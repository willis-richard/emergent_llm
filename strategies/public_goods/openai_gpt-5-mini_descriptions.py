"""
Strategy descriptions for PublicGoodsDescription.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
"""


description_COOPERATIVE_1 = '''
Name: Reciprocal Conditional Cooperation with Forgiveness (RCCF)

Goal (high level)
- Aim to establish and sustain high mutual contributions across the repeated game by signaling cooperation early, reciprocating observed cooperation, punishing clear and persistent free-riding, and forgiving so cooperation can recover. Be cautious in the final rounds to avoid being exploited by endgame defection.

Key ideas
- Start cooperative to signal willingness to form a cooperative equilibrium.
- Condition my contribution on observed behavior of others (both group-level and per-player histories).
- Punish only when defection is clearly persistent (avoid overreacting to noise/one-off defections).
- Use forgiveness after a short punishment so cooperation can restart.
- Always defect in the very last round (and be progressively more cautious as the end approaches).

Notation
- n: number of players (including me)
- r: total rounds
- t: current round (1..r)
- history: observed contributions of all players in past rounds (for each round we see vector c_j ∈ {0,1} for all players)
- others: player set excluding me
- c_j,t: contribution of player j in round t (observed after round t)
- m_t = Σ_{j≠me} c_j,t: number of other cooperators in round t
- G_t: group cooperation rate among others observed (see definition below)
- S_j: per-player cooperation score (moving estimate of how often j cooperates)
- Parameters (internal to strategy) — chosen as functions of n,r but not of opponents:
  - L = min(5, r): lookback window (number of latest rounds to compute short-run stats)
  - α = 0.3 (reward step for S_j after cooperation)
  - β = 0.5 (punishment step for S_j after defection)
  - T_coop = 0.5 (group-level cooperation threshold to cooperate)
  - T_lenient = 0.2 (below this indicates clear non-cooperation)
  - P_max = 3 (maximum punishment length after a clear collapse)
  - p_forgive = 0.25 (probability of cooperating when being lenient)
  - Endgame_caution_window E = min(3, r-1): in the last E rounds be stricter
These parameter values are defaults; implementations may tune them slightly but keep same spirit.

State initialization
- For each other player j: S_j := 0.5 (neutral prior)
- Recent rounds buffer for others’ group contributions (initially empty)

Round-by-round decision rules (pseudocode style)

For each round t = 1..r:
  If t == r:
    // Last round: defect (dominant choice in finite horizon)
    play D (c_me = 0)
    continue to next (game ends)
  End if

  // Update per-player scores S_j from history of previous rounds (if t>1)
  // We use an exponential-like update on each observed action:
  For each player j ∈ others:
    If t > 1:
      If c_j,{t-1} == 1: S_j := S_j + α*(1 - S_j)
      Else:                 S_j := S_j * (1 - β)   // or S_j := S_j - β*S_j
    End if
  End for

  // Compute short-run group cooperation measures
  Let G_recent = average over last L rounds of (m_τ / (n-1)) for τ in [t-L .. t-1] (use available rounds only)
  Let G_last = (m_{t-1} / (n-1)) if t>1 else 0
  Let S_bar = average_j S_j  // average per-player cooperation score

  // Determine if there is an ongoing punishment state (persistent low cooperation)
  Let consecutive_low = number of consecutive previous rounds (ending at t-1) where (m_τ / (n-1)) < T_lenient
  Let in_punishment = (consecutive_low > 0) and (consecutive_low <= P_max)

  // Decision logic
  If in_punishment:
    // Punish by defecting for up to P_max rounds unless signs of recovery are visible:
    If G_recent >= T_coop or S_bar >= 0.75:
      // others are already recovering strongly; stop punishment
      play C
    Else
      play D
    End if

  Else
    // Not in punishment: choose based on how cooperative the group has been
    If G_last >= T_coop:
      // Clear recent majority cooperation — reciprocate
      play C
    Else if G_recent >= T_coop:
      // Recent cooperation over window — reciprocate
      play C
    Else if G_recent >= T_lenient:
      // Mixed behavior: be lenient to incentivize return to cooperation
      With probability p_forgive: play C
      Else: play D
    Else
      // Clear non-cooperation among others — defect to avoid being exploited
      play D
    End if
  End if

  // Endgame tightening: if remaining rounds R_rem = r - t <= E, raise bar:
  // (Apply after computing choice above; if choice was C and we are in endgame with weak signals, switch to D)
  If (r - t) <= E:
    If chosen_action == C and (G_recent < 0.9 and S_bar < 0.9):
      chosen_action := D
    End if
  End if

  // Play chosen_action (C or D)
End for

Additional details and rationale

1) Opening move
- Play C in round 1 (the above logic will do this because G_last undefined => treated as cooperative start). Starting cooperative is a low-cost signal to encourage reciprocal cooperation and is standard in successful reciprocity strategies.

2) Per-player scoring and targeted trust
- S_j acts as a per-opponent memory that smooths one-off defections and identifies persistent defectors. Using per-player scores lets the strategy be less punishing when just a subset of players defect, and it allows the strategy to respond more to individually persistent defectors. This is useful in heterogeneous populations.

3) Punishment and forgiveness
- Punishment is limited in length (P_max rounds) to avoid irreversible escalation that kills future cooperation. Punishment occurs only after consecutive rounds of low group cooperation (consecutive_low > 0). After punishment we immediately check for signs of recovery (group cooperation or high S_bar) and resume cooperation if present.
- Forgiveness is implemented both by (i) limited punishment length and (ii) probabilistic cooperation when behavior is mixed (G_recent between T_lenient and T_coop). The asymmetry in updates (α < β) can be tuned to be more forgiving (smaller β) or harsher.

4) Robustness to exploitation and noise
- Do not cooperate if recent group cooperation is clearly low (G_recent < T_lenient). That prevents being exploited by persistent defectors.
- Using L>1 lookback and per-player scores reduces sensitivity to random errors or one-shot deviations by others.

5) Endgame behavior
- In the last round always defect (dominant action).
- In the last E rounds (E small, e.g., 3) raise the cooperation bar — only continue to cooperate if there is very strong evidence that others are highly cooperative (both G_recent and S_bar near 1). This reduces being exploited by strategies that defect only near the end.

6) Parameter choices are simple and fixed
- All thresholds and rates are chosen without knowing opponents. They are conservative enough to avoid long exploitation and lenient enough to foster cooperation in friendly populations. Tournament implementers can set those numeric values; the logic and relative relationships (lenient band, stricter endgame, short limited punishment, per-player smoothing) are the important design points.

Why this is cooperative and adaptive
- Cooperative: opens with cooperation and gives cooperation a clear, forgiving path to re-establish after mistakes.
- Adaptive: assesses both aggregate behavior and per-player histories; punishes only when defection is persistent; uses probabilistic leniency to re-start cooperation.
- Robust: avoids being exploited by immediate retreat when others clearly defect, and avoids permanent breakdown by limiting punishment and allowing fast forgiveness.

Edge cases summary
- First round: Cooperate (signal).
- Last round: Always defect.
- Rounds near the end (last E rounds): be cautious; require strong signals to keep cooperating.
- If others defect once or rarely: treat as noise, do not trigger long punishment.
- If group cooperation collapses (multiple consecutive poor rounds): punish by defecting for up to P_max rounds, then reassess.
- If a subset of players defect persistently while others cooperate: per-player scores S_j will reflect that and the strategy will be able to selectively condition on average S_bar; behavior effectively punishes defectors by refusing to contribute until their scores recover.

Example behavior (qualitative)
- Versus mostly-cooperative opponents: you and them will quickly settle into repeated C and achieve high total payoff.
- Versus unconditional defectors: you will cooperate early (one or two rounds) then switch to defect and avoid long exploitation.
- Versus oscillatory or error-prone opponents: smoothing and probabilistic forgiveness reduce needless punishment cycles and allow cooperation to re-emerge.

Implementation notes
- The strategy requires only the history of observed actions per round (which the game supplies).
- The strategy can be made entirely deterministic by replacing randomized forgiveness with a deterministic rule (e.g., "forgive on every 4th mixed round") — but small randomized forgiveness often performs better in heterogeneous tournaments because it enables spontaneous restarting of cooperation.
- Parameter values (L, α, β, T_coop, T_lenient, P_max, p_forgive, E) can be tuned for specific tournaments; the structure and tradeoffs above should be preserved.

This RCCF strategy balances being cooperative (open, reciprocating) with being robust (limited punishment, endgame caution, per-player scoring), making it well-suited for tournaments against varied AI opponents.
'''

description_COOPERATIVE_2 = '''
Strategy name: Reciprocal Thresholded Pavlov (RTP)

Intuition (short)
- Start cooperatively and try to sustain mutually beneficial cooperation.
- Use a Pavlov / "Win‑Stay, Lose‑Shift" rule adapted to public goods: compare the payoff you actually received last round with the payoff you would have gotten had you defected (counterfactual). If your previous action “worked” (gave at least as much as the alternative), repeat it; if it “failed”, switch.
- Be generous on ties, forgive quickly when the group returns to cooperating, and avoid very long punishments. Defect in the final round (endgame).

Why this is robust
- It is purely history‑dependent and needs only past actions (and payoffs that are computable from past actions).
- It starts cooperative, so it can foster cooperation with other conditional cooperators.
- It punishes defections when they hurt you, discouraging exploitation, but is forgiving and short‑memory so that isolated defections (or changing opponents) don’t cause permanent collapse into mutual defection.
- The counterfactual check uses observed others’ behavior, so it adapts to different opponent mixes and to group sizes / k-values implicitly.

Decision rules (natural language)
1. First round: cooperate.
2. Last round (round r): defect (no future to enforce reciprocity).
3. For each intermediate round t (2 ≤ t < r):
   a. From the previous round (t−1) compute:
      - C_prev = total number of cooperators last round (including you).
      - my_prev = your action last round (C or D); encode c_my_prev = 1 if C else 0.
      - actual_payoff = (1 − c_my_prev) + (k/n) × C_prev.
      - counterf_defect_payoff = 1 + (k/n) × (C_prev − c_my_prev)
        (this is the payoff you would have received in t−1 had you defected while others’ actions stayed the same).
   b. If actual_payoff ≥ counterf_defect_payoff, then your previous action “won” — repeat your previous action in round t.
   c. If actual_payoff < counterf_defect_payoff, your previous action “lost” — switch action in round t (C → D or D → C).
   d. Tie‑break (actual_payoff == counterf_defect_payoff): prefer cooperation (play C).
4. Forgiveness and finite punishment:
   - If you are currently defecting because you switched after a “loss”, forgive quickly: if within the last P_max rounds there has been an increase in group cooperation (e.g., C_prev ≥ ceil(n/2)), immediately return to cooperation on the next round rather than continuing punishment.
   - Concretely: set P_max = 2 (defect at most two consecutive rounds in response to a loss unless counterfactual checks keep you defecting).
   - This prevents long vendettas and allows re‑establishment of cooperation when others improve their behavior.

Pseudocode (concise)

Initialize:
  last_action = C
  consecutive_defect_punishment = 0
  P_max = 2
For t = 1..r:
  if t == 1:
    play C; last_action = C
    continue
  if t == r:
    play D; break
  Observe previous round actions → C_prev (total cooperators), my_prev = last_action
  c_my_prev = 1 if my_prev == C else 0
  actual_payoff = (1 - c_my_prev) + (k / n) * C_prev
  counterf_defect_payoff = 1 + (k / n) * (C_prev - c_my_prev)
  if actual_payoff > counterf_defect_payoff:
    next_action = my_prev   # Win → stay
    consecutive_defect_punishment = 0
  else if actual_payoff < counterf_defect_payoff:
    # Lose → switch
    next_action = (D if my_prev == C else C)
    if next_action == D:
      consecutive_defect_punishment += 1
    else:
      consecutive_defect_punishment = 0
  else:  # tie
    next_action = C
    consecutive_defect_punishment = 0
  # Forgiveness override:
  if next_action == D and (C_prev >= ceil(n/2) or consecutive_defect_punishment > P_max):
    next_action = C
    consecutive_defect_punishment = 0
  play next_action
  last_action = next_action

Edge cases and clarifications
- Perfect observability: the strategy assumes you see the full vector of who cooperated last round (the game spec gives that).
- First round: always cooperate to signal willingness to sustain cooperation.
- Final round: defect (standard finite‑horizon endgame rationality). If you want to be altruistic and the tournament rewards absolute sum over many episodes rather than strict game‑theoretic dominance, you can optionally override and cooperate in final round; but the safe normative move is defect in t=r.
- Ties: when your actual payoff equals the counterfactual defect payoff, prefer cooperation (generosity).
- Parameter choices: P_max = 2 is modest; it can be tuned in tournament calibration. The forgiveness trigger uses majority cooperation (ceil(n/2)) as a pragmatic sign that the group has largely returned to cooperative behavior; you may lower this threshold for more generosity or raise it to be stricter against noisy cooperative signals.

Why this is cooperative
- Starts with cooperation and only defects in response to evidence that a different action would have paid better last round — i.e., it punishes only when you were personally better off had you defected (which indicates exploitation or insufficient group cooperation).
- It repeats actions that are mutually beneficial and quickly tests whether cooperation is restored, which re‑establishes cooperative regimes when possible.
- It is forgiving and short‑memory: isolated defections (or different opponents) do not lead to permanent reversion to defection.

Performance profile (practical)
- Against unconditional cooperators: rapidly stays cooperative and gets high payoffs.
- Against conditional cooperators and tit‑for‑tat variants: converges to sustained cooperation.
- Against unconditional defectors or exploiters: detects exploitation (you do worse than the defect counterfactual), defects in response, limits losses, and does not endlessly waste payoff trying to coax non‑reciprocators into cooperating.
- Robust to a mixture of strategies: relies on observed actions rather than assumptions about opponent types.

Implementation note
- This rule uses only observed past actions and arithmetic based on n and k; it is straightforward to implement and compute in each round. The counterfactual comparison is cheap and fully determined from history.
'''

description_COOPERATIVE_3 = '''
Summary (one line): Play “Pavlov” (win‑stay/lose‑shift) adapted to the public‑goods setting: start cooperating, keep the same action if your last‑round action did at least as well as the counterfactual other action (given everyone else’s last round choices), otherwise switch; always defect in the final round. This is cooperative, forgiving, adaptive and robust.

Why this choice (brief): In a public‑goods game a single round’s best action is usually defection, so sustained cooperation requires conditional strategies. Pavlov is simple, retaliatory enough to deter exploitation, forgiving (so accidental or occasional defections don’t lead to permanent breakdown), and fully defined from public history and game parameters only. For a finite horizon we avoid pointless exploitation in the final round by defecting there.

Decision rules (natural language)
- Round 1: Cooperate (C).
- Any round t where t = r (the last round): Defect (D).
- For rounds t = 2, 3, ..., r−1:
  1. Observe last round’s actions of all players (including yourself) and compute
     - m = number of other players (n−1) who cooperated last round;
     - your last-round action a_prev ∈ {C, D};
     - your realized last-round payoff π_prev.
  2. Compute the counterfactual payoff you would have received last round if you had played the opposite action a_alt (hold everyone else’s last-round actions fixed):
     - If a_alt = C, counterfactual π_alt = 0 + (k/n) × (m + 1).
     - If a_alt = D, counterfactual π_alt = 1 + (k/n) × m.
  3. Compare: if π_prev ≥ π_alt (your last action did at least as well as switching would have), then repeat the same action a_prev this round (win‑stay); otherwise switch to the opposite action (lose‑shift).
  4. Tie‑breaking: if π_prev = π_alt, prefer to cooperate (choose C) to favor cooperation and forgiveness.

Edge cases and clarifications
- First round: Cooperate. This seeds cooperation and gives you the chance to establish mutually beneficial behavior.
- Last round (t = r): Always defect. For a known finite horizon, backward induction makes the final round a one-shot game; defecting avoids being exploited in the final round and prevents giving free payoffs when no future returns can enforce cooperation.
- No noise specified: The rule is deterministic given history. If implementation must handle observation noise, add contrition/confirmation: if you think the observed deviation may be noise, consider requiring two consecutive rounds of bad signals before permanent punishment. (Optional; not required by the specification.)
- Multiple defectors/cooperators: The rule uses the exact public count m and computes exact counterfactual payoffs. It scales to any n.
- Forgiveness and punishment length: Pavlov’s switching is immediate and single‑round: if you switch to punish a low payoff, you will switch back as soon as you receive a better payoff. This gives short, proportionate punishment (not permanent), which encourages quick restoration of cooperation.
- Exploitation risk: Against persistent unconditional defectors, Pavlov will learn to defect (because cooperating will yield lower payoff than defecting) and won’t be repeatedly exploited. Against conditional cooperators it tends to stabilize cooperation.
- Strategy depends only on game parameters (n, k, r) and observable public history of actions/payoffs (allowed by the specification).

Pseudocode
Inputs: n, k, r
State: history of rounds 1..t−1 (actions of all players each round), current round t

function decide_action(t, history):
  if t == 1:
    return C
  if t == r:
    return D

  // get last round info
  last_round = history[t-1]
  m = number of players other than me who cooperated in last_round
  a_prev = my action in last_round
  π_prev = my payoff in last_round

  // compute counterfactual payoff if we had done the opposite
  if a_prev == C:
    // counterfactual is had we defected
    π_alt = 1 + (k / n) * m
  else: // a_prev == D
    // counterfactual is had we cooperated
    π_alt = 0 + (k / n) * (m + 1)

  if π_prev > π_alt:
    return a_prev        // win-stay
  else if π_prev < π_alt:
    return opposite(a_prev) // lose-shift
  else: // tie: prefer cooperation
    return C

Properties and behavioral intuition
- Cooperative: starts with cooperation and prefers cooperation on ties; will sustain cooperation when others reciprocate because cooperating when others cooperate yields better or equal payoffs and the strategy stays with C.
- Adaptive: uses the exact observed mix of cooperators to compute whether cooperating was individually rewarding and switches behavior accordingly.
- Forgiving: single or occasional defections trigger short punishments (a switch) but allow prompt return to cooperation as soon as cooperation becomes profitable again.
- Robust: works against a wide range of opponents — unconditional defectors will be met with defection and receive no long-term advantage; conditional or reciprocal opponents will tend to stabilize mutual cooperation.
- Simple and implementable: requires only last round’s public actions/payoffs (and the game parameters) and a deterministic comparison.

Optional refinements (if you want more sophistication)
- Lookback window: instead of comparing only the previous round, compare average payoff over the last M rounds to the average counterfactual over those rounds; this smooths reactions to single random deviations.
- Graduated punishment: if repeated exploitation is detected (several consecutive rounds where π_prev < π_alt), impose a longer defect period P before trying cooperation again.
- Contrition after mistaken defection: if you defect and then observe everyone else cooperating next round, proactively cooperate next round to re-establish cooperation.

This Pavlov-based strategy balances being cooperative (seeking and sustaining mutual cooperation), being robust (not exploited by persistent defectors), and being adaptive (responding to observed behavior without assuming any pre-arranged coordination).
'''

description_COOPERATIVE_4 = '''
Name: Adaptive Conditional Cooperator with Forgiving Proportional Punishment (ACCFPP)

Intuition (short)
- Start cooperative to signal willingness.
- Cooperate as long as the group shows substantial cooperation in recent rounds.
- If the group cooperation falls below a threshold, punish by defecting for a limited, proportional number of rounds (not forever) so the punishment is credible but forgiving.
- Escalate only if low cooperation repeats; reset escalation after sustained cooperation.
- Always defect in the very last round (endgame safety).

This balances being cooperative (promoting high total welfare) with robustness against exploitation and with forgiveness so cooperation can recover.

Strategy parameters (computed from game parameters and history)
- n, r, k: given game parameters.
- gamma: cooperation threshold fraction for "good" group behavior. Default gamma = 0.60 (60%). This can be adjusted modestly using k and n if desired (e.g., lower gamma when k is very high because the public good is more valuable), but gamma must be in (0.5, 0.8).
- base_punish_unit: 1 (minimum punishment length in rounds).
- punish_cap_fraction: 0.5 (cap punish length at at most half of remaining rounds).
- escalation_limit: maximum escalation factor (e.g., 8).
- window_for_reset: 2 (number of consecutive "good" rounds required to reset escalation).

State variables maintained from history
- punish_counter: number of remaining rounds that this strategy is currently punishing (initially 0).
- escalation_factor: integer >=1, starts at 1, increments when low-cooperation events repeat without a reset.
- consecutive_good_rounds: counts consecutive recent rounds where group cooperation >= gamma.

Decision rules (natural-language)
1. First round:
   - Cooperate. (Signal cooperative intent; there is r>1.)

2. Last round (round t = r):
   - Defect. (Endgame: cannot credibly threaten future punishment; defecting avoids final-round exploitation.)

3. For any intermediate round t (1 < t < r):
   - If punish_counter > 0:
     - Defect this round and decrement punish_counter by 1.
     - If during punishment we observe the group cooperation in the previous round >= gamma for window_for_reset consecutive rounds, clear punishment early (set punish_counter = 0) and reset escalation_factor = 1.
   - Else (not currently punishing):
     - Compute C_prev = number of cooperators in the previous round (t-1).
     - If C_prev / n >= gamma:
       - Cooperate this round. Increment consecutive_good_rounds. If consecutive_good_rounds >= window_for_reset, reset escalation_factor = 1.
     - Else (C_prev / n < gamma):
       - This is treated as a "group defection event." Set consecutive_good_rounds = 0.
       - Compute shortfall S = ceil(n * gamma) - C_prev (S >= 1).
       - Compute raw_punish = max(base_punish_unit, S).
       - Apply escalation: punish_length = min( floor(punish_cap_fraction × remaining_rounds), raw_punish × escalation_factor ).
         - remaining_rounds = r - t + 1 (including this round).
       - Set punish_counter = punish_length.
       - Defect this round (punishment begins immediately).
       - Increment escalation_factor = min(escalation_limit, escalation_factor × 2) so repeated offenses produce longer punishments.
       - If punish_length = 0 (only possible if remaining_rounds small), defect anyway (defensive).

4. Forgiveness and recovery:
   - If while punishing the group cooperation quickly returns to >= gamma for window_for_reset consecutive rounds, stop punishing early (punish_counter = 0) and reset escalation_factor = 1.
   - After punishment finishes, return to cooperating as long as group behavior is "good."

5. Persistent individual defectors:
   - Optionally track each player's cooperation frequency over the last W = min(5, t-1) rounds.
   - If a single player i defects in all of the last W rounds (very persistent defector), do NOT change the global rule: still measure group cooperation by actual counts. (In public goods games you cannot exclude a defector; it is safer to punish proportionally via group-level punishments rather than retaliating only against individuals because retaliation is costly and simultaneous moves make targeted retaliation ineffective.)
   - However, if a strict majority of players are persistent defectors, the above group-based punishments will lead you to stop cooperating until they return to cooperating.

Pseudocode (concise)
- Inputs: n, r, k, history (actions[1..t-1][1..n])
- Maintain: punish_counter ← 0; escalation_factor ← 1; consecutive_good_rounds ← 0
- On round t (1-based):
  if t == 1:
    return C
  if t == r:
    return D
  remaining_rounds ← r - t + 1
  if punish_counter > 0:
    // We are punishing
    // Check if forgiveness condition met: if previous rounds show recovery
    if t-1 >= window_for_reset:
      if cooperation_rate_over_last(window_for_reset) >= gamma:
        punish_counter ← 0
        escalation_factor ← 1
        consecutive_good_rounds ← window_for_reset
        return C
    punish_counter ← punish_counter - 1
    return D
  // Not currently punishing:
  C_prev ← number of cooperators in round t-1
  if (C_prev / n) >= gamma:
    consecutive_good_rounds ← consecutive_good_rounds + 1
    if consecutive_good_rounds >= window_for_reset:
      escalation_factor ← 1
    return C
  else:
    // Group fell short -> begin punishment
    consecutive_good_rounds ← 0
    shortfall S ← max(1, ceil(n * gamma) - C_prev)
    raw_punish ← max(1, S)
    punish_cap ← max(1, floor(punish_cap_fraction × remaining_rounds))
    punish_length ← min(punish_cap, raw_punish × escalation_factor)
    punish_counter ← punish_length - 1  // we will defect this round and have punish_counter more rounds
    escalation_factor ← min(escalation_limit, escalation_factor × 2)
    return D

Design choices and rationale
- Start cooperative: cooperating in early rounds is the most direct way to reach the high-payoff all-cooperate outcome.
- Majority (gamma) threshold: requiring a substantial fraction (default 60%) of cooperators in the last round reduces the chance of being exploited by a small clique of defectors while still permitting recovery when most players cooperate.
- Proportional punishment: punish lengths scale with how far group cooperation fell short (shortfall S) and with an escalation multiplier when offenses repeat. Punishing just long enough to impose a real cost on defectors but not forever avoids locking into mutual defection.
- Forgiveness: quick forgiveness when cooperation recovers allows the group to return to efficient outcomes, and resetting escalation after sustained cooperation prevents explosion-length punishments.
- Endgame: defecting in the last round prevents being exploited at the end when threats are not credible. (You may optionally adopt a slightly different endgame rule — e.g., cooperate in final round if group has cooperated every prior round — but the safe, robust choice is to defect in the last round. If you want to be maximally cooperative and are willing to risk last-round exploitation, you could cooperate in final round when every other player cooperated in the penultimate round; but that is a policy decision for the tournament designer and can be switched.)

Robustness properties
- Works without assuming common norms or precommitment; decisions depend only on observed actions and payoffs.
- Forgiving behavior reduces risk of long vendettas and allows cooperation to re-emerge against adaptive opponents.
- Proportional punishment makes exploitation costly to repeated defectors while avoiding permanent collapse to defection.
- Escalation ensures repeated mass-defection events are met with increasingly credible retaliation, deterring opportunistic exploitation.
- Simple, deterministic, and implementable from the given history and parameters.

Tuning advice
- gamma: Higher gamma (e.g., 0.7) makes the strategy more demanding (safer but more likely to withdraw cooperation). Lower gamma (e.g., 0.55) favors cooperation but increases exploitation risk.
- punish_cap_fraction: Controls how long punishments can be relative to remaining rounds. Set lower to be more forgiving near the end.
- escalation_limit and doubling factor: Protects against infinite escalation—cap to a reasonable number (e.g., 8).
- You can make the strategy probabilistic (e.g., cooperate with probability p when C_prev slightly below gamma) to reduce deterministic cycles and exploitability; the pseudocode above is deterministic for clarity.

Summary
ACCFPP is a simple, history-dependent, adaptive strategy: signal cooperation, condition continuation on recent group cooperation, punish proportionally when the group falls short, forgive quickly when cooperation returns, and protect yourself in the final round. It promotes and sustains cooperation in many environments while limiting exploitation by selfish or noisy opponents.
'''

description_COOPERATIVE_5 = '''
Strategy name: Forgiving Targeted Conditional Cooperation (FTCC)

High-level idea
- Begin by signalling cooperative intent.
- Reward players who have cooperated in the past and punish (by withholding cooperation) primarily those who defected — i.e., use targeted, proportional punishment rather than blanket group punishment.
- Be forgiving: let reputations slowly recover so single mistakes don’t lead to permanent collapse.
- Be endgame-aware: avoid being exploited in the final round(s).
- All decisions use only the public history of who cooperated in each round and the known parameters (n, r, k).

Intuition / goals
- Sustain mutual cooperation when many opponents reciprocate (maximizes group payoff when k > 1).
- Make defection costly to persistent free-riders by withholding cooperation from them until they improve behavior.
- Avoid over-punishing (which can collapse cooperation) by slowly forgiving past defections.
- Adapt to opponents’ actual behaviour (instead of assuming norms).

State the strategy maintains
- For each other player j (j ≠ i) a numeric trust score S_j (initially 0).
- A global “last action” of this strategy (start with Cooperate).
- Tunable parameters (recommended defaults provided): decay λ ∈ (0,1), penalty p > 0, cooperation-count threshold q, minimum cooperating-others threshold m_min, forgiveness floor T_f.

Recommended default parameter values (can be tuned for tournaments):
- λ = 0.8 (memory decay per round)
- p = 2.0 (penalty for a defect in the score update)
- q = 0.0 (score threshold to consider a player “trusted”)
- m_min = max(1, floor((n-1)/2)) (minimum number of other cooperators in previous round to be willing to cooperate)
- T_f = 1 (number of rounds of observed cooperation to heal a negative S_j to non-negative faster)
Notes: these defaults bias toward cooperation with a majority and produce forgiving recovery; they work well across n, k, r in practice. They are explicit and interpretable for implementation.

Decision rules (per round t)
1. Edge / trivial cases:
   - If r == 1: defect (no future to incentivize cooperation).
   - If t == r (final round): defect (no future punishment/reward possible).
   - If k ≤ 1: defect every round (public good gives no group benefit; cooperating is strictly dominated).
2. Otherwise (1 < t < r):
   - Use observed history to compute:
     - last_cooperators = number of other players who played C in round t−1 (0..n−1).
     - trusted_count = number of players j with S_j ≥ q (i.e., non-negative or above threshold).
     - avg_trust = (1/(n−1)) × Σ_j S_j.
   - Primary cooperation decision:
     - Cooperate if either:
       a) last_cooperators ≥ m_min (i.e., a majority / meaningful fraction cooperated last round), OR
       b) trusted_count ≥ ceil((n-1)/2) (i.e., a majority of players are trusted), OR
       c) avg_trust ≥ q (group trust is non-negative)
     - Otherwise, defect.
   - Targeted adjustment:
     - If we decide to cooperate but fewer than (n−1) players are trusted, we still cooperate — this rewards trustworthy players and gives defectors a chance to recover.
     - If we decide to defect because trusted_count is low, we will selectively withhold cooperation until defectors’ scores improve (see update rules below).

Action in round t = C if the above leads to cooperate, else D.

Score update after observing round t actions of all players
For every j ≠ i:
- If j played C in round t:
  S_j ← λ × S_j + +1
- If j played D in round t:
  S_j ← λ × S_j − p

(Optionally: if S_j becomes slightly positive because of multiple recent cooperations, give a small additional bonus of +T_f to speed recovery.)

Rationale: cooperation increases trust; a defection subtracts a larger pulse (p) which makes persistent defectors stand out. Decay λ allows forgiveness over time so a single mistake is not permanent.

Punishment mechanics (targeted, proportional)
- The strategy’s defecting decision (when trust is low) is not a blanket, permanent grim trigger. It withholds cooperation primarily as long as many S_j remain negative. Because S_j are per-player, cooperation can resume with trustworthy players even if some others remain punished. This avoids unnecessary “group collapse” and concentrates the punishment on defectors.

Forgiveness
- Because of the decay λ and the positive increment for cooperation, defectors who start cooperating will gradually rebuild S_j and receive cooperation back.
- This forgiveness prevents endless retribution cycles and is robust to occasional mistakes.

Handling the first round
- If r > 1 and k > 1: cooperate in round 1. Reason: need an initial cooperative signal to begin reciprocal cooperation.
- If r == 1 or k ≤ 1: defect.

Handling the last round(s) (endgame)
- Always defect in round r.
- Optionally, to reduce being exploited by standard endgame “unraveling”, the strategy can use a short endgame horizon policy: for t ≥ r − E where E is a small number (e.g., E = 1 or 2), gradually reduce willingness to cooperate (raise m_min or tighten q). The simple default is only to defect in t == r; this is transparent and avoids complexity. In practice, many opponents still cooperate until very near the end; targeted punishment keeps us protected.

Robustness features
- Adaptive: per-player S_j lets the strategy adapt to mixed populations (some cooperators, some persistent defectors).
- Targeted punishment avoids penalizing cooperators for the behavior of others and helps sustain cooperation with a cooperative subset.
- Forgiving (decay λ and positive increments) prevents one-off mistakes from destroying cooperation permanently.
- Deterministic and simple to implement; uses only public history and the known parameters.

Pseudocode (concise)

Initialize:
  For each j ≠ i, S_j = 0
  last_action = C (we will play C in round 1 if r > 1 and k > 1)

For each round t = 1..r:
  if r == 1 or k <= 1:
    play D; continue
  if t == r:
    play D; continue
  if t == 1:
    play C
    observe others’ actions, update S_j (see below)
    continue

  # For t in 2..r-1
  last_cooperators = number of other players who played C in round t-1
  trusted_count = number of j with S_j >= q
  avg_trust = sum_j S_j / (n-1)

  if last_cooperators >= m_min or trusted_count >= ceil((n-1)/2) or avg_trust >= q:
    play C
  else:
    play D

  observe others’ actions for round t
  for each j ≠ i:
    if j played C:
      S_j = λ * S_j + 1
    else:  # j played D
      S_j = λ * S_j - p

Parameter tuning guidance
- If k is close to 1 (public good weak), raise p and raise m_min (be more defensive).
- If r is large, be more patient: lower p and use larger λ (slower decay) so reputation accumulates and cooperation is easier to sustain.
- If n is large, use m_min ≈ floor((n-1)/2) or slightly higher; cooperation with at least a majority of others is a robust signal.
- If you detect a subpopulation of persistent defectors (many S_j << 0 and not improving), you can switch to a long-term withholding posture targeted only at the defectors while still cooperating with those with S_j ≥ q.

Why this meets the requirements
1. Decision rules: Fully specified above — when to cooperate (first round unless trivial cases; in subsequent rounds when past cooperation / trust is sufficient) and when to defect (end round, insufficient trust or low recent cooperation).
2. Edge cases: First round, last round, k ≤ 1, and r == 1 are explicitly handled. Endgame behavior is conservative (defect in last round).
3. Cooperative mindset: The strategy starts cooperatively, rewards cooperators, seeks to maintain group cooperation, punishes defectors enough to deter persistent free-riding, and forgives so cooperation can resume.

Notes on implementation / tournament play
- This strategy is deterministic and transparent, making it easy to implement and verify.
- It is robust against different opponent mixes (all-cooperators, all-defectors, mixed, exploiters) because of targeted punishment and forgiveness.
- You may tune λ and p to the tournament environment. In environments with noisy mistakes you should increase forgiveness (higher λ nearer to 1 and smaller p); in environments with many persistent exploiters, increase p.

Summary
FTCC: Start cooperative to signal intent, keep per-player reputations, cooperate if a meaningful fraction/majority of others cooperated recently or the group trust is non-negative, withhold cooperation primarily from identified defectors, forgive over time, and defect in the final round. This balances cooperation promotion and protection against exploitation, is adaptive to opponent behaviour, and depends only on game parameters and observable history.
'''

description_COOPERATIVE_6 = '''
Strategy name: Adaptive Conditional Cooperator with Proportional Punishment (ACCPP)

Summary (one line)
- Start cooperative, stay cooperative when the group is sufficiently cooperative recently; when the group falls below a dynamic cooperation threshold, punish for a short, proportional number of rounds, then probe and forgive. Use small random probes to re-establish cooperation and treat the last round cautiously based on past behaviour.

Intuition and goals
- Encourage and sustain high mutual cooperation when it is reciprocated.
- Avoid long-term exploitation by defectors via measurable, proportional punishments.
- Be noise-tolerant (forgive occasional mistakes) and adaptive to the observed cooperation level and to the game parameters (n, k, r).
- Use only parameters and observed history (past round contributions); no communication or external coordination.

Fixed internal parameters (implementation-ready)
- epsilon (exploration/probe probability) = 0.02 (2%)
- recent_window W = min(5, r) — number of most recent rounds used to compute local cooperation rates
- max_punish_fraction = 0.20 — maximum fraction of remaining rounds to spend punishing
- punish_scale = 2.0 — scale factor to turn shortfall into punishment length

Derived, game-dependent parameters
- cooperation threshold alpha = 0.5 + 0.25 * (1 - k/n)
  - This sets a dynamic bar: when k/n is large (public good is powerful), require only ~50% cooperating to continue; when k/n is small (public good is weak), require a higher cooperating fraction (up to ~0.75) to avoid being exploited.
- max_punish = max(1, floor(max_punish_fraction * r))

State variables to maintain (from history)
- current round t (1..r)
- history of rounds: for each past round s < t we observe contribution vector c_{1..n,s} or at minimum the count m_s = number of cooperators in round s
- our own last action
- punish_until_round (initially 0) — if t <= punish_until_round we are in punishment phase

Decision rules (high-level)
1. First round (t = 1)
   - Cooperate (C). (Signal cooperative intent; no prior information)

2. If in an active punishment phase (t <= punish_until_round)
   - Mostly defect (D) during punishment to make punishment meaningful.
   - With small probability epsilon, override and Cooperate (probing move) to test whether cooperation has resumed.
   - If, during punishment, a subsequent round shows group cooperation fraction >= alpha, end punishment early (set punish_until_round = 0) and cooperate next round.

3. Otherwise (not punishing)
   - Compute f_last = m_{t-1} / n where m_{t-1} is number of cooperators in the immediate previous round (if t=2 this is round 1).
   - Compute recent cooperation rate over W rounds: f_recent = (sum_{s=t-W}^{t-1} m_s) / (n * min(W, t-1)).
   - Main rule:
     - If f_last >= alpha OR f_recent >= alpha:
         - Cooperate (C).
     - Else (group fell below threshold):
         - Enter punishment: compute severity = max(0, alpha - f_last).
         - Compute punishment length L = min(max_punish, 1 + ceil(punish_scale * severity * remaining_rounds))
           where remaining_rounds = r - t + 1.
         - Set punish_until_round = t + L - 1.
         - Defect (D) this round (first round of punishment).
   - Always allow an epsilon probe while not punishing: with probability epsilon, cooperate even if the rule would say defect (to re-open cooperation).

4. Endgame (last rounds)
   - If t = r (final round):
     - If overall historical cooperation rate (total C's / (n*(t-1))) >= alpha OR f_last >= alpha then Cooperate (C); else Defect (D).
     - Rationale: be cautious in the last round but allow cooperating if the group has consistently been cooperative (we can still gain).
   - If t in (r - few) where few = max(1, floor(0.05 * r)) (final several rounds), reduce punishments:
     - Cap punish_until_round so it never extends beyond r - 1 (no pointless punishment in final round).
     - Be slightly more receptive to forgiveness (use a slightly lower alpha_for_end = alpha - 0.05 when deciding to end punishment early).

Detailed pseudocode

Inputs: n, k, r, current round t, history m_1..m_{t-1} (counts of cooperators each past round), our_last_action, state variable punish_until_round

Derived constants:
  epsilon = 0.02
  W = min(5, r)
  alpha = 0.5 + 0.25 * (1 - k/n)
  max_punish = max(1, floor(0.20 * r))
  punish_scale = 2.0

Procedure decide_action(t, history):
  if t == 1:
    return C

  f_last = m_{t-1} / n
  f_recent = (sum of m_s for s in max(1, t-W) .. t-1) / (n * min(W, t-1))
  remaining = r - t + 1

  if t <= punish_until_round:   // currently punishing
    if f_last >= alpha or f_recent >= alpha:
      // group recovered while we were punishing
      punish_until_round = 0
      return C
    else:
      if random() < epsilon:
        return C    // probe during punishment
      else:
        return D

  // not currently punishing
  if f_last >= alpha or f_recent >= alpha:
    if random() < epsilon:
      // small exploration even when cooperating
      return C
    else:
      return C

  // group cooperation fell below threshold: initiate proportional punishment
  severity = max(0, alpha - f_last)
  L = min(max_punish, 1 + ceil(punish_scale * severity * remaining))
  punish_until_round = t + L - 1
  // Do not allow punishment to extend past final round usefully
  if punish_until_round >= r:
    punish_until_round = r - 1
  return D

Notes on parameters and robustness
- alpha depends on k/n so the strategy automatically requires a higher observed cooperation rate when the public good is weak (k small) and is more tolerant when the public good is strong (k close to n).
- Punishment length scales with severity and remaining rounds so it is proportional (not harshly infinite like grim-trigger), avoiding permanent retaliation and enabling recovery.
- Epsilon probes (small random cooperations) let the strategy re-initiate cooperation after possibly noisy defections by others.
- Using f_last and f_recent makes the strategy responsive to sudden changes while smoothing over single-round mistakes.
- The upper cap max_punish prevents excessively long punishments late in the game.
- Endgame handling is cautious: in the absolute last round the strategy defects unless there is evidence of stable cooperation; this modestly reduces exploitation risk in a finite horizon while still allowing cooperation if the group has been reliably cooperative.

Why this will do well in a tournament
- Encourages cooperation with simple clear signals (start cooperating, reward cooperation).
- Retaliates in proportion to observed defection to deter serial exploitation but forgives to allow re-establishment of cooperation (good against both unconditional defectors and conditional cooperators).
- Adapts to the public-good potency (k) so threshold and tolerance change appropriately.
- Noise-tolerant and uses limited punishment (avoids long vendettas that lower long-run payoff).
- Uses only previous actions (public history) and game parameters, so it is implementable in the contest environment.

Variants / tunings you might try
- Increase epsilon (more probing) if other strategies often get stuck in mutual defection.
- Adjust alpha formula toward more or less strictness based on prior tournament experience.
- Use a longer W (recent window) to be more stable or shorter to be more reactive.

This description is directly implementable: it specifies all decision branches, derived parameters from (n,k,r), and a deterministic punish-until mechanism augmented with small-probability probes for reconnection and noise-tolerance.
'''

description_COOPERATIVE_7 = '''
Strategy name: Forgiving Majority Conditional Cooperator (FMCC)

Intuition (short)
- Start by cooperating to signal willingness to cooperate.
- Reciprocate cooperation from the group when a clear majority of other players have cooperated recently.
- Punish defections briefly and proportionally so defectors lose short-term gains but the group can recover quickly.
- Track individuals to tolerate a small set of persistent defectors (so you keep cooperating with the cooperating majority rather than letting a few exploiters collapse cooperation).
- In the known final round, avoid being exploited: if everyone has been perfectly cooperative so far, cooperate; otherwise defect.

This strategy depends only on game parameters (n, r, k) and the observed history (each player’s actions in previous rounds).

Parameters used internally (computed from n, r, k)
- w (memory window): min(3, r) — look at up to the last 3 rounds to estimate recent behavior (short window reacts quickly).
- majority_threshold: 0.5 (cooperate when at least a simple majority of others have cooperated recently). This value can be adjusted upward for more conservative play; 0.5 is robust and allows cooperating minorities to rejoin.
- persist_defect_threshold: 0.5 (if a given opponent defected in at least half of observed rounds, mark them as a persistent defector).
- base_punish_len: 1 (minimum one-round punishment).
- punish_scale: ceil((n-1) * 0.5) used to scale punishment severity only when many others defected; used in formula below.

Decision rules (concrete)
Maintain these data structures:
- For each other player j: defect_count_j = number of rounds j played D so far.
- For each past round t: store actions of all players.
- punish_counter (integer) initialized 0: number of rounds left in an active punishment phase.
- r_remain = rounds remaining including current.

Every round t (1..r) do:

1) Edge-case checks:
   - If t == 1 (first round): Cooperate. (Signal cooperative intent.)
   - If t == r (final round):
       - If every player (including you) cooperated in all rounds 1..r-1 (i.e., perfect cooperation history), then Cooperate in final round.
       - Else Defect in final round (last-round defection to avoid exploitation).

2) Update persistent-defector marks:
   - For each j ≠ i: if defect_count_j / max(1, rounds_observed_for_j) ≥ persist_defect_threshold, mark j as persistent_defector.

3) If punish_counter > 0:
   - Defect this round.
   - Decrement punish_counter by 1.
   - (Still update counts after the round.)
   - End decision.

4) Otherwise (no active punishment), compute recent cooperation rates:
   - Consider the memory window of last w rounds (or fewer if t-1 < w): examine for each of those rounds how many of the other players cooperated.
   - Let recent_coop_by_others = for each round r' in window, count coopers among other players; then take the average fraction f = average_over_window( (#cooperators_among_others) / (n-1) ).
   - If there are persistent defectors, compute f among the non-persistent players only:
       - Let N_nonpersist = number of other players not marked persistent. Compute f_nonpersist using those players in the same way; set effective_fraction = f_nonpersist. If N_nonpersist == 0 then effective_fraction = 0.

5) Decision based on effective_fraction:
   - If effective_fraction ≥ majority_threshold:
       - Cooperate this round.
   - Else:
       - Defect this round, and (if the shortfall is nontrivial) start a proportional punishment:
           - Let shortfall = max(0, majority_threshold - effective_fraction).
           - Set punish_counter = base_punish_len + ceil(shortfall * punish_scale).
             (This makes a single defection produce a short 1-round punishment, many simultaneous defections produce longer punishments.)
       - End decision.

6) After the round finishes, update defect_count_j for each j based on observed actions of that round, so history informs future rounds.

Rationale and properties
- Cooperative signal: Cooperates in round 1 to show willingness to coordinate on the Pareto-superior all-C outcome.
- Reciprocation: Uses recent majority behavior to decide; if most others cooperate, FMCC cooperates, sustaining cooperation with groups that reciprocate.
- Forgiving: Punishments are short (minimum one round) and proportional. Quick forgiveness prevents long mutual-defection cycles caused by a single mistake or a single exploiter.
- Robust to partial defection: By tracking persistent defectors and excluding them when computing cooperation fraction, FMCC can continue to cooperate with a cooperating majority even if a minority of players exploit. This prevents collapse of cooperation due to a few bad actors.
- Adaptive: Memory window w = 3 balances responsiveness and stability; punish length scales with observed shortfall so the strategy adapts to the severity of defection waves.
- Final-round behavior: Defects in the final round unless history shows everyone cooperated all prior rounds. This avoids a guaranteed last-round exploit while still rewarding perfectly cooperative histories (cooperate in final round if full cooperation persisted).

Pseudocode
(Informal, implementable)

initialize:
  for each j != me: defect_count[j] = 0
  punish_counter = 0
  w = min(3, r)
  majority_threshold = 0.5
  persist_defect_threshold = 0.5
  base_punish_len = 1
  punish_scale = ceil((n-1) * 0.5)

for round t = 1 to r:
  rounds_observed = t - 1

  if t == 1:
    action = C
    play action; observe others; update defect_count; continue

  if t == r:  // final round
    if rounds_observed == 0 or (every player cooperated in all rounds 1..r-1):
      action = C
    else:
      action = D
    play action; update defect_count; continue

  // mark persistent defectors
  persistent = set()
  for j != me:
    if defect_count[j] / max(1, rounds_observed) >= persist_defect_threshold:
      persistent.add(j)
  nonpersistent_players = set of others minus persistent

  if punish_counter > 0:
    action = D
    punish_counter -= 1
    play action; update defect_count; continue

  // compute effective recent cooperation fraction
  window_start = max(1, t - w)
  if nonpersistent_players not empty:
    denom = size(nonpersistent_players)
    sum_frac = 0
    for round s in window_start..t-1:
      coop_count = number of nonpersistent players who played C in round s
      sum_frac += coop_count / denom
    effective_fraction = sum_frac / (t - window_start)
  else:
    effective_fraction = 0

  if effective_fraction >= majority_threshold:
    action = C
    play action; update defect_count; continue
  else:
    action = D
    // start punishment proportional to shortfall
    shortfall = max(0, majority_threshold - effective_fraction)
    punish_counter = base_punish_len + ceil(shortfall * punish_scale)
    play action; update defect_count; continue

Notes for implementers
- All counts and averages can be floating; comparisons use >= with exact threshold.
- The constants (w, majority_threshold, persist_defect_threshold, base_punish_len, punish_scale) are chosen for robustness; they can be tuned for specific tournaments. FMCC’s design prioritizes preserving cooperation with a cooperating majority while limiting exploitation by defectors.
- The persistent-defector mechanism avoids discarding cooperation entirely when a small fraction of players are malicious or always-defect.
- The punishment is group-level (you cannot punish a single player’s payoff only), but by defecting while cooperating with most others you reduce the benefit of being a defector and encourage return to cooperation.

Why this strategy is cooperative and robust
- It opens with cooperation and returns to cooperation rapidly after transgressions, thereby tending to sustain mutually beneficial all-C outcomes when opponents are willing.
- It avoids blind “grim” strategies that never forgive and lose long-run payoff.
- It is resilient to mixed populations: if a cooperating majority exists, FMCC will cooperate with them even in the presence of a minority of defectors; if defections are widespread, FMCC defects to avoid being exploited.
- By defecting in the last round except under perfect history, FMCC protects itself from unavoidable endgame exploitation while still rewarding spotless cooperative histories.

This strategy is straightforward to implement from the described rules and requires only per-round action history and simple counters.
'''

description_COOPERATIVE_8 = '''
Name: Adaptive Threshold Reciprocity (ATR)

Short description
- ATR is a forgiving, reciprocity-based strategy that tries to sustain high group cooperation while limiting exploitation. It starts cooperatively, conditions on the observed cooperation level of the other players (not on any promises), punishes deviations but forgives quickly, and never allows punishment to escalate indefinitely. It defects in the last round (standard endgame correction). ATR uses only the game parameters (n, r, k) and the observed action history.

Intuition
- Begin by offering cooperation to establish a cooperative norm.
- Monitor how many of the other players cooperated in recent rounds.
- Cooperate when a clear majority (or only a few others) cooperated recently.
- If the group cooperation falls below the threshold, punish by defecting, but limit and quickly forgive punishment so occasional mistakes or one-off free rides do not destroy cooperation.
- Always defect in the final round (no future to enforce reciprocity).

Decision rules (natural language)
- Round 1: Cooperate.
- Last round (round r): Defect.
- For each intermediate round t (2 ≤ t ≤ r−1):
  1. Compute how many of the other players cooperated in the previous round:
     others_coop = (# of players who played C in round t−1) − (my action in t−1, 1 if I cooperated else 0).
     (If you prefer to base on a longer window, you can average over the last w rounds; default ATR uses the previous round for responsiveness.)
  2. If others_coop ≥ (n−1) − d_grace, cooperate. (That is, if at most d_grace other players defected last round, be lenient and cooperate.)
  3. Else if others_coop/(n−1) ≥ τ, cooperate. (That is, if the fraction of other cooperators last round is at least τ, cooperate.)
  4. Otherwise (others_coop/(n−1) < τ), enter punishment mode: defect this round.
  5. Punishment mode is limited: if punishment is triggered, defect for at most P consecutive rounds (unless cooperation recovers earlier). After a punishment round, if others’ cooperation meets the threshold again, forgive and resume cooperation.
- Parameters explained and recommended default values:
  - τ (cooperation fraction threshold): 0.50 (cooperate if a majority of other players cooperated last round).
  - d_grace (grace number of tolerated defectors): 1 (tolerate up to one defector among others without punishing).
  - P (max consecutive punishment rounds): min(3, remaining rounds − 1). (Limits the length of punishment to avoid long vendettas.)
  - Optionally w (memory window): 1 (only last round), but can be set to a small integer (e.g., 3) if you want smoother responses.

Pseudocode (concise)
Initialize:
  consecutive_low = 0  // number of immediately preceding rounds where others_coop/(n-1) < τ
For each round t = 1..r:
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue
  // compute others' cooperation in last round
  total_coop_prev = number of players who played C in round t-1
  my_prev = 1 if I played C in round t-1 else 0
  others_coop = total_coop_prev - my_prev
  frac = others_coop / (n - 1)
  // leniency: tolerate up to d_grace defectors
  if others_coop >= (n - 1) - d_grace:
    // high cooperation among others — cooperate
    play C
    consecutive_low = 0
    continue
  // threshold test
  if frac >= τ:
    play C
    consecutive_low = 0
    continue
  // below threshold -> punish
  consecutive_low += 1
  if consecutive_low <= P:
    play D
    continue
  else:
    // punishment cap reached; attempt to rebuild cooperation
    play C
    consecutive_low = 0
    continue

Design notes and rationale
- Start cooperative to signal willingness to build a cooperative norm. In many heterogeneous tournaments, opening with cooperation invites reciprocators and yields higher joint return.
- Use majority threshold τ = 0.5 because in an n-player public goods setting a majority cooperating signals a functioning cooperative group; it is robust and parameter-agnostic. The use of d_grace ensures tolerance for 1 defector (common noise or occasional free-riding).
- Limited punishment (P small) prevents endless retaliation. Short, predictable punishment discourages persistent free-riding but restores cooperation quickly if others resume cooperating.
- The strategy depends only on observed history and parameters (n, r); it does not assume shared conventions or external coordination.
- Last-round defection is necessary because there is no future to enforce cooperation.
- Optional refinements:
  - Use a longer memory window w (e.g., w = min(3, r−1)) and compute frac as the average fraction of other cooperators over the last w rounds for smoother behavior.
  - Make τ slightly adaptive to k: when k is closer to n (public good is more productive), you can lower τ to encourage cooperation (e.g., τ = 0.45 + 0.05*(1 - k/n)), but defaults above are robust.
  - Use a small random forgiveness probability ε (e.g., 0.02) to break cycles of mutual punishment in fully deterministic settings.

Examples of behavior
- All-cooperators (everyone plays C): ATR cooperates every round until the last, producing high group payoff.
- All-defectors: ATR defects after the first round (it will defect whenever others are below threshold), avoiding sustained exploitation.
- Single consistent defector among many cooperators: ATR tolerates up to d_grace defectors; if the defection is rare, ATR will continue to cooperate and keep group payoff high. If many defect persistently, ATR punishes for up to P rounds and then tries to restore cooperation.
- Occasional mistakes/noise: Short punishment + forgiveness lets cooperation recover after isolated mistakes.

Why this is robust and cooperative
- It is cooperative-minded (offers cooperation, forgives quickly, seeks to rebuild cooperation).
- It is punitive enough to discourage persistent free-riding (short, credible punishment).
- It is tolerant to noise and single-player deviations, avoiding breakdown of cooperation from isolated defects.
- It requires only observed plays (no communication), handles arbitrary numbers of opponents, and adapts to changing behavior without assuming common norms.

Implementation checklist for tournament coding
- Track the global action history per round (so you can compute total_coop_prev and my_prev).
- Maintain consecutive_low counter.
- Set τ = 0.5, d_grace = 1, P = min(3, remaining rounds − 1) unless you choose to adjust them for specific tournament properties.
- Always output D in the final round.

This strategy balances pro-cooperation incentives with guarded retaliation and rapid forgiveness — a practical, adaptive approach for multi-player repeated public goods competitions.
'''

description_COOPERATIVE_9 = '''
Strategy name: Forgiving Conditional Cooperator (FCC)

Summary intuition
- Signal cooperation by starting cooperatively, try to sustain high group cooperation by cooperating when others have been cooperating, but avoid long-term exploitation by short, proportional punishments when group cooperation falls.  
- Be forgiving (short punishments, quick reevaluation) so cooperation can be re-established after mistakes or noise.  
- Reduce forgiveness near the end of the finite game (endgame awareness) to avoid being exploited when future enforcement is weak.  
- All choices use only the game parameters (n, k, r) and observed history (who played C/D each past round).

Important derived quantities
- w (memory window) = min(5, r - 1). (How many past rounds we average over; at least 1 if r>1.)
- coop_fraction(t') = (number of players who played C in round t') / n.
- coop_rate = average of coop_fraction over the last w rounds (excluding the current choice).
- T_base = 0.5 + 0.3 * (1 - k/n). (A baseline cooperation threshold that rises when the public good is less efficient — we require more evidence of others' cooperation when k is small.)
  - Clamp T_base to [0.4, 0.8].
- T_final_increase = 0.15 if remaining rounds ≤ max(1, floor(r/10)) else 0. (Makes us less forgiving near the end.)
- T = clamp(T_base + T_final_increase, 0.4, 0.95).
- Punishment length scale: P_max = 3 (maximum consecutive defections used to punish).
- Forgiveness threshold T_forgive = 0.5 (we resume cooperation if group-level cooperation recovers above this).

State variables tracked by the strategy
- punishment_remaining: integer number of rounds left in an active punishment phase (initially 0).
- last_actions and full history of others' actions (available by assumption).

Decision rules (natural-language + pseudocode)

Initialize:
- punishment_remaining ← 0
- Cooperate in round 1 (to signal willingness to cooperate).

Each round t (2 ≤ t ≤ r):
1. Compute w = min(5, r-1). Compute coop_rate = average over the previous min(w, t-1) rounds of coop_fraction (i.e., fraction of players cooperating, averaged by round). If t-1 < w just average over the available rounds.

2. Update threshold:
   T_base = 0.5 + 0.3 * (1 - k/n)
   T_base = clamp(T_base, 0.4, 0.8)
   If remaining_rounds = r - t + 1 ≤ max(1, floor(r/10)), set T = T_base + 0.15 else T = T_base
   T = clamp(T, 0.4, 0.95)

3. Endgame rule (final round t = r):
   - If punishment_remaining > 0 then defect (no point cooperating while punishing).
   - Else if coop_rate ≥ 0.95 (i.e., near-unanimous cooperation recently) then cooperate (mutual-final cooperation signal).
   - Else defect. (Default to defect in final round unless there's strong evidence everyone will cooperate.)

4. Punishment mechanics and main decision (for non-final rounds):
   - If punishment_remaining > 0:
       - Play D (defect).
       - punishment_remaining ← punishment_remaining − 1
   - Else (not currently punishing):
       - If coop_rate ≥ T: play C (cooperate).
       - Else (coop_rate < T): start a punishment phase:
           - Measure shortfall s = T − coop_rate (≥0).
           - Set punishment_remaining ← min(P_max, ceil( (s / T) * P_max )).
             (At least 1 round of punishment; longer punishment if the shortfall is large; capped at P_max and also not exceeding remaining rounds minus 1.)
           - Immediately play D this round (beginning punishment). If punishment_remaining was set > 0, it will be decremented on subsequent rounds as above.

5. Forgiveness / exit from punishment:
   - After punishment_remaining reaches 0, re-evaluate coop_rate. If coop_rate ≥ T_forgive then resume cooperation. If not, you will re-enter punishment per rule 4 (this means punishment can repeat but only in short bursts and only as long as group cooperation remains low).

Edge cases and clarifications
- First round: Always cooperate to signal cooperative intent.
- Last round: Default to defect unless history shows near-unanimous cooperation recently (coop_rate ≥ 0.95) and you are not currently punishing. This protects against guaranteed final-round unilateral exploitation but allows mutual-final cooperation if it is very clearly established.
- Very small r (e.g., r = 2 or 3): w will be small (≥1). You still start cooperating round 1. For r = 2, you likely defect in round 2 unless the other cooperated and coop_fraction was near 1.
- Remaining rounds cap: When computing punishment_remaining, never set it larger than remaining_rounds - 1 (no need to punish in final round if doing so would be pointless). If remaining_rounds is small, punishment is correspondingly short.
- No randomization required. The strategy is deterministic given parameters and history.
- Noise robustness: The memory window w > 1 (when available), forgiving small punishments, and the moderate T_forgive reduce over-reaction to occasional mistakes. Punishment lengths are short and proportional to the degree of observed shortfall in cooperation.

Why this is cooperative and robust
- Cooperative: Starts by cooperating and cooperates whenever the group-level cooperation signal exceeds a threshold. This helps build and sustain cooperation against reciprocating strategies that also respond to signals.
- Robust to exploitation: If others defect enough that the average cooperation falls under the threshold, FCC defects in short, visible punishment phases. These punishments reduce the short-term gains of persistent free-riders and thus deter exploitation.
- Forgiving: Punishments are short and conditional; the strategy resumes cooperation quickly when the group returns to acceptable cooperation levels. This allows recovery from mistakes and supports re-establishment of cooperation.
- Endgame-aware: The strategy reduces forgiveness near the final rounds to avoid being exploited when future enforcement is weak, but still allows mutual final-round cooperation if the history gives very strong evidence of universal cooperation.

Pseudocode (compact)

Initialize:
  punishment_remaining = 0

Round t:
  if t == 1:
    play C
    continue

  remaining_rounds = r - t + 1
  w = min(5, r - 1)
  lookback = min(w, t-1)
  coop_rate = average over last lookback rounds of (number_of_C / n)

  T_base = 0.5 + 0.3*(1 - k/n)
  T_base = clamp(T_base, 0.4, 0.8)
  if remaining_rounds ≤ max(1, floor(r/10)):
    T = clamp(T_base + 0.15, 0.4, 0.95)
  else:
    T = T_base

  if t == r:                         # final round
    if punishment_remaining > 0:
      play D
      punishment_remaining -= 1
    else if coop_rate >= 0.95:
      play C
    else:
      play D
    continue

  if punishment_remaining > 0:
    play D
    punishment_remaining -= 1
    continue

  if coop_rate >= T:
    play C
    continue

  # coop_rate < T: start punishment
  s = T - coop_rate
  P = min(P_max, ceil((s / T) * P_max))
  P = min(P, remaining_rounds - 1)   # don't punish into the final useless round
  if P < 1:
    P = 1
  punishment_remaining = P - 1       # we play one D now and P-1 further rounds
  play D

End pseudocode

Tuneable knobs (implementer may adjust)
- w (memory depth), P_max (punishment aggressiveness), T_base formula, and T_forgive can be tuned for particular tournament environments. The defaults above aim for balance: enough memory to detect trends, short punishing to avoid permanent feuds, threshold responsive to k (public-good efficiency) and endgame.

Behavioral examples (quick intuition)
- If most players repeatedly cooperate, FCC cooperates each round and earns/encourages high group payoff.  
- If a small set of players begin free-riding persistently, coop_rate falls, FCC responds with a few rounds of defection to reduce the free-riders’ payoff advantage and encourage correction. If they correct, FCC forgives and returns to cooperation.  
- If everyone defects, FCC quickly defects as well (no wasted sacrifices).  
- Near the final round FCC is cautious: it will defect unless near-unanimous cooperation has been reliably observed.

This strategy is fully specified using only n, k, r and the observed history and is deterministic and implementable. It balances cooperative intent, protection from exploitation, forgiveness for noise, and endgame awareness.
'''

description_COOPERATIVE_10 = '''
Name: Adaptive Threshold with Forgiving Punishment (ATFP)

Overview (intended properties)
- Starts by signalling cooperation.
- Cooperates when the recent group cooperation level meets a threshold that is tuned by the game parameters (n, k).
- When others fall below the threshold, issues a short, limited punishment (defection) then forgives and returns to cooperation to allow recovery.
- Adapts threshold stringency to the public-good efficiency k: when the project is more efficient (k closer to n) it tolerates lower observed cooperation; when the project is less efficient (k small) it requires stronger evidence of others cooperating.
- Avoids harsh permanent punishments (resilient to noise) but punishes repeated or persistent free-riding.
- In the final round it is cautious: defects unless recent cooperation is strong.

All decisions depend only on (n, r, k) and the public history of past round actions (who cooperated each round) and observed payoffs.

Derived parameters (computed once from n, k, r)
- w = min(5, r-1)  // look-back window for recent history (use up to last 5 rounds)
- θ = clamp(0.25 + 0.75*(1 - k/n), 0.2, 0.9)
    - Intuition: θ decreases when k/n is larger (public good more efficient), allowing tolerance for fewer cooperators.
- T = ceil(θ * (n-1))  // required number of OTHER players cooperating (excludes me) to consider “good” cooperation
- punish_base = 2  // base punishment severity multiplier
- punish_max = min(4, r-2)  // maximum punishment length (never exceeds remaining rounds)

State maintained (internal; can be computed from history):
- punish_until_round (initially 0): if current round t ≤ punish_until_round, play D (we are executing a punishment).

Decision rules (natural language + pseudocode)

Pseudocode style:

Initialize:
  w = min(5, r-1)
  theta = clamp(0.25 + 0.75*(1 - k/n), 0.2, 0.9)
  T = ceil(theta * (n-1))
  punish_until_round = 0

For each round t = 1..r:
  // Helper: others_cooperators(t') = number of players other than me who cooperated in round t'
  if t == 1:
    play C   // start by signalling cooperation

  else if t <= punish_until_round:
    play D   // still in a punishment period

  else if t == r:
    // final-round rule: cooperate only if recent cooperation strong
    compute others_coop_last = others_cooperators(t-1)
    if others_coop_last >= T:
      play C
    else:
      play D

  else:
    // Evaluate recent history
    lookback = min(w, t-1)
    sum_others = sum_{u = t - lookback}^{t-1} others_cooperators(u)
    recent_avg = sum_others / lookback

    // compute frequency of under-threshold rounds in window
    under_count = number of u in {t-lookback,...,t-1} with others_cooperators(u) < T
    mis_rate = under_count / lookback

    if recent_avg >= T:
      play C   // group has met our cooperation standard recently; reciprocate

    else:
      // Start a short, proportional punishment
      P = min( max(1, ceil(mis_rate * punish_base)), punish_max, r - t )  
         // at least 1 round, scales with how often others missed T, capped
      punish_until_round = t + P - 1
      play D

Explanation of the numbers and behavioral rationale
- Why start with cooperation? To signal willingness to cooperate and allow formation of mutually beneficial reciprocity.
- Why threshold T based on k and n? The private cost of cooperation is fixed (giving up the private 1), while the public benefit depends on k. When k is large (k ≈ n) the public good is efficient and the strategy should be tolerant (lower threshold) because even a few cooperators produce large returns; when k is small (but still >1), cooperation is fragile and the strategy demands stronger evidence of others cooperating.
- Why use a small look-back w and limited punishments? A small window (up to 5 rounds) balances responsiveness and noise tolerance. Limited, proportional punishments (1–4 rounds) prevent collapse into permanent mutual defection from a single mistake, and allow the strategy to re-establish cooperation quickly if opponents respond. The punishment length increases if under-threshold rounds are frequent, so persistent free-riding is met with stronger (but still temporary) response.
- Why defect in the last round unless strong evidence of cooperation? In a finitely repeated game, the last round is vulnerable to exploitation. This rule avoids giving away the endgame payoff unless others have just shown strong, recent reciprocity.

Edge cases and corner behavior
- Very small r: If r ≤ 2, w becomes 1 or 0; the strategy still cooperates in round 1, and in round r uses the last-round check (or defects if no recent evidence). punish_max is constrained so we never schedule punishments beyond the game end.
- Noise / transient deviations: A single accidental defection by another player causes at most a short punishment (usually 1 or 2 rounds), after which this strategy returns to cooperation—so it tolerates mistakes.
- Persistent defectors: Persistent low cooperation raises mis_rate and so increases punishment length up to punish_max. That reduces exploitation risk and lowers the return to unconditional cooperation by the opponent.
- Mixed opponents: The threshold-based rule works against heterogeneous populations. If the population as a whole sustains sufficient cooperation (recent_avg ≥ T), ATFP cooperates and reaps high payoffs. If the group is below threshold, ATFP defects to avoid being exploited while still giving opportunities to re-establish cooperation.

Variants / parameter tuning (implementation notes)
- w, punish_base and punish_max are tunable. Larger w makes the strategy slower to react but more robust to short fluctuations. Larger punish_max makes punishment more severe but risks prolonged collapse.
- The formula θ = 0.25 + 0.75*(1 - k/n) is intentional but adjustable. It yields lower required peer-cooperation when k is high and higher required peer-cooperation when k is low. Implementers may substitute a different monotone mapping from k/n to θ.
- If you want a strictly more forgiving version, lower punish_base (e.g., 1) and/or increase w.
- If experiments show many opponents exploit the endgame, make last-round criterion stricter (require average of last w rounds ≥ T rather than only last round).

Summary
- ATFP is an adaptive reciprocity strategy: cooperate initially; continue to cooperate when recent others’ cooperation meets a parameterized threshold that accounts for k and n; respond to deviations with short, proportional punishments and then forgive; be cautious in the final round unless recent cooperation evidence is strong.
- It is robust to noise, flexible across values of k and n, and designed to sustain cooperation with reciprocating opponents while limiting exploitation by defectors.
'''

description_COOPERATIVE_11 = '''
Summary of intent
- Goal: achieve and sustain high group cooperation while avoiding long-term exploitation.
- Style: start cooperatively, monitor everyone’s behaviour, apply short graded punishments when the group falls below a cooperation threshold, then forgive. Become conservative in the known endgame so you are not trivially exploited in final rounds.
- Uses only game parameters (n, r, k) and observed history (past rounds’ actions). No external assumptions about opponents.

Key ideas (intuition)
- Cooperate to signal willingness and to obtain the high payoff of mutual cooperation (when others reciprocate).
- Punish when the recent group cooperation is too low (punishment discourages free-riding).
- Keep punishments short and proportional so cooperation can recover; permanent grim trigger is avoided because it collapses group gains with small mistakes.
- Near the final rounds, reduce risk-taking (because backward-induction pressure grows), so be more conservative about cooperating.

Parameters the strategy uses (derived from n, r, k)
- G = 1: initial grace rounds (cooperate in round 1).
- W = min(5, r): window length for short-term behaviour estimation (last W rounds).
- T_frac: required fraction of cooperators in recent rounds to keep cooperating. Set
    T_frac = 0.5 + 0.4 * (k - 1) / (n - 1)
  (Interpretation: the higher k is relative to 1, the more willing we are to require and sustain cooperation. This lies between 0.5 and 0.9 roughly for typical k.)
- T_total = ceil(T_frac * n): minimum number of cooperators in the previous round (including other players plus potential self) that we treat as “acceptable”.
- P_max = min(ceil(r/5), 4): maximum punishment length (a small finite number so we don’t punish forever).
- Endgame window E = min(3, r): last E rounds are “conservative” (reduce exploitation risk).
- Forgiveness: after a punishment period ends, resume cooperation immediately if recent behaviour has improved.

High-level decision rules
1. Round 1: Cooperate (signal cooperative intent).
2. Last round (t = r): Defect (one-shot dominant action).
3. Endgame rounds (t in r-E+1 .. r-1): Conservative mode — cooperate only if the immediately previous round was full cooperation (every player cooperated). If not full cooperation previously, defect. This avoids being exploited close to the end.
4. Normal rounds (not in the endgame and t > 1):
   - If currently in an active punishment period (you are punishing because of past low cooperation), defect this round and decrement punishment counter.
   - Otherwise look at the immediately previous round’s count C_prev (total number of cooperators last round).
     - If C_prev >= T_total, cooperate this round (rewarding cooperation).
     - If C_prev < T_total, initiate punishment: set a punishment counter P = 1 + ceil( (T_total - C_prev) / n * P_max ). Immediately defect this round and continue defecting for the P rounds (counting this round in P). The punishment length is proportional to how far last round’s cooperation was below the threshold, bounded by P_max. After punishment ends, resume cooperation (forgiveness), provided cooperative signals reappear.

5. Additional smoothing (robustness): If many small mistakes (single-round deviations) are observed across players but the W-window average group cooperation rate remains high (>= T_frac), treat it as a mistake and do not punish. In practice: compute avg_coop = average total cooperators over last min(W, t-1) rounds; if avg_coop >= T_frac * n then treat current under-threshold C_prev as temporary noise and choose to cooperate instead of starting punishment.

Pseudocode

Inputs: n, r, k, history (list of previous rounds; each round is a vector of n actions where action is 1 for C and 0 for D)
Derived:
  G = 1
  W = min(5, r)
  T_frac = 0.5 + 0.4 * (k - 1) / (n - 1)
  T_total = ceil(T_frac * n)
  P_max = min(ceil(r/5), 4)
  E = min(3, r)

State variables (maintained between rounds):
  punish_remaining = 0   # rounds left to punish (0 if not punishing)

Function DecideAction(t, history):
  if t == 1:
    return C
  if t == r:
    return D
  # Endgame conservative mode
  if t >= r - E + 1 and t <= r - 1:
    if last_round_all_cooperated(history):
      return C
    else:
      return D

  # If currently punishing, continue punishment
  if punish_remaining > 0:
    punish_remaining -= 1
    return D

  # Look at previous round cooperators
  C_prev = number_of_cooperators_in_round(history[-1])

  # Compute short-term average cooperator count across window
  window_len = min(W, len(history))
  avg_coop = average_total_cooperators(history[-window_len:])
  if avg_coop >= T_frac * n:
    # recent behaviour is overall cooperative -> forgive single lapses
    return C

  # If previous round meets threshold, cooperate
  if C_prev >= T_total:
    return C

  # Otherwise initiate proportional punishment
  gap = max(0, T_total - C_prev)  # how far below threshold
  # punish length proportional to gap, at least 1, bounded by P_max
  P = min(P_max, 1 + ceil((gap / n) * P_max))
  punish_remaining = P - 1   # we already defect this round, so set remaining
  return D

Helper definitions:
  last_round_all_cooperated(history):
    return number_of_cooperators_in_round(history[-1]) == n

  number_of_cooperators_in_round(round_actions):
    return sum(round_actions)  # actions are 1 for C, 0 for D

  average_total_cooperators(last_k_rounds):
    return mean(sum(actions) for each round in last_k_rounds)

Why this is cooperative and robust
- Cooperative: starts by cooperating, rewards rounds with many cooperators by cooperating again, and only defects when the group has recently underperformed. This tends to sustain mutual cooperation with reciprocators.
- Punishment is short and proportional: that discourages exploitation but allows a quick recovery. That avoids spirals to permanent defection that destroy group payoff after a single mistake.
- Forgiveness/smoothing: the algorithm ignores isolated lapses when recent average cooperativeness is high, preventing overreaction to noise (useful against random defectors or accidental mistakes).
- Endgame caution: in the last few rounds we are conservative, reducing the chance of being exploited by strategies that defect late but were cooperative earlier.
- Parameter adaptation: T_frac depends on k and n: we require higher cooperative support when k is higher (because the collective benefit is larger and cooperation is more valuable), but always require at least a simple majority baseline.

Notes and tuning
- The numeric constants (e.g., W=5, P_max up to 4, T_frac baseline 0.5) are conservative defaults chosen for robustness. Tournament implementers may tune them if desired.
- If you prefer probabilistic forgiveness, substitute a small cooperation probability during punishment after the first punishment round; that can help reestablish cooperation with noisy opponents.
- If you have evidence opponents are completely non-cooperative (avg_coop very low across long history), the strategy will effectively defect most rounds — rational to avoid persistent exploitation.

Edge cases explicitly handled
- First round: cooperate (signal).
- Final round: defect (one-shot best response).
- Endgame rounds: require full previous-round cooperation to keep cooperating.
- Single accidental defections: smoothed by window-average check to avoid unnecessary long punishments.
- Multiple simultaneous defectors: punishment length scales with how far the group fell below the cooperation threshold.

Conclusion
This “Reciprocal Conditional Cooperator with graded punishment and forgiveness” is simple to implement, starts cooperatively, punishes proportional to recent under-performance, forgives quickly to restore cooperation, and becomes conservative near the known end. It uses only the game parameters and observed history and is robust across a wide range of opponent behaviours.
'''

description_COOPERATIVE_12 = '''
Name: Adaptive Proportional Conditional Cooperator (APCC)

High-level idea
- Start friendly (cooperate) and try to sustain mutual cooperation when the group is cooperating.
- Measure recent group cooperation (excluding yourself) over a short rolling window.
- Cooperate when recent group cooperation is sufficiently high. If group cooperation falls, apply a proportional, short punishment (defect) whose length grows with the magnitude of the drop, then forgive and test whether cooperation returns.
- Be slightly more conservative in the final rounds (endgame awareness) to reduce being exploited at the end, but continue to cooperate if the group shows a strong cooperative signal.
- All decisions depend only on the known parameters (n, r) and the public history of actions.

Why this is robust and cooperative
- It rewards groups that sustain high cooperation, encouraging mutually beneficial outcomes.
- Punishments are proportional and short (not a permanent grim trigger), which resists long cycles of mutual retaliation and tolerates occasional mistakes.
- The rolling-window statistic makes the strategy adaptive to slowly changing opponent behavior.
- Endgame-awareness lowers exploitation risk near terminal rounds but still allows cooperation when it is widely reciprocated.

Parameters (derived from game parameters, but fixed for implementation)
- memory window w = min(5, r) — over how many past rounds we measure the group
- baseline threshold theta = 0.50 — require roughly majority cooperation among others to be confident to cooperate
- endgame rounds L = min(3, r) — last L rounds are endgame
- endgame threshold theta_end = 0.70 — be more demanding near the end
- small-margin delta = 0.10 — a narrow band below threshold where we “test” cooperation probabilistically
- test probability p_test = 0.5 — chance to cooperate when in the test band
- punishment scale S = w (or 5) — converts shortfall below threshold into punishment length
- minimum punishment length 1 when a clear shortfall observed

Decision rules (natural language)
1. First round: cooperate (signal goodwill).
2. Each round t > 1:
   a. Compute the recent others’ cooperation rate rho over the past window:
      - Look at the last m = min(w, t-1) rounds.
      - Let total_other_coops = total number of contributions by the other (n-1) players across those m rounds.
      - rho = total_other_coops / ((n-1) * m)  (range 0..1).
   b. If currently in an active punishment period (we maintain a punishment counter), then defect and decrement the counter by 1.
   c. Otherwise set current threshold:
      - If t > r - L (we are in the last L rounds), use theta_cur = theta_end, else theta_cur = theta.
   d. Compare rho to theta_cur:
      - If rho >= theta_cur: cooperate.
      - Else if rho >= theta_cur - delta: cooperate with probability p_test (a probabilistic test to check if cooperation will resume).
      - Else (rho < theta_cur - delta): start punishment:
         • Set punishment_length = max(1, ceil((theta_cur - rho) * S)).
         • Set punishment counter = punishment_length.
         • Defect this round (the first punishment round).
   e. After punishment_counter reaches 0, the next non-punishment decision will again consult rho and may cooperate (forgiveness). We do not carry forward permanent grudges.

Edge cases and special handling
- Very early rounds (t small): the window m will be small (m = t-1). Still compute rho consistently; this gives the strategy rapid adaptation from the earliest observations.
- If r is very small (e.g., r = 2 or 3) we still use w = min(5,r) and L = min(3,r) — the strategy will be naturally short-sighted but still cooperative in early rounds and slightly conservative in final rounds.
- Last round (t = r): we use the endgame threshold theta_end. If rho indicates strong recent cooperation (>= theta_end) the strategy cooperates; otherwise it defects. This avoids unconditional last-round cooperation that can be exploited.
- Single accidental defections by others: because punishment lengths are proportional and typically small, single mistakes trigger short punishments and then forgiveness — the strategy is tolerant of noise or errors.
- Coordinated, persistent exploitation by many opponents: the strategy responds by defecting in proportion to how badly the group cooperation falls; persistent exploitation will result in sustained defection (protecting you), but because punishment is proportional and re-testing occurs, the strategy will resume cooperation only when the group behavior improves.

Pseudocode (concise)
- Initialize punishment_counter = 0
- Set w = min(5,r), theta = 0.50, L = min(3,r), theta_end = 0.70, delta = 0.10, p_test = 0.5, S = w
- For each round t = 1..r:
    if t == 1:
        action = C
    else:
        m = min(w, t-1)
        total_other_coops = sum over rounds (t-1 downto t-m) of contributions by other (n-1) players
        rho = total_other_coops / ((n-1) * m)
        if punishment_counter > 0:
            action = D
            punishment_counter -= 1
        else:
            theta_cur = (theta_end if t > r - L else theta)
            if rho >= theta_cur:
                action = C
            else if rho >= theta_cur - delta:
                action = C with probability p_test, else D
            else:
                punishment_length = max(1, ceil((theta_cur - rho) * S))
                punishment_counter = punishment_length - 1   # we defect this round, so remaining count is length-1
                action = D
    play action

Implementation notes and tuning
- The numeric constants (w, theta, theta_end, delta, p_test, S) are chosen to balance cooperation and protection. In tournament settings you can tune them; the defaults given are robust starting points.
- If you expect many pure cooperators and want to be more cooperative, lower theta (e.g., 0.45) and shorten punishment lengths. If you suspect many exploiters, raise theta and S.
- The probabilistic test band prevents permanent miscoordination after marginal drops and helps re-establish cooperation without over-punishing.

Final remarks
- APCC is explicitly cooperative in intent (starts cooperatively, aims to sustain high group cooperation) but robust: it proportionally punishes low group cooperation, forgives, and re-tests. It avoids brittle all-or-nothing triggers (like grim) and is endgame-aware so it does not leave you repeatedly exploited near termination. It only relies on public history and game parameters, so it meets the tournament constraints.
'''

description_COOPERATIVE_13 = '''
Name: Adaptive Conditional Cooperation with Graduated Forgiveness (ACCGF)

Intuition (short)
- Start cooperative and try to keep cooperative outcomes when a clear majority or sustained pattern of cooperation exists.
- If cooperation falls below a reasonable threshold, signal disapproval by defecting for a short, graded punishment period proportional to the shortfall.
- Track individual behavior (simple strike counts) so persistent defectors are isolated while temporarily forgive occasional lapses.
- Always allow rapid return to cooperation when others recover. In the last round defect (one-shot best-response).

This strategy is parameterized only by (n, r, k) and the recent history of all players' moves and payoffs.

Parameters (computed from game parameters; transparent and simple)
- coop_threshold = ceil(n/2)   // need a simple majority signal to keep cooperating
- W = min(5, r)               // sliding window length for smoothing short-run noise
- strike_limit = max(2, floor(W/2))  // how many recent defections by a player count as “suspicious”
- max_punish = min(5, r-1)    // cap on punishment length
These values are conservative defaults intended to be robust across n, r, k. They can be tuned but do not require knowledge of opponents' internal algorithms.

State maintained
- strikes[j] for each other player j ∈ {1..n}, initialized 0 (count of recent suspicious defections)
- punishment_timer (integer ≥ 0), initialized 0
- last_actions[t-1][j] available from history (previous rounds)
- t = current round index (1..r)

Decision rules (natural language)
1. First round (t = 1)
   - Cooperate. This signals willingness to cooperate.

2. Last round (t = r)
   - Defect. In a finite horizon, the last round is one-shot; defect to avoid exploitation.

3. Updating strikes (done after observing round t-1, before choosing in round t)
   - For each player j (excluding me), consider their action history over the last W rounds (or fewer if t-1 < W).
   - If player j defected more than expected relative to peers (e.g., j defected while at least coop_threshold players cooperated in that past round), increment strikes[j] by 1 (bounded).
   - If player j cooperated in recent rounds consistently, decrement strikes[j] (down to 0). Strikes are a short memory, not permanent labels.

   Rationale: this differentiates targeted, repeat defectors from occasional mistakes.

4. Decide whether to cooperate in round t (t not 1 or r)
   - If punishment_timer > 0:
       - Play D (defect) this round (we are in a punishment phase).
       - After observing the round, reduce punishment_timer by 1 unless the group already recovered (see forgiveness rule below).
   - Else (punishment_timer == 0):
       - Compute recent_cooperators = number of players (including me if I cooperated last round) who cooperated in the most recent completed round OR use the average number of cooperators over the last W rounds (use the average if you want more smoothing). Use the average to reduce sensitivity to one noisy round.
       - If recent_cooperators >= coop_threshold AND the number of players with strikes > strike_limit is small (e.g., ≤ floor(n/3)):
           - Play C (cooperate). This preserves cooperation when a clear majority is cooperating and there are not many persistent defectors.
       - Else:
           - Signal disapproval by entering a short, graded punishment:
               - Set shortfall = max(0, coop_threshold - recent_cooperators).
               - punishment_timer = min(max_punish, 1 + shortfall)  // longer punishment when the shortfall is larger
               - Play D this round (start punishment).
   - Forgiveness trigger (during punishment timer > 0): if while punishing we observe recovery (e.g., in some round the number of cooperators ≥ coop_threshold), clear punishment_timer and reduce strikes for cooperating players.

5. Individual rehabilitation
   - A player with strikes[j] > strike_limit is treated as “suspicious.” The strategy reduces willingness to cooperate if many players are suspicious, but does not permanently exclude anyone.
   - A suspicious player can rehabilitate by showing cooperation for S = max(2, W/2) consecutive rounds; when that happens, strikes[j] are decremented and they are again treated as normal.

6. Safety against exploitation
   - Prevent permanent exploitation by limiting cooperation if many players are persistent defectors (we will defect until the group recovers).
   - Limit punishment lengths and allow rapid forgiveness to avoid long retaliatory cycles.

Pseudocode (concise)
Note: history[t-1][j] is action of player j in round t-1 ('C' or 'D').

Initialize:
  strikes[j] = 0 for all j ≠ me
  punishment_timer = 0

For each round t = 1..r:
  if t == 1:
    play C
    continue to next round
  if t == r:
    play D
    continue

  // update strikes using last W rounds of history
  for each player j ≠ me:
    defections_in_window = count of 'D' by j in rounds max(1,t-W) .. t-1
    // consider also whether j defected while many others cooperated in those rounds
    suspicious_defs = count of rounds in that window where j == 'D' AND (#cooperators_in_that_round >= coop_threshold)
    strikes[j] = min(max_strike, strikes[j] + suspicious_defs - rehabilitation_count)
    strikes[j] = max(0, strikes[j])

  // decide
  if punishment_timer > 0:
    play D
    // after observing outcomes: punishment_timer = punishment_timer - 1 unless recovery condition met
  else:
    recent_cooperators = average number of cooperators (including me if I cooperated) over last W rounds (or number in last round)
    suspicious_count = count of j with strikes[j] > strike_limit
    if recent_cooperators >= coop_threshold AND suspicious_count <= floor(n/3):
      play C
    else:
      shortfall = max(0, coop_threshold - round(recent_cooperators))
      punishment_timer = min(max_punish, 1 + shortfall)
      play D

  // On observing the round's actions (for forgiveness and rehabilitation processing):
  if punishment_timer > 0 and observed_cooperators_this_round >= coop_threshold:
    punishment_timer = 0
    // reward cooperating players by decreasing their strikes
    for j with action 'C' in this round:
      strikes[j] = max(0, strikes[j] - 1)

Why this is cooperative
- It starts cooperative, signals cooperation early, and only defects as a calibrated response to insufficient cooperation by others.
- Punishments are short, proportional to the degree of collapse, and immediately forgivable when others resume cooperation — this supports recovery and stable cooperation with reciprocating opponents.
- Tracking individuals ensures persistent defectors are not rewarded forever while allowing occasional lapses to be forgiven; that prevents exploitation by mixed strategies that sometimes defect.

Why this is robust
- Uses majority threshold (ceil(n/2)) rather than fragile unanimity; thus it tolerates a minority of defectors/noise.
- Uses sliding-window averages (W) to avoid overreacting to single noisy rounds.
- Graduated punishment avoids overly harsh permanent punishments (which can provoke long mutual-defection cycles) while being strong enough to discourage repeated mass defection.
- Does not rely on prearranged schedules or shared norms — only on observed behaviour — so it adapts to arbitrary independent strategies.

Edge cases and variants
- If you want to be more aggressive (risking some exploitation to push cooperation), lower coop_threshold (e.g., ceil(n*0.4)) or increase max_punish. If you want to be more forgiving, increase W, lower strike sensitivity, and shorten punishments.
- If r is very small (say r ≤ 3), cooperation is hard to maintain. With extremely short games you can choose to cooperate until the last 1 or 2 rounds and then defect; keep punishments very short.
- If repeatedly facing groups with many persistent defectors (high suspicious_count), the algorithm will spend more rounds defecting — this is rational defensive behavior.

Summary
ACCGF is a simple, parameter-light, adaptive conditional cooperation strategy: start cooperating, continue while a clear majority cooperates, send short graded punishments when cooperation collapses, track individuals to isolate repeat defectors, and forgive quickly once cooperation resumes. It is designed to sustain high mutual payoffs with reciprocating opponents while avoiding long exploitation and destructive punishment-prisoner dynamics.
'''

description_COOPERATIVE_14 = '''
Name: Adaptive Generous Conditional Cooperator (AGCC)

Goal: Sustain high levels of mutual cooperation when others reciprocate, punish clear exploitation quickly, forgive and try to re‑establish cooperation, and avoid being permanently exploited. Uses only game parameters (n, r, k) and observed history of actions.

High‑level summary
- Be “nice”: start by cooperating to signal cooperation (except in very short games).
- Condition cooperation on recent observed cooperation by others (majority/near‑majority).
- If exploited (you cooperated while many others defected), punish for a short fixed number of rounds.
- Be forgiving: stop punishment as soon as the group’s recent cooperation recovers above threshold.
- If cooperation has broken down, occasionally try a cooperative “restart” (randomized) to seed recovery.
- Never cooperate in the last round (avoid last‑round exploitation).

Parameters used by the strategy (internal; deterministic choices that can be tuned)
- W = min(4, r-1): history window length used to estimate recent group cooperativeness.
- θ = 0.50: baseline threshold for the fraction of other players cooperating (in recent history) required to cooperate.
  - Slight adjustment rule: if k is relatively large (public return strong), be a bit more generous: set θ = max(0.3, 0.50 - 0.10 * floor((k-1)/((n-1)/2))). (This only lowers the threshold by at most 0.1–0.2 when k is large.)
- g = 1: tolerance count (allow one occasional defection from individuals without immediate punishment).
- P = 2: punishment length (defect for P rounds when a clear exploitation is detected).
- p_restart = 0.10: probability of cooperating when the group has been mostly defecting recently (to try to restart cooperation).
(Implementers can tune these; the rules below specify how the parameters are used.)

State variables (maintained from history)
- punish_remain: integer counter of remaining punishment rounds (initially 0).
- last_action_i: your last action (C or D).

Decision rules (per round t)
1. Edge cases
   - If t == r (last round): play D. (No future to incentivize cooperation.)
   - If r == 2: play D in both rounds. (In very short games, reciprocity is weak; this is conservative. If you prefer more cooperative play in tournaments, set this exception aside and treat r==2 like longer games.)

2. If punish_remain > 0:
   - Play D.
   - Decrement punish_remain by 1.
   - (Do not change punish_remain based on others while it is >0; evaluation occurs after punishment rounds.)

3. Otherwise (no active punishment) — compute recent group cooperativeness
   - For each past round s in [max(1, t-W) ... t-1], compute f_s = (number of cooperators among the other n-1 players in round s) / (n-1).
   - Let q = average_s f_s (if t==1 then no past rounds; handle below).
   - Let last_round_others = fraction of other players who cooperated in round t-1 (if t>1).

4. First round (t == 1):
   - If r >= 3: play C (signal niceness).
   - If r == 2: play D (see edge-case above).

5. Normal decision (t not last, punish_remain == 0, t > 1)
   - Exploitation detection (trigger punishment):
     - If in the immediately preceding round (t-1) you played C and last_round_others <= 1/3 (i.e., at least ~2/3 of others defected while you cooperated), then set punish_remain = P and play D this round. (This is a fast, proportional response to clear exploitation.)
   - Otherwise, conditional cooperation:
     - If q >= θ: play C (group has been cooperating recently).
     - If q < θ:
       - If q is very low (e.g., q <= 0.10) — group has mostly defected recently — then:
         - With probability p_restart, play C (attempt to restart cooperation).
         - Otherwise play D.
       - Else (intermediate q): play D.

6. Forgiveness / exit punishment:
   - After punish_remain reaches 0, resumptively follow the normal decision rule. If q >= θ then cooperate immediately (forgiveness).

Pseudocode (concise)
- Initialize punish_remain = 0.
- For each round t = 1..r:
  - if t == r: play D; continue
  - if r == 2: play D each round; continue
  - if punish_remain > 0: play D; punish_remain -= 1; continue
  - if t == 1:
      if r >= 3: play C else play D
      continue
  - compute q = average fraction of other players cooperating over last W rounds
  - compute last_round_others = fraction of other players cooperating in round t-1
  - if last_action_i == C and last_round_others <= 1/3:
      punish_remain = P
      play D
      continue
  - if q >= θ:
      play C
  - else if q <= 0.10:
      with probability p_restart: play C
      else play D
  - else:
      play D

Rationale and properties
- Niceness: starts with cooperation (unless game too short), which allows reciprocal strategies to coordinate on high-payoff mutual cooperation.
- Conditional cooperation: using a recent-window average q makes the strategy robust (it responds to group trends rather than being fooled by single accidental cooperations).
- Quick, limited punishment: immediate but short punishment (P rounds) deters persistent exploitation but avoids long vendettas that reduce group welfare.
- Forgiving: exits punishment as soon as group behavior improves, enabling return to high cooperation.
- Restart mechanism: random occasional cooperation when most defect helps overcome coordination deadlocks (a deterministic always-defect world would never restart).
- Last round defection: neutralizes last-round exploitation; this is standard and avoids giving free payoffs in the final round.
- Parameter sensitivity: θ = 0.50 is a simple majority rule; when public return k is large the small lowering of θ makes the strategy more willing to cooperate because the social gains of cooperation are higher.

Edge cases and special notes
- n = 2 reduces to a standard two‑player repeated PD: this strategy is effectively a generous tit‑for‑tat type (start cooperate, retaliate to clear exploitation by opponent, forgive).
- Very small r (r = 2): the strategy plays D both rounds by default (conservative). If a tournament rewards risk-taking, remove the r == 2 exception.
- No randomization requirement: the only randomized element is p_restart; implementers may replace it with a deterministic periodic restart (e.g., cooperate every 10th round) if randomness is not allowed.
- If you want to be more exploit‑resistant, increase P or increase θ; to be more cooperative, reduce P and θ, increase p_restart.

Why this strategy is suitable for tournaments
- It will achieve high payoffs when paired with reciprocal strategies (it signals cooperation, sustains cooperation when reciprocated).
- It punishes clear exploiters quickly so it avoids being a continual sucker.
- It is forgiving and includes a mechanism to restart cooperation after a breakdown, which helps recover from noise or mistakes.
- It does not rely on any off‑equilibrium coordination beyond observed actions; decisions depend only on parameters and observable history.

Implementation checklist
- Maintain history of actions of all players; compute fractions and rolling averages over W rounds.
- Track your own last action and a punish_remain counter.
- Implement the conditional rules in the given order (punishment overrides, then exploitation detection, then conditional cooperation, then restart randomness).
- Tune W, θ, P, p_restart as desired for the tournament environment.
'''

description_COOPERATIVE_15 = '''
Strategy name: Conditional Cooperate with Graded Punishment and Forgiveness (CGPF)

Intuition (short)
- Start by cooperating and try to stabilize a high level of public contribution.
- Only keep cooperating when the group shows substantial reciprocation (a moving “majority” standard that is relaxed slightly when the public return k is large).
- If the group falls below that standard, punish by defecting for a short, graded number of rounds proportional to how many players defected; but forgive quickly once cooperation resumes.
- In the final round, default to defection unless the previous round showed unanimous cooperation (in which case cooperate to capture mutually available surplus).

This strategy depends only on the game parameters (n, r, k) and the observed history of past rounds’ actions.

Decision rules (natural language)
1. Initialization
   - Round 1: cooperate (C).

2. Threshold for “acceptable” group behavior
   - Compute the marginal per-capita return MPCR = k/n.
   - Set a cooperation threshold fraction f_threshold = 0.5 + 0.25*(1 - MPCR).
     - This lies between 0.5 and 0.75. When MPCR is high (public good is valuable), the threshold stays near 0.5 (we tolerate more noise). When MPCR is low (cooperation is fragile), the threshold becomes stricter (up to 0.75).
   - Let others_needed = ceil((n-1) * f_threshold). This is the minimum number of other players who must have cooperated in the previous round for us to deem the group sufficiently reciprocal.

3. Normal (non-punishing) play in interior rounds (2 ≤ t ≤ r-1)
   - Observe the number of cooperators in the previous round, m_prev (includes all players; you can infer how many others cooperated by subtracting your own past action if needed).
   - Let others_coop_prev = number of cooperators among the other n-1 players in the previous round.
   - If we are not currently in a punishment phase:
     - If others_coop_prev ≥ others_needed: play C (cooperate) this round.
     - Else: enter punishment mode: set punish_length = 1 + ceil((n - m_prev)/2) and play D this round. (Punishment length grows with how many players defected last round.)
   - If we are currently in a punishment phase:
     - Play D; decrement punish_length.
     - If at any point during punishment we observe a full-cooperation round (m_prev == n), immediately cancel punishment and resume cooperating next round (forgiveness on unanimous return).

4. Last round (t = r)
   - Default: play D (defection) because there is no future to sustain punishment.
   - Exception: if the immediately preceding round had unanimous cooperation (m_prev == n), play C to capture the extra mutual payoff (this is safe only when everyone just showed full cooperation).

5. Forgiveness and reset
   - Whenever we see a round with m_prev == n (unanimous cooperation), clear any active punishment and return to normal cooperative stance next round.
   - If punish_length expires without the group restoring cooperation above the threshold, return to normal checks (we may cooperate again if others meet the threshold).

Pseudocode

Inputs: n, r, k
State variables: punish_length ← 0

Helper:
  MPCR ← k / n
  f_threshold ← clamp(0.5 + 0.25*(1 - MPCR), 0.5, 0.75)
  others_needed ← ceil((n-1) * f_threshold)

For each round t = 1..r:
  observe history of previous rounds; let m_prev = #cooperators in round t-1 (if t==1, no m_prev)
  others_coop_prev ← if t==1 then undefined else m_prev - (your action in t-1 if you know it)

  if t == 1:
    play C
    continue

  if t == r:
    if m_prev == n:
      play C
    else:
      play D
    continue

  # interior rounds
  if punish_length > 0:
    # we are punishing
    if m_prev == n:
      # immediate forgiveness on unanimous return
      punish_length ← 0
      play C
    else:
      play D
      punish_length ← punish_length - 1
  else:
    # not currently punishing
    if others_coop_prev ≥ others_needed:
      play C
    else:
      # trigger graded punishment
      punish_length ← 1 + ceil((n - m_prev) / 2)  # includes current round; decremented next round
      play D

Why these choices (brief)
- Start cooperating to signal willingness to achieve the socially efficient outcome.
- A majority/threshold rule is simple, easy to observe, and robust to various opponent types; the threshold is tightened when k is small (cooperation is less profitable) and loosened when k is large (cooperation is more valuable).
- Graded punishment (punish length increases with number of defectors) communicates a proportionate response: severe breaches cause longer punishment, small lapses cause short punishment. This discourages exploitation but avoids the harshness of infinite “grim trigger.”
- Fast forgiveness (return to cooperation when the group shows clear restoration) prevents lock-in cycles and allows the strategy to regain mutual cooperation quickly.
- Last-round defection is the safe default in finite-horizon games; the unanimous-exception allows capturing mutually beneficial surplus if everyone signaled cooperation in the penultimate round.

Robustness properties
- Against unconditional defectors: the strategy defects often enough (punishes) to avoid being exploited repeatedly, limiting loss relative to pure cooperation.
- Against conditional cooperators: it will coordinate on mutual cooperation if they reciprocate a majority of the time.
- Against noisy or mixed strategies: graded punishments and forgiveness avoid endless retaliation loops and adapt to the observed cooperation level.
- Uses only observable history and parameters (n, r, k); no out-of-band coordination required.

Tuning notes (optional)
- The specific form of f_threshold and the punish_length formula are conservative, simple defaults that work across a wide parameter range. An implementer can tune f_threshold (closer to 0.5 for more forgiving play, closer to 0.75 for stricter) or scale punishment lengths if a tournament environment demands more or less aggressiveness.
'''

description_COOPERATIVE_16 = '''
Strategy name: Adaptive Threshold with Forgiving Punishment (ATFP)

Short summary
- Start by cooperating to seed cooperation.
- In each subsequent round cooperate if the previous round showed sufficient group cooperation; otherwise punish by defecting for a short, proportional period.
- After punishment, forgive quickly and try to re-establish cooperation (with a small probabilistic “probe” if group remains uncooperative).
- Always defect in the final round (no future to enforce cooperation), and gradually reduce cooperation during a short endgame window to avoid exploitation from endgame defections.
- Parameters are explicit and depend only on n, r, k and observed history; they can be tuned if desired.

Intuition / design goals
- Be cooperative: seed and maintain mutual cooperation when others reciprocate.
- Be robust: punish defections to deter exploitation, but keep punishments short so the group can recover from mistakes and avoid long mutually harmful wars.
- Be adaptive: the cooperation threshold and punishment severity respond to observed group behavior and to how valuable the public good is (k).
- Handle last-round endgame rationality while preserving cooperation earlier.

Internal parameters (default values and motivations)
- τ (cooperation threshold): current-round cooperators fraction in previous round must be ≥ τ to cooperate this round. Default: τ = 0.5 adjusted downward when the public good is relatively large:
  - τ = clamp(0.5 - 0.2*(k - n/2)/(n/2), 0.2, 0.6)
  - Rationale: when k is large (public good yields big returns), be a bit more tolerant (lower τ) because cooperation is more valuable; keep τ in [0.2,0.6].
- P_max (max punishment length): default 3 rounds (caps runaway punishment).
- Punishment length formula P = min(P_max, ceil((τ - f) * n)) where f is fraction of cooperators last round. So bigger shortfalls yield longer punishment, but never beyond P_max.
- Forgiveness probability γ: when a punishment finishes, cooperate next round with probability γ = 0.2 (gives chance to re-start cooperation).
- Probe probability ε: each round, with small probability ε = 0.02 take the opposite action as a probe (cooperate if you otherwise would defect, defect if you otherwise would cooperate). This lets you detect if others have shifted strategies.
- Endgame window E: last E rounds we behave more conservatively. Set E = min(3, max(1, floor(0.05 * r))). In that window we require stronger evidence of cooperation (e.g., full cooperation last round) to cooperate, and in the final round defect for sure.

State variables tracked from history
- t: current round index (1..r)
- history of actions per round (for all players); in practice we need only previous round’s count of cooperators m_{t-1}.
- punish_count: number of remaining punishment rounds (initially 0).

Decision rules (natural language)
1. First round (t = 1):
   - Cooperate (C). This seeds cooperation and signals willingness to cooperate.

2. Last round (t = r):
   - Defect (D). There is no future to enforce cooperation.

3. Endgame window (t in r - E + 1 ... r - 1):
   - Be conservative. Cooperate only if the previous round had near-universal cooperation (e.g., m_{t-1} = n). Otherwise defect (or follow an active punishment if one is ongoing).
   - This avoids being exploited by strategies that switch to defection near the end.

4. Normal rounds (1 < t < r - E + 1):
   - Observe f = m_{t-1} / n (fraction of players who cooperated last round; m includes yourself if you cooperated).
   - If punish_count > 0:
       - Play D, decrement punish_count by 1, continue.
   - Else:
       - If f >= τ: play C (reward cooperation).
       - Else (f < τ): enter punishment: set punish_count = P (computed below) and play D now (first punishment round).
   - After punishment_count reaches 0 we do not remain forever defecting: cooperate next round with probability γ (forgiveness), otherwise follow the rules above (so forgiveness may re-start cooperation).

5. Probing:
   - Independently each round, with small probability ε flip the chosen action to perform a probe. Probes detect whether others have changed to cooperation and allow quick recovery.

Punishment length formula
- P = min(P_max, ceil((τ - f) * n))
  - If f is just slightly below τ, punishment is short (often 1). If f is far below τ, punishment is longer up to P_max.

Pseudocode (concise)
Inputs: n, r, k
Internal/defaults: τ as defined above; P_max=3; γ=0.2; ε=0.02; E = min(3, max(1, floor(0.05*r)))
State: punish_count ← 0

On round t (1..r):
  if t == 1:
    action ← C
    return possibly_probe(action)
  if t == r:
    action ← D
    return action  // final round, no probe needed
  // determine fraction f from previous round
  m_prev ← number of players who played C in round t-1
  f ← m_prev / n

  // endgame conservative behavior
  if t > r - E:
    if punish_count > 0:
      action ← D
      punish_count ← punish_count - 1
      return possibly_probe(action)
    if m_prev == n:  // previous round was unanimous cooperation
      action ← C
    else:
      action ← D
    return possibly_probe(action)

  // normal rounds
  if punish_count > 0:
    action ← D
    punish_count ← punish_count - 1
    return possibly_probe(action)

  if f >= τ:
    // reward cooperation
    // after a period of punishment we still probabilistically forgive:
    action ← C with probability 1
  else:
    // initiate proportional punishment
    P ← min(P_max, ceil((τ - f) * n))
    punish_count ← P - 1   // we use one punishment round now, remaining are P-1
    action ← D

  return possibly_probe(action)

Function possibly_probe(action):
  with probability ε:
    return flip(action)  // flip C↔D for a probe
  else:
    return action

Notes and rationale / why this is cooperative and robust
- Cooperates initially and rewards groups that show adequate cooperation in the previous round, which encourages sustained mutual cooperation.
- Punishments are proportional and short (bounded by P_max). That deters persistent defectors while avoiding long, destructive retaliatory cycles that reduce overall payoff.
- Forgiveness and a small probing probability allow fast recovery after accidental defections or when co-players have changed strategies.
- The threshold τ is lowered when k is large (public good is more valuable), making the strategy more tolerant when cooperation gives bigger group payoff.
- Endgame behavior acknowledges finite-horizon incentives (last round defection) and reduces being exploited by those who defect near the end, while still trying to preserve cooperation before the endgame.
- The strategy depends only on the game parameters (n, r, k) and observed history (counts of cooperators), not on assumptions about others’ internal rules.

Parameter tuning guidance
- Lower τ → more forgiving, but more vulnerable to exploitation. Raise τ → stricter, faster to punish, safer versus free-riders but risks breaking cooperative equilibria.
- Increase γ (forgiveness) and ε (probing) to recover cooperation faster if the environment has many near-cooperative agents or noise.
- Increase P_max to make punishments stronger if many opponents are persistent defectors; decrease it if retaliation wars are common.
- For very short r (small number of rounds), set E larger (closer to r) so endgame effects are considered early; for very long r, E can be small.

Edge cases
- n = 2: All formulas still apply (fraction f ∈ {0, 0.5, 1}). τ defaults yield reasonable tit-for-tat-like behavior.
- r small (e.g., r=2 or 3): E will be at least 1, so the strategy will cooperate round 1 and defect round r; middle rounds follow the above (but with short horizon punishment).
- If you cannot observe others’ actions exactly (contrary to spec), replace f with estimated cooperators; if noisy observations exist, increase γ and ε to make the strategy forgiving.

Implementation notes
- The strategy only needs the previous-round count of cooperators (m_prev) and an internal punish_count. It is easy to implement in a tournament.
- Randomness (ε, γ) should be pseudo-random with a seeded RNG if deterministic reproducibility is needed.

Conclusion
ATFP is a simple, parameterized strategy that seeds cooperation, enforces it with short proportional punishments, forgives to recover cooperation, adapts tolerance according to k, and protects itself against last-round exploitation. It is designed to do well in mixed populations of cooperative and selfish opponents without relying on communication or coordinated schedules.
'''

description_COOPERATIVE_17 = '''
Strategy name: Adaptive Conditional Cooperator with Proportional Punishment (ACCPP)

Goal (cooperative mindset)
- Try to sustain near-universal cooperation early and in the middle of the game.
- Quickly and proportionally punish clear free-riding to make one-shot defection unattractive.
- Forgive after a limited, predictable punishment so cooperation can be re‑established.
- Be tolerant of a small number of occasional defections (noise or mistakes).
- Avoid being exploited in the endgame by switching to defection when remaining rounds make punishment ineffective.

Parameters (computed from game inputs n, k, r)
- L_deterrent = minimal integer L >= 1 such that L * (k - 1) > (n - k)/n.
  - Derivation: the one-shot gain from defecting while everyone else cooperates is G = (n - k)/n. A punishment that imposes (k - 1) loss per round for L rounds must satisfy L*(k - 1) > G to deter that deviation.
  - Formula in code: L_deterrent = max(1, ceil(((n - k)/n) / (k - 1)))
- tolerance (number of other players' defections we tolerate before punishing) = 1 if n >= 4, = 0 if n ∈ {2,3}.
  - This gives robustness to single mistakes in larger groups while still reacting to systematic free-riding.
- window W = 1 (we use the most recent round's observed contributions to decide). Optionally this can be extended to a short window (e.g., W = min(3, r)) if implementer prefers smoothing.
- Endgame horizon H = L_deterrent. When remaining rounds <= H the strategy switches to conservative play (see "Last rounds" below).

State variables maintained
- mode ∈ {Cooperate, Punish}
- punish_remaining: integer ≥ 0 (rounds left in punishment)
- last_rounds_history: record of previous rounds' contributions (for windowing if desired)

Decision rules (high level)
1. First round (t = 1): Cooperate (signal intent to cooperate).
2. Last rounds (if remaining rounds including current round <= H): Defect.
   - Rationale: when there are too few rounds left, punishment cannot credibly deter one-shot defection; switch to safe action (defect).
3. If punish_remaining > 0 (we are in punishment mode):
   - Play D (Defect).
   - After playing D, decrement punish_remaining by 1.
   - If punish_remaining becomes 0, set mode = Cooperate (forgive and try to re-establish cooperation).
4. If not currently punishing (mode = Cooperate):
   - Observe the most recent round (or the last W rounds aggregated). Let d = number of other players (j ≠ i) who played D in that observation window (count distinct defecting players per most recent round if W = 1; if W > 1 use recent average or majority).
   - If d ≤ tolerance: Play C (Cooperate).
   - If d > tolerance: Start a punishment:
     - Compute multiplier M = ceil(d / (tolerance + 1)). (More defectors => longer punishment.)
     - Set punish_remaining = M * L_deterrent.
     - Set mode = Punish.
     - Immediately play D this round (first round of punishment).
     - (During punishment, if subsequent rounds reveal even more defectors among others, extend punish_remaining proportionally by the same rule.)
5. If none of the above (normal cooperative state): Play C.

Pseudocode (concise)
- Inputs: n, k, r, current_round t (1..r), history H (every player's past actions)
- Precompute:
  - L_deterrent = max(1, ceil(((n - k)/n) / (k - 1)))
  - tolerance = 1 if n >= 4 else 0
  - H_endgame = L_deterrent
- State: mode ← Cooperate; punish_remaining ← 0
- Each round:
  - remaining = r - t + 1
  - if t == 1: action ← C; continue
  - if remaining <= H_endgame: action ← D; (do not change mode/punish state)
  - else if punish_remaining > 0:
      action ← D
      punish_remaining ← punish_remaining - 1
      if punish_remaining == 0: mode ← Cooperate
  - else:  // mode == Cooperate
      // Observe last round (or W-round summary). Let d = # of other players who defected.
      d ← count_defectors_among_others_in_most_recent_round(H)
      if d <= tolerance:
         action ← C
      else:
         M ← ceil(d / (tolerance + 1))
         punish_remaining ← M * L_deterrent - 1   // minus 1 because we count current round as first punished round
         mode ← Punish
         action ← D

Notes and refinements
- Proportional punishment: Punishment length increases with how many others defected. A lone defector triggers minimal punishment; an organized mass-defection triggers longer punishment.
- Forgiveness: After a finite punish_remaining reaches zero we return to cooperation. This avoids permanent breakdown from a single lapse.
- Extension for noisy environments: Increase tolerance to 2 or use window W = 3 with threshold on average defection rate to reduce sensitivity to random mistakes.
- Tracking during punishment: If during punishment others continue to defect (or defect more), extend punish_remaining by the same M * L_deterrent rule (so punishment can escalate while group is breaking down).
- Last-round behavior: We play D in final H rounds because when remaining rounds ≤ H the future cost of defection is too small to credibly deter a one-shot gain. H is chosen from L_deterrent; it is the minimal number of remaining rounds needed for a punishment of length L_deterrent to be effective.
- Security vs. cooperation trade-off: The strategy is intentionally cooperative-looking and generous at the start to form mutual cooperation with like-minded opponents, but it imposes short, predictable punishments that are just strong enough (by the L_deterrent calculation) to remove incentives for profitable unilateral deviation when punishments can be carried out. That makes it robust against opportunistic strategies while forgiving mistakes.

Why this is robust and adaptive
- Starts with cooperation to attract cooperative opponents.
- Uses local, observable statistics (who defected last round) and tolerates one mistake so it is not brittle.
- Computes minimal punishment length from only game parameters n and k. The punishment is tailored to remove a defector’s one-shot benefit; it is not arbitrarily long (so it’s not needlessly retaliatory) and thus allows rapid return to cooperation.
- Punishment scales with the number of defectors, so it responds appropriately to both individual and group deviations.
- Forgiveness ensures the strategy does not lock into mutual defection permanently and can re-establish cooperation if others return to cooperating.
- Edge-case handling (first and last rounds) is explicit so behavior is consistent and predictable for opponents and for tournament implementations.

Example (n=6, k=2)
- Compute L_deterrent: G = (6-2)/6 = 2/3; k-1 = 1; L_deterrent = ceil(2/3 / 1) = 1.
- tolerance = 1.
- So a single defection among others triggers a 1-round punishment (we defect that round), then we forgive. Two or more defectors among others trigger longer punishments equal to M*1 rounds (M = ceil(d/2)).
- Cooperate in round 1; if at any point many defect, respond quickly but briefly; in final 1 round (remaining <=1) we defect.

Implementation notes
- The algorithm requires only n,k,r and the full action history (which the game provides).
- All thresholds and punishment lengths are deterministic and computed from n,k,r (and the observed d), so this is fully implementable without extra tuning.
- Optional: implement a small randomization in forgiveness (e.g., with small probability p forgive earlier) to discourage steady exploitation by highly adversarial strategies — but the deterministic version above is sufficient and easier to analyze.

Summary
- ACCPP is a cooperative-first, tolerant, proportionate-punishment, forgiving strategy.
- It is parameterized by n,k,r only (with history dependence), deters profitable one-shot deviations using minimal punishments, scales punishment to the size of defections, forgives to restore cooperation, and avoids being exploited in the endgame.
'''

description_COOPERATIVE_18 = '''
Name: Generous Conditional Cooperator with Limited Punishment (GCC-LP)

Short description
- Start by trying to cooperate to build cooperation.
- Cooperate as long as a clear majority (or everyone) cooperated in the immediately preceding round.
- If cooperation falls below that threshold, apply a short, proportional punishment (defect for a small number of rounds) and then return to cooperating if cooperation recovers.
- Always defect in the final round. Use short punishments (not permanent) so cooperation can be restored — this reduces the incentive to exploit and is robust to a wide range of opponent behavior.

Rationale
- Cooperation is fragile in a finite public-goods game, so the strategy is generous (to foster cooperation) but defensive (to limit exploitation).
- Majority threshold focuses cooperation effort where most players are cooperating; proportional, limited punishments deter persistent free-riders but permit recovery.
- Forgiveness prevents permanent war of attrition (which would give low payoffs against many strategies).
- Last-round defection is unavoidable in finite horizon games; we avoid futile retaliation there.

Parameters derived from game
- n (number of players) and r (rounds) are given.
- Threshold T = ceil(n/2) (majority). This is simple, parameter-free, and interpretable for all n.
- Max punishment length P_max = 3 (small, fixed). Keeps punishments short and limits endgame unwinding.
- Punishment length after an underthreshold round: P = min(P_max, max(1, ceil((n - S_prev)/2))) where S_prev is number of cooperators observed in previous round (including self). This makes punishment longer when many defected, shorter when only a few defected.

Decision rules (natural language)
1. First round (t = 1)
   - Play C (cooperate).

2. Any round t = r (the final round)
   - Play D (defect). (No future to enforce cooperation.)

3. For rounds 1 < t < r:
   - If currently in an active punishment phase (you are obliged to punish for some remaining rounds), play D and reduce remaining punishment counter by 1.
   - Otherwise inspect the immediate past round (t-1) and let S_prev be the total number of cooperators in that round (number from 0 to n).
     - If S_prev == n (everyone cooperated last round), play C.
     - Else if S_prev >= T (majority cooperated last round), play C.
     - Else (cooperation fell below threshold):
       - Compute P = min(P_max, max(1, ceil((n - S_prev)/2))).
       - Enter a punishment phase of length P (set punishment_remaining = P - 1, because you will punish this round immediately).
       - Play D for this (punishing) round.

4. After a punishment phase expires:
   - If the last observed round (the one just played by all players) has S_last >= T, resume cooperating.
   - If cooperation is still below threshold, extend punishment using the same rule (recompute P from the latest S_last and set punishment_remaining accordingly) — so punishments are renewed by repeated underthreshold rounds but always remain short.

5. Special handling when r is very small:
   - If r = 2: play C in round 1, D in round 2 (consistent with general rules: cooperate to try to establish cooperation, but final round is D).
   - If r = 3: follow the general rules but note punishments will be truncated if remaining rounds are fewer than required — punish only within remaining rounds.

Pseudocode

Initialize:
  punishment_remaining = 0

For each round t from 1 to r:
  if t == r:
    action = D
  else if punishment_remaining > 0:
    action = D
    punishment_remaining -= 1
  else if t == 1:
    action = C
  else:
    S_prev = number_of_cooperators_in_round(t-1)  # observed
    if S_prev == n or S_prev >= T:
      action = C
    else:
      # trigger proportional short punishment
      P = min(P_max, max(1, ceil((n - S_prev)/2)))
      action = D
      punishment_remaining = max(0, P - 1)  # we punish this round, so remaining = P-1

  play action

After round t completes, update history and loop.

Notes and clarifications
- The threshold is based only on the immediately previous round (not long memory) to keep adaptation fast and simple. This makes the strategy responsive to dramatic changes without long grudges.
- Punishment length is proportional to the number of defectors in the previous round: more defectors → longer (but capped) punishment. That discourages mass defection while not locking into permanent retaliation.
- Forgiveness: punishments are finite and only re-triggered by subsequent underthreshold rounds — this allows recovery and cooperative cycles.
- Always defect in the final round because no future rounds exist for enforcement.
- The constants (majority threshold, P_max = 3, proportional formula) are conservative but work across values of n and k. Tournament implementers can tune P_max (e.g., 2–5) if they want stronger/weaker punishment.

Why this is cooperative and robust
- Cooperative: It starts by cooperating and continues to cooperate when the group shows cooperative behavior (everyone or at least a majority). This produces high group payoffs when reciprocated.
- Robust: It punishes defectors to deter exploitation, but the punishments are short and proportional, so the strategy recovers when cooperation resumes. It does not rely on pre-commitment, communication, or common conventions across opponents.
- Adaptive: It reacts to observed group behavior (S_prev), scales punishment to severity, and stops punishing when cooperation returns.
- Fits a tournament environment: tolerant toward occasional defections (noise or opportunistic plays) but able to punish consistent freeloading, and avoids endless mutual destruction in later rounds.

Optional small refinements
- Use a probabilistic generosity: when S_prev == T (just a majority), cooperate with high probability (e.g., 0.9) to reduce exploitation risk from oscillatory players. (This introduces randomness; use only if the tournament supports stochastic strategies.)
- Track individual players who always defect and exclude them from cooperation (i.e., reduce effective S_prev by ignoring never-cooperators). This increases robustness against a few persistent free-riders but requires per-player tracking.

This strategy balances being cooperative (to achieve high collective payoffs) and being defensive (to limit exploitation), is simple to implement from the game parameters and observed history, and is adaptive to a wide range of opponent behaviors.
'''

description_COOPERATIVE_19 = '''
Name: Adaptive Majority-Triggered Conditional Cooperator (AMTCC)

High-level idea
- Default to cooperation (to signal and try to build mutual cooperation).
- Cooperate only when there is credible evidence that a sufficient fraction of players have been cooperating recently.
- If that evidence disappears, impose a finite, proportional punishment (defection) to deter free-riding.
- Include periodic small-probability forgiveness/tests so the strategy can re-establish cooperation after noise or mistakes.
- Always defect in the final round (no future to enforce reciprocity).

This strategy depends only on: game parameters (n, r, k) and the complete history of past actions (everyone’s contributions each round).

Intuition / why it is cooperative and robust
- It tries to sustain mutually beneficial full/coordinated cooperation when most others reciprocate.
- It protects itself against sustained exploitation by defectors via finite punishments that reduce exploiters’ short-run advantage while allowing restoration (forgiveness).
- The thresholds and punishment lengths are adaptive functions of k and r so behavior is tuned to how valuable cooperation is and how many rounds remain.
- Small random forgiveness reduces lock-in on mutual defection caused by mistakes or adversarial exploitation.

Core parameters (computed from n, r, k)
- w = min(10, r-1)  // look-back window for recent behavior
- alpha = clamp(0.6 + 0.4*(1 - k/n), 0.5, 0.95)
  - (If k/n is small => cooperating is costly, so require a higher observed cooperation rate; if k/n is closer to 1 => require less strict evidence.)
- M = ceil(alpha * n)  // minimum number of players (including self) we want to see cooperating on average
- max_punish = min(5, max(1, r/4)) (rounded down)  // cap on punishment length, scaled to remaining horizon
- eps_forgive = min(0.08, 4.0/r)  // small prob of forgiveness/ testing; shrinks for very long games

State maintained
- punishment_counter (integer ≥ 0, initially 0): rounds remaining in an ongoing punishment phase
- punish_escalation (integer ≥ 0, initially 0): increases a bit if repeated offenses occur (used to increase punishment length modestly)

Decision rules (natural language)
1. Final round:
   - If current round t == r: play D (defect).

2. First round:
   - If t == 1: play C (start cooperatively).

3. If currently punishing:
   - If punishment_counter > 0: play D and decrement punishment_counter by 1.
   - After punishment_counter reaches zero, return to normal assessment next round.

4. Normal assessment (not in punishment, 1 < t < r):
   - Compute for each player j (including self) their cooperation rate over the last w rounds (if fewer than w rounds exist, use all available past rounds).
   - Count coop_count = number of players whose cooperation rate ≥ alpha.
   - If coop_count ≥ M: play C (cooperate) this round.
   - If coop_count < M: begin a punishment phase:
     - Set punishment_length = min(max_punish, 1 + (M - coop_count) + punish_escalation).
     - Set punishment_counter = punishment_length and play D this round.
     - Increment punish_escalation by 1 (bounded growth) so repeated norm violations lead to modestly longer punishments.
   - Additionally: on any decision (cooperate or defect), with independent probability eps_forgive override the chosen action and play C (a small random forgiveness/test). If you use that forgiveness and the group responds by higher cooperation subsequently, reset punish_escalation to 0.

Edge cases and clarifications
- Window shorter than available history: when t-1 < w, use all t-1 past rounds.
- If r is small (e.g., r = 2 or 3), parameters adapt: w ≤ r-1, max_punish small; behaviour becomes cautious (cooperate only when clear signal exists).
- If many rounds remain, punishments are effective; if near the last rounds punishment lengths are automatically short (max_punish bounded by remaining horizon).
- The strategy never punishes forever (no grim trigger). Punishment is finite and can escalate moderately on repeated offenses but also can be reset by a forgiveness-test that succeeds.
- All decisions use only the observed history of contributions (who played C vs D each past round) and the parameters n, r, k.

Pseudocode
Inputs: n, r, k, history (list of past rounds where each round is a vector of 0/1 contributions for players 1..n); my_index

Initialize:
  w = min(10, r-1)
  alpha = clamp(0.6 + 0.4*(1 - k/n), 0.5, 0.95)
  M = ceil(alpha * n)
  max_punish = min(5, max(1, floor(r/4)))
  eps_forgive = min(0.08, 4.0/r)
  punishment_counter = 0
  punish_escalation = 0

Function decide(t, history):
  if t == r:
    return D

  // small randomness for forgiveness/testing
  forgive_roll = random_uniform(0,1) < eps_forgive

  if t == 1:
    if forgive_roll:
      return C
    else:
      return C

  if punishment_counter > 0:
    punishment_counter -= 1
    if forgive_roll:
      return C
    else:
      return D

  // Normal assessment
  consider_rounds = last min(w, t-1) rounds from history
  for each player j in 1..n:
    coop_rate[j] = (sum of j's contributions in consider_rounds) / len(consider_rounds)

  coop_count = count of j with coop_rate[j] >= alpha

  if coop_count >= M:
    if forgive_roll:
      // small chance to test generosity (still C)
      punish_escalation = max(0, punish_escalation - 1)
      return C
    else:
      punish_escalation = max(0, punish_escalation - 1)
      return C
  else:
    // start finite punishment
    punishment_length = min(max_punish, 1 + (M - coop_count) + punish_escalation)
    punishment_counter = punishment_length - 1  // we are defecting this round, so remaining is length-1
    punish_escalation = punish_escalation + 1
    if forgive_roll:
      // sometimes we test instead of immediate full defection
      return C
    else:
      return D

Why this is a practical choice for tournaments
- It is cooperative: it actively tries to achieve mutual cooperation when most other players reciprocate.
- It is robust: it defends against persistent defectors via measurable group-based punishments and escalation, so defectors do not permanently exploit you.
- It is adaptive: thresholds depend on k (how attractive cooperation is) and on recent history (windowed rates). Punishment lengths scale with both the number of apparent defectors and remaining horizon.
- It is forgiving: finite punishments and small-probability tests allow recovery from mistakes and opportunistic collapses.
- It is implementable from available information only (observed contributions each round and the known n, r, k).

Tuning notes (implementer can adjust)
- w: larger windows make the strategy slower to react but more stable; smaller windows make it more responsive.
- alpha formula: the given mapping from k/n to alpha is conservative; an implementer may tune constants if they expect different opponent populations.
- eps_forgive and max_punish: increase forgiveness if there is expected noise; increase max_punish if very long games and stronger deterrence is needed.

Summary
AMTCC starts cooperative, continues cooperating when a sufficiently large fraction of players have been cooperating recently (threshold adaptively set by k/n), and punishes with finite, escalating defection when that signal disappears. Built-in randomized forgiveness lets cooperation be re-established. It is simple, uses only publicly observed history and parameters, and balances pro-sociality with protection against exploitation.
'''

description_COOPERATIVE_20 = '''
Strategy name: Conditional Cooperative with Proportional Punishment and Forgiveness (CC-PPF)

High-level idea
- Start by signaling cooperation.
- Cooperate as long as the recent observed cooperation level among the other players stays above a threshold.
- If the group falls below the threshold, punish proportionally to the size of the deviation (short, targeted defection streaks), then forgive and return to cooperation once cooperation resumes.
- Tolerate small occasional defections (do not overreact), and probe occasionally if the group has collapsed to see whether cooperation is still possible.
- In the last few rounds behave conservatively to avoid predictable exploitation (cooperate in final rounds only if recent history shows near-universal cooperation).

This strategy depends only on the game parameters (n, r, k) and the observed history (who contributed in each prior round) and uses only simple summary statistics of history (recent cooperation rates).

Parameters (computed from game parameters; these are defaults you can tune if desired)
- Window length L = min(5, r - 1). Use up to last 5 rounds (or fewer if r small).
- Tolerance tau = 0.25 (tolerate up to 25% defectors in a round). Threshold T = 1 - tau = 0.75 (i.e., require ≥75% of others cooperating).
- Maximum punishment length P_max = min(3, r). Punish at most 3 rounds.
- Endgame length E = min(2, max(1, floor(r/10))). In the final E rounds act conservatively.
- Probe probability p_probe = 0.05 (when the group has collapsed, occasionally cooperate to test).
These are computed from n and r only (k does not change the logic but affects incentives; you may increase strictness when k is low).

State maintained
- punish_until_round (initially 0): if current round ≤ punish_until_round, play D (punishing).
- (history is given externally: contributions of all players in each prior round)

Decision rules (natural language)
1. First round (t = 1)
   - Play C (cooperate) to signal willingness to sustain cooperation.

2. Final rounds (t > r - E)
   - Conservatism near the end reduces exploitation from known finite horizon:
     - For the final round t = r: cooperate only if every player cooperated in round r − 1; otherwise defect.
     - For the other last-E rounds: cooperate only if the fraction of other players who cooperated in round t − 1 is ≥ T; otherwise defect.

3. If currently in punishment (t ≤ punish_until_round)
   - Play D (continue punishment).
   - After punish_until_round is passed, fall back to the normal decision logic.

4. Normal rounds (1 < t ≤ r − E and not in punishment)
   - Compute last_round_fraction_others = fraction of the other n − 1 players who cooperated in round t − 1.
   - Compute recent_fraction_others = average, over the last L rounds, of the fraction of others cooperating in each of those rounds.
   - If last_round_fraction_others ≥ T:
       - Play C (cooperate).
     (This enforces contingent cooperation: if most others cooperated last round, continue cooperating.)
   - Else (last_round_fraction_others < T): a deviation occurred — respond as follows:
       - def_count = number of other players who defected in the previous round = (n − 1) − (# cooperators among others in t − 1).
       - Set punishment length P = min(P_max, max(1, def_count)). (Punish at least one round, up to P_max, proportional to the number of defectors.)
       - Set punish_until_round = t + P − 1.
       - Play D (start/continue punishment).
   - Exception (probing to recover cooperation): if recent_fraction_others < 0.20 (the group has largely collapsed), then with small probability p_probe play C in this round to test whether others will respond; otherwise play D. (This prevents permanent deadlock against strategies that can be persuaded back to cooperation but avoids frequent exploitation.)

5. Forgiveness / Recovery
   - If punishment completes (t > punish_until_round) and recent_fraction_others ≥ T, return to cooperation automatically.
   - If, after punishment, others resume cooperation for L rounds, continue cooperating indefinitely (subject to the same monitoring).

Rationale and robustness
- Start-cooperate: gives cooperative strategies a chance to coordinate and achieve higher payoffs.
- Conditional cooperation on recent majority cooperation (T) encourages mutual cooperation without naive unconditional cooperation.
- Proportional punishment: punishes deviations promptly and non-permanently; keeps punishment short so the group can recover cooperation quickly and avoids long mutual losses due to overly harsh “grim” strategies.
- Tolerance (tau): avoids frequent oscillations and needless retaliation for small, possibly noisy deviations.
- Probing: prevents becoming permanently locked into defection when most opponents are willing to cooperate but a few rounds of defection started a downward spiral.
- Endgame conservatism: avoids being exploited in the known final rounds while still allowing last-round mutual cooperation if the history strongly supports it.
- The strategy is adaptive (responds to measured cooperation levels) and does not rely on side channels, conventions, or any assumptions about fairness beyond observed actions.

Pseudocode (concise)

Initialize: punish_until = 0
Let L = min(5, r-1), T = 0.75, P_max = min(3, r), E = min(2, max(1,floor(r/10))), p_probe = 0.05

For each round t = 1..r:
  if t == 1:
    play C
    continue
  if t <= punish_until:
    play D
    continue
  if t > r - E:   // endgame
    if t == r:
      if all players cooperated in round r-1 then play C else play D
    else:
      if (cooperators among others in round t-1) / (n-1) >= T then play C else play D
    continue
  // normal rounds
  last_frac = (cooperators among others in round t-1) / (n-1)
  recent_frac = average over last L rounds of ((cooperators among others that round)/(n-1))
  if recent_frac < 0.20:
    with probability p_probe play C else play D
    continue
  if last_frac >= T:
    play C
  else:
    def_count = (n-1) - (cooperators among others in round t-1)
    P = min(P_max, max(1, def_count))
    punish_until = t + P - 1
    play D

Notes for implementation
- The strategy uses only observed actions and simple counters; it is efficient to implement.
- The numerical defaults (T = 0.75, tau = 0.25, P_max = 3, etc.) are intentionally conservative and robust; you may tune them for a particular tournament (e.g., increase tolerance when n is large or increase punishment when k is small).
- The probing step uses randomness; if randomness is disallowed, replace “with probability p_probe” by “cooperate deterministically every M rounds” (e.g., every 7th round) when recent_frac is low.

Closing comment
This strategy is cooperative in spirit: it opens with cooperation, rewards cooperation, punishes violations proportionally and briefly, forgiving and probing to recover cooperation. It is robust against pure defectors (it stops being exploited), resilient to noisy or occasional deviations (tolerance and short punishment), and practical for tournaments with unknown opponent mixes.
'''

description_COOPERATIVE_21 = '''
Strategy name: Responsive Proportional Reciprocity (RPR)

High-level idea
- Start by signalling cooperation. Cooperate as long as recent group behaviour shows strong cooperation. If the group drops cooperation, respond with a proportional, temporary punishment (defection) whose length scales with how many players defected. After the punishment you probe (cooperate once) to allow recovery; if cooperation returns, resume. Always defect in the final round (no future to enforce reciprocity). The thresholds and punishment severity are derived from (n,k) so the rule is adaptive to the game’s public-good return.

Why this is cooperative and robust
- Cooperative: it begins by cooperating and resumes cooperation after successful recovery, so when others are willing to cooperate it fosters high collective payoffs.
- Robust: it refuses to be exploited by small numbers of persistent defectors because punishments are proportional to the size of the deviation; it is forgiving (one-cooperate probe after punishment) so it recovers from transient defections or noise.
- Adaptive: the cooperation threshold depends on k (the public-good multiplier), so the strategy is stricter when the public-good is less profitable and more lenient when the public-good return is large.

Decision rules (natural language)
1. First round (t = 1): Cooperate.

2. Final round (t = r): Defect.

3. For any intermediate round t (2 ≤ t ≤ r-1):
   - Let s_prev be the number of cooperators observed in the previous round (round t-1).
   - Let d = n − s_prev be the number of defectors in the previous round.
   - Maintain a counter punishment_remaining (initially 0).
   - If punishment_remaining > 0:
       - Defect this round and decrement punishment_remaining by 1.
     Else (no active punishment):
       - Compute a cooperation threshold:
         threshold_fraction = 1 − k/(2n)
         threshold_count = ceil(n * threshold_fraction)  (equivalently ceil(n − k/2))
       - If s_prev == n (everyone cooperated last round): Cooperate.
       - Else if s_prev ≥ threshold_count: Cooperate (majority-style leniency tuned by k).
       - Else (s_prev < threshold_count): Enter punishment:
           - Set punishment_remaining = min(r − t, d)  (punish for up to the remaining rounds, but no more than number of defectors).
           - Defect this round.

4. After the punishment countdown reaches 0: play one "probe" round—cooperate once to see if others return to cooperation. If the probe elicits cooperation by others above the threshold, resume normal cooperative mode. If not, the same punishment rule re-triggers.

Optional small-probability generosity (to escape pathological cycles)
- Optionally, to avoid long alternating cycles in deterministic environments, you can add a small-probability generosity: during punishment you cooperate with small probability g (e.g., g = 0.05). This is optional; tournaments without randomness can also use the deterministic rule above.

Pseudocode (structured)
- Inputs: n, k, r
- State: punishment_remaining ← 0
- threshold_fraction ← 1 − k/(2n)
- threshold_count ← ceil(n * threshold_fraction)

For each round t = 1..r:
  if t == 1:
    action ← Cooperate
  else if t == r:
    action ← Defect
  else:
    observe s_prev ← number of cooperators in round t−1
    d ← n − s_prev

    if punishment_remaining > 0:
      (optional:) with prob g cooperate (generosity) else:
      action ← Defect
      punishment_remaining ← punishment_remaining − 1
    else:
      if s_prev == n:
        action ← Cooperate
      else if s_prev ≥ threshold_count:
        action ← Cooperate
      else:
        punishment_remaining ← min(r − t, d)
        action ← Defect

  play action
  (After the round, if punishment_remaining just reached 0, the next non-final round will be the probe round: follow the same rules above—the probe is implemented by the fact that when punishment_remaining == 0 the algorithm will attempt cooperation if s_prev ≥ threshold_count or s_prev==n; the probe is simply cooperating once by virtue of no active punishment.)

Edge cases and clarifying notes
- r = 2: Round 1 cooperate, round 2 defect (the strategy respects last-round defection).
- Very small r (e.g., r = 3): Same logic; punishments are truncated by remaining rounds due to min(r − t, d).
- If the group includes a stubborn subset of defectors who never change, punishments will be applied repeatedly (and the strategy will alternate punishment and probes), which prevents consistent exploitation while still allowing recovery if some players change.
- The threshold_count = ceil(n − k/2): when k is large (public good yields near-full social return) threshold_count becomes closer to a simple majority; when k is small the threshold requires near-unanimity before cooperating, making the strategy more cautious when contributions are harder to justify.
- The punishment length = number of observed defectors (clipped by remaining rounds) gives proportionality: small deviations get small punishments; larger disruptions get longer punishments.

Rationale summary
- Cooperate initially to signal willingness.
- Use a k-sensitive threshold so the strategy is appropriately lenient when cooperation delivers large returns and strict when the public good is weak.
- Punish proportionally to deter exploitation; forgive quickly to restore mutual cooperation.
- Always defect in the last round because reciprocity cannot be enforced then.

This strategy is implementable from the game parameters (n,k,r) and observable history (counts of cooperators each round), is deterministic (unless you include the optional generosity), and balances cooperation, deterrence and forgiveness in a way designed to earn high total payoffs in tournaments against varied opponent behaviours.
'''

description_COOPERATIVE_22 = '''
Name: Adaptive Win–Stay Lose–Shift with Aspirations (AWSLS-A)

Short summary:
- Begin by attempting cooperation to invite mutual contribution.
- Use a win–stay, lose–shift rule: if your last-round payoff met or exceeded an internal aspiration level, repeat your previous action; otherwise switch.
- Update the aspiration as a bounded moving average of observed payoffs so the strategy adapts to the population of opponents.
- Always defect in the final round (no future to sustain reciprocity).
- This approach is simple, forgiving, hard-to-exploit long-run, and adapts to many opponent behaviours without requiring coordination.

Rationale (intuitive):
- Win–stay, lose–shift stabilizes mutual cooperation: if everyone cooperates, payoffs are high and the rule keeps cooperation. If someone defects and your payoff falls, you switch to punish; when payoffs recover you return to cooperation (forgiveness).
- The aspiration level adapts to the environment (how well the group is doing) so the strategy does not lock into unrealistic expectations or get stuck being exploited.
- Defecting in the last round is necessary in finite-horizon games to avoid being exploited in a one-shot final interaction.

Decision rules (natural language + pseudocode):

Notation
- n, r, k are known game parameters.
- t is the current round index, t = 1..r.
- action_i,t ∈ {C, D} is this strategy’s action in round t.
- π_i,t is this strategy’s observed payoff in round t.
- A_t is the aspiration level used to evaluate round t’s payoff when deciding round t+1; A_1 is initial aspiration before any payoff observed.
- alpha ∈ (0,1) is aspiration learning rate (suggest default alpha = 0.3).
- clamp(x, low, high) = min(max(x, low), high).
- Let payoff_min = 1 (payoff when everyone defects in a round).
- Let payoff_max = k (payoff when everyone cooperates in a round).

Initialization
- A_1 := payoff_max = k (start with optimistic aspiration of full cooperation payoff).
- Choose alpha (default 0.3). This controls how quickly aspiration adapts. Lower alpha => slower, more patient; higher => rapid adaptation.
- action_i,1 := C (cooperate in the first round to invite cooperation).

Main loop (for t = 1..r)
1. If t == r (the final round) then
     action_i,r := D
   and stop (no update after last round).
2. Else if t == 1 then
     play action_i,1 := C (already set).
3. Else (t ≥ 2 and not last round)
   - Observe your last-round payoff π_i,t-1 and your previous action action_i,t-1.
   - If π_i,t-1 ≥ A_{t-1} then
        action_i,t := action_i,t-1  // win–stay: repeat what worked
     else
        action_i,t := opposite(action_i,t-1)  // lose–shift: switch to punish/try alternative
4. After observing the round t payoff π_i,t, update aspiration for next decision:
     A_{t+1} := clamp( (1 - alpha) * A_t + alpha * π_i,t, payoff_min, payoff_max )

Utility helpers
- opposite(C) = D, opposite(D) = C
- clamp keeps A between [1, k] so aspiration never goes below the mutual-defection baseline or above mutual-cooperation maximum.

Parameter suggestions and variants
- Default alpha = 0.3. If you expect highly noisy environments or slow-moving populations, use lower alpha (0.05–0.2). If opponents change rapidly, increase alpha (0.4–0.6).
- Optionally require strict improvement: treat π_i,t-1 > A_{t-1} as “win” and π_i,t-1 ≤ A_{t-1} as “lose” (the provided rule uses ≥ as win to favor stability).
- A more forgiving variant: do “lose” only after two consecutive rounds with π < A, to reduce punishment cycles from single mistakes. This increases forgiveness at the cost of slower response to persistent defectors.

Edge cases
- First round: cooperate to signal cooperative intent (action_i,1 = C).
- Last round: defect (action_i,r = D), because no future to reward cooperation.
- Near-end behavior: the strategy will naturally tend to defect if payoffs drop as the end approaches because aspiration updates reflect realized payoffs; nothing extra is required other than defecting in the final round.
- If repeatedly exploited (your payoffs fall and aspiration drops), the strategy switches to defecting in response and aspiration will adapt downward; if the environment becomes more cooperative later, aspiration rises and the strategy returns to cooperating.
- If mistakenly forced into a short disagreement cycle, the moving-average aspiration plus win–stay dynamics provides built-in forgiveness and recovery in a few rounds.

Why this is cooperative and robust
- Cooperative: It proactively tries cooperation (round 1) and stays cooperative when cooperation produces satisfactory payoffs (mutual cooperation yields the maximum payoff k, which keeps A high and causes repeated cooperation).
- Robust to exploitation: If other players defect, your payoff drops below aspiration and you switch to defecting, preventing long-run exploitation.
- Adaptive: The aspiration moving average uses only your own observed payoffs and adapts to the mixture of opponents faced, so the strategy performs sensibly against a wide range of behaviours (always-defect, always-cooperate, tit-for-tat types, stochastic strategies).
- Forgiving: One or a couple rounds of bad payoff do not permanently lock you into punishment; A updates allow return to cooperation when payoffs improve.

Implementation notes
- The rule needs only your own past actions and payoffs (both observation types are allowed by the game spec). It does not require or assume any shared norms or communications.
- The strategy is deterministic given alpha and initial conditions. If desired, small randomization (ε-greedy exploration) can be added to probe opponents, but deterministic win–stay, lose–shift with aspiration already balances exploration and exploitation via aspiration updates.
- The clamp bounds [1, k] keep aspiration interpretable and numerically stable.

Summary of full pseudocode (compact)

Initialize:
  A := k
  alpha := 0.3 (example)
  action := C  // play C in round 1

For t = 1..r:
  if t == r:
    play D and exit
  else if t == 1:
    play action (C)
  else:
    // t >= 2
    if last_payoff >= A:
      action := last_action
    else:
      action := opposite(last_action)
    play action

  observe payoff π
  A := clamp((1 - alpha)*A + alpha*π, 1, k)
  last_action := action
  last_payoff := π

This strategy should be straightforward to implement, fosters and preserves cooperation when it benefits you and the group, punishes defectors, and adapts to diverse opponent populations while avoiding being persistently exploited.
'''

description_COOPERATIVE_23 = '''
Strategy name: Aspiration-based Win‑Stay Lose‑Shift (AWL) for Public Goods

Summary (intuition)
- Start by signaling cooperation.
- Use an aspiration level (a moving target payoff). If your last-round payoff met or exceeded aspiration, repeat your previous action (win → stay). If it fell short, switch action (lose → shift).
- Update the aspiration by a small learning rate toward the experienced payoff so the strategy adapts to the population: it punishes low-payoff environments and forgives when cooperation returns.
- Always defect in the final round (no future to sustain reciprocity).
This approach is simple, adaptive, forgiving (so cooperation can be re-established), and robust against exploitation because low payoffs trigger switching.

Parameters to set (depends only on n, k, r and designer choice)
- A0: initial aspiration. Recommended: A0 = (1 + k)/2 (midpoint between the all-defect payoff 1 and the all-cooperate payoff k).
- λ: aspiration learning rate, 0 < λ ≤ 0.3. Recommended λ = 0.2 (fast enough to adapt, slow enough to avoid overreacting).
- t: round index 1..r.
These are fixed before play and use only observable history.

Decision rules — verbal description
1. Round 1: Cooperate (C). This signals willingness to cooperate.
2. For each round t where 1 < t < r:
   - Observe your payoff π_{t-1} from the previous round and remember your previous action a_{t-1}.
   - If π_{t-1} ≥ aspiration A, then play the same action a_t = a_{t-1} (win → stay).
   - If π_{t-1} < aspiration A, then switch action a_t = opposite(a_{t-1}) (lose → shift).
   - After the action is played (or immediately after observing π_{t-1}), update aspiration:
     A ← (1 − λ) × A + λ × π_{t-1}.
3. Round r (last round): Defect (D) — the last round is a one-shot so defecting is dominant.

Additional practical details and edge-handling
- If the strategy ever reaches a run of alternating actions because aspiration oscillates, the learning rate λ damps oscillation over time. You can reduce λ if oscillation is problematic.
- When r is small (e.g., r = 2 or 3), the last-round defect is still used; the strategy may cooperate in earlier rounds but will not attempt to sustain cooperation beyond the last round.
- The strategy uses only your own past payoffs and your own past actions (both are part of common history). It does not require forming beliefs about each opponent or explicit targeting; it reacts to realized success.
- This strategy is deterministic once A0 and λ are fixed.

Why this is cooperative and robust
- Cooperative intent: By cooperating in round 1 and repeating actions that yield satisfactory payoffs, AWL tends to maintain mutual cooperation with reciprocating opponents (they produce high payoffs → you stay cooperating).
- Punishment and protection: If opponents defect and you obtain low payoff, the rule “lose → shift” will push you to defect next round, denying free-riders further gains.
- Forgiveness and recovery: Because aspiration is updated toward experienced payoffs (not a permanent grim trigger), once cooperation resumes and payoffs rise, AWL will switch back to cooperation and stabilize there.
- Adaptivity: The aspiration dynamically adapts to the environment (mixtures of cooperators and defectors) so the strategy is not locked to a brittle threshold. It works against conditional cooperators, exploiters, and adaptive opponents.
- Final-round rationality: Defecting in the last round prevents last-round exploitation and is consistent with backward-induction.

Pseudocode

Initialize:
  A ← (1 + k) / 2        # initial aspiration
  λ ← 0.2                # learning rate (example)
  a_prev ← C             # play C in round 1

For t in 1..r:
  if t == 1:
    play a_prev (C)
    observe payoff π_1
    if r == 1: break
    A ← (1 − λ) * A + λ * π_1
    continue

  if t == r:
    play D
    observe payoff π_r
    break

  # t in 2..r-1
  # Decide based on last round payoff vs aspiration
  if π_{t-1} >= A:
    a_t ← a_prev        # stay with previous action
  else:
    a_t ← opposite(a_prev)  # switch action

  play a_t
  observe payoff π_t
  # update aspiration
  A ← (1 − λ) * A + λ * π_t
  a_prev ← a_t

Notes for implementers
- "opposite(C)" = D and vice versa.
- Use the exact payoff π_t available (the game gives full payoff info).
- You may tune λ downward (e.g., 0.1) if you expect many short-term fluctuations and want more inertia; increase λ if you want faster adaptation.
- A0 can be tuned. Using (1 + k)/2 is neutral; if you want to bias toward more cooperation, set A0 closer to k. If you want to be cautious, set A0 closer to 1.

Behavioral examples
- If you play against steadfast cooperators: Round 1 C yields high payoff; π_1 ≥ A, so you keep cooperating — mutual cooperation is sustained.
- If you play against exploiters who defect from the start: Round 1 C yields low payoff; π_1 < A, so you switch to D next round and punish; aspiration falls toward low payoffs, so you won’t be repeatedly exploited.
- If you play against a mix of conditional cooperators who sometimes cooperate: aspirations move gradually and the strategy will alternate as needed, encouraging cooperation where it is reciprocated and defecting when it is not.

Conclusion
AWL is simple, parameter-sparse, and balances signaling, retaliation, and forgiveness. It relies only on game parameters and history (your past actions and payoffs), is adaptive to different opponent behaviors, and is explicitly cooperative-minded (initial cooperation and willingness to return to cooperation when it becomes mutually beneficial), while protecting you from long-term exploitation and defecting rationally in the final round.
'''

description_COOPERATIVE_24 = '''
Name: Adaptive Conditional Cooperator with Proportional Punishment (ACCPP)

High-level idea
- Start by signalling cooperation. Try to maintain mutual cooperation whenever a clear majority of other players reciprocate.
- When you see significant defection, punish proportionally but briefly so you discourage exploitation without provoking long vendettas.
- Always forgive and test for recovery: after a limited punishment you try cooperation again.
- Be slightly more forgiving when the public-good multiplier k is large (because the cost of cooperating is smaller relative to group benefit).
- In the very last round you follow a short, history-sensitive rule (cooperate only if recent group cooperation is very high); this avoids being exploited in the known terminal round while staying cooperative when mutual cooperation is likely.

Decision variables computed from parameters (deterministic, known to the algorithm)
- n (players), r (rounds), k (multiplier). Let g = k / n (0 < g < 1).
- Window length for recent history: w = min(5, r-1). (If r=2, then w=1.)
- T_coop (cooperation threshold): how large a recent cooperation rate among others we require to cooperate.
  - Set T_coop = clamp(0.625 - 0.15*g, 0.45, 0.75).
    - This makes the strategy somewhat more forgiving when g is larger (cooperation is less costly).
- T_tolerate (tolerance for one-round defects before punishing): T_tolerate = 0.4 (fixed).
- Max punishment length: P_max = max(1, ceil(r/10)). (Allows longer punishments in longer games.)
- Punishment scaling: punish length grows with fraction of defectors.

State kept between rounds
- punishment_timer (integer ≥ 0) – number of rounds left to punish. Start at 0.
- last_action (C or D) – what you played in last round.
- (history) full record of all players' actions each past round (available by assumption).

Rules (natural language, then compact pseudocode)

1) First round (t = 1)
- Play C (cooperate). This signals willingness to cooperate and attempts to achieve mutual cooperation early.

2) Each round t > 1
- Let for each prior round τ, S_τ = fraction of the other n−1 players who cooperated in round τ.
- Compute f_recent = average of S_τ over the last w rounds (or over all past rounds if fewer than w exist).
- Compute offending_rate_last = 1 − S_{t-1} = fraction of other players who defected in the immediately previous round.

- If punishment_timer > 0:
  - Play D (defect) this round.
  - Decrement punishment_timer by 1 at the end of the round.

- Else (not currently punishing):
  - If t is the last round (t == r):
    - Last-round rule: cooperate only if S_{t-1} ≥ max(0.8, T_coop + 0.15). Otherwise defect.
      - Rationale: do not be naively exploited in the final, unpunishable round; but if the group very consistently cooperated last round, reward them once.
  - Else (not final round):
    - If f_recent ≥ T_coop:
      - Play C (cooperate). Rationale: the group has been cooperating enough to justify continued cooperation.
    - Else:
      - If offending_rate_last ≤ (1 − T_tolerate) (i.e., only a small fraction defected last round):
        - Play C (give single-defection grace). This treats occasional/accidental defection leniently.
      - Else (significant defections last round):
        - Enter proportional punishment:
          - Compute p = offending_rate_last (fraction of other players who defected last round).
          - Set punishment_timer = min(P_max, 1 + round( (p - T_tolerate) * (n) ) ).
            - This produces a punishment of at least 1 round and longer if many players defected; capped by P_max.
          - Play D this round (start punishment immediately).

3) After a punishment period completes:
- On the first round after punishment_timer reaches 0, play C (test for recovery), regardless of the last round's defections.
- If the test is reciprocated (f_recent in subsequent rounds ≥ T_coop), remain cooperating; otherwise re-enter proportional punishment as above.

4) Additional practical twists to improve robustness
- Weighted recent history: you may weight the most recent round more heavily when computing f_recent (e.g., weight last round 2x) so the strategy adapts faster to changes.
- If you observe sustained near-zero cooperation for many rounds (e.g., f_recent < 0.05 for several windows), switch to permanent defection to stop being a chronic sucker. (This only happens when group is wholly non-cooperative.)

Pseudocode (compact)

Initialize: punishment_timer = 0
Play round 1: action = C

For each round t = 2..r:
  compute S_last = fraction of other players cooperating in round t-1
  compute f_recent = average S over last w rounds (or all past if < w)
  offending_rate_last = 1 - S_last

  if punishment_timer > 0:
    action = D
    punishment_timer = punishment_timer - 1

  else:
    if t == r:   # last round
      if S_last >= max(0.8, T_coop + 0.15): action = C
      else: action = D

    else:  # not last round and not punishing
      if f_recent >= T_coop:
        action = C
      else:
        if offending_rate_last <= (1 - T_tolerate):  # small number of defectors last round
          action = C
        else:
          # proportional punishment
          p = offending_rate_last
          punishment_timer = min(P_max, 1 + round((p - T_tolerate) * n))
          action = D

  record action; proceed to next round

When punishment_timer reaches 0, the algorithm cooperates on the next round as a recovery test.

Why this is cooperative and robust
- Cooperative: starts with cooperation, prefers cooperation when a clear majority reciprocates, and forgives occasional lapses. It always attempts to return to cooperation after punishment.
- Robust: punishes defections proportionally and promptly, which deters exploitation. Punishments are capped and followed by a recovery test to avoid long mutual defection cycles.
- Adaptive: thresholds depend on the public-good effectiveness g = k/n so the strategy becomes more forgiving when cooperation yields larger per-player returns. The windowed average and weighted recent round let the algorithm adapt to rapid or gradual changes in opponents’ behavior.
- Tournament-friendly: It performs well against unconditional cooperators (gains by cooperating), unconditional defectors (punishes and stops giving them gains), and conditional strategies (mutual cooperation can be established). It also guards against exploitation in the final round, while still rewarding predictable cooperative groups.

Parameter tuning notes (implementation guidance)
- The constants (w = 5, T_tolerate = 0.4, P_max = ceil(r/10), the T_coop linear formula) are chosen for general robustness. In a specific tournament you can tune:
  - Increase T_coop to be stricter if many opponents are opportunistic.
  - Decrease punishment lengths to avoid long retaliatory cycles in noisy environments.
  - Increase punishment lengths in very long tournaments to sustain incentives for cooperation.

Edge cases summary
- First round: always cooperate.
- Short games (r small): window length w reduces to available rounds; P_max likely = 1 so punishment is brief.
- Last round: defect unless the immediate past shows very strong cooperation (S_last ≥ max(0.8, T_coop + 0.15)).
- If almost everyone defects for an extended period, the strategy defects to avoid continually losing.
- After any punishment, the strategy tests cooperation once — it does not punish forever.

This strategy balances being reliably cooperative (so it can reach and sustain high-payoff mutual cooperation) with protection against exploitation (by calibrated, proportional punishment and quick forgiveness). It only uses game parameters (n, r, k) and observed history, so it is implementable in the tournament environment you described.
'''

description_COOPERATIVE_25 = '''
Name: Adaptive Generous Conditional Cooperator (AGCC)

Goal (design objective)
- Promote and sustain mutual cooperation (high total group payoff) whenever opponents reciprocate.
- Avoid long-term exploitation by defectors by applying short, proportional punishments.
- Be forgiving and adaptive to noisy or occasional lapses, but become increasingly cautious as the game approaches its end.
- Use only game parameters (n, k, r) and observable history of past rounds.

Intuition (short)
- Start by signalling cooperation.
- Use a short moving-window estimate of how cooperative the group has been recently.
- Cooperate when recent cooperation is sufficiently high; otherwise defect for a short, proportional punishment period.
- Be forgiving (do not escalate punishment indefinitely) so cooperation can re-establish.
- In the last round, require strong recent evidence of sustained cooperation to cooperate; otherwise defect to avoid exploitation in the known final stage.

Parameters computed from game parameters
- W = min(4, r - 1)  // memory window (look back this many rounds; if r small, shrink window)
- target_rate = min(0.8, 0.5 + 0.25*(k / n))
    - Rationale: require a simple majority-like recent cooperation rate; higher k (larger public good benefit) justifies a slightly higher cooperation expectation.
- P = min(3, max(1, ceil((n - k) / 2)))
    - Rationale: punishment length (in rounds) increases with how much k is below n (when k is much smaller than n, free-riding is more attractive so require somewhat stronger short punishment). Bound to 1–3 rounds to avoid long-term mutual harm.
- forgiveness_tolerance = 1  // ignore isolated single-round dips if the moving average stays above target_rate

State kept by the strategy
- history of observed cooperators each past round (including whether you cooperated)
- punish_counter (integer >= 0): when >0, you are in punishment mode and play D; counts down each round

Decision rules (natural language then concise pseudocode)

Natural language
1. First round: Cooperate. This signals goodwill.
2. For each subsequent round t < r (not the last round):
   a. If punish_counter > 0: play D and decrement punish_counter by 1.
   b. Else compute the recent cooperation fraction f = average over the last W rounds of (number_of_cooperators / n).
      - If f >= target_rate: play C (cooperate).
      - If f < target_rate: enter punishment: set punish_counter = P (do not include current round as already being decided) and play D this round.
   c. If there is a single isolated dip (one round with slightly lower cooperators) but the W-round average remains >= target_rate, ignore it (this is covered by f).
3. Last round t = r:
   - Cooperate only if the last W rounds were all cooperators (i.e., full, recent unanimous cooperation). Otherwise defect. (This avoids being exploited in the known terminal round while allowing cooperation in the end only when it's very likely everyone will cooperate.)
4. If r is very small (e.g., r = 2), W will be 1 and decisions naturally become based on the immediately previous round; P will be at least 1 so a single defection triggers a one-round punishment.

Concise pseudocode

Initialize:
  W = min(4, r - 1)
  target_rate = min(0.8, 0.5 + 0.25*(k / n))
  P = min(3, max(1, ceil((n - k) / 2)))
  punish_counter = 0
  history = []  // each element = number_of_cooperators in that round

For round t = 1..r:
  if t == 1:
    action = C
  else if t == r:  // last round (endgame-aware)
    if last W rounds exist and for each s in last W rounds history[s] == n:
      action = C
    else:
      action = D
  else:  // intermediate rounds
    if punish_counter > 0:
      action = D
      punish_counter -= 1
    else:
      // compute average cooperation fraction over last W rounds
      let L = min(W, length(history))
      f = (1 / L) * sum_{j=1..L} (history[-j] / n)
      if f >= target_rate:
        action = C
      else:
        action = D
        punish_counter = P - 1
        // we already defect this round, so we set counter to remaining punishment rounds

  Play action, observe total_cooperators_this_round (m)
  append m to history

Notes and clarifications
- punish_counter semantics: After deciding to punish, you defect that round and set punish_counter to P-1 so total consecutive defect rounds for punishment = P.
- Averaging: the average f uses the observed total cooperators per round (so includes your own past choices and others’ past choices). This makes the rule robust to small sampling noise.
- Forgiveness is built-in: punishments are short and you return to cooperation if the moving average recovers.
- Endgame: the last-round conservative rule protects you from being exploited by pure defectors exploiting the finite horizon, yet allows cooperation in the final round when everyone has a strong recent record of cooperating.
- Deterministic and implementable: all quantities are derived from n, k, r and the observable history.

Why this is cooperative and robust
- Cooperative: it starts by cooperating and will sustain cooperation when a majority / sustained fraction of the group reciprocates. That leads to the mutually beneficial all-C outcome when many opponents are conditional cooperators.
- Robustness: short, proportional punishments deter repeated exploitation without causing long revenge cycles that destroy group payoffs. Using a moving average avoids punishing for single accidental or isolated defections.
- Adaptive: target_rate and punishment length depend on k and n, so the strategy becomes slightly more demanding or forgiving depending on how valuable the public good is and how many players there are.
- Endgame-awareness: the last-round rule prevents being systematically exploited in the known terminal stage while still allowing cooperation if the group has truly established trust.

Practical tuning suggestions (optional)
- If the tournament tends to have many generous opponents, increase target_rate up toward 0.7–0.8 to favor stronger cooperation.
- If opponents are very noisy, increase W and reduce P to become more forgiving.
- If many opponents are exploitative, increase P (bounded by r) or tighten the last-round rule.

This strategy is simple, transparent, adaptive to n, k, r, and furnished with short, proportional punishments and forgiveness so it tends to produce high group payoff against many kinds of opponents while avoiding long-term exploitation.
'''

description_COOPERATIVE_26 = '''
High-level idea
- Be clearly cooperative: start by cooperating and try to sustain high cooperation when others reciprocate.
- Be conditional and retaliatory: punish rounds with many defectors, but punish only briefly and forgive quickly so cooperation can recover.
- Be safe in the endgame: defect in the last round (no future to enforce reciprocity); adapt last few-round behavior conservatively to avoid being exploited.
- Use only game parameters (n, r, k) and observable history (who cooperated each past round).

Core intuition
- In a single round an individual is always better off defecting, so cooperation must be sustained by reciprocity.
- Punishment must be strong enough to deter sustained defection but limited (not infinite) so noise or a single mistake does not collapse cooperation permanently.
- A small tolerance for a few defectors lets the group sustain cooperation when there are a few free-riders, which is often needed in multi-player public-goods settings.

Parameters used by the strategy (implementation can expose these for tuning)
- tol_frac (default 0.20): fraction of players tolerated as defectors in a round without triggering immediate long punishment. tol = max(1, floor((n-1) * tol_frac)).
- P_max (default 3): maximum number of consecutive punishment rounds triggered by a bad round.
- Forgive-on-recovery: if others return to cooperative levels during punishment, cancel punishment early.
- Endgame_defect_rounds (default 1): number of final rounds in which this strategy defects (default = last round). This prevents exploitation in the terminal round where reciprocity cannot be enforced.
- Majority fallback: require at least ceil(n/2) cooperators in the previous round as a secondary signal to cooperate when tol condition is not met.

Recommended automatic adjustments (optional, improves robustness)
- If k is large (public good very beneficial), be more tolerant: increase tol_frac toward 0.4 as k → n; if k is small (near 1) be slightly less tolerant.
- Cap P_max by r (P_max = min(P_max, max(1, floor(r/10)))) so punishment length scales with game length.

Natural-language decision rules (concise)
1. First round: Cooperate.
2. Final rounds: Defect in the last Endgame_defect_rounds rounds (by default, defect in the final round).
3. For any non-final round t > 1:
   a. Let coop_prev = number of players who cooperated in round t−1 (including yourself).
   b. If coop_prev >= n - tol: interpret previous round as “good enough” (at most tol defectors) → cooperate.
   c. Else if coop_prev >= ceil(n/2): (majority cooperated) → cooperate (secondary, more forgiving signal).
   d. Else: previous round was bad (many defectors). Enter a punishment phase:
       - Set punish_rounds = min(P_max, 1 + (n - coop_prev - tol)) (i.e., escalate punishment by how many players exceeded tolerated defectors, up to P_max).
       - Defect for punish_rounds consecutive rounds.
       - During punishment, monitor coop_prev each round; if coop_prev returns to >= n - tol during punishment, cancel remaining punishment immediately and return to cooperate.
4. Always be forgiving: after punishment ends (either completed or cancelled), return to the cooperation rule above (step 3).
5. When in doubt (ambiguous histories or tiny r), fall back to cooperate early and be cautious only very late in the game.

Pseudocode

Initialize:
  tol = max(1, floor((n-1) * tol_frac))
  P_max = min(P_max, max(1, floor(r/10)))   # optional scaling
  punish_remaining = 0

For each round t = 1..r:
  if t == 1:
    play C
    continue
  if t > r - Endgame_defect_rounds:
    # In designated terminal rounds defect to avoid exploitation.
    play D
    continue

  coop_prev = number of cooperators in round t-1 (including self)
  if punish_remaining > 0:
    # we are currently punishing unless fast-forgiven
    if coop_prev >= n - tol:
      punish_remaining = 0
      play C
    else:
      punish_remaining = punish_remaining - 1
      play D
    continue

  # Not punishing now; decide based on previous round
  if coop_prev >= n - tol:
    play C
  else if coop_prev >= ceil(n/2):
    play C
  else:
    # Trigger punishment
    extra_defectors = max(0, n - coop_prev - tol)
    punish_remaining = min(P_max, 1 + extra_defectors) - 1
      # we will defect this round and punish_remaining more rounds after this
    play D

Notes on edge cases
- r = 2: Round 1 cooperate, round 2 defect (last-round rule). This is safe and consistent.
- Very small n (n = 2): tol = 1 by formula, so strategy behaves like generous tit-for-tat / conditional cooperator — cooperates first and punishes a defection for a short time.
- Very short games: P_max scales with r, so punishment will not be longer than the remaining horizon.
- Noisy environments: brief punishments and immediate forgiveness when cooperation resumes avoid permanent breakdown from one-off deviations.
- Persistent exploiters: punishments scale with the number of defectors above tolerance; repeated exploitation will provoke repeated short punishments that reduce the exploiter’s long-run payoff.

Why this will perform well in a tournament
- It opens cooperatively (invites cooperation from reciprocators).
- It rewards high-cooperation rounds (cooperates when most cooperated last round).
- It tolerates a small number of free-riders (prevents collapse over a single defection) yet punishes larger or repeated defections.
- Punishment is limited and forgiving, avoiding endless mutual retaliation and allowing cooperation to recover.
- Endgame defensiveness prevents guaranteed exploitation in the terminal round.
- The strategy relies only on public history and parameter knowledge; it adapts quickly to opponent behavior without requiring explicit coordination or shared norms.

Tuning tips (if you can tune before tournament)
- Increase tol_frac (toward 0.3–0.4) when k is high to allow more tolerance for occasional defectors and preserve cooperation.
- Increase P_max slightly for long r (so punishment credibly influences long-run incentives).
- Decrease tol_frac and P_max for very small r to avoid wasting rounds on punishment when few rounds remain.

This strategy is straightforward to implement, explainable, and balances cooperation promotion, robustness to exploitation, and forgiveness to recover cooperation.
'''

description_COOPERATIVE_27 = '''
Name: Adaptive Conditional Cooperator with Forgiveness (ACCF)

Intuition (short)
- Signal cooperation immediately, try to sustain group cooperation by cooperating when a clear majority of others have been cooperating recently.
- If cooperation falls, apply a short, proportional punishment (defect for a few rounds) to deter free-riding.
- Forgive quickly when others restore cooperative behaviour so cooperation can recover.
- Refuse to be repeatedly exploited by persistent defectors: if many players are persistently non‑cooperative, stop cooperating until they change.
- In the final round (no future), defect (no incentive to sacrifice when there is no future punishment).

This strategy depends only on game parameters (n, k, r) and observed history (actions of all players by round). It is adaptive, forgiving, and robust against a range of opponent behaviours.

Parameters (computed once from inputs)
- L = min(5, r - 1) — lookback window for recent group behaviour (short and responsive).
- W = min(10, r - 1) — window for detecting persistent defectors (longer window).
- theta = 0.5 — require at least 50% of other players cooperating recently to be willing to cooperate.
- P_base = 2 — base punishment length (rounds).
- P_max = min(5, r - 1) — maximum punishment length.
- phi = 0.3 — threshold to mark a player as a persistent defector (cooperation rate below phi over W rounds).
- d_thresh = max(1, floor((n - 1) / 3)) — if more than d_thresh players are persistent defectors, treat group as hostile.
(These numeric choices are defaults; they are intentionally simple, interpretable, and scale sensibly with n and r.)

State variables (maintained during play)
- mode ∈ {COOPERATE, PUNISH} (initially COOPERATE).
- punish_remaining (integer, initially 0).
- history: for each past round t and each player j we observe c_j,t ∈ {0,1}.

Decision rules (natural language)
1. Terminal-round rule:
   - If this is the last round (t = r): defect (D). Rationale: no future to enforce cooperation; last-round defection yields a weakly higher payoff if others cooperate.

2. First-round rule:
   - Round 1: cooperate (C). Rationale: signal willingness to cooperate, give cooperative opponents a chance to reciprocate.

3. Each non-terminal round t > 1:
   a. Compute recent group cooperation among other players:
      - Let S = rounds max(1, t - L) ... t - 1.
      - avg_others = (1 / (|S| * (n - 1))) * Σ_{s∈S} Σ_{j≠i} c_j,s
        (fraction of other-players’ contributions in the lookback window).
   b. Detect persistent defectors:
      - For each player j ≠ i compute coop_rate_j over W most recent rounds (or fewer if t-1 < W).
      - Let persistent_count = number of j with coop_rate_j < phi.
      - If persistent_count > d_thresh: treat the environment as hostile -> defect this round (and remain cautious until persistent_count falls).
   c. If mode = PUNISH and punish_remaining > 0:
      - Defect this round (D).
      - Decrement punish_remaining by 1.
      - After punish_remaining reaches 0, re-evaluate avg_others; if avg_others ≥ theta then switch mode to COOPERATE; else increase punishment length (see punishment sizing below) and remain in PUNISH.
   d. If mode = COOPERATE:
      - If avg_others ≥ theta: cooperate this round (C).
      - If avg_others < theta: defect this round, switch to PUNISH and set punish_remaining according to punishment sizing (below).

Punishment sizing and escalation
- When entering punishment because avg_others < theta, set punish_remaining = min(P_max, P_base + ceil((theta - avg_others) * 10)).
  - Rationale: the smaller avg_others is relative to theta, the longer the punishment (but bounded).
- If punishment ends but avg_others still < theta, increase punishment length by 1 (capped at P_max) and continue punishing. This prevents quick cycling of exploitation while keeping punishments short and recoverable.

Forgiveness and re-entry to cooperation
- The strategy always gives a clear quick path back to cooperative mode: after punishment ends, if recent average cooperation among others returns to ≥ theta, switch back to COOPERATE immediately.
- If persistent_count falls below or equal to d_thresh, treat group as repairable again.

Edge cases
- Very small r (e.g., r = 2): L and W become 1; the algorithm still cooperates in round 1 and defects in round 2.
- If r is so small that P_base or P_max exceeds remaining rounds, cap punishments to remaining rounds.
- If the population is nearly all defectors (avg_others ≈ 0), the strategy will defect (defensive) rather than repeatedly sacrifice.
- If there are a few persistent defectors but the majority remain cooperative, ACCF will cooperate with the majority and punish only briefly to penalize defectors. If persistent defectors form a significant minority (more than d_thresh), the strategy switches to defensive mode and stops cooperating until behaviour improves.

Pseudocode

(Assume inputs n,k,r and access to history of c_j,t for all previous rounds t=1..t-1)

initialize:
  mode = COOPERATE
  punish_remaining = 0

on each round t:
  if t == r:
    action = D
    return action

  if t == 1:
    action = C
    return action

  S = rounds max(1,t-L) .. t-1
  |S| = number of rounds in S
  avg_others = (sum_{s in S} sum_{j≠i} c_j,s) / (|S| * (n-1))

  compute coop_rate_j for each j≠i over last min(W, t-1) rounds
  persistent_count = count of j with coop_rate_j < phi

  if persistent_count > d_thresh:
    action = D
    // stay in COOPERATE state? stay cautious but keep mode unchanged; punish_remaining = 0
    return action

  if mode == PUNISH and punish_remaining > 0:
    action = D
    punish_remaining -= 1
    if punish_remaining == 0:
      if avg_others >= theta:
        mode = COOPERATE
      else:
        punish_remaining = min(P_max, punish_remaining + 1) // escalate and stay punishing
        mode = PUNISH
    return action

  // mode == COOPERATE
  if avg_others >= theta:
    action = C
    return action
  else:
    // start punishment
    mode = PUNISH
    punish_remaining = min(P_max, P_base + ceil((theta - avg_others) * 10))
    action = D
    return action

Why this is cooperative and robust
- Cooperative motive: ACCF cooperates from the start and cooperates whenever a clear majority of others has been cooperating recently. That encourages mutual cooperation and obtains the group payoff when many players reciprocate.
- Robust deterrence: Short, proportional punishments are designed to make being a persistent free-rider costly enough to deter exploitation in repeated interaction, but not so long that cooperation cannot be healed.
- Forgiveness: Quick return rules prevent permanent breakdown after mistakes or one-off deviations, enabling recovery to high-payoff mutual cooperation.
- Protection against persistent defectors: If many players are persistently non-cooperative, ACCF stops cooperating, cutting losses rather than giving sustained value to exploiters.
- Endgame safety: Defecting in the final round prevents being exploited when future punishments are impossible.

Tunable choices
- theta, L, W, P_base, P_max, phi, d_thresh are explicit and interpretable; tournament designers or implementers can tune them based on observed opponent patterns or desired trade-off between bold cooperation and safety.
- For conservative play (less risk of exploitation), raise theta and/or P_base and/or phi; for more generous cooperation, lower theta and shorten punishments.

Expected tournament behaviour
- Against cooperative or conditional-cooperative opponents, ACCF will rapidly reach and maintain mutual cooperation and earn near-group-optimal payoffs.
- Against pure defectors, ACCF will stop cooperating after short exposure and minimize loss.
- Against opportunistic or noisy opponents, ACCF’s limited punishments and forgiveness allow cooperation to recover while deterring exploitation.

This strategy is implementable from the game parameters and the full public history, requires no communication, adapts to a variety of opponent behaviours, and clearly prioritizes cooperation while guarding against exploitation.
'''

description_COOPERATIVE_28 = '''
Name: Generous Conditional Cooperate-and-Exclude (GCCE)

Short description
- Start by signalling cooperation.
- Continue cooperating while the group shows substantial recent cooperation.
- If the group falls below a cooperation threshold, punish with a short, escalating sequence of defections.
- Always include forgiveness tests so the strategy can re‑establish cooperation.
- Threshold and punishment lengths adapt to the public-good effectiveness k and to recent history so the strategy is neither gullible nor permanently vengeful.

Intuition
- The one-shot dominant move is D, so cooperation must be sustained by reciprocity. GCCE tries to build and maintain a high-contribution equilibrium when others reciprocate, but it punishes persistent free-riding quickly and in a limited way so as to deter exploitation and allow recovery.
- The cooperation threshold is tied to k: when the public good is more efficient (k relatively large) the strategy is more willing to tolerate some defectors because the group benefit of cooperation is larger.

Notation
- n, r, k: game parameters
- t: current round (1..r)
- history: for each past round s < t we observe full action profile; we can count cooperations by players
- A_i,t ∈ {C, D} denotes player i’s action in round t; "others" refers to players ≠ me
- L: lookback window length (number of previous rounds used); default L = min(5, t-1)
- p_hat: estimated recent cooperation rate among others (fraction of other-player actions that were C over the last L rounds)
- θ: cooperation threshold (fraction). Computed from (n,k) below.
- punish_counter: remaining rounds to play D as punishment (initially 0)
- escalate_count: how often punishment has been triggered recently (for escalation); reset when cooperation is reestablished
- ε: forgiveness / probe probability while punishing (small)
- P_base: base punishment length (small integer)

Parameters (suggested defaults)
- L = min(5, t-1)
- θ = clamp(0.5 * (n / k), 0.2, 0.8)
  - Explanation: if k is small relative to n (public good less efficient) require higher observed cooperation before you keep cooperating; if k is large, be more lenient. The clamp keeps thresholds sensible for extremes.
- P_base = 2 (punish at least 2 rounds when exploitation detected)
- escalate_limit = 3 (max extra escalation steps)
- ε = 0.05 (5% random cooperation during punishment to test recovery)
- punish_counter initially 0, escalate_count initially 0

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This is a clear cooperative signal and helps bootstrap mutual cooperation in most populations.
2. For t ≥ 2:
   a. Compute L and p_hat = (number of C by others during last L rounds) / ((n-1) * L).
   b. If punish_counter > 0:
       - With probability ε play C (a forgiveness probe).
       - Otherwise play D.
       - Decrement punish_counter by 1.
       - If punish_counter hits 0 and p_hat ≥ θ on the same decision, reset escalate_count = 0.
   c. If punish_counter == 0:
       - If p_hat ≥ θ: play C (cooperate).
       - If p_hat < θ: this is evidence of recent group under-contribution → set punish_counter = min(P_base + escalate_count, r - t) and play D this round. Also increment escalate_count = min(escalate_count + 1, escalate_limit).
3. Last-round special handling (t = r):
   - If punish_counter > 0 or p_hat < θ: play D (defect) — no future to enforce cooperation.
   - Exception: if p_hat is extremely high (e.g., p_hat ≥ 0.99 or the last round was unanimous cooperation) you may choose to play C to capture the mutually higher payoff; this allows benevolent final-round mutual gains when the group has been reliably cooperative. (This keeps the final-round behavior responsive rather than mechanical.)
4. Recovery & forgiveness:
   - The ε probe while punishing gives a small chance to re-establish cooperation even during punishment.
   - When punish_counter reaches 0 and p_hat ≥ θ, treat the group as 'recovered' and reset escalate_count to 0 so future punishments are small again.
5. Handling repeated exploitation:
   - escalate_count increases each time you trigger punishment (up to escalate_limit), making subsequent punishments for repeated violations slightly longer; this raises the cost of persistent exploitation to deter repeat offenders.
6. Robustness to noise and stochastic opponents:
   - Short window L and small P_base avoid overreacting to single shocks.
   - ε probes and limited escalation avoid infinite punishment cycles.

Pseudocode

Initialize:
  punish_counter = 0
  escalate_count = 0
  For t = 1..r:
    if t == 1:
      action = C
      continue to next round
    L = min(5, t-1)
    p_hat = (number of C by others in last L rounds) / ((n-1)*L)
    θ = clamp(0.5 * (n / k), 0.2, 0.8)
    if t == r:  // last round
      if punish_counter > 0 or p_hat < θ:
        action = D
      else if p_hat >= 0.99:  // nearly unanimous cooperation recently
        action = C
      else:
        action = D
      // no need to update punish_counter for last round
    else:  // intermediate round
      if punish_counter > 0:
        if random() < ε:
          action = C
        else:
          action = D
        punish_counter -= 1
        if punish_counter == 0 and p_hat >= θ:
          escalate_count = 0
      else:  // not currently punishing
        if p_hat >= θ:
          action = C
        else:
          // trigger punishment
          punish_counter = min(P_base + escalate_count, r - t)
          escalate_count = min(escalate_count + 1, escalate_limit)
          action = D

Design choices and rationale
- Start-cooperate: gives the best chance to establish mutual cooperation early; most cooperative strategies reciprocate.
- Threshold θ linked to k: more willingness to cooperate when the public good is valuable.
- Short lookback (L ≤ 5): lets the strategy respond to recent trends without overfitting to old behavior.
- Limited, escalating punishments: short punishments deter occasional or opportunistic defects but do not lock you into mutual retaliation with noisy or adversarial opponents; escalation deters persistent freeloaders.
- Forgiveness probe ε: breaks deadlocks, allows accidental defections to be forgiven, and supports re-coordination.
- Last-round conservative default: avoid being exploited in the final round; allow cooperation only when near-unanimous recent cooperation is observed.

Edge cases
- Very small r (r = 2): the strategy cooperates round 1 and then likely defects round 2 unless the opponent cooperated unanimously in round 1—this balances signalling and protection.
- Very large n: θ scales via n/k; clamping limits ensure the threshold stays in a reasonable band.
- Unpredictable / random opponents: short punishment and forgiveness prevent endless conflict; cooperation will be attempted when the observed cooperation rate rises.
- Deterministic defectors: GCCE will detect low p_hat and punish; punishments are short but escalate, depriving defectors of the repeated benefits of exploitation and stabilizing against sustained free-riding.
- All-cooperators population: GCCE cooperates almost always and reaps the higher mutual payoff.
- Exploitative populations: GCCE protects itself early and tries to re-establish cooperation only when the group returns to sufficient cooperative behavior.

How this is cooperative
- GCCE privileges cooperation when the group shows cooperative behavior, and signals cooperation from the start.
- It punishes only as needed, with clear, bounded punishments and forgiveness, which helps maintain long-term cooperative outcomes against a variety of opponent types.
- It is adaptive: thresholds respond to k and history; punishments escalate when necessary; forgiveness allows re-coordination.

Implementation notes
- The strategy depends only on parameters (n, k, r) and observed history (others’ past actions). No communication or coordination with opponents is required.
- Tunable parameters (L, P_base, ε, clamp bounds) can be adjusted for different tournaments; the defaults trade responsiveness and robustness.
- Optionally, one can replace the scalar p_hat by a weighted recent average or track per-player cooperation rates; the high-level decision rules remain the same.

This GCCE strategy is simple to implement, transparent, and designed to balance generosity and protection so it performs well across diverse opponent behaviours while strongly favoring cooperative outcomes when others reciprocate.
'''

description_COOPERATIVE_29 = '''
Name: Adaptive Conditional Cooperator (ACC) — a forgiving, targeted, and adaptive conditional-cooperation rule for the repeated N‑player public goods game.

High-level idea
- Signal willingness to cooperate (start cooperating).
- Cooperate when the group recently showed substantial cooperation (so cooperation is likely to be reciprocated).
- Detect persistent free‑riders using per-player histories and respond with limited, proportional punishment (defecting for a short punish period).
- Be forgiving and periodically probe to re‑establish cooperation if opponents change behaviour.
- Defect in the final round (no future to incentivize cooperation).

Derived algorithm parameters (computed from known game parameters n, r, k)
- W (window length for history checks) = min(10, max(1, r-1)). Use recent W rounds to estimate behaviour.
- q (group cooperation threshold fraction) = 0.50 (majority). You cooperate next round if at least q fraction of players cooperated in the previous round(s).
- P (punishment length for a flagged player) = 2 (or P = min(4, max(2, round(r/25))) if you want r-dependent).
- γ (personal-cooperation tolerance) = 0.4 (a player cooperating less than 40% in the recent window is treated as a persistent defector).
- p_probe (probability to probe despite bad history) = 0.05 (small chance to cooperate to test whether cooperation can resume).
- forgiving policy: after P rounds of punishment toward a flagged player, remove the flag if that player’s cooperation rate in the window improved above γ or if global cooperation improved.

State to maintain
- For each player j (including yourself if convenient), count coop_j = number of times j cooperated in the last W rounds (sliding window).
- For last round, record m_{t-1} = total cooperators.
- For each player j, a flag punish_j with remaining punishment rounds (integer ≥ 0).
- A global punishment flag optional when many players defect (see below).

Decision rules (natural language)
1. First round (t = 1)
   - Cooperate. This signals cooperative intent; it is low-cost to the strategy because it seeks sustained mutual cooperation thereafter.

2. Last round (t = r)
   - Defect. There is no future to incentivize others, so defecting avoids being exploited. (If you prefer a tournament that rewards mutual last-round cooperation, this can be overridden, but standard game-theoretic logic says defect.)

3. Detection & flagging (run before choosing action at round t ≥ 2)
   - Use the last W rounds (or fewer if t-1 < W) to compute each player j’s cooperation rate R_j = coop_j / min(W, t-1).
   - If R_j < γ and j has defected more than once in the very recent rounds, set punish_j = P (start/refresh punishment timer for j).
   - If more than half of players are flagged (major systemic defection), set a global-punish mode (brief collective retaliation for P rounds). This prevents being the only punisher in large coordinated defection by others; it also conserves leniency if only one or two are misbehaving.

4. Main action rule (choose C or D at round t)
   - If any punish_j > 0 AND the expected effect of your cooperating would be to give a flagged defector a benefit relative to you (i.e., you are being exploited), defect. That is, when you are in active punishment of at least one flagged player, choose D until the offending players’ punish_j timers expire or their rates improve.
   - Else, compute m_{t-1} (number of cooperators in previous round). If m_{t-1} ≥ ceil(q * n) then Cooperate (C).
   - Else (m_{t-1} < ceil(q * n)):
       - With probability p_probe, Cooperate (this is a low-frequency test to see if cooperation can be re-established).
       - Otherwise Defect (D).
   - If in global-punish mode, defect until the global-punish timer expires; then resume normal rule and allow probes.

5. Punishment mechanics and forgiveness
   - When a player j is flagged, punish_j is set to P. Each round punish_j > 0 causes you to defect; after each round decrement punish_j by 1.
   - If during punishment j’s recent cooperation rate R_j rises above γ (they started cooperating), clear punish_j and resume cooperating following the main rule.
   - Never punish forever for a single mistake — punishment length is finite (P). After P rounds, the player is forgiven unless they continue to defect repeatedly.
   - If multiple players are flagged but group global cooperation improves (m_{t-1} surpasses threshold), clear flags gradually to restore cooperation.

6. Edge behavior near the end-game
   - Because last-round defection is dominant, begin phasing out long punishments so you are not exploited in final few rounds:
       - If current round t satisfies r - t ≤ P (i.e., within P rounds of the end), reduce punish_j timers so they expire by the final round (or stop initiating new punishments). Continue to defect in final round.

Why this is cooperative and robust
- Cooperative: It starts by cooperating and cooperates whenever the group has recently shown substantial cooperation. That maximizes the chance of fostering and maintaining high-cooperation equilibria.
- Robust: It detects repeat free‑riders via per-player statistics and punishes only those players and only for a limited period (proportional punishment) rather than permanently (avoids over-punishing cooperative or noisy opponents).
- Adaptive: Uses sliding-window statistics and probes periodically to detect changes in opponent behaviour (so it can re-establish cooperation with strategies that become cooperative later).
- Forgiving: Finite punishments plus clearing conditions allow return to cooperation after mistakes or strategy changes.
- Simple & implementable: Only requires counting cooperations over the last W rounds and a few small counters.

Pseudocode

Initialize:
  W = min(10, max(1, r-1))
  q = 0.5
  P = 2
  γ = 0.4
  p_probe = 0.05
  for each player j: coop_j = 0, punish_j = 0

Round t (for t = 1..r):
  if t == 1:
    play C
    record others’ actions after round
    update coop_j
    continue

  // update coop_j from history of last W rounds before deciding (sliding window)
  compute m_prev = number of cooperators in round t-1
  for each player j:
    compute R_j = coop_j / min(W, t-1)   // use stored sliding counts

  // flag persistent defectors
  for each player j != me:
    if R_j < γ and player j defected more than once in last min(W,t-1) rounds:
      punish_j = P  // reset/refresh punishment timer

  // global-punish check
  if count{j: punish_j > 0} > n/2:
    set global_punish_timer = P

  // Decrement timers from previous round (do this after last round actions in implementation)
  // (timers should be decremented end-of-round in implementation details)

  // Decide action
  if t == r:
    action = D
  else if global_punish_timer > 0:
    action = D
  else if exists j with punish_j > 0:
    action = D
  else if m_prev >= ceil(q * n):
    action = C
  else:
    with probability p_probe:
      action = C
    else:
      action = D

  // After decisions are executed and actions observed:
  // update coop_j counts for sliding window
  // decrement punish_j and global_punish_timer by 1 (if > 0)
  // if any punish_j > 0 and that player's R_j >= γ, set punish_j = 0

Notes and tuning suggestions
- W, P, γ, q, and p_probe are tunable. If you expect more noise, increase forgiveness (longer windows, lower γ, longer P to avoid overreacting to single mistakes). If opponents are highly exploitative, lower p_probe and make punishments slightly less forgiving.
- If you prefer to be more risk-tolerant and try to sustain near-full cooperation, raise q (e.g., 0.6–0.8). If you prefer to be more protective of yourself, lower q or shorten probe.
- Optionally track per-player recent defections more strictly (e.g., consecutive defections) to flag exploiters quicker.
- The strategy is intentionally simple and local (sliding-window), which helps in tournaments where opponents are many and varied.

Summary
ACC is a conditional cooperator that: (1) opens by cooperating to signal willingness, (2) cooperates whenever recent group behaviour indicates a viable cooperative environment (majority cooperated), (3) punishes repeat defectors with short, finite punishments targeted at offenders, (4) forgives and probes occasionally so cooperation can be re-established, and (5) defects on the last round. This balances being cooperative and avoiding exploitation while remaining adaptive to a wide range of opponent behaviours.
'''

description_COOPERATIVE_30 = '''
Strategy name: Reciprocal Gradual with Proportional Forgiveness (RGPF)

High-level idea
- Start cooperative and try to sustain collective cooperation by rewarding cooperation and applying short, proportional punishments when the group is exploited.
- Punishments are targeted to restore cooperation (not to inflict maximal damage) and are forgiving so cooperation can recover.
- The strategy adapts its leniency and punishment length to the game parameters (n, k) so it is more forgiving when the public good is more valuable (k closer to n) and tougher when cooperation is harder to sustain (k small).
- No communication or coordination is assumed; all decisions use only the game parameters and observed history.

Notation and bookkeeping
- r rounds, n players indexed 1..n. I am player i.
- Round index t = 1..r.
- History: for each past round s < t we observe the vector of actions a_j,s ∈ {C,D} for each player j.
- For convenience:
  - coop_count(s) = number of players who played C in round s.
  - coop_count_others(s) = coop_count(s) - 1 if I cooperated in s, else coop_count(s) (i.e., number of other players who cooperated).
  - For an interval of recent rounds S = {t-L,...,t-1} define others_recent_rate = (sum over s∈S sum_{j≠i} 1[a_j,s = C]) / (L*(n-1)), the fraction of “other-player cooperative moves” in that window.
- Internal state:
  - punish_timer (integer ≥ 0): number of remaining rounds I will defect as a punishment. When >0 I defect regardless of other checks (unless early-forgiveness conditions below apply).
  - last_punishment_initiator_round (for bookkeeping; optional).

Parameter choices (deterministic functions of n, k, r)
- Window length for recent behavior: L = min(5, max(1, floor(r/10))). (If r small, uses full history.)
- Generosity factor: g = (k - 1) / (n - 1). This maps k∈(1,n) to g∈(0,1). Higher g means cooperation is relatively more valuable.
- Cooperation threshold (how much recent cooperation by others we require to cooperate): theta = clamp(0.5 - 0.2*g, 0.25, 0.55). (This makes the strategy more forgiving when k is large.)
- Punishment base length: P_base = clamp(ceil((1 - g) * 3), 1, 3). (When k small -> longer punishment; when k close to n -> shorter.)
- Maximum punishment length: P_max = min( max(1, ceil(r/10)), 3).

Decision rules (per round t)

1. First round (t = 1)
- Play C (cooperate). Rationale: signalling willingness to cooperate and giving cooperation a chance to emerge.

2. Last round (t = r)
- Default: play D (defect). Rationale: no future to enforce cooperation. Exception: if every prior round s=1..r-1 had coop_count(s) = n (i.e., perfect cooperation history by everybody), play C as a mutually beneficial “reward” (optional reward only for strict, consistent cooperation by all). This prevents needless exploitation if the whole group has been fully cooperative every round.

3. Intermediate rounds (1 < t < r)
- If punish_timer > 0:
  - By default, play D and decrement punish_timer by 1.
  - Early forgiveness rule: recompute others_recent_rate over last L rounds (ending at t-1). If others_recent_rate ≥ theta_return where theta_return = theta + 0.15 (a stricter threshold for ending punishment early), then cut punish_timer to 0 and cooperate this round instead. Rationale: if others quickly return to cooperative behavior, stop punishing early to restore cooperation.
- Else (not currently punishing):
  - Compute coop_last = coop_count(t-1) (number of players who cooperated in previous round).
  - Compute others_recent_rate over last L rounds.
  - If coop_last == n (everyone cooperated last round): play C.
  - Else if others_recent_rate ≥ theta: play C. (If the recent pattern of others shows sufficient cooperation, rejoin.)
  - Else (others_recent_rate < theta): we judge the group is not cooperative enough and we initiate a proportional punishment:
      - Set defectors_last = n - coop_last (number of defectors in previous round).
      - Choose punishment length P = min(P_max, max(1, P_base + floor(defectors_last / 2))). (Punishment length rises gently with how many defected.)
      - Set punish_timer = P - 1 (we will defect this round and P-1 subsequent rounds). Play D this round.

Additional behaviors and tie-breaking
- If a computed numeric threshold equals exactly the measured rate, favor cooperation (choose C) to bias toward re-establishing cooperation.
- If there is a clear pattern of targeted defectors (a small subset of players always defect while others cooperate), this strategy still punishes via group defection until the group recovers; targeted retaliation is infeasible because moves are simultaneous and public good yields are shared, so this strategy punishes by refusing to contribute until others show sufficient high cooperation rate.
- The strategy never punishes indefinitely; punishments are short and bounded (P_max) to avoid cascades of mutual defection.

Pseudocode (concise)
Initialize punish_timer = 0.
For each round t = 1..r:
  if t == 1:
    play C
    continue
  if t == r:
    if for all s in 1..r-1: coop_count(s) == n:
      play C
    else:
      play D
    continue
  // t in 2..r-1
  compute L = min(5, max(1, floor(r/10)))
  compute others_recent_rate over rounds max(1,t-L) .. t-1
  if punish_timer > 0:
    if others_recent_rate >= theta + 0.15:
      punish_timer = 0
      play C
    else:
      play D
      punish_timer -= 1
    continue
  // not punishing
  coop_last = coop_count(t-1)
  if coop_last == n:
    play C
    continue
  if others_recent_rate >= theta:
    play C
    continue
  // initiate proportional punishment
  defectors_last = n - coop_last
  P = min(P_max, max(1, P_base + floor(defectors_last / 2)))
  punish_timer = P - 1
  play D
  continue

Why this strategy is cooperative
- It starts by cooperating and cooperates whenever the recent history indicates a reasonable probability that others will also cooperate, thereby enabling the high group payoff of mutual cooperation to be realized.
- It punishes defections so that pure defectors do not reap continual benefit without cost; punishment is proportional to the severity of the lapse, which makes punishment credible but not excessively destructive.
- It is forgiving: punishments are short, can be cut short if others quickly return to cooperative behavior, and cooperation is restored as soon as group signals reasonable cooperative intent.
- The strategy adapts to game parameters: it becomes more forgiving and quicker to rejoin cooperation when k is large (cooperation is more valuable), and more disciplinary when k is small (cooperation is harder to sustain).

Robustness properties
- Works without assuming shared norms: decisions use only observable actions and the built-in thresholds.
- Resilient to noise-less but erratic opponents: brief proportional punishments prevent persistent exploitation while forgiveness allows recovery.
- Avoids permanent mutual defection loops because punishments are bounded and early forgiveness is possible.
- Avoids being exploited in the last round by defecting there (except to reward unanimous long-run cooperation).

Practical notes for implementation
- All thresholds and small constants (e.g., the 0.15 margin for early forgiveness, bounds of P_max=3) are tunable; they are chosen to balance enforcement and forgiveness. Tournament implementers can tune them if desired, but they should remain functions of (n,k,r) only.
- The window L trades reactivity vs. stability: small L is more reactive to single-round deviations; larger L smooths over occasional lapses. The chosen L = min(5, floor(r/10)) is conservative and works well across many r.

Summary
RGPF cooperates by default, punishes proportionally and briefly when the group shows insufficient cooperation, and forgives quickly when cooperation resumes. It adapts to the public-good multiplier k and the group size n, aiming to maximize sustained group payoff when others are willing to cooperate, while protecting itself against persistent exploitation.
'''

description_COOPERATIVE_31 = '''
Strategy name: Forgiving Conditional Cooperator (FCC)

Idea (short): Start by showing cooperative intent, then reciprocate group-level cooperation but punish shortfalls gently and allow recovery. Defect in the final round to avoid endgame exploitation. The rule uses only n, r, k and the observed history of past rounds (who cooperated each round).

Key constants (computed from parameters)
- H = min(3, r − 1)  // history window (uses up to 3 most recent rounds)
- q = 0.50           // target fraction of other players cooperating (threshold to cooperate)
- δ = 0.15           // forgiveness slack: allow small transient drops below q without immediate punishment
- Tfinal = 1         // number of final rounds to always defect (set to 1 — the last round)

High-level decision rules
1. Rounds t where t > r − Tfinal (i.e., last round): always defect.
2. Round 1: cooperate (signal willingness to cooperate).
3. For rounds 2..r − Tfinal:
   - Compute cooperation_rate_others = average over the last H rounds of (number of other players who cooperated) / (n − 1).
     - If fewer than H past rounds exist, average over the rounds that do exist.
   - If cooperation_rate_others >= q − δ, then cooperate this round.
   - Otherwise defect this round.
   - Once defecting because cooperation_rate_others < q − δ, continue to defect only until the cooperation_rate_others (recomputed each round using the sliding H-window) recovers to >= q. As soon as the recovery condition holds, switch back to cooperating.

Rationale and properties
- Cooperative mindset: Start by cooperating to encourage reciprocal cooperation. The rule cooperates whenever the recent behavior of the group indicates that a substantial fraction (≥ ~50%) of others are cooperating.
- Reciprocity: The decision depends on observed past cooperations of others; this rewards cooperative groups and punishes persistent defection.
- Forgiveness and noise tolerance: A small slack δ prevents overreaction to single-round dips or occasional rogue defections. The sliding window H = 3 makes the response depend on recent trend rather than a single round.
- Graduated punishment and recovery: Punishment is temporary and proportional to how long the group remains below threshold; the strategy does not use a permanent (grim) trigger, so cooperation can be re-established.
- Endgame awareness: The last round is defected to avoid the standard final-round exploitation. (Tfinal may be increased if desired, but setting Tfinal = 1 is minimal and standard.)
- Robustness: Works well against pure cooperators, conditional cooperators, and many defectors. A population-level explanation: it helps sustain cooperation when a sizeable minority or majority reciprocate, and avoids being exploited indefinitely by pure defectors because it withdraws cooperation when group behavior falls.

Pseudocode

Inputs: n, r, history (list of rounds 1..t−1, each round is vector of actions C/D for players 1..n)
Let i be the index of this player
Constants:
  H = min(3, r − 1)
  q = 0.50
  δ = 0.15
  Tfinal = 1

Function choose_action(t, history):
  if t > r − Tfinal:
    return D            // last-round defection
  if t == 1:
    return C            // open by cooperating
  // compute cooperation rate among other players over the last up to H rounds
  rounds_to_use = min(H, t − 1)
  sum_frac = 0
  for s from t − rounds_to_use to t − 1:
    coop_others = number of players j ≠ i who played C in round s
    sum_frac += coop_others / (n − 1)
  cooperation_rate_others = sum_frac / rounds_to_use
  if cooperation_rate_others >= q − δ:
    return C
  else:
    return D

Notes on tuning
- H (history window): H = 3 is a balance between reactivity and stability. If the tournament environment is highly noisy, increase H to be more stable; if opponents adapt quickly, decrease H to be more reactive.
- q (threshold): q = 0.5 asks for roughly half of others to cooperate. If you want stricter cooperation (only cooperate when many do), increase q toward 0.6–0.8. If you want to be more generous, lower q to 0.4.
- δ (forgiveness slack): protects against one-off drops. Lower δ if you want less forgiveness.
- Tfinal: normally 1 (defect in last round). If tournament experiences often reward last-round cooperation or you expect others also to cooperate in last round, you can experiment with Tfinal = 0, but that increases risk of being exploited.

Examples (intuition)
- All-C opponents: FCC cooperates every non-final round → good group payoff.
- All-D opponents: FCC cooperates in round 1 only, sees cooperation_rate_others = 0, defects thereafter → avoids long exploitation.
- Mixed / conditional opponents: FCC quickly sustains cooperation if a stable core cooperates (cooperation_rate_others >= q). If cooperation slips, FCC withdraws contributions but forgives short, isolated dips.

Why this is suitable for a tournament
- It is simple, transparent, and only depends on publicly observable history and game parameters.
- It rewards and stabilizes reciprocation but does not allow unbounded exploitation.
- It tolerates occasional mistakes and permits cooperation to return after punishment.
- It respects the last-round logic to avoid being exploited unilaterally at the end.

You can implement FCC directly from the pseudocode above. Adjust H, q, δ and Tfinal if you want the agent to be more/less forgiving or more/less demanding about group cooperation.
'''

description_COOPERATIVE_32 = '''
Name: Adaptive Group Reciprocity with Proportional Punishment (AGR-PP)

Intuition (short)
- Start by trying to establish cooperation.
- Cooperate as long as a clear majority of other players have been cooperative recently.
- When cooperation falls below the threshold, punish by defecting for a limited number of rounds proportional to the shortfall, then forgive and try to re-establish cooperation.
- Always defect in the final round (no future to enforce cooperation).
- Use short memory so strategy adapts quickly to opponent behaviour, and keep punishments proportional so we avoid persistent mutual loss.

This strategy depends only on game parameters (n, r, k) and the observed history of actions.

Parameters (set once, can be tuned)
- memory m = min(5, r - 1) — use last m rounds of history (excluding current round).
- cooperation threshold T = 0.5 — require at least a simple majority of others cooperating recently to keep cooperating.
  - (Optional tuning: if you want to bias toward cooperation when k is large relative to n, you may lower T slightly; if you want to be more cautious when k is small, raise T.)
- maxPunish = min(⌈r/3⌉, r - 1) — maximum punishment length.
- forgivenessProbeProb ε = 0.05 — small probability to probe with a cooperation during punishment (optional, randomized forgiveness).
These values are defaults; the algorithm will work with them and they keep behavior robust and adaptive.

Full decision rules
1. Terminal-round rule
   - If current round t = r (final round), play D (defect). There is no future to sustain cooperation.

2. First round
   - Round t = 1: play C (cooperate) to attempt to establish cooperation.

3. For rounds 2 ≤ t ≤ r - 1
   - Maintain a punishment counter punishLeft (initially 0). If punishLeft > 0:
     - Optionally with small probability ε play C as a probe (forgiveness probe); otherwise play D.
     - Decrease punishLeft by 1 after the action.
     - After punishLeft reaches 0 we will evaluate history next round to decide to resume cooperation or re-punish.
   - If not currently punishing (punishLeft = 0):
     - Compute coop_rate = (number of contributions by all other players in the last m rounds) / (m × (n - 1)).
       - If t - 1 < m (not enough rounds), compute coop_rate over the available rounds.
     - If coop_rate ≥ T:
       - Play C (cooperate).
     - Else (coop_rate < T):
       - Set punishment length:
         - rawShortfall = T - coop_rate (in (0,1])
         - punishLeft = min(maxPunish, max(1, ceil(rawShortfall × r))) — i.e., punish at least 1 round and scale the length by the shortfall and remaining horizon.
         - Immediately play D this round (begin punishment).

4. Recovery/forgiveness after punishment
   - After finishing a punishment phase (punishLeft reaches 0), resume cooperation only if recent coop_rate ≥ T (computed with the newest data). If others have returned to cooperation, continue cooperating. If not, re-enter punishment following the same rule above.
   - The optional randomized probe during punishment allows for restarting cooperation even if opponents are slow to resume.

Rationale and properties
- Cooperative: The strategy starts cooperative and only defects in response to insufficient cooperation by others. It therefore attempts to realize the higher group payoff produced by mutual cooperation.
- Reciprocal: It conditions on observed behaviour of others (not on labels or identities). It only punishes when there is evidence a majority is not cooperating recently.
- Proportional punishment: The punishment length grows with the shortfall in recent cooperation (rawShortfall × r) but is capped. This avoids permanent retaliation that destroys long-run gains and also prevents tiny deviations from causing no response.
- Forgiving: After punishment, the player looks for evidence of resumed cooperation and immediately resumes cooperating when others do so. The optional small-probability probe helps recover cooperation if all players are using permissive or noisy strategies.
- Robust to exploitation: By defaulting to D in the final round, the strategy avoids certain last-round exploitation. By punishing defections, it deters persistent exploiters (since persistent mutual defection yields low total payoff).
- Adaptive memory: Using a small memory (m ≤ 5) makes the strategy responsive to changes in opponent behaviour (e.g., if many opponents switch to cooperate, you will resume quickly).
- Parameter sensitivity: T = 0.5 is a simple majority rule. If you observe many opponents who are highly cooperative (k close to n), you can lower T to be more cooperative; if k is small (temptation high), raise T to be more defensive. The default T = 0.5 balances cooperation and safety in mixed tournaments.

Pseudocode

Initialize:
  punishLeft ← 0
  m ← min(5, r - 1)
  T ← 0.5
  maxPunish ← min(ceil(r/3), r - 1)
  ε ← 0.05  (optional; set to 0 for deterministic behavior)

For each round t = 1..r:
  if t == r:
    action ← D
    output action; continue
  if t == 1:
    action ← C
    output action; continue

  if punishLeft > 0:
    with probability ε:
      action ← C  # forgiveness probe
    else:
      action ← D
    punishLeft ← punishLeft - 1
    output action
    continue

  # compute coop_rate among other players over last up to m rounds
  L ← min(m, t - 1)  # number of prior rounds available
  totalOthersCoop ← sum over previous L rounds of (number of other players who played C in that round)
  coop_rate ← totalOthersCoop / (L * (n - 1))

  if coop_rate ≥ T:
    action ← C
    output action
  else:
    rawShortfall ← T - coop_rate  # in (0,1]
    punishLeft ← min(maxPunish, max(1, ceil(rawShortfall * r)))
    action ← D    # begin punishment immediately
    punishLeft ← punishLeft - 1   # we count this round as one punished round
    output action

Notes and special-case handling
- If r is small (e.g., r = 2), the strategy cooperates in round 1 and defects in round 2 (as required by final-round rule). This chooses to try to get a cooperative first round payoff from nice opponents while avoiding guaranteed exploitation in the final round.
- If you prefer a more defensive stance near the endgame, you can choose to treat the last K rounds as terminal-like (e.g., always defect during the last 2 rounds). That reduces cooperative gains but can be safer against strategies that exploit endgame opportunities.
- The randomness ε is optional. Setting ε > 0 makes the strategy more robust when opponents use generous or noisy strategies and helps re-establish cooperation. Set ε = 0 if you require determinism.

Example behavior
- If most players cooperate consistently, coop_rate stays high (≥ T) and the strategy cooperates every round up to r - 1, yielding high group payoffs.
- If many players start defecting, coop_rate drops; the strategy defects and punishes proportionally for a short number of rounds, then checks if others resume cooperation. If they do, the strategy resumes cooperating; if not, it keeps punishing until remaining rounds become few.

Why this is a good tournament entry
- Encourages and stabilizes mutual cooperation where others are willing to reciprocate.
- Limits exploitation by defectors via measurable, proportional punishments.
- Recovers quickly from disturbances because punishments are finite and the strategy is forgiving.
- Works without communication, only on observed history, and adapts to a variety of opponent classes (cooperative, exploitative, conditional, noisy).

You can implement this description directly as an algorithm. Adjust T, m, maxPunish, and ε if empirical tuning against the tournament population suggests different trade-offs between being forgiving and being robust to exploitation.
'''

description_COOPERATIVE_33 = '''
Name: Adaptive Generous Conditional Cooperator (AGCC)

Goal (high level)
- Promote and sustain group cooperation whenever enough other players appear willing to cooperate, but avoid long-term exploitation by defectors.
- Be forgiving (recover from mistakes) yet retaliatory enough to discourage persistent free-riding.
- Use only game parameters (n, k, r) and public history of players’ actions/payoffs.

Design principles
- Start by signaling cooperation (to give cooperative opponents a chance).
- Condition future cooperation on recent observed group cooperation levels (not on identities).
- Use a short memory window so the strategy adapts quickly to changes in opponents’ behaviors.
- Be forgiving: tolerate occasional drops and allow recovery.
- Defect on the final round (no future to reward cooperation).

Parameters used by the strategy (computed from game inputs)
- L = min(5, r − 1): lookback window (uses up to 5 most recent rounds; if fewer past rounds, use those).
- γ = max(0.50, k / n): cooperation-support threshold (minimum fraction of cooperators in recent history the agent wants to see before cooperating). Rationale: require at least majority or the per‑capita marginal multiplier k/n, whichever is larger.
- slack = 1 / (2L): forgiveness slack (small tolerance for occasional drops).
- recovery_len = 3: minimum number of rounds of improved group cooperation required before trying to resume cooperation after a deliberate switch to defection.

State the agent stores
- history: list of observed total cooperators each past round (or equivalently each player’s actions per round).
- my_last_action (C or D).
- defect_since_round: round index of the first round in the current defect streak (or null if currently cooperating).

Decision rules (natural-language + pseudocode)

Summary:
- Round 1: cooperate.
- Rounds 2..r−1: compute recent average fraction of cooperators (excluding only using recorded group counts); if recent cooperation ≥ γ − slack then cooperate; otherwise defect. If we are currently defecting, only return to cooperation after we have observed at least recovery_len rounds within the lookback window where cooperation ≥ γ + slack (i.e., a clear recovery).
- Round r (final): defect.

Pseudocode

Inputs: n, k, r
State maintained across rounds: history (list of total_cooperators per past round), my_last_action, defect_since_round

Function decide_action(round t):
    if t == 1:
        action = C
        my_last_action = C
        return action

    if t == r:
        // No future to reward: defect
        action = D
        my_last_action = D
        defect_since_round = defect_since_round or t
        return action

    // Compute lookback window (most recent up to L rounds)
    consider_rounds = last min(L, t-1) rounds from history
    // For each round in consider_rounds we have total_cooperators (an integer 0..n)
    // compute average fraction of cooperators (including all players)
    avg_frac = (1 / (n * len(consider_rounds))) * sum(total_cooperators for each round in consider_rounds)

    // If currently cooperating, check whether to continue
    if my_last_action == C:
        if avg_frac >= γ - slack:
            action = C           // others are cooperating enough — keep cooperating
            my_last_action = C
            return action
        else:
            // recent cooperation dropped below threshold — switch to defect to avoid exploitation
            action = D
            my_last_action = D
            defect_since_round = t
            return action

    // If currently defecting, we only return to cooperation when there's clear recovery
    if my_last_action == D:
        // Check for recovery signal: require that among the last L rounds,
        // at least recovery_len rounds have fraction_cooperators >= γ + slack
        count_strong_rounds = number of rounds in consider_rounds where (total_cooperators / n) >= γ + slack
        if count_strong_rounds >= recovery_len:
            // Try resuming cooperation (test if others truly recovered)
            action = C
            my_last_action = C
            defect_since_round = null
            return action
        else:
            action = D
            my_last_action = D
            defect_since_round = defect_since_round or t
            return action

Updating history after the round
- After each round, append the observed total_cooperators to history (this is public information).
- Update my_last_action appropriately (already done in decision).

Notes, choices and rationale

1) Why start with cooperation?
   - It signals willingness to form cooperative equilibria. Many independent cooperative strategies also start by cooperating; by doing so AGCC can achieve mutual cooperation with such opponents.

2) Why defect on the final round?
   - In a finite repeated game without commitment, last-round cooperation cannot be rewarded; defecting in the final round is the individually dominant choice. AGCC takes that rational action to avoid being exploited on the final move. (If you prefer to keep the possibility of cooperation in the last round to preserve group total payoff against sufficiently cooperative opponents, you could alter this, but that risks exploitation.)

3) Why condition on recent group cooperation fraction (not individuals)?
   - The game is symmetric and players cannot communicate or form binding agreements; conditioning on group-level statistics is robust to permutations of opponents and to the presence of newcomers or occasional noisy players.

4) Why γ = max(0.50, k/n)?
   - If a majority cooperates, the social signal is strong enough to sustain cooperation. If k/n is relatively large (per-capita return of a contribution is high), it makes sense to be more tolerant (i.e., accept slightly lower observed rates) because the cooperative return is larger. This ties responsiveness to the marginal return parameter k. The 0.50 floor ensures we don’t try to cooperate when less than half the group is cooperating unless k/n is already large.

5) Forgiveness & recovery
   - slack and recovery_len prevent permanent collapse after a single accidental defection or after a short opportunistic drop. They provide a clear, observable recovery signal before AGCC resumes cooperation.

6) Lookback L
   - A short window keeps the strategy adaptive rather than locking into long grudges. L = 5 was chosen as a balance: responsive but not hypersensitive. If r is small, L scales down.

7) Robustness
   - AGCC works reasonably against many opponent types:
     - Unconditional cooperators: AGCC will reach and maintain mutual cooperation.
     - Unconditional defectors: AGCC quickly defects and avoids continual exploitation.
     - Conditional cooperators / tit-for-tat types: AGCC’s initial cooperation and reciprocity rules allow mutual cooperation to emerge.
     - Noisy environments: forgiveness slack and short memory reduce lock-ins from random mistakes.

Variants and tunable options (implementation choices)
- If implementer prefers randomized forgiveness, replace deterministic recovery test with probabilistic re-entry to cooperation when avg_frac is near threshold.
- The constants (L = 5, slack = 1/(2L), recovery_len = 3) can be tuned by tournament experience; smaller slack = less forgiveness, larger recovery_len = slower to resume cooperation.
- If the implementer wants a less exploit-prone variant in short games, they can defect in last two rounds instead of only the last round (this is more conservative but reduces possible exploitation near the end).

Short worked example (n=6, k=2, r large)
- γ = max(0.5, 2/6 = 0.333) = 0.5. Start cooperating round 1.
- If in recent rounds at least half the group cooperated on average, continue to cooperate; if the observed fraction falls below 0.5 persistently, switch to defect but return to cooperate if cooperation recovers strongly for several rounds.

Final remark
The strategy is explicit, depends only on (n, k, r) and public history, and balances cooperative intent with protections against exploitation. It is designed for implementation in tournament environments where opponents are heterogeneous and potentially adaptive.
'''

description_COOPERATIVE_34 = '''
Name: Adaptive Threshold Reciprocity with Forgiveness (ATRF)

Overview (informal)
- Start by signalling cooperation. Try to sustain high group cooperation by cooperating when enough others have been cooperating recently.
- If cooperation breaks down (too few cooperators), punish briefly by defecting to discourage free-riding.
- Be forgiving and error-tolerant: revert to cooperation when others return to cooperating.
- In the final round, play a best-response to the empirical behaviour observed (so you are not exploited in the obvious endgame), rather than mechanically cooperating in a guaranteed-loss final move.
- All decisions use only the game parameters (n, k, r) and the publicly observed history (who cooperated in each past round).

State the strategy maintains
- For each player j (including self), track total times j cooperated and recent cooperation counts.
- Maintain the global recent cooperation history: for the last W rounds, how many cooperated each round.
- A small “punishment counter” when the strategy is in a punishment phase.

Tunable constants (recommended defaults)
- W (recent-window) = min(5, r) — smooths noise and short-term fluctuations.
- P (punishment length) = 1 or 2 rounds (default 2) — short, so punishment is effective but not permanently destructive.
- F (forgiveness confirmation) = 2 rounds — require F rounds of improved cooperation to stop punishing.
- baseline_threshold = 0.5 (majority). Adjust this by the per-round cooperation cost/benefit:
    threshold = clip(baseline_threshold + 0.4*(1 - k/n), 0.5, 0.95)
  Explanation: when k/n is close to 1 cooperation is less costly and we are more lenient (threshold near 0.5). When k/n is small we demand stronger evidence of group cooperation before risking cooperating.
- Exploitation-window L = max(3, ceil(r/5)) — if long-run cooperation rate falls below a low exploitation threshold, switch to safe mode (see below).

Decision rules (per round t)
Input available at start of round t: parameters n, k, r and full history H = actions of all players in rounds 1..t-1.

1) If t = 1 (first round): Cooperate.
   Rationale: signal cooperative intent; many opponents reciprocate initial cooperation.

2) Before choosing, compute recent statistics:
   - For each past round s in {max(1,t-W) ... t-1}, let m_s = number of players who cooperated in round s.
   - Let recent_mean = mean_s (m_s / n) (fraction of players cooperating on average over the recent window). If t-1 < 1 then recent_mean = 1 (so first round cooperates).
   - Let last_round_fraction = (m_{t-1} / n) if t>1 else 1.
   - Let longrun_fraction = total_cooperations_by_all_players / ((t-1) * n) (overall fraction so far), defined 1 if t=1.
   - Maintain punishment_counter which when >0 forces defect this round and is decremented at end of the round.

3) If punishment_counter > 0: Defect this round.
   - After taking the action, decrement punishment_counter by 1.
   - Continue to step 6 (update state after outcome).

4) Otherwise (punishment_counter == 0) decide whether to cooperate or defect:
   - Core cooperation test (short-term reciprocity):
       If last_round_fraction >= threshold:
           Cooperate (others cooperated last round sufficiently).
       Else
           Enter punishment: set punishment_counter := P (punishment length) and Defect this round.
   Rationale: if a strong fraction cooperated in the last round we reciprocate; if not, we respond by defecting briefly to discourage free-riding.

5) Final-round exception (t = r):
   - Rather than mechanically following the above, make a final-round best-response to the empirical behaviour:
       - Estimate expected number of other cooperators this final round by recent_mean * (n-1) (or by last_round m_{r-1} - include weight on last round if desired).
       - Compute expected payoff if you Cooperate vs Defect assuming others behave as estimated:
           Let est_others = round(expected number of other cooperators).
           π_if_C = 0 + (k/n)*(est_others + 1)
           π_if_D = 1 + (k/n)*est_others
       - Choose the action (C or D) that yields larger immediate payoff. If equal, choose C to remain cooperative.
   Rationale: the last round has no future so choose the action that maximizes the immediate payoff against the expected behaviour of others.

6) Exploitation detection and safe mode:
   - If longrun_fraction < min( threshold - 0.1, 0.25) for at least L rounds (i.e., the group has persistently cooperated very little), switch to Safe Mode: always defect for the remainder of the game (this prevents long-run exploitation).
   - Exit Safe Mode only if over a recovery window of at least F consecutive rounds the group fraction >= threshold again; then resume normal mode.
   Rationale: avoid being endlessly exploited by many defectors while allowing recovery.

7) Forgiveness:
   - If you are in punishment (punishment_counter > 0 or just finished punishment), monitor the next F rounds: if in those F rounds recent_mean (computed at each step) >= threshold_for_forgiveness where threshold_for_forgiveness = threshold - 0.05 (slightly easier), then clear punishment_counter (set 0) and resume cooperation.
   Rationale: allow recovery from accidental defection and be forgiving when others return to cooperative behaviour.

8) Per-player nuance (optional enhancement):
   - If one or a few players are persistent defectors but the majority cooperates, you can continue cooperating as long as recent_mean >= threshold. If a small set of persistent defectors cause recent_mean to fall but overall others are cooperative, punishment will be short and forgive quickly. This helps sustain cooperation when a small number of “antisocial” players exist.

Pseudocode (concise)
Initialize: punishment_counter = 0, safe_mode = false
For each round t = 1..r:
  compute m_s for s in recent window; recent_mean, last_round_fraction, longrun_fraction
  if safe_mode:
    action := D
    if recent_mean >= threshold for F consecutive rounds:
      safe_mode := false
  else if t == 1:
    action := C
  else if t == r:  // final round: best-response to estimated others
    est_others := round(recent_mean * (n-1))
    if 0 + (k/n)*(est_others + 1) >= 1 + (k/n)*est_others:
      action := C
    else
      action := D
  else if punishment_counter > 0:
    action := D
    punishment_counter := punishment_counter - 1
  else if last_round_fraction >= threshold:
    action := C
  else:
    // start punishment
    punishment_counter := P - 1   // we defect now and will defect P-1 more rounds
    action := D
  // take action, observe actions for round t, update counts, longrun_fraction etc.
  if longrun_fraction < min(threshold - 0.1, 0.25) for L rounds:
    safe_mode := true

Why this strategy is cooperative and robust
- Cooperative-first: starts by cooperating and reciprocates cooperation from others, encouraging collective high payoffs.
- Responsive: uses recent group behaviour to respond quickly to defections and recover quickly when others return to cooperation.
- Forgiving: punishment is short and followed by explicit forgiveness conditions, reducing vulnerability to noise and accidental defection.
- Adaptive to parameters: the threshold depends on k/n so the strategy demands more visible cooperation when the public-good multiplier is small (cooperation is relatively less attractive), and relaxes demands when cooperation is more beneficial.
- Final-round prudence: avoids being trivially exploited in the inevitable endgame by using an empirical best-response, but still cooperates in the final round when there is evidence that others will cooperate.
- Safe against exploitation: when the group is persistently uncooperative, the strategy protects itself by switching to safe mode to avoid long-run losses.

Implementation notes / parameter tuning
- W, P, F, and threshold adjustment constants are tunable for the tournament. Default values above are chosen to balance responsiveness and stability in noisy environments.
- If the tournament has very noisy players (random errors), increase W and use smaller P and more generous forgiveness.
- If opponents are mostly short-term exploiters, reduce forgiveness and shorten cooperation windows.
- Optionally add small stochastic cooperation (cooperate with small probability ε when borderline) to avoid deterministic exploitation by certain strategies; but deterministic ATRF already includes conditional forgiveness to deal with noise.

Summary
ATRF is a conditional-cooperation rule that:
- opens with cooperation,
- reciprocates when the recent group cooperation fraction is above an adaptively chosen threshold (dependent on k and n),
- punishes briefly when cooperation drops,
- forgives when others recover,
- in the final round chooses the immediate best-response to estimated behaviour to avoid simple endgame exploitation,
- switches to safe-mode if the group is persistently uncooperative.

This design aims to sustain high group payoffs when many players are willing to reciprocate, while being robust against persistent defectors and accidental errors.
'''

description_COOPERATIVE_35 = '''
Strategy name: Forgiving Proportional Trigger (FPT)

Short description
- Start cooperatively.
- Cooperate as long as past rounds show near-universal cooperation.
- If a meaningful shortfall of cooperators appears, punish by defecting for a proportionate, finite number of rounds, then forgive and return to cooperation.
- Near the end of game (when there are not enough remaining rounds to make punishment credible) switch to unconditional defection (endgame).

This strategy depends only on (n, k, r) and the observed history of who cooperated each past round.

Key intuition and design choices
- Mutual cooperation (everybody C) yields a per-round payoff k (>1) which dominates mutual defection (payoff 1) in the long run, so the goal is to sustain high cooperation.
- A one-shot deviation from mutual cooperation gives an immediate gain g = 1 − k/n. To deter deviations we must threaten a finite future cost. The minimum punishment length required (in rounds), when the punishment entails loss (k − 1) per punished round, is roughly
  base_punish = ceil( (1 − k/n) / (k − 1) ).
  (If k is close to 1 this number can be large; if k is close to n it is small.)
- Because this is a finite-horizon game, punishments must be credible: if too close to the final rounds there is no future to punish. Therefore if remaining rounds are ≤ base_punish we defect (endgame).
- Punishment is finite, proportional and forgiving: we punish for a number of rounds that grows with the number of defectors observed (so punish more when more players deviate), then immediately return to cooperation. We also tolerate a single isolated defection (forgiveness) to avoid brittle cycles caused by single deviations.

Full decision rules (natural language)
Precompute:
- base_punish = max(1, ceil( (1 − k/n) / (k − 1) )). (If k very near 1 this can be large; take at least 1.)
State:
- punishment_timer (integer, initially 0): rounds remaining that you will defect as punishment.

Each round t (1 ≤ t ≤ r) do:
1. If t = 1:
   - Cooperate (no history yet).
2. Else (t > 1):
   - Let H = r − t + 1 be the number of rounds remaining including current.
   - If H ≤ base_punish:
       - Endgame: defect (you cannot credibly enforce punishments with so few rounds left).
   - Else if punishment_timer > 0:
       - Defect this round and set punishment_timer := punishment_timer − 1.
   - Else (no active punishment and not endgame):
       - Let S_prev = number of cooperators observed in the previous round (round t − 1).
       - If S_prev ≥ n − 1:
           - Cooperate (either everyone cooperated or only one defected — we forgive single isolated defection).
       - Else (S_prev ≤ n − 2):
           - Compute shortfall s = n − S_prev (number of defectors observed last round, at least 2).
           - Set punishment_timer := min(H − 1, base_punish × s).
             (We cap the timer by H − 1 so we never try to punish past the end of the game.)
           - Defect this round (first round of punishment) and decrement punishment_timer by 1 immediately (since we've used one of the punishment rounds).
3. After a punishment period completes, the next non-punishment opportunity you will cooperate (forgive) and resume the above rules.

Pseudocode

Initialize:
  base_punish = max(1, ceil((1 - k/n) / (k - 1)))
  punishment_timer = 0

For each round t = 1..r:
  H = r - t + 1  # rounds remaining including this one

  if t == 1:
    action = C
  else if H <= base_punish:
    # endgame: defect because punishments are not credible
    action = D
  else if punishment_timer > 0:
    action = D
    punishment_timer -= 1
  else:
    S_prev = number_of_cooperators_in_round(t - 1)
    if S_prev >= n - 1:
      action = C
    else:
      s = n - S_prev  # number of defectors observed last round (>=2 here)
      punishment_timer = min(H - 1, base_punish * s)
      # take one punishment round now
      action = D
      punishment_timer -= 1

Return action

Additional implementation notes and corner cases
- Last round (t = r): H = 1; since H ≤ base_punish you will defect. This is consistent with backward induction for finite horizon; but the strategy still enables cooperation in earlier rounds where punishment is credible.
- If r is small and base_punish ≥ r, the algorithm will defect from the start (it recognizes that credible punishment is impossible given the short horizon).
- The forgiveness for a single defection (S_prev ≥ n − 1) prevents destructive cycles triggered by isolated mistakes or one-off weird strategies. If you want less tolerance you can lower the forgiveness threshold to S_prev = n.
- The punishment is proportional to the observed shortfall (number of defectors), so coordinated multi-player deviation is punished more heavily than a single defection.
- Because punishment length is finite and capped by remaining rounds, we avoid endless grim-trigger-style permanent retaliation that is fragile to mistakes.
- This strategy requires only the public history of who cooperated each round (perfect monitoring is assumed by the game spec).

Why this is cooperative and robust
- Cooperative: It begins cooperating and returns to cooperation after finite punishments; it aims to sustain full or near-full cooperation when the horizon is long enough for punishments to be credible.
- Robust to many opponent behaviors:
  - It deters persistent exploitation because deviations trigger proportional punishment whose length is set to be sufficient (given parameters) to make a one-shot gain unattractive.
  - It is forgiving to single isolated defections (reducing the likelihood of long mutual punishment cycles) and quickly returns to cooperation.
  - It adapts to the size of the deviation: more defectors => longer punishment, so it reacts appropriately to mass defections.
  - It recognizes endgame limits and plays defect near the end when enforcement is impossible, avoiding being exploited by opponents who always defect at the end.
- Simple to implement and requires only game parameters and observed history.

Possible small variants (tunable)
- Change forgiveness: require full unanimity (S_prev = n) before cooperating again (harsher), or require majority S_prev ≥ ceil(n/2) (more tolerant).
- Scale punishment nonlinearly with shortfall (e.g., quadratic) if you want stronger deterrence for coordinated attacks.
- Add a small randomization in the first post-punishment cooperative round (cooperate with high probability) to reduce deterministic retaliation cycles in tournaments with mixed strategies.

This strategy balances enforceability, forgiveness and proportionality; it is practical and adaptable across a wide range of (n, k, r) values and opponent behaviors.
'''

description_COOPERATIVE_36 = '''
Overview (goal)
- Start by signalling willingness to cooperate.
- Cooperate when the group shows sufficient recent cooperation.
- If the group falls below a cooperation threshold, apply a short, proportional punishment sequence (defect) to deter freeloaders.
- Use occasional probabilistic forgiveness (probing) to restore cooperation quickly after punishment and to avoid endless retaliation.
- Protect yourself in the literal last round (no future) except as a reward if cooperation has been stable immediately beforehand.
All decisions depend only on (n, k, r) and the public history of actions.

Key design choices and rationale
- Threshold rule: cooperate if at least a fraction theta of players cooperated last round. Theta is adaptive to k: when the public-good multiplier k is large, cooperation is more valuable and we require a stronger signal before cooperating; when k is small (but > 1) we use a conservative minimum threshold of 50%.
- Limited punishment: punish for a short number of rounds (punish_len) rather than forever (avoids long mutually-damaging wars).
- Probabilistic forgiveness (probe): after punishment we sometimes cooperate to test whether others will return to cooperation. This helps re-establish cooperation with forgiving opponents and avoids permanent cycles.
- Endgame protection: by default defect in the last round (no future), but reward persistent groups that have been fully cooperative in the immediate past M rounds.
- Contrition: if you were the only defector last round (or one of very few), be willing to cooperate (contrite action) to re-establish cooperation quickly.

Concrete parameter formulas (implementer can tune)
- theta_frac = clamp(1 - 1/k, 0.5, 0.9)
  - clamp(x,a,b) = min(max(x,a),b)
  - Intuition: 1 - 1/k increases with k. Bound between 0.5 and 0.9 to keep behavior sensible.
- threshold_count = ceil(theta_frac * n)
- M = min(3, max(1, floor(r/10)))   // consecutive fully cooperative rounds needed to reward in final round; at least 1, up to 3
- punish_len = min(5, max(1, ceil(0.1 * r))) // short punishment proportional to total rounds, but ≤ 5
- p_probe = 0.20 // probability to probe (forgiveness) after a punishment period
These numbers balance firmness and forgiveness; you can change them, but the strategy logic should remain.

State your strategy maintains
- punishment_timer: number of remaining punishment rounds (initially 0)
- last_probe_round: last round you performed a probe (to avoid probing every round)
- last_action: what you played last round (C or D)

Decision rules (natural language)
At the start of each round t = 1..r do:
1. If t == 1:
   - Play C (signal cooperation).
2. Else if t == r (final round):
   - If the previous M rounds exist and in each of those rounds all n players played C, then play C (reward persistent full cooperation).
   - Otherwise play D (no future to enforce cooperation).
3. Else (1 < t < r):
   - If punishment_timer > 0:
       - Play D and decrement punishment_timer by 1.
       - After punishment_timer becomes 0, schedule a probe opportunity (see probing below).
   - Else (not in an active punishment):
       - Let m_prev = number of players who played C in round t-1.
       - If m_prev >= threshold_count:
           - Play C (group showed enough cooperation).
       - Else if (m_prev >= threshold_count - 1) AND (you played D last round but many others cooperated):
           - Play C (contrition: if you were a lone/small-number defector, be willing to return to cooperation).
       - Else:
           - Enter punishment: set punishment_timer = punish_len - 1 (you will play D now plus remaining punish_len-1 rounds) and play D now.
   - Probing (after punishment_timer reaches 0): with probability p_probe, in the next non-punishment round play a probe C to see if others respond; otherwise stay with threshold rule.

Pseudocode (compact)
- Inputs: n,k,r, history (list of rounds; each round r_j is array of n actions {C,D})
- Initialize: punishment_timer = 0, last_probe_round = -inf
- Compute parameters:
    theta_frac = clamp(1 - 1/k, 0.5, 0.9)
    threshold_count = ceil(theta_frac * n)
    M = min(3, max(1, floor(r/10)))
    punish_len = min(5, max(1, ceil(0.1 * r)))
    p_probe = 0.20
- For current round t:
    if t == 1:
        action = C
    else if t == r:
        if t-1 >= M and for all s in {t-M,...,t-1}: number_of_C(s) == n:
            action = C
        else:
            action = D
    else:
        if punishment_timer > 0:
            action = D
            punishment_timer -= 1
        else:
            m_prev = number_of_C(t-1)
            my_last = my action in round t-1
            if m_prev >= threshold_count:
                action = C
            else if (m_prev >= threshold_count - 1) and (my_last == D):
                action = C   // contrition
            else:
                // start punishment
                action = D
                punishment_timer = punish_len - 1
        // probing: only consider a probe immediately after punishment_timer reached 0
        if punishment_timer == 0 and last_probe_round < t-1 and random() < p_probe:
            action = C
            last_probe_round = t

Notes about robustness
- Starts cooperative to create opportunities to reach high-payoff mutual cooperation with reciprocal opponents.
- Punishment is short and predictable so it deters persistent defectors but does not escalate into permanent losses when mixed groups exist.
- Probing (forgiveness) prevents permanent deadlocks and allows resumption of cooperation if opponents react positively.
- Contrition avoids “I defected once so we all war forever” scenarios and speeds recovery after accidental or opportunistic defects.
- Endgame protection prevents guaranteed exploitation in the last round but still rewards groups that maintained full cooperation immediately before the final round.
- Adaptive theta_frac uses k: when k is larger (cooperation yields higher return), the strategy requires a bigger signal of group cooperation before continuing, reducing exploitation risk.

Edge cases and small-r adjustments
- Very small r (r = 2 or 3): punish_len and M clamp to small values so the strategy behaves conservatively (you can still be exploited by final-round defectors but you reduce losses by defecting in the final round unless full stability is proven).
- If many players always defect, the strategy will quickly detect insufficient m_prev and settle into occasional probing and mostly defection to avoid being exploited.
- If many players always cooperate, this strategy will mostly cooperate and thus obtain near-social-optimum payoffs.
- If the tournament has a mix of reciprocators and exploiters, the combination of short punishment and probes is designed to maximize payoff vs. reciprocators while limiting exploitation by freeloaders.

Implementation tips
- Use the public action history to compute m_prev and the M-round full-cooperation check.
- Track punishment_timer and last_probe_round per match.
- Use a proper pseudorandom generator for p_probe draws.
- Tune punish_len and p_probe conservatively if you find you are being exploited too often or trapped in long mutual defections.

Final summary (one-line)
Cooperate initially; continue cooperating when the group shows a strong cooperative signal (threshold adaptive to k), punish briefly when the group falls below the threshold, use occasional probabilistic probes (forgiveness) to restore cooperation, be contrite when you were the lone defector, and default to defection in the last round except to reward immediately sustained full cooperation. This balances a cooperative mindset, deterrence of freeloaders, and robustness to varied opponent behaviors.
'''

description_COOPERATIVE_37 = '''
Strategy name: Adaptive Conditional Cooperator with Forgiveness (ACCF)

Short description / intuition
- Start by trying to cooperate and aim to sustain high group cooperation by conditionally cooperating when others do.  
- If you are clearly exploited (you cooperated while many others defected and you obtained a lower payoff than a defector would have gotten), punish for a short, fixed probation.  
- After punishment, forgive gradually: adapt your propensity to cooperate toward the observed group cooperation rate.  
- Always defect in the final round (no credible future punishment there).  
This is simple, uses only the known parameters and the observable history (counts of cooperators each round and your own payoff), and balances being cooperative, retaliatory and forgiving.

Parameters used by the strategy (computed from game parameters or set as defaults)
- n, r, k — game parameters (given).
- alpha ∈ (0,1] — learning / forgiveness rate (suggested default 0.3). Higher alpha = faster adaptation/forgiveness.
- punish_fraction ∈ (0,1] — fraction of remaining rounds used for a deterministic punishment after a clear exploitation (suggested default 0.25).
- exploitation_margin — a small positive tolerance to avoid punishing for tiny payoff noise (suggested default 0.001).
Notes: implementers may tune alpha and punish_fraction. Reasonable defaults above work across many n, k, r.

State variables maintained across rounds
- p ∈ [0,1] — current cooperation propensity (initially 1.0, meaning strong willingness to cooperate).
- punish_counter (integer ≥ 0) — number of remaining rounds we will play deterministic D as punishment (initially 0).
- last_m (integer) — number of cooperators in the last round (initially undefined).
- last_payoff (float) — your payoff in last round (initially undefined).

High-level decision rules (when exactly to cooperate vs defect)
1. Last-round rule: If current round t == r (the final round), play D.
   - Rationale: no future punishment is possible in a finite game; cooperating on t = r is exploitable with no enforcement.

2. Punishment rule: If punish_counter > 0, play D and decrement punish_counter by 1. Do not update p while in deterministic punishment rounds (except decrementing the counter).
   - Rationale: short, predictable punishment is an enforceable deterrent and makes behavior interpretable to others.

3. Otherwise (not in last round and not in punishment):
   - Probabilistic variant: cooperate with probability p, defect with probability 1 − p.
   - Deterministic variant (if randomness should be avoided): play C if p ≥ 0.5, else play D.
   - Rationale: using a graded propensity rather than unconditional TFT makes the strategy forgiving and robust to occasional mistakes by others.

How history updates p and triggers punishment (observations at the end of each round)
After each round t (except after final round where you do not need to update future decisions):

A. Observe m_t = number of cooperators in round t and your payoff π_t.

B. Check for exploitation signal:
   - You can compute the payoff you would have obtained by defecting given others' choices that round:
     - If you played C in round t, then the number of other cooperators was m_t − 1. A defector’s payoff in that realized profile would have been π_D = 1 + (k/n) × (m_t − 1).
     - If your actual payoff π_t < π_D − exploitation_margin, treat this as a clear exploitation event (you cooperated but would have been better off by defecting by more than the small margin).
   - If exploitation detected:
     - Set punish_counter = max(1, round(punish_fraction × (r − t))) — i.e., a small number of forthcoming rounds proportional to remaining game length (at least 1).
     - Reduce cooperation propensity p ← p × 0.5 (or p ← max(0, p − 0.5) equivalently). This ensures immediate visible retaliation and lowers future cooperation probability.
     - Do not update p using the normal learning step for this round (punishment takes precedence).
   - Rationale: punish only when you were demonstrably exploited; if you were one of many cooperators but still reasonably benefited, do not punish.

C. If exploitation not detected:
   - Update p toward observed group cooperation rate:
     - Let observed_rate = m_t / n.
     - p ← p + alpha × (observed_rate − p).
     - (Clip p to [0,1].)
   - Rationale: this moves your cooperation propensity gradually toward the actual cooperation level you are seeing, so you rise with cooperators and retreat when group cooperation collapses.

Edge cases and special considerations
- First round (t = 1): p is initialized to 1. So you start by cooperating (or cooperating with probability 1). This signals cooperative intent and gives the group a chance to coordinate without precommitments.
- Last round (t = r): always defect. This acknowledges the finite-horizon endgame where cooperation cannot be enforced.
- When r is very small (e.g., r = 2 or 3): punish_fraction × (r − t) may produce very small punish_counter; that’s intended — punishments should be short when few rounds remain.
- If many players defect persistently: p will adapt downward toward the observed low cooperation rate, so you will not waste rounds contributing into a mostly defecting group.
- If cooperation resurges among others: p will increase toward that level (forgiving).
- If a single round has a small anomalous drop (noise or isolated mistake): because of the alpha update and the exploitation_margin, the strategy tends to forgive small slips instead of triggering permanent retribution.
- Deterministic vs probabilistic choice: randomness helps avoid lock-step defection cycles between retaliatory players; if deterministic actions are required, use p ≥ 0.5 as the decision rule.

Pseudocode (probabilistic variant)
Initialize:
  p ← 1.0
  punish_counter ← 0

For each round t = 1..r do:
  if t == r:
    action ← D
  else if punish_counter > 0:
    action ← D
    punish_counter ← punish_counter − 1
  else:
    action ← C with prob p, else D

  play action; observe m_t (number of cooperators), my_payoff π_t, and record whether I played C last round etc.

  if t == r:
    break (no further updates needed)

  if I played C this round:
    other_cooperators ← m_t − 1
    payoff_if_defected ← 1 + (k/n) × other_cooperators
    if π_t < payoff_if_defected − exploitation_margin:
      // exploitation detected
      punish_counter ← max(1, round(punish_fraction × (r − t)))
      p ← max(0, p × 0.5)
      continue to next round (skip normal update)
  // no exploitation
  observed_rate ← m_t / n
  p ← p + alpha × (observed_rate − p)
  clamp p to [0,1]

Deterministic variant: replace the probabilistic draw with action ← C if p ≥ 0.5 else D.

Why this strategy is cooperative and robust
- Cooperative: It starts cooperative, and it cooperates as long as the group shows cooperation. It increases cooperation propensity when it observes many cooperators.
- Disciplined: Clear, predictable punishment after being clearly exploited deters persistent free-riding—punishment length scales with remaining rounds so it is meaningful early in the game and short near the end.
- Forgiving and adaptive: Uses a learning rate alpha to move p toward observed cooperation; punishment reduces p but future cooperation restores it. This avoids permanent stalemates and tolerates occasional mistakes/noise.
- Parameter-light and observable-history-only: Uses only n, r, k and past round outcomes (counts and payoffs), so implementable under the stated information assumptions.
- Robust to diverse opponent types: Works against unconditional defectors (reduces wasted cooperation), conditional cooperators (reciprocates and fosters mutual cooperation), and stochastic players (adapts smoothly).

Implementation notes
- Choose alpha moderately (0.2–0.4) for a balance of responsiveness and stability. Larger alpha is faster to follow others but may overreact to noise.
- Choose punish_fraction smallish (0.2–0.4) so punishments are meaningful but not so long that recovery is unlikely.
- exploitation_margin should be small (e.g., 1e-3) unless numeric noise is large.
- If randomness is not permitted, use the deterministic threshold p ≥ 0.5 to decide cooperation; to reduce tie cycles you can break ties in favor of cooperation (i.e., C if p ≥ 0.5).

Summary
ACCF is a “start-cooperative, retaliate-on-clear-exploitation, forgive-and-adapt” strategy. It is easy to implement, interpretable by other agents, and balances cooperative aims with robustness against opportunists. It exploits repeated interaction (except last round) to sustain cooperation while avoiding persistent exploitation.
'''

description_COOPERATIVE_38 = '''
Strategy name: Optimistic Threshold with Proportional Punishment and Forgiveness (OT-PPF)

High-level idea
- Start cooperative and give opponents the benefit of the doubt.
- Continue cooperating when a clear fraction of other players have been cooperating recently.
- If others fall below a cooperation threshold, respond with a short, proportional punishment (defect for a few rounds) to make defection costly for them.
- Be explicitly forgiving: probe to re-establish cooperation after punishment and reduce punishment length toward the end of the game.
- If one-shot incentives already make cooperation strictly better (k/n > 1), always cooperate.

This strategy only uses the game parameters (n, r, k) and the public history of actions, and adapts to observed opponent behavior.

Detailed decision rules

1) Trivial cooperative regime
- If k/n > 1: cooperate every round. (Cooperating is a strict individual payoff improvement in every single round.)

2) Reciprocal regime (k/n ≤ 1)
Definitions and internal state:
- punish_counter (integer ≥ 0): how many remaining rounds you will defect as a punishment; initial 0.
- last_actions[t] (for t < current round): observed actions of all players in previous rounds (public history).
- Let others_coop_count(t') = number of players other than you who played C in round t'.
- Let coop_fraction_window = average over a recent window W of rounds of (others_coop_count / (n-1)).
- Window size W = min(5, current_round - 1). (Use as many recent rounds as available, up to 5.)
- Thresholds:
  - θ_coop = 0.50 (require at least half of the other players to be cooperating recently to treat the community as cooperative).
  - θ_low = 0.40 (if community cooperation is below this, treat as clear defection).
- Punishment sizing:
  - remaining_rounds = r - current_round + 1 (including the present round).
  - base_punish = max(1, floor(remaining_rounds * 0.15)). (Punishment scales with remaining horizon but is modest.)
  - If coop_fraction_window < θ_low, set punish_counter = min(max_punish, ceil(base_punish * (θ_low - coop_fraction_window) / θ_low)),
    where max_punish = max(1, floor(remaining_rounds / 3)).
  - Practically: small downward deviations lead to 1–2 rounds of punishment; larger, sustained defection leads to longer but bounded punishment. Punishment shrinks near the end of the game.

Round-by-round decision (for current round t):
- If k/n > 1: play C.
- Else (k/n ≤ 1):
  1. If t == 1: play C (start optimistic).
  2. If t == r (last round): play D. (Backward induction: last round cooperation cannot be enforced unless k/n > 1.)
  3. If punish_counter > 0: play D this round and decrement punish_counter by 1.
  4. Else compute coop_fraction_window over the last W rounds.
     - If coop_fraction_window ≥ θ_coop: play C (reward cooperation).
     - Else if coop_fraction_window < θ_low:
         - Set punish_counter as above (proportionate to how far below θ_low).
         - Play D this round (begin punishment).
     - Else (θ_low ≤ coop_fraction_window < θ_coop):
         - Forgive and play C (be generous when community cooperation is moderate).
  5. After any punishment finishes, always perform a one-round cooperative probe (play C) and then reassess the coop_fraction_window including the probe result. If others have improved to ≥ θ_low you return to cooperation; otherwise, re-initiate punishment with a slightly longer punish_counter (up to max_punish).

Optional stochastic forgiveness (if implementations allow randomness)
- In the intermediate zone (θ_low ≤ coop_fraction_window < θ_coop), instead of deterministic C you may play C with high probability p = 0.9 and D with probability 0.1 to hedge against exploitation while signaling willingness to cooperate. This increases robustness against pure exploiters while keeping the cooperative signal clear.

Pseudocode (concise)

Initialize punish_counter = 0.
For each round t = 1..r:
  if k/n > 1:
    play C
    continue
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue
  if punish_counter > 0:
    play D
    punish_counter -= 1
    continue
  W = min(5, t-1)
  coop_fraction_window = average_{u=t-W..t-1} (others_coop_count(u) / (n-1))
  if coop_fraction_window >= 0.50:
    play C
  else if coop_fraction_window < 0.40:
    remaining = r - t + 1
    base_punish = max(1, floor(remaining * 0.15))
    max_punish = max(1, floor(remaining / 3))
    punish_counter = min(max_punish, ceil(base_punish * (0.40 - coop_fraction_window) / 0.40))
    play D
  else:
    play C  // forgiving region; optionally play C with p=0.9, D with p=0.1

Why this design is cooperative and robust
- Cooperative: the strategy starts cooperative, rewards visible cooperation, and uses modest, proportional punishment to deter and correct free-riding. The forgiveness and probing ensure the strategy returns to cooperation quickly after problems are resolved.
- Robust: uses only public history (no assumptions about norms), adapts punishment length to how severe and persistent defection is, and shortens punishments near the end of the game to avoid wasting rounds when little future influence remains.
- Exploit-resistant: will punish sustained defection. The punishment is proportional (so not overly harsh, preventing permanent mutual defection) and escalates if opponents persist.
- Tournament-friendly: will earn high payoffs against cooperators by cooperating, and will not be naively exploited by defectors.

Edge cases / special notes
- Very small r: Window W and punish lengths automatically shrink, so the strategy becomes forgiving and avoids long futile punishments when few rounds remain.
- Very large n: coop_fraction uses proportions (n-1 denominator) so the rule scales.
- If many opponents are heterogenous, the strategy effectively treats the community as cooperative only if a majority of others cooperate recently; that is robust to a minority of persistent defectors.
- If randomness is allowed, the optional small-probability defections in the “moderate” zone reduce exploitability while keeping the cooperative signal intact.
- If k/n >> 1: unconditional cooperation is both individually and socially optimal and the strategy cooperates throughout.

Summary
- Always cooperate when single-round incentives already favor cooperation (k/n > 1).
- Otherwise, start cooperative, keep cooperating while a recent majority of others do, punish short and proportionally when cooperation falls clearly below a threshold, and forgive quickly to restore cooperation. Defect in the final round (unless k/n > 1). This approach balances being cooperative, deterring defectors, and recovering from breakdowns of cooperation.
'''

description_COOPERATIVE_39 = '''
Strategy name: Parameterized Finite-Deterrence (PFD)

Intuition (short)
- Start by cooperating to signal willingness to cooperate.
- Use finite, parameterized collective punishment when defections occur so that a unilateral short-term gain is outweighed by the expected cost of punishment.
- Be lenient about the very last rounds (endgame): when too few rounds remain to make punishment credible, stop cooperating to avoid being exploited.
- Scale punishment severity with how many others defected in the last observed round to make punishment proportional to the offense.
- After a finite punishment phase, forgive and return to cooperation; repeat as needed.

This strategy depends only on game parameters (n, k, r) and the public history of moves; it is fully implementable as an algorithm.

Key computed quantity
- Basic deterrence length P0 = ceil( (1 - k/n) / (k - 1) ).
  - Derivation sketch: immediate one-shot gain from deviating (when others would cooperate) equals 1 - k/n. Per-round cooperative surplus that punishers can deny equals k - 1. To deter a one-shot defection, P rounds of collective punishment should satisfy P×(k-1) ≥ 1 - k/n, hence P ≈ (1 - k/n)/(k - 1). We take the ceiling to get an integer number of punishment rounds. If k is close to 1 this may be large; that correctly reflects that strong punishment is needed to deter defectors when the public-good multiplier is small.

Top-level decision rules (natural language)
1. Initialization:
   - Compute P0 = ceil((1 - k/n)/(k - 1)). If k ≤ 1 (outside the assumed parameter range) treat P0 as infinite and defect always.
   - Maintain a state variable:
     - mode ∈ {COOPERATE, PUNISH, ENDGAME}
     - punish_remaining (integer), initially 0
   - At the start (round 1) set mode = COOPERATE unless r ≤ P0, in which case set mode = ENDGAME.

2. Every round t (1 ≤ t ≤ r):
   - Let R = r - t + 1 be number of rounds remaining including this one.
   - Observe last round's outcome (for t = 1 there is no past observation).
   - If R ≤ P0: switch immediately to ENDGAME (defect all remaining rounds). Reason: you cannot credibly impose the deterrent length P0 any more, so cooperating is exploitable.
   - If mode == PUNISH:
     - If punish_remaining > 0: play D this round, decrement punish_remaining by 1. If punish_remaining becomes 0 after decrement, set mode = COOPERATE (for the next round), unless R-1 ≤ P0 in which case switch to ENDGAME.
   - Else if mode == COOPERATE:
     - If this is the last round (R == 1): play D (ENDGAME behavior).
     - Otherwise:
       - If in the immediately previous round (t-1) at least one other player defected, compute m_defected = number of players (excluding you) who played D in round t-1.
         - Set punish_length = min(R-1, P0 × max(1, m_defected)).
           - (We scale P0 by number of defectors to make punishment proportional when multiple players defect at once. That increases robustness to coordinated attacks.)
         - Set punish_remaining = punish_length, set mode = PUNISH, and play D this round (the first round of punishment).
       - Else (no others defected last round): play C.

3. ENDGAME:
   - In ENDGAME mode play D every remaining round.

Edge cases and clarifications
- First round (t = 1): there is no history. If r > P0, play C; otherwise (r ≤ P0) go to ENDGAME and play D. This avoids opening yourself up to profitable one-shot deviations that cannot be credibly punished.
- Last round: always defect. This is unavoidable in a finite-horizon simultaneous public-goods game; the strategy accepts that and works to sustain cooperation before the final few rounds.
- If punish_remaining is set to 0 by computation (because R-1 = 0), the strategy will have already switched to ENDGAME due to the R ≤ P0 check.
- If multiple players defect in the same round, punish_length increases proportionally (P0 × m_defected). This raises the expected cost of coordinated defections and improves robustness against groups of exploiters.
- If k is close to 1 then P0 will be large (possibly larger than r), so the strategy will defect throughout (because credible punishment is infeasible). That is appropriate: when k is very small cooperation is hard to sustain.
- The algorithm is deterministic and only uses n, k, r and history of past actions (public monitoring). It requires no off-path commitments from other players.

Pseudocode (compact)

Compute P0 = ceil((1 - k/n) / (k - 1))
mode = COOPERATE if r > P0 else ENDGAME
punish_remaining = 0

for t = 1..r:
  R = r - t + 1
  if R <= P0:
    mode = ENDGAME
  if mode == ENDGAME:
    play D
    continue
  if mode == PUNISH:
    if punish_remaining > 0:
      play D
      punish_remaining -= 1
      if punish_remaining == 0:
        mode = COOPERATE if (R-1) > P0 else ENDGAME
      continue
    else:
      mode = COOPERATE
  # mode == COOPERATE
  if R == 1:
    play D
    continue
  if t > 1 and (# players ≠ me who played D in round t-1) > 0:
    m_defected = number of other defectors in round t-1
    punish_length = min(R-1, P0 * max(1, m_defected))
    punish_remaining = punish_length
    mode = PUNISH
    play D
  else:
    play C

Why this is cooperative
- The default behavior (COOPERATE) is to contribute every round as long as credible punishment is possible. That maximizes group welfare (all-cooperate yields the social optimum each round).
- Punishments are finite and return to cooperation. This avoids permanent collapse of cooperation after a single mistake or attack.
- Punishments are proportional to the scale of the offense (more defectors produce longer punishment), so cooperation among the majority is not unduly undermined by a small minority unless they persist.

Why this is robust
- The punishment length P0 is derived from the game parameters so it is tailored to the strength of incentives (k) and the number of players (n). It is the minimal integer punishment length that can offset a one-shot deviation when the cooperative baseline is available, so it is efficient (not longer than needed).
- If remaining rounds are too few to credibly punish, the strategy stops cooperating (ENDGAME). That prevents being exploited in the endgame.
- Scaling punishment to the number of defectors makes coordinated defection less attractive.
- The strategy is deterministic and transparent in its dependency on publicly observable history, which helps other adaptive agents learn the consequences of defection and thus promotes stable cooperation.

Variants / tuning knobs (optional)
- Leniency: to be more tolerant of isolated errors you can require m_defected ≥ T_tol (e.g., T_tol = 1 or larger) before punishing. With perfect monitoring and no noise, T_tol = 1 is appropriate; with noise you may raise T_tol or use a short “warning” before full punishment.
- Punishment scaling: instead of linear scaling by m_defected, you can use a saturating function (e.g., punish_length = P0 × min(m_defected, M_max)) to limit collateral damage.
- Alternative P0 rounding: you could use a slightly larger P0 to be more conservative against exploitation; the trade-off is more frequent temporary collective welfare loss during punishment.

Summary
- Cooperate by default when you can credibly punish (r > P0).
- If anyone defects in the immediately preceding round, impose a finite, proportional collective punishment of length P = min(remaining_rounds-1, P0 × m_defected).
- After the finite punishment, forgive and return to cooperation.
- If too near the end to credibly punish (remaining rounds ≤ P0), defect for the rest of the game (ENDGAME).
This strategy is adaptive, parameterized by (n,k,r), and balances cooperation with robustness against exploitation.
'''

description_COOPERATIVE_40 = '''
Strategy name: Adaptive Threshold Reciprocity (ATR)

Goal summary
- Signal cooperation, sustain cooperation when others reciprocate, avoid long-term exploitation, and forgive briefly so cooperation can recover.
- Uses only parameters (n, k, r) and the observable history (who cooperated each previous round).
- Starts cooperative, conditions future cooperation on recent group cooperation rates, punishes proportionally and temporarily, and defects in the final round (safe endgame).

High-level intuition
- In one-shot the dominant action is D; repeated interaction allows stable cooperation if players reciprocate.
- ATR (1) starts friendly to encourage reciprocators, (2) cooperates when the recent group cooperation rate indicates others are reliably reciprocating, (3) defects when the group reliably fails to reciprocate, (4) punishes briefly rather than forever (forgiveness), and (5) defects in the last round to avoid certain exploitation when opponents are fully strategic about the endgame.

Key derived internal parameters (computed from n,k,r)
- w (window length) = min(5, r-1). Use last w rounds to estimate recent behavior.
- gamma (cooperation threshold) = clamp(0.5 - 0.3 * (k-1)/(n-1), 0.2, 0.8).
  - As k increases (public good is more productive), ATR becomes more forgiving (lower gamma) because cooperation is more valuable.
- delta (one-shot tolerance) = 1/n. Small tolerance for noise in a single round.
- P (punishment length) = min(3, max(1, floor(r/10))). Short, proportional punishment period; grows slowly with total rounds.
- Endgame margin E = 1 (the final round index r). In round r ATR will defect (safe default).

All clamp() results are rounded to convenient integer thresholds where needed (e.g., gamma translated into number of cooperators).

Decision rules — precise

Notation
- t = current round (1..r)
- history[t'] = vector of cooperators in round t' (values c_j ∈ {0,1} for j=1..n)
- my_action[t'] ∈ {C,D}
- others_coops[t'] = sum_{j ≠ me} c_j,t' (number of cooperators excluding me in round t')
- coop_rate_window = average over last w rounds of (sum_{j=1..n} c_j,t')/n (fraction of all players who cooperated)
- coop_count_window = average over last w rounds of sum_{j=1..n} c_j,t' (average number of cooperators)
- safe_last_round = (t == r)
- recent_drop = indicator that coop_rate_window decreased by more than delta relative to the previous window

Main policy (natural-language then pseudocode)

1) First round (t = 1)
- Play C (cooperate). This is a cooperative signal and allows fast mutual cooperation with reciprocators.

2) Last round (t = r)
- Play D (defect). Finite-horizon backward induction makes cooperating in final round risky; defect protects from endgame exploitation. (If your tournament designer later wants an alternative, this line can be replaced by an optional exception when the entire history shows near-perfect cooperation and opponents never defected; but default is D.)

3) Intermediate rounds (1 < t < r)
- Compute coop_rate_window using the most recent w completed rounds (rounds t-1 down to max(1,t-w)).
- If coop_rate_window ≥ gamma:
    - Cooperate (C). The group has been cooperating enough recently to make reciprocation promising.
- Else if coop_rate_window < gamma:
    - If we are currently inside a punishment episode (i.e., we already initiated a punishment after detecting serious exploitation and have not yet completed P punishment rounds), continue the punishment by Defect (D).
    - Otherwise:
        - If the latest single-round coop fraction (round t-1) dropped sharply compared to the prior window (recent_drop) or if round t-1 had very few cooperators (e.g., total_cooperators_{t-1} ≤ max(1, floor(n*0.2))):
            - Start a punishment episode: Defect for P rounds (or until the end of the game if fewer than P rounds remain).
        - Else (small, isolated fluctuation):
            - Forgive the fluctuation and play what you played in the previous round (win-stay): repeat your previous action (if your previous action was C, do C; if D, stay D). This avoids overreacting to one-off deviations but preserves punitive capability for repeated failures.

4) Punishment dynamics and forgiveness
- A punishment episode consists of defecting for P rounds after detecting a robust drop in cooperation.
- After P rounds of defecting, re-evaluate coop_rate_window; if coop_rate_window ≥ gamma, resume cooperation; otherwise, repeat the cycle: another P rounds of defection if exploitation persists.
- If coop_rate_window shows a sustained recovery (two consecutive windows above gamma), resume cooperating immediately.

5) Edge cases
- If you were the only cooperator in the previous round (total_cooperators_{t-1} == 1):
    - Treat this as a strong sign of exploitation: start a punishment episode immediately (Defect for P rounds).
- If there is perfect cooperation in every past round (cooperation rate = 1 for all completed rounds) and t is close to final round (t >= r-2):
    - You may optionally cooperate in round r-1 to sustain payoff, but still defect in final round r. (Default ATR still defects in r-1 only if coop_rate_window < gamma; otherwise cooperates.)

Pseudocode

Initialize:
  w = min(5, r-1)
  gamma = clamp(0.5 - 0.3 * (k-1)/(n-1), 0.2, 0.8)
  delta = 1/n
  P = min(3, max(1, floor(r/10)))
  punishment_remaining = 0

On round t:
  if t == 1:
    play C
    return

  if t == r:    # last round
    play D
    return

  # compute coop rates on previous rounds
  window_start = max(1, t - w)
  completed_rounds = [window_start .. t-1]
  coop_rate_window = average over completed_rounds of (sum_{j=1..n} c_j / n)
  coop_rate_prev_window = average over [max(1, window_start - w) .. window_start - 1] if that window exists else coop_rate_window

  total_coop_last = sum_{j=1..n} c_j at round t-1
  was_only_cooperator = (total_coop_last == 1)

  if punishment_remaining > 0:
    play D
    punishment_remaining -= 1
    return

  if coop_rate_window >= gamma:
    play C
    return

  # coop_rate_window < gamma: consider punishment or mild response
  if was_only_cooperator or (coop_rate_window + delta < coop_rate_prev_window) or (total_coop_last <= max(1, floor(0.2 * n))):
    # significant or repeated drop => start punishment
    punishment_remaining = P - 1   # will defect this round and P-1 more rounds
    play D
    return
  else:
    # marginal or one-off drop: win-stay/lose-shift
    if my_action[t-1] == C:
      play C
    else:
      play D
    return

Rationale and expected behavior
- Starting with C allows quickly establishing mutual cooperation with reciprocators.
- Gamma adapts to k: when public goods are more productive (larger k), ATR is more forgiving (lower gamma) because cooperation yields larger group gains.
- The sliding window w keeps the strategy responsive to recent trends while smoothing single-round noise.
- Punishment of fixed short length P discourages persistent exploitation but avoids permanent breakdowns due to accidental slips or strategic provocation. Short punishments are effective in multi-player settings where permanent punishment is hard and costly.
- Win-stay behavior in borderline cases prevents overreacting to small deviations and helps stabilize cooperation.
- Defecting in the last round is conservative and avoids guaranteed exploitation by backward- or forward-looking defectors.

Why this is adaptive and robust
- Adapts to opponents’ empirically observed cooperation frequency (windowed rate).
- Forgives occasional deviations so non-strategic errors or probing do not collapse cooperation.
- Proportionate punishments deter repeated free-riding without long-term loss from over-punishing.
- Parameter gamma scales with k so the strategy is more cooperative when the public good is more valuable.
- Safe endgame behavior protects against last-round exploitation in a finite-horizon tournament.

Implementation notes
- The exact numerical clamps and constants (0.5 and 0.3 in gamma formula, w = 5, P choices) are conservative defaults that work well across a range of n,k,r; they can be tuned for particular tournaments if desired.
- If noise / implementation errors are expected, increase delta or widen w to avoid false punishments.
- If the tournament contains many short games (small r), reduce P relative to r (already handled by using min/floor rules).
- Optionally log round-by-round coop_rate_window to allow more sophisticated trend detection (e.g., linear trend) if allowed.

Final summary
- ATR cooperates first, uses a short sliding-window estimate of group cooperation to decide whether to cooperate, punishes significant or repeated drops with short defections, forgives small/isolated deviations by staying with prior behavior, scales tolerance with k (more productive public goods => more forgiveness), and defects in the last round to avoid endgame exploitation. This combination is cooperative, adaptive, and robust against a wide range of opponent behaviors.
'''

description_COOPERATIVE_41 = '''
Strategy name: Counterfactual Win‑Stay/Lose‑Shift with controlled forgiveness (C-WSLS-F)

High-level idea
- Start cooperative to signal willingness to cooperate.
- After each round, compute the payoff you actually received and the payoff you would have received had you played the opposite action given everybody else’s observed actions that round (counterfactual).
- If your actual payoff was at least as large as the counterfactual, repeat your previous action (win–stay). If your actual payoff was smaller than the counterfactual, switch (lose–shift).
- Add two light meta-rules to make the strategy robust in multi‑player repeated play:
  1. Forgiveness: if cooperation among others rebounds (a short window of relatively high cooperation), abandon short punishments and resume cooperation.
  2. Endgame safety: in the last round defect (no future to influence), and reduce forgiveness as the horizon shortens.

Why this is cooperative and robust
- Cooperative: it starts by cooperating and returns to cooperation whenever others show cooperative behaviour. It rewards reciprocating opponents because mutual cooperation gives repeated gains.
- Robust: the counterfactual rule prevents you from being persistently exploited — you switch when you were better off by doing the opposite. That punishes unilateral defectors. Forgiveness prevents endless punishment cycles and allows recovery when opponents return to cooperation.
- Adaptive: rule uses observed behaviour each round (full history is available) and naturally scales punishment/forgiveness with what you observe; it requires no assumptions about opponents’ norms or explicit coordination.

Detailed decision rules

Notation
- t = current round (1..r).
- r = total rounds (known).
- n = number of players.
- k = multiplication factor.
- For the previous round t-1:
  - a_prev ∈ {C, D} = your action in round t-1.
  - others_prev = vector of other players’ actions in round t-1.
  - m_prev = number of cooperators among the other n−1 players in round t-1.
  - π_prev = your actual payoff in round t-1.
- counterfactual_opposite = the payoff you would have got in round t-1 had you played the opposite action, given others_prev.

Compute counterfactual payoff formula (given observed others_prev):
- If a_prev == C then counterfactual_opposite = payoff if you had played D:
    = 1 + (k/n) * (m_prev)  [because total cooperators would be m_prev]
- If a_prev == D then counterfactual_opposite = payoff if you had played C:
    = 0 + (k/n) * (m_prev + 1)  [total cooperators would be m_prev + 1]

Core WSLS rule
- If t == 1:
  - Play C (cooperate) to open with a cooperative signal.
- Else if 1 < t < r (not the last round):
  - If π_prev >= counterfactual_opposite:
      - Repeat a_prev (stay).
  - Else:
      - Switch from a_prev (shift): play the opposite of a_prev this round.
- Else (t == r, last round):
  - Play D (defect). There is no future to influence; defect dominates given others’ fixed actions.

Forgiveness / recovery rule (limits punishment and restores cooperation)
- Maintain a short rolling window W (e.g., W = 2 rounds) of recent rounds’ observed cooperation rates among others (fraction of other players who cooperated each round).
- Forgive and resume cooperation whenever:
  - You currently are defecting because of a recent loss, AND
  - In the most recent W rounds the average fraction of cooperators among others >= θ_f,
  - AND there are at least 2 rounds remaining (t ≤ r-2).
- Suggested parameter: θ_f = 0.5 (i.e., majority of others cooperating in the short window).
- In that case, play C to help restore mutual cooperation.

Limited persistent-defection safeguard
- If others have been near-universal defectors for S consecutive rounds (e.g., S = 3) and few rounds remain, switch to a defensive mode: defect in all remaining rounds. This avoids wasting remaining rounds being exploited when no reciprocation is observed. (Set S moderately small, e.g., 3).

Putting the rules together (pseudocode)

Parameters (suggested defaults)
- W = 2 (forgiveness window)
- θ_f = 0.5 (forgiveness threshold)
- S = 3 (persistent-defection threshold)

Pseudocode:
1. If t == 1:
     play C
     continue
2. If t == r:
     play D
     continue
3. Compute m_prev = number of other players who cooperated in round t-1
   Compute counterfactual_opposite as described above
4. If π_prev >= counterfactual_opposite:
     action = a_prev  # stay
   Else:
     action = opposite(a_prev)  # shift
5. Compute average_coop_recent = average fraction of cooperators among others over last W rounds
   If action == D and average_coop_recent >= θ_f and t <= r-2:
       action = C  # forgive and try to rebuild cooperation
6. If in the last S rounds (or until t) the fraction of cooperating others each round <= small_eps (e.g., <= 1/(n) or zero),
   and remaining rounds ≤ S:
       action = D  # switch to defensive permanent defection for remaining rounds
7. Play action.

Notes on parameter choices and adaptivity
- W = 2 and θ_f = 0.5 are conservative defaults: they give a short, quick chance for cooperation to rebound without making you naive.
- S = 3 prevents repeatedly being exploited when most players are persistent defectors.
- These parameters can be adjusted: increase W and θ_f to make the strategy more forgiving when population tends to be cooperative; reduce them when tournament opponents are exploitative.
- The counterfactual comparison is exact because perfect monitoring gives you the other players’ actions. This avoids guessing opponents’ intentions.

Examples of behavior
- Against unconditional cooperators: you start C; π_prev will be at least as good as the counterfactual, so you stay C and keep cooperating—mutual cooperation maintained.
- Against unconditional defectors: you start C, you get exploited in round 1 (π_prev smaller than counterfactual), so you switch to D. With persistent defection you remain D (possibly permanently near the end) and avoid further losses.
- Against conditional reciprocators (tit-for-tat or similar): you start C; when they cooperate you continue to cooperate; if they defect you punish by switching, but forgiveness allows you to return to cooperation when they do.
- Endgame: you defect in final round so you are not exploited for no future benefit.

Why this is good tournament fodder
- It signals cooperation early (helps create mutually cooperating pairs/groups).
- It is retaliatory but not vicious: punishments are immediate and effective (you switch when exploited) but limited and reversible (forgiveness window).
- Uses exact counterfactual information (available under perfect monitoring) to decide whether you benefited — this makes punishment proportional and avoids punishing when you actually benefited.
- Endgame handling prevents needless losses in the final round(s) where reciprocity cannot be sustained.

Implementation note for a programmer
- Keep a history array for all rounds’ actions to compute m_prev and recent cooperation rates.
- Compute π_prev and the counterfactual precisely using the formula in the spec.
- Make the forgiveness and persistent-defection checks after computing the WSLS decision to allow quick return to cooperation when warranted.

This strategy is simple to implement, parameter-light, and aligns with cooperative goals while protecting against being exploited — making it well suited for tournaments mixing a wide range of opponent behaviours.
'''

description_COOPERATIVE_42 = '''
High-level goal
- Encourage and sustain high group cooperation by signalling willingness to cooperate, punishing observed group-level defection enough to deter exploitation, and forgiving so cooperation can restart. The rule set is deterministic, simple to implement from history, and adapts punishment severity to the game parameters (n, k, r).

Key ideas
- Start by cooperating to signal cooperation.
- Use a group-level “majority” check on the previous round to decide whether the group is cooperating.
- If the group defected (below threshold), punish with a short, finite burst of defections. The punishment length scales with how tempting defection is (depends on k and n) but is capped to avoid endless mutual punishment.
- Forgive early if the group returns to cooperating during punishment.
- Always defect in the final round (no future to enforce reciprocity).
- Never rely on assumed norms or external coordination — only use recorded history of actions.

Parameters (computed from game inputs)
- q = 0.5 (cooperation fraction threshold; i.e., “majority cooperated last round”).
  - If previous-round fraction of cooperators ≥ q → generous, cooperate.
  - If < q → treat as a group defection that merits punishment.
- P_base = ceil(n / (n - k))   (scales punishment with how close k is to n)
- P_max = 4                    (cap to avoid excessive punishment)
- P = min(P_max, max(1, P_base))  (punishment length in rounds)
  - Rationale: when k is close to n, individual benefit from cooperation is higher and defecting is less tempting, so severe punishment isn’t necessary; when k is small relative to n, temptation to defect is high, so we lengthen punishment—but cap it to keep the strategy forgiving.
- When starting a punishment, do not schedule punishment that would extend into the last round. Set actual punishment length to min(P, remaining_rounds - 1).

State variables the strategy maintains
- punish_remaining: integer number of rounds left to defect as punishment (0 if not punishing).
- played_last_round: the action you took last round (used only to maintain state; decisions are based on punish_remaining and observed last-round cooperators).
(These are derivable from the algorithmic implementation and the history you observe.)

Decision rules (natural-language + pseudocode)
- Round 1: Cooperate.
- For each subsequent round t = 2..r:
  1. If t == r (final round): Defect.
  2. Else if punish_remaining > 0:
     - If last-round group cooperation fraction ≥ q: (group returned to cooperating) set punish_remaining = 0 and Cooperate.
     - Else: Defect and decrement punish_remaining by 1.
  3. Else (not currently punishing):
     - Compute coop_fraction_prev = (# cooperators in round t-1) / n.
     - If coop_fraction_prev ≥ q: Cooperate.
     - Else (coop_fraction_prev < q):
         - remaining_rounds = r - t + 1  (including current round)
         - set punish_remaining = min(P, remaining_rounds - 1)
         - Defect this round and decrement punish_remaining by 1 (since you consume the first punishment round now).

Pseudocode
(Assume access to: n, k, r, round index t (1..r), history list of each round's actions; punish_remaining initialized = 0)

Initialize:
  q = 0.5
  P_base = ceil(n / (n - k))
  P = min(4, max(1, P_base))
  punish_remaining = 0

On each round t:
  if t == 1:
    action = C
    // no change to punish_remaining
    return action

  if t == r:
    action = D
    punish_remaining = 0
    return action

  if punish_remaining > 0:
    coop_frac_prev = (number of C in round t-1) / n
    if coop_frac_prev >= q:
      // forgive early
      punish_remaining = 0
      action = C
      return action
    else:
      action = D
      punish_remaining -= 1
      return action

  // not currently punishing
  coop_frac_prev = (number of C in round t-1) / n
  if coop_frac_prev >= q:
    action = C
    return action
  else:
    remaining_rounds = r - t + 1
    punish_remaining = min(P, max(0, remaining_rounds - 1))
    // immediately punish this round (consume 1)
    if punish_remaining > 0:
      punish_remaining -= 1
      action = D
      return action
    else:
      // no room to punish (only final round left) — default to D
      action = D
      return action

Notes and edge-case handling
- First round: cooperate. This is a clear signal and attracts conditional cooperators.
- Last round: defect. With no future rounds to enforce cooperation, defect is the dominant one-shot move.
- Near the end: punish_remaining is truncated so that punishment never runs into round r (we leave the last round free to be treated as the final defection).
- Forgiveness: while punishing, if you observe the group returned to cooperating (≥ q) in the most recent round, stop punishment immediately and resume cooperation.
- Ties at q (exactly half cooperate) favor cooperation (generosity).
- The algorithm only uses the previous round’s group cooperation count and an internal punish counter; it does not assume or need patterns beyond the public history.

Why this will be robust and cooperative
- It is cooperative-minded: it starts cooperative and cooperates whenever the group recently showed adequate cooperation (majority).
- It is retaliatory enough to deter persistent defectors: a below-threshold round triggers a finite, parameterized punishment that scales with how tempting defection is (via P_base).
- It is forgiving and avoids lock-in: punishment is finite and can be cancelled early if others resume cooperating; this helps restore cooperation after mistakes or exploration by opponents.
- It is adaptive: reacts to the actual observed group behavior each round rather than assuming agent-specific identities or fixed schedules.
- It is simple and interpretable, making it robust in tournaments against diverse unknown strategies.

Variants / tuning (optional)
- If you observe many opponents that respond slowly, increasing P_max to 6 can strengthen deterrence; if the tournament has noise (random mistakes), lowering P_max or making q slightly lower (e.g., 0.45) makes the strategy more forgiving.
- You can replace the majority threshold q with a higher threshold when you want stricter demands for cooperation (e.g., q = 0.6) or lower q if you need to be more accommodating.

Summary
- Cooperate first. Cooperate whenever the previous round showed majority cooperation. If the group fell below majority, punish with a short, finite sequence of defections whose length depends on n and k, but forgive early if group cooperation reappears. Always defect in the final round. This balances cooperation, deterrence, and forgiveness while relying only on observable history and game parameters.
'''

description_COOPERATIVE_43 = '''
Strategy name: Forgiving Threshold Reciprocity (FTR)

Summary (one line)
- Start cooperatively to signal willingness; condition future cooperation on recent group cooperation; punish short and proportionally when group cooperation falls below threshold; forgive fast and limit punishment (and always defect in the last round).

Intuition and design goals
- Be visibly cooperative so you can gain large payoffs with cooperative opponents.
- Detect and respond to exploitation quickly so you are not a free-ride target.
- Keep punishments short, proportional and forgiving so cooperation can be re-established and mutual payoffs recovered.
- Be simple, deterministic (with optional tiny randomness for forgiveness), and only use observable history + game parameters (n, r, k).

High-level decision rules
- First round: cooperate (signal).
- Last round (round r): defect (no future incentive).
- During play: compute the fraction of other players who cooperated last round. If that fraction is high, cooperate. If it is low, defect for a short, escalating but capped punishment. If it is intermediate, keep your previous action (inertia). After good rounds, forgive and return to cooperation quickly.
- Punishment is group-level (not trying to single out individuals) but escalates if bad behaviour persists. Punishments are capped to avoid endless wars and to limit lost opportunities.

Parameters used by the strategy (derived from game parameters and simple defaults)
- grace_period = min(3, r-1) — unconditional cooperation at the start (signal).
- lookback_window W = min(5, r-1) — used for smoothing if desired (primary rule uses previous round but can average last W rounds optionally).
- tau_high = 0.6 — threshold of other-players-cooperating (fraction) above which you cooperate.
- tau_low = 0.4 — threshold below which you start punishment.
- max_punish = min(3, r-1) — maximum consecutive punishment rounds allowed.
- escalation_step = 1 — how much to increase punishment for repeated low cooperation rounds.
- forgiveness_prob epsilon = 0.05 (optional) — tiny probability to cooperate during punishment to allow reconciliation in noisy environments.
Notes on parameters:
- tau_high and tau_low are a small band to avoid flip-flopping on borderline noise; they may be tuned for tournaments. They are independent of n and k to keep rules simple; in practice they perform robustly across many n,k because they react to observed behaviour (not payoff computation). If preferred, tau_high can be increased with k because a higher k raises the value of cooperation.

State the strategy keeps across rounds
- last_action (C or D), initialized to C (because we cooperate first round).
- punish_counter (integer), initialized 0.
- consecutive_bad_rounds (integer), count of consecutive rounds where fraction of other cooperators <= tau_low.
- rounds_played t from 1..r.

Decision rule (natural language)
1. If t == r (last round): play D.
2. Else if t <= grace_period: play C.
3. Else if punish_counter > 0:
   - With probability 1 - epsilon: play D (continue punishment) and decrement punish_counter.
   - With probability epsilon: play C (forgiveness attempt) and leave punish_counter unchanged or decrement by 1 (implementation choice: decrement by 1 to keep punishments finite).
4. Else (no active punishment):
   - Compute p = fraction of other players who cooperated in the previous round. (Optionally compute p as the average fraction over the last W rounds for smoothing.)
   - If p >= tau_high: play C; reset consecutive_bad_rounds = 0.
   - Else if p <= tau_low:
       - consecutive_bad_rounds += 1
       - Set punish_counter = min(max_punish, escalation_step * consecutive_bad_rounds)
       - Play D this round (start punishment).
   - Else (tau_low < p < tau_high): play last_action (inertia); do not change consecutive_bad_rounds (or reset to 0 if you prefer leniency). This avoids flip-flopping on noisy borderline rounds.
5. Update last_action to the action you took for use next round.

Pseudocode (deterministic core; optional small randomness shown)
- Initialize:
  last_action = C
  punish_counter = 0
  consecutive_bad_rounds = 0
- For each round t = 1..r:
  if t == r:
    action = D
  else if t <= grace_period:
    action = C
  else if punish_counter > 0:
    with probability epsilon:
      action = C
      punish_counter = max(0, punish_counter - 1)
    else:
      action = D
      punish_counter -= 1
  else:
    observe prev_round_others_cooperators = count_cooperators_previous_round_excluding_self
    p = prev_round_others_cooperators / (n - 1)
    (optionally: p = average of this fraction over last W rounds)
    if p >= tau_high:
      action = C
      consecutive_bad_rounds = 0
    else if p <= tau_low:
      consecutive_bad_rounds += 1
      punish_counter = min(max_punish, escalation_step * consecutive_bad_rounds)
      action = D
      punish_counter -= 1   # we start punishment this round
    else:
      action = last_action   # inertia on borderline
  play action
  last_action = action

Edge cases and clarifications
- First round: cooperate (t = 1).
- Last round: defect (t = r). Rationale: no future to enforce cooperation.
- Two-player case (n = 2): this reduces to standard Tit-for-Tat/WSLS-like behavior; it will cooperate to start, retaliate briefly to defection, then forgive.
- Very short games (r small): grace_period and max_punish are capped by r-1 so the strategy behaves sensibly; if r = 2, you cooperate round 1 and defect round 2 (rational endgame behavior).
- Observation noise: optional epsilon forgiveness helps recover cooperation if some defections are accidental or noisy.
- If opponents are unconditional defectors: the strategy will rapidly detect low p and defect (short punishment), then keep defecting (since p remains low) and avoid exploitation. You won’t be repeatedly exploited.
- If opponents are conditional cooperators (reciprocators): initial cooperation will open cooperation; the threshold & short punishments will repair occasional defections and sustain mutual cooperation.
- If opponents are exploitative (cooperate sometimes to lure then defect): punishments escalate for persistent low cooperation and cap at max_punish, after which your inertia and forgiveness try to see if cooperation resumes; this avoids never-ending wars while making exploitation costly.
- If opponents play arbitrary/random strategies: the thresholding and inertia avoid excessive flip-flopping and achieve good expected payoffs.

Why this is cooperative and robust
- Cooperative: you begin by signaling cooperation and will cooperate whenever the recent group behaviour justifies it (p >= tau_high). That allows you to reap high mutual-payoff rounds with cooperators.
- Robust to exploitation: the strategy detects low group cooperation and retaliates quickly, which reduces the incentive for others to free-ride on you.
- Limited punishment and forgiveness: this keeps lost opportunities for mutual gain limited and allows recovery from mistakes and noise; it also prevents escalating wars that reduce average payoffs for everyone.
- Simple and history-based: uses only observable past contributions and parameters n,r,k, so it can be implemented in a tournament without external coordination.

Tunable variants (optional)
- Use moving-average p over last W rounds instead of single previous round to reduce noise sensitivity.
- Increase max_punish or escalation_step to be harsher against persistent defectors.
- Tie tau_high to k (e.g., increase tau_high as k increases) to value cooperation more when it is socially more beneficial.
- Implement targeted punishment: if desired, track per-player cooperation rates and reduce cooperation conditional on specific defectors (this is more complex but can be more efficient at discouraging persistent individual free-riders).

Final remarks
- This strategy is deliberately pragmatic: it refuses to be exploited, it signals and rewards cooperation, forgives to restore cooperation, and applies simple, transparent rules that will perform well against a wide variety of opponent types in a tournament.
'''

description_COOPERATIVE_44 = '''
Name: Adaptive Conditional Cooperator with Graduated Punishment and Forgiveness (ACG-PF)

Goal (design intent)
- Encourage and sustain near–full cooperation (maximize group payoff) while being robust to exploitation.
- Signal willingness to cooperate (first move cooperate), punish persistent free-riding, but forgive and re-test so cooperation can be restored.
- Use only public history, game parameters (n, r, k) and internal state (punishment timers); no communication or coordination required.

High-level idea
- Start cooperatively to signal intent.
- Track recent group cooperation rates (excluding self) in a short sliding window.
- Cooperate if recent behavior suggests others are reciprocating sufficiently.
- If others fall below a cooperation threshold, impose a short, graded punishment (defect for several rounds). After punishment, perform a one-round cooperative test. If the test is met, return to cooperation; if not, increase punishment length.
- Always defect in the final round (standard backward-induction endgame).
- Parameters adapt to n, k and remaining rounds so the strategy scales and is not overly harsh near the end.

Core parameters (default formulas you can tune when implementing)
- Window length w = min(4, max(1, r-1)). (Use up to last 4 rounds, fewer if game is short.)
- Cooperation threshold α = clamp(0.6 + 0.4*(k-1)/(n-1), 0.5, 0.9).
  - Rationale: when k is larger, cooperation is more valuable so require higher observed reciprocity; threshold stays in [0.5,0.9].
- Base punishment length scale L0 = 1 (minimum punishment 1 round).
- Punishment growth factor: increase punish length modestly each repeated offense.
- Small probabilistic forgiveness rate p_test = 0.05 (occasional one-round cooperative probe during punishment sequence, optional).

State variables
- phase ∈ {COOP, PUNISH}
- punish_timer (integer ≥ 0): rounds remaining to punish (defect) before test
- last_punish_length (integer): length used last time (for escalation)
- offense_count (integer): number of recent punish cycles
- (history): observed actions of all players each past round

Decision rules (natural-language + step-by-step pseudocode)

Definitions:
- t = current round (1..r)
- remaining = r - t + 1
- For a round s, let C_s = total number of players who played C in round s.
- For a window of rounds T = {max(1,t-w) ... t-1} (previous w rounds or as many as exist), compute:
  others_coop_rate = (sum_{s in T} (C_s - (I_self_cooperated_in_round_s ? 1 : 0))) / (|T| * (n-1))
  (If T is empty — e.g., t=1 — define others_coop_rate = 1 as an optimistic prior.)

Top-level pseudocode (deterministic core with optional small-probability probes)

On each round t do:
1. Endgame rule:
   - If t == r: play D (defect). Reason: no future to reward or punish.
2. First round:
   - If t == 1: play C; initialize phase = COOP, punish_timer = 0, last_punish_length = 0, offense_count = 0.
3. If punish_timer > 0:
   - (a) With tiny probability p_test (e.g., 0.05) you may override to play C as a probe; otherwise play D.
   - (b) Decrement punish_timer by 1.
   - (c) If punish_timer becomes 0 after decrement, set phase = TEST (see step 4).
   - Return chosen action for this round.
4. If phase == TEST (punishment just finished):
   - Play C for this round (one-round cooperative test).
   - After observing other players' C count this round:
     - Compute others_coop_rate over the most recent w rounds including this test round.
     - If others_coop_rate ≥ α:
       - Phase <- COOP; punish_timer <- 0; last_punish_length <- max(1, last_punish_length - 1) if you want de-escalation; offense_count <- 0.
     - Else:
       - Enter punishment again: set phase <- PUNISH; offense_count++.
       - Set new punish_timer = min( remaining-1, last_punish_length + ceil( (α - others_coop_rate) * r/2 ) + 1 )
         (That is: punish length grows with the shortfall and previous punishment; always leave at least one round for a future test if possible.)
       - last_punish_length <- punish_timer.
   - Return action C for this round.
5. If phase == COOP:
   - Compute others_coop_rate over the last w rounds (excluding t).
   - If others_coop_rate ≥ α:
     - Play C.
   - Else:
     - Enter punishment:
       - phase <- PUNISH
       - offense_count++.
       - Set punish_timer = min( remaining-1, L0 + ceil((α - others_coop_rate) * r/2) + floor(offense_count/1.5) )
         (shorter punish early in the game, somewhat longer with repeated offenses, but never longer than remaining-1 rounds so a test round can occur)
       - last_punish_length <- punish_timer.
       - Play D this round (first punishment round).
6. (Optional softening near end of game)
   - If remaining ≤ 2: be more forgiving — reduce punish_timer target to 0 or 1 (so punishment doesn't waste remaining opportunities for cooperation).

Remarks on parameter choices and behavior
- Initial cooperation (round 1) signals willingness to coordinate.
- The threshold α ensures we only cooperate when others have been reciprocating reasonably often; it increases moderately with k so the strategy demands more reciprocation when cooperation is more valuable.
- Punishments are graduated (punish length depends on how badly others deviated and grows if deviation repeats). This is less brittle than Grim Trigger and allows recovery.
- The mandatory one-round test after punishment prevents endless cycles of mutual punishment and allows restoration of cooperation.
- Small probabilistic probes (p_test) while punishing can discover unexpectedly cooperative opponents faster; keep p_test small to avoid exploitation.
- Last-round defection avoids being exploited when future retaliation is impossible.

Edge cases and special handling
- Very short games (r small): window w is small; punish lengths are capped so tests are still possible. If remaining rounds <=2, the strategy reduces punishments so it does not squander remaining opportunities to cooperate.
- No prior history (t=1): treat others_coop_rate = 1 to encourage initial cooperation.
- If almost everyone else always defects: punishments will be triggered, and the strategy will shift to defecting most rounds (protecting the strategy from exploitation).
- If many independent conditional cooperators are present, the policy will converge to sustained cooperation through mutual signals and restorative tests.

Why this is cooperative and robust
- Cooperative: it starts cooperative, aims to restore and maintain high cooperation (targeting high group payoffs). It is forgiving and includes explicit test rounds to re-establish cooperation.
- Robust: it defends itself against pure defectors by imposing punishments and eventually defaulting to defection when others do not reciprocate. Punishment lengths are proportional and capped to avoid overreaction.
- Adaptive: uses sliding-window statistics of observed behavior; thresholds scale with k and n; punishments escalate with repeated offenses and are reduced on successful reconciliation.

Example concrete instantiation (plug-in numeric defaults)
- w = min(4, r-1)
- α = clip(0.6 + 0.4*(k-1)/(n-1), 0.5, 0.9)
- L0 = 1, p_test = 0.05

This yields a ready-to-implement decision routine: compute recent cooperation rates, compare to α, and follow the COOP / PUNISH / TEST cycle described above.

Overall the ACG-PF strategy is simple to implement, focuses on group-level reciprocity, punishes defections sufficiently to deter exploitation, and is forgiving to allow cooperation to reemerge — properties suited for noisy, heterogeneous tournaments of independent AI strategies.
'''

description_COOPERATIVE_45 = '''
Strategy name: Adaptive Threshold Reciprocity (ATR) — a conditional-cooperation rule that aims for sustained high cooperation, punishes shortfalls proportionately, forgives noise, and respects the last-round endgame.

Intuition (short):
- Start by signalling cooperation.
- In each non-final round, cooperate if recent group behaviour shows a strong tendency to cooperate; otherwise apply a short, proportional punishment (defect) that is limited in length and followed by a test for rehabilitation.
- Forgive occasional deviations (random generosity) to avoid permanent breakdown from noise or mistakes.
- Defect in the last round (no future to enforce cooperation).

Key internal parameters (computed once from inputs or set constants)
- n, r, k: game parameters (given).
- WINDOW W = min(5, r-1) — lookback window for “recent behaviour”.
- THRESHOLD alpha = 0.60 — require at least 60% cooperation in recent window to continue cooperating (choice balances sensitivity and robustness; implementers may tune 0.5–0.75).
- MAX_PUNISH = 3 — maximum consecutive punishment rounds to avoid over-punishing.
- FORGIVE_PROB p = 0.10 — small random forgiveness probability to recover from cycles or noise.
- CONTRITE_ROUNDS = 2 — after a punishment period if we determine we were “wrongly” punished while we cooperated, we play cooperatively for CONTRITE_ROUNDS to signal contrition.

Observable history available each round t:
- For each previous round s < t: full vector of actions (C or D) for all players.
- Keep internal state across rounds: punish_counter (how many rounds of punishment remain), last_punish_start_round, contrite_counter.

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This seeds cooperation and is the default cooperative stance.
2. Final round (t = r): Defect (D). With no future, cooperating cannot be enforced, so defect to avoid being exploited.
3. Intermediate rounds (1 < t < r):
   - If currently in an active punishment episode (punish_counter > 0): play D, decrement punish_counter by 1. After punitive rounds finish, set contrite_counter = CONTRITE_ROUNDS (see contrition rule).
   - Else if contrite_counter > 0: play C and decrement contrite_counter by 1.
   - Else evaluate recent group cooperation:
     a. Compute recent cooperation fraction g = (sum of cooperators over rounds max(1,t-W) .. t-1) / (n * number_of_rounds_considered). (If t-1 < 1, treat g = 1 so we cooperate.)
     b. If g >= alpha: play C (reward cooperation).
     c. Else: decide to punish:
        - Compute shortfall s = alpha - g (a number in (0, alpha]).
        - Set punish_length = min( MAX_PUNISH, 1 + ceil( s / alpha * MAX_PUNISH ) ). (A proportional punishment: bigger shortfalls give up to MAX_PUNISH rounds.)
        - Initialize punish_counter = punish_length and play D this round (start punishment).
   - Additionally: with small probability p execute a one-shot forgiveness: if decision above says D, flip to C with probability p (this prevents permanent defection cycles and tolerates mistakes).
4. Contrition: whenever a punishment episode ends, we play C for CONTRITE_ROUNDS to signal rehabilitation. This helps end retaliatory cycles.

Pseudocode (concise)

Initialize:
- punish_counter = 0
- contrite_counter = 0

On each round t (1..r), given history H of past rounds:
if t == 1:
    action = C
elif t == r:
    action = D
else:
    if punish_counter > 0:
        action = D
        punish_counter -= 1
        if punish_counter == 0:
            contrite_counter = CONTRITE_ROUNDS
    elif contrite_counter > 0:
        action = C
        contrite_counter -= 1
    else:
        // compute recent cooperation fraction g
        s0 = max(1, t - W)
        rounds_considered = t - s0
        total_cooperators = sum_{s = s0}^{t-1} (# of C in round s)
        g = total_cooperators / (n * rounds_considered)
        if g >= alpha:
            action = C
        else:
            // start proportional punishment
            shortfall = alpha - g
            punish_length = min(MAX_PUNISH, 1 + ceil(shortfall / alpha * MAX_PUNISH))
            punish_counter = punish_length - 1   // we will defect this round, so store remaining
            action = D
    // small random forgiveness
    if action == D and rand() < p:
        action = C

Return action.

Comments and rationale
- Cooperative mindset: ATR begins by cooperating, rewards rounds with substantial cooperation in the group, and aims to restore and maintain high collective cooperation rather than maximize short-term exploitation.
- Proportional punishment: Punishments are short (<= MAX_PUNISH) and proportional to how far recent cooperation falls short of the target; this discourages persistent defectors while avoiding long vendettas that lower total payoffs.
- Forgiveness and contrition: Random forgiveness avoids lock-in to mutual defection after accidental deviations; contrition (two cooperative rounds after punishment) signals willingness to resume cooperation so others can safely reciprocate.
- Robustness: Uses only aggregate group statistics and short memory, so it tolerates individual deviant strategies, mis-coordination, and moderate noise. The window W allows responsiveness while smoothing single-round fluctuations.
- Endgame handling: Defect in final round (standard rational endgame move). If you prefer more “endgame cooperation” in very small r settings, parameters can be adjusted (but last-round defection is safer).
- Parameter tuning: Default alpha = 0.60, W = 5, MAX_PUNISH = 3, p = 0.10 are conservative choices that work in many tournaments. If you expect highly cooperative opponents, alpha can be lowered toward 0.5 to be more lenient; if the population is very noisy, increase p or W.

Why this performs well in diverse environments
- Against unconditional defectors: ATR defects after short punishment and doesn’t get repeatedly exploited.
- Against conditional cooperators (e.g., other tit-for-tat-like players): ATR quickly converges to mutual cooperation because it rewards rounds with high cooperation and returns to cooperation after short punishment periods.
- Against noisy or mistaken behaviour: The combination of WINDOW smoothing, forgiveness probability, and contrition prevents long-lasting collapse from one-off mistakes.
- Against exploitative strategies: Proportional punishments and the short memory limit losses while still giving honest cooperators a path back to mutually beneficial play.

Implementation notes
- The only required inputs are n, r, k and the full history of past actions.
- Randomness (for forgiveness) should be seeded for reproducibility if desired.
- Implementers may tune alpha, W, MAX_PUNISH and p based on the tournament environment or run a short calibration (e.g., larger p in noisy populations).

Summary
ATR is a simple, interpretable, cooperative strategy: start cooperative, condition future cooperation on recent group cooperation, punish shortfalls briefly and proportionally, forgive occasionally, and defect in the last round. It is adaptive and robust across a wide range of opponent behaviors without requiring communication or externally imposed coordination.
'''

description_COOPERATIVE_46 = '''
High-level idea (informal)
- Start cooperative to signal willingness to cooperate.
- Cooperate while the group shows substantial cooperation (at least a simple majority of players cooperated on the previous round).
- If the group falls below that cooperation threshold, respond with a short, proportional punishment: defect for a small number of rounds that grows with how severely the group deviated, but is capped so punishment cannot wipe out the rest of the game.
- Forgive quickly when the group recovers: as soon as observed cooperation meets the threshold again, resume cooperation.
- In the final round, behave cooperatively only if the group just proved cooperative; otherwise avoid being exploited.

This is a conditional-cooperation strategy with proportional punishment and prompt forgiveness. It is adaptive (reacts to how many defected), robust (tolerates some defectors, punishes clear deviations), and only depends on game parameters and observed history.

Parameters used by the strategy (derived only from n, r, k)
- tau (cooperation threshold): 0.5 (require a simple majority cooperating in the previous round to consider the group cooperative). Rationale: midpoint between all-defect and all-cooperate; tolerates a minority of defectors.
- P_max (maximum punishment length): max(1, ceil(r/10)). Rationale: punishment scales with horizon but cannot consume the whole remaining game; allows recovery.
(These are fixed functional choices; they can be adjusted before a tournament if desired. The strategy itself uses only tau and P_max plus observed history.)

Precise decision rules
Notation:
- t = current round (1..r)
- history[t'] = vector of the n observed actions in round t' (C or D), for all t' < t
- coop_count(t') = number of C in history[t']
- r_remaining = r - t + 1 (including current round)

Rules (natural language)
1. First round (t = 1): play C.
2. For t > 1:
   - Let m = coop_count(t-1) be the number of cooperators observed last round.
   - If t == r (final round):
       - If m >= tau * n, play C (cooperate in the final round only when the group just demonstrated majority cooperation).
       - Else play D (avoid being exploited in the one-shot endgame).
   - Else (not the final round):
       - If m >= tau * n: play C (group is cooperative — reciprocate).
       - If m < tau * n:
           - Compute shortfall = ceil(tau * n) - m (number of cooperators “missing” below threshold).
           - Set punishment_length = min(P_max, max(1, shortfall)).
           - Enter a punishment phase: defect for punishment_length consecutive rounds (including current round), unless the group prematurely recovers to m >= tau * n in some later observed round — in that case immediately stop punishment and resume cooperation.
3. Forgiveness / recovery:
   - During punishment, after each round observe coop_count. If coop_count >= tau * n at the start of any round, stop punishing and cooperate that round and onward (until another deviation).
4. Persistent deviation:
   - If the group repeatedly fails to reach tau for long periods, this rule will cycle: punish for short windows then test for recovery; punishments are proportional and bounded so the strategy does not lock into forever defection (not a strict grim trigger), but it imposes cost on persistent defectors.
5. Bookkeeping:
   - The strategy needs to count consecutive punished rounds and track whether it is currently in a punishment phase, to know whether to defect or test again.

Pseudocode (concise)
Initialize:
  tau = 0.5
  P_max = max(1, ceil(r/10))
  punishment_timer = 0   // rounds left to defect as punishment

For each round t = 1..r:
  if t == 1:
    play C
    continue

  // update based on last round
  m = number of C in history[t-1]

  // if currently punishing, but group recovered, stop punishment
  if m >= tau * n:
    punishment_timer = 0

  if t == r:   // final round special case
    if m >= tau * n:
      play C
    else:
      play D
    continue

  if punishment_timer > 0:
    // continue punishment unless group already recovered above
    play D
    punishment_timer -= 1
    continue

  // not punishing now; decide based on last round
  if m >= tau * n:
    play C
  else:
    shortfall = ceil(tau * n) - m
    punishment_length = min(P_max, max(1, shortfall))
    punishment_timer = punishment_length - 1   // we will defect this round and have (length-1) to follow
    play D

Why this strategy is cooperative and robust
- Cooperative signal: it starts cooperating and reciprocates cooperation; when the majority cooperates it continues doing so, supporting mutual high payoffs (all-C yields payoff k each).
- Tolerant to noise/minor defection: it tolerates up to (about) half the group defecting without immediate long punishments; single or small numbers of defectors produce only short, proportional punishments rather than permanent collapse to defection.
- Proportional punishment: the punishment scales with how many defected relative to the threshold; a larger breakdown triggers longer punishment, discouraging coordinated mass defection.
- Forgiveness: quick return to cooperation once the group improves prevents endless mutual retaliation and lets the group recover to near-optimal payoffs.
- Endgame prudence: in the last round the strategy cooperates only if the group just demonstrated cooperation, avoiding exploitation in the final one-shot where defection is dominant.
- Low parameter complexity: only uses n, r, k indirectly (r to set punishment cap P_max). All decisions require only observed past actions and the two derived constants tau and P_max.

Possible modest variants (tunable)
- Lower tau (more tolerant) when k is large (public good is very valuable) and raise tau when k is close to 1 (cooperation less valuable). Example: tau = 0.5 - 0.1*(k - n/2)/n clipped to [0.3,0.7]. (Not required; left as a tuning option.)
- Increase P_max for longer games or if tournament opponents are expected to require stronger punishments.
- Use a longer memory test (e.g., average cooperation over last L rounds instead of only previous round) to smooth fluctuations; this makes the strategy more tolerant to single-round noise.

Summary
This strategy is a simple, interpretable conditional-cooperator:
- start by cooperating,
- cooperate when the last round showed majority cooperation,
- punish proportionally and briefly when cooperation falls below majority,
- forgive immediately when cooperation returns,
- in the final round only cooperate if the group just proved cooperative.

It balances being cooperative and forgiving with a measured deterrent against exploitation, and uses only game parameters and observed history.
'''

description_COOPERATIVE_47 = '''
Strategy name: Adaptive Conditional Cooperator (ACC) — a forgiving, population-aware Win‑Stay/Lose‑Shift rule tuned to the public‑goods payoff structure.

Summary (informal)
- Start by cooperating to signal willingness.
- In every non‑final round, decide based on (a) how many players cooperated in the most recent round and (b) your last action. Cooperate when the recent group signal is strong enough, defect otherwise.
- If you change to defect because the group fell short, stay in the punishment state until you see a clear sign the group has returned to cooperating; then forgive and resume cooperating.
- Always defect in the final round (no future to reward cooperation).

Why this works
- It is cooperative: it begins by cooperating, returns to cooperation when the group shows willingness, and prefers sustained mutual cooperation when it is reciprocated.
- It is robust: it punishes clear and repeated exploitation but is forgiving and re‑establishes cooperation when opponents do. The cooperation threshold adapts to k and n so that the rule asks for more evidence of group cooperation when the multiplier k is small (cooperation is less productive) and accepts smaller cooperating minorities when k is large.
- It depends only on game parameters (n, k, r) and the observable history (who cooperated each round).

Parameters used by the strategy
- n, k, r — game parameters (given).
- m_threshold = max(1, ceil(n / k)). Interpretation: the minimum number of cooperators in the last round (counting everyone) required as a signal that cooperation is worth trying. This threshold decreases as k grows (public good is more productive → fewer cooperators needed to make cooperation attractive).
- (Optional) window W for smoothing noisy observations (default W = 1). Using W > 1 smooths occasional mistakes by looking at the average cooperation over the last W rounds rather than a single round.

Decision rules (plain language)
1. First round: play C (cooperate).
2. Last round (t = r): play D (defect).
3. For any non‑final round t > 1:
   a. Let m be the number of players who cooperated in the immediately preceding round (or the average number cooperating across the last W rounds if using smoothing).
   b. If m >= m_threshold: treat the group signal as “cooperative” → play C (cooperate).
   c. If m < m_threshold: treat the group signal as “non‑cooperative” → play D (defect).
4. Forgiveness / recovery: once you observe m >= m_threshold you immediately resume cooperation on the next non‑final round. There is no permanent grim trigger; punishment is temporary and contingent on observable group recovery.
5. If you switched to D because m < m_threshold, remain D until you see m >= m_threshold (i.e., you punish by withholding cooperation until a clear cooperative signal appears).
6. Optional mild leniency: if you personally cooperated last round and were one of the only cooperators (e.g., you cooperated but m is just below threshold and r - t is large), you might still wait one round to see if the group recovers; this is captured by using smoothing window W > 1.

Pseudocode (deterministic)
Inputs: n, k, r, history of rounds (for each past round t': list of who played C)
Compute m_threshold := max(1, ceil(n / k))
Set W := 1 (or small integer like 2–3 if you expect noisy play)

For current round t = 1..r:
  if t == 1:
    action := C
  else if t == r:
    action := D
  else:
    Let m := average over last W rounds of (number of cooperators in that round)
      (if W = 1 then m is the number of cooperators in round t-1)
    if m >= m_threshold:
      action := C
    else:
      action := D
  Play action

Notes and refinements
- Smoothing (W > 1): if opponents sometimes make mistakes or stochastic moves, setting W = 2 or 3 prevents single accidental defections from collapsing cooperation. Use W = min(3, r-1).
- Tuning m_threshold: ceil(n/k) is conservative and interpretable: higher k → smaller threshold. If you want to be more demanding (stronger punishment), raise the threshold by 1 or use ceil(α·n/k) with α>1; if you want more generosity, use floor(n/k) or reduce α.
- Endgame handling: always defect on the final round. (You may optionally defect on the last two rounds if r is small and you want extra safety, but that reduces cooperation opportunities.)
- Against pure defectors: the strategy quickly converges to D and stops being exploited.
- Against reciprocators and conditional cooperators: the strategy re‑establishes and sustains cooperation when a sufficient fraction of players reciprocate.
- Transparency: the rule is simple and observable from play history, so other conditional cooperators can detect and reciprocate it.

Example (n = 6, k = 2, r arbitrary)
- m_threshold = ceil(6 / 2) = 3.
- Start C. If the previous round had at least 3 cooperators, play C; otherwise play D. If the group recovers to 3+ cooperators you resume C.

Why not always cooperate or use a one‑shot trigger?
- Always cooperating is trivially exploited (one‑shot incentive to defect).
- Grim‑trigger (defect forever after a single defection) is harsh and can lock the group into mutual defection after noise or a single opportunistic defection.
- ACC balances protection (temporary punishment) with forgiveness (return to cooperation when the group shows willingness), which is important in multi‑agent tournaments where you will face a mix of exploiters, reciprocators and noise.

Implementation checklist for the tournament
- Compute m_threshold = max(1, ceil(n / k)).
- Choose smoothing window W (default 1; use 2–3 if you expect noisy opponents).
- Maintain history of cooperations per round (or compute on the fly).
- Follow the pseudocode; ensure final round is always D.

This strategy is simple to implement, adapts to k and n, is cooperative-minded, punishes exploitation, and forgives when others return to cooperative behavior.
'''

description_COOPERATIVE_48 = '''
Name: Lenient Proportional-Tit‑for‑Tat with Forgiveness (LPTFT-F)

Intuition (short)
- Start cooperatively and try to sustain high group cooperation.
- Be forgiving and tolerant of small or accidental defections to avoid needless collapse.
- Respond to larger or repeated defections with proportional, short punishments that are targeted at restoring cooperation (not destroying payoffs).
- Use a short endgame rule to avoid being exploited at the known final rounds.
- Use a small amount of randomized forgiveness during punishments so the strategy can re‑establish cooperation if others are willing.

All decisions depend only on game parameters (n, r, k) and observed history of actions. No communication or external coordination is required.

Parameters (computed from game parameters)
- W = min(5, max(1, floor(r/5))) — recent-window length to measure behavior (practical default: up to 5 rounds).
- tolerated_defect_fraction = min(0.25, 1/(2*sqrt(n))) — small tolerance for occasional defects (scales with n).
- punishment_base = max(1, ceil(r/6)) — a base scale for punishment lengths (short relative to total rounds).
- punishment_scale = 0.5 — multiplies observed defect severity to set punishment length.
- forgiveness_prob = 0.08 — small probability to cooperate during a punishment round (random test of recovery).
- endgame_len = min(3, r-1) — final rounds where we switch to conservative play to guard against endgame exploitation.

(You may tune constants; they were chosen to be conservative and robust across many n,r,k.)

State kept between rounds
- punish_until_round (integer): the round number until which the strategy is actively punishing (0 if not punishing).
- last_punish_start (round index): for bookkeeping if needed.

High-level decision rules
1. Round 1: Cooperate (C). Start by signaling willingness to cooperate.
2. For every round t > 1:
   a. If t <= punish_until_round:
      - With probability forgiveness_prob play C (test for recovery).
      - Otherwise play D (continue punishment).
   b. Else (not currently punishing):
      - Compute recent behavior over the last W rounds (or as many rounds as exist if t-1 < W):
         - For each other player j compute recent_coop_j = (# times j played C in those rounds) / window_size.
         - Let recent_group_coop_fraction = (1/(n-1)) * sum_j recent_coop_j.
         - Let recent_round_defectors = number of players who defected in the most recent completed round (t-1).
         - Let recent_round_defect_fraction = recent_round_defectors / n.
      - If we are in the endgame (r - t + 1 <= endgame_len):
         - Cooperate only if recent_group_coop_fraction >= 0.9 (very high cooperation signal) AND last round had few defectors (recent_round_defect_fraction <= tolerated_defect_fraction). Otherwise defect.
      - Else (normal phase):
         - If recent_group_coop_fraction >= 1 - tolerated_defect_fraction:
             - Cooperate (reward near-universal cooperation).
         - Else if recent_round_defect_fraction <= tolerated_defect_fraction:
             - Cooperate (tolerate a few defects so accidental or isolated deviations don’t collapse cooperation).
         - Else (significant defection observed):
             - Enter proportional punishment:
                * severity = recent_round_defect_fraction  (in (0,1])
                * P = min( r - t, max(1, ceil(punishment_base * punishment_scale * severity)) )
                * Set punish_until_round = t + P - 1
                * Play D this round (start punishment). Future rounds until punish_until_round will follow the punishment rule in step 2a.
3. After the punishment window ends, resume the normal checks in step 2b; if others have mostly returned to cooperation resume cooperating.

Why these rules?
- Start cooperatively to encourage mutual cooperation (this yields the highest joint payoff when others reciprocate).
- Tolerant thresholds prevent single or isolated defections (which could be mistakes or exploratory) from triggering long, self‑damaging punishments.
- Punishment length is proportional to the observed severity (fraction of defectors), bounded and short relative to r so we don't waste too many rounds punishing.
- Randomized forgiveness during punishment offers a low-cost test whether opponents are trying to return to cooperation, helping avoid infinite punishment cycles.
- Endgame conservatism reduces exploitation in the last known rounds where backward induction pressure is largest.

Pseudocode (per round t observing history H up to round t-1)
Inputs: n, r, k, t, history H (H contains actions of each player by round)
State: punish_until_round (init 0)

window_size := min(W, t-1)  // number of past rounds to inspect; if t=1 then handled above
if t == 1:
    action := C
    return action

if t <= punish_until_round:
    // in punishment phase
    if random() < forgiveness_prob:
        action := C
    else:
        action := D
    return action

// compute recent stats
if window_size >= 1:
    for each player j != me:
        recent_coop_j := (# of C by j in rounds t-window_size .. t-1) / window_size
    recent_group_coop_fraction := avg_j recent_coop_j
else:
    recent_group_coop_fraction := 1.0  // only happens if t==2 and we treat round 1 full-coop if it was

// recent completed round statistics
last_round_actions := actions in round t-1
recent_round_defectors := count of D in last_round_actions
recent_round_defect_fraction := recent_round_defectors / n

remaining_rounds := r - t + 1

// endgame behavior
if remaining_rounds <= endgame_len:
    if recent_group_coop_fraction >= 0.9 and recent_round_defect_fraction <= tolerated_defect_fraction:
        action := C
    else:
        action := D
    return action

// normal-phase rules
if recent_group_coop_fraction >= 1 - tolerated_defect_fraction:
    action := C
    return action

if recent_round_defect_fraction <= tolerated_defect_fraction:
    // small slip — be lenient
    action := C
    return action

// significant defection -> proportional punishment
severity := recent_round_defect_fraction  // fraction in (0,1]
P := min(remaining_rounds - 1, max(1, ceil(punishment_base * punishment_scale * severity)))
punish_until_round := t + P - 1    // set punishment window
action := D   // begin punishment now
return action

Notes on edge cases and additional considerations
- If many players are defectors permanently, LPTFT-F will often default to D (to avoid being exploited repeatedly). But it will still test for recovery due to the forgiveness_prob and periodic rechecks.
- If players coordinate on long-run cooperation, LPTFT-F cooperates every round (except occasional forgiveness randomness) and thus achieves the cooperative payoff.
- The punishment is collective (we defect ourselves) rather than expensive targeted retaliation (there is no cheap way to punish an individual in this public-goods payoff structure). Punishing is costly to ourselves, so punishments are short and proportional to avoid excessive self-harm.
- The strategy is deterministic except for occasional random forgiveness; that small randomness is only used while punishing and helps detect reconciliations.
- The constants (W, tolerated_defect_fraction, punishment_base, punishment_scale, forgiveness_prob, endgame_len) are chosen for robustness; you can tune them for particular tournaments (e.g., increase punishment_base if you want harsher enforcement when k is large).

Cooperative mindset summary
- LPTFT-F always begins cooperative, rewards widespread cooperation, tolerates small slips, punishes proportionally when defection is significant, and uses forgiveness to re-establish cooperation quickly. It also respects endgame concerns so it is not trivially exploited in final rounds. This combination is adaptive and robust against a wide range of opponent behaviours while explicitly trying to sustain efficient, cooperative outcomes when other players are willing to reciprocate.
'''

description_COOPERATIVE_49 = '''
High-level idea
- Start cooperatively to signal willingness to sustain public good.
- Condition future cooperation on nearby history of others’ cooperation (who is reliably cooperative), not on single-round noise.
- Punish defections in a measured, targeted and temporary way (short, proportional punishments), then forgive so cooperation can recover.
- Always defect in the final round (no future to enforce cooperation).
This keeps the strategy cooperative with reciprocation and forgiveness, but avoids being a long-term sucker or being easily exploited.

Parameters the algorithm uses (derived from game parameters and history)
- n, r, k: given by the environment.
- L = min(5, r − 1): history window length (use up to last 5 rounds; fewer if game short).
- θ = 0.6: cooperation-rate threshold for labeling a player “reliable cooperator.”
- φ = 0.6: target fraction of players (including self) whose cooperation we want to expect before cooperating.
- γ = 0.4: “mass-defection” sensitivity (fraction of others defecting that triggers stronger punishment).
- P_min = 0.2: minimum probabilistic cooperation when borderline (keeps generosity).
- Punishment base length P_base = 2 rounds (punishments are short).

These numeric values are recommendations. Implementation may tune them for the tournament environment, but the decision rules below assume such defaults.

Observables (available from history)
- For every prior round t, we observe each player j’s action c_j,t ∈ {0,1}.
- For current round number t (1..r), let history be rounds 1..t−1.

Definitions used in rules
- coop_rate_j = fraction of rounds in the last L rounds in which j cooperated (if fewer than L rounds exist, use all available prior rounds).
- reliable_j = (coop_rate_j ≥ θ).
- expected_reliable_count = 1 + number of other players j with reliable_j true (the “1” counts ourselves if we decide to cooperate).
- last_round_cooperators = number of players who cooperated in round t−1 (0 if t=1).
- last_round_defectors = n − last_round_cooperators.

Decision rules (deterministic core with mild randomness near thresholds)

1) First round (t = 1)
- Cooperate. Rationale: open with cooperation to try to establish mutual cooperation.

2) Last round (t = r)
- Defect. Rationale: no future rounds to enforce cooperation; standard backward-induction endgame behavior.

3) Intermediate rounds (1 < t < r)

A. Compute reliability and estimates
- For each player j ≠ me compute coop_rate_j over the last L rounds.
- Set reliable_j = (coop_rate_j ≥ θ).
- expected_reliable_count = 1 + Σ_{j≠me} reliable_j.

B. If expected_reliable_count ≥ ceil(n × φ)
- Cooperate. Rationale: enough players appear reliably cooperative to justify contributing to sustain high group payoff.

C. Else (expected_reliable_count < ceil(n × φ))
- Evaluate recent shock:
  - If last_round_defectors / n ≥ γ (a large fraction defected last round, mass defection):
      - Enter short punishment: Defect for P = min(P_base + ceil(n × (last_round_defectors / n − γ)), r − t) rounds (but never longer than remaining rounds). The punishment duration increases mildly with the severity of the mass defection. While punishing, mark all players who defected last round as “unreliable” but continue collecting their history.
  - Else (only a few defectors or borderline case):
      - Use probabilistic cooperation: compute p = P_min + (expected_reliable_count / n) × (1 − P_min). With probability p: cooperate; else defect.
      - Rationale: when group reliability is borderline, keep some generosity to avoid collapse from noise and mistakes while protecting against exploitation.

D. Targeted response to persistent individual defectors
- If some players j have coop_rate_j below an extended tolerance (e.g., below 0.2 over last L rounds) and have been repeatedly defecting while others are reliable, treat them as persistent exploiters. If persistent exploiters exist and their number is small, we still follow the group rule above but do not increase our cooperation probability to be larger than necessary (i.e., we rely on the group threshold to decide). The short punishment (C) reduces benefit to exploiters.

E. Forgiveness and recovery
- After any punishment period ends, recompute coop_rate_j over the recent L rounds. If many players again meet reliable_j, resume cooperation under rule B. Forgiveness is automatic due to rolling window: as players resume cooperating their coop_rate_j rises above θ.

Implementation notes (pseudocode sketch)

Inputs each round: t, r, n, history matrix c[j][1..t−1]
Compute L_eff = min(L, t−1)
For each j ≠ me:
  coop_rate_j = sum_{s=t−L_eff to t−1} c[j][s] / L_eff (if L_eff = 0 treat as 0)
  reliable_j = (coop_rate_j ≥ θ)
expected_reliable_count = 1 + sum_j reliable_j
last_round_cooperators = sum_j c[j][t−1] (0 if t==1)
last_round_defectors = n − last_round_cooperators

if t == 1:
  action = C
else if t == r:
  action = D
else:
  if expected_reliable_count ≥ ceil(n * φ):
    action = C
  else if last_round_defectors / n ≥ γ:
    // Punish briefly
    P = min(P_base + ceil(n * (last_round_defectors / n − γ)), r − t)
    // Maintain a punishment counter in state; if currently punishing, decrement and keep defecting
    action = D
  else:
    p = P_min + (expected_reliable_count / n) * (1 − P_min)
    action = C with probability p, else D

Edge cases / clarifications
- Short games (r ≤ 3): L is small; still start cooperative, defect final round. The probabilistic generosity helps avoid being exploited in very short sequences.
- Small groups (n = 2): thresholds scale (ceil(n*φ) is 2×0.6 → ceil(1.2) = 2), so you cooperate only if the other is reliable (as desired).
- Noise / errors: Because we use a short rolling window (L) and probabilistic cooperation when borderline, the strategy tolerates occasional mistakes and does not punish forever for single errors.
- Exploitation resistance: Persistent defectors reduce expected_reliable_count, causing our strategy to defect until they become reliable; mass defections trigger a short collective punishment to reduce free-rider payoffs quickly.
- No reliance on coordination: The strategy uses only observed actions and its own internal counters; it does not require messaging, shared schedules, or precommitments.

Why this strategy is cooperative yet robust
- Cooperative: it starts cooperatively, returns to cooperation quickly when others are sufficiently cooperative, and uses short forgiveness windows so cooperation can recover after mistakes.
- Robust: it conditions on actual behavior (per-player reliability) and punishes proportionally and temporarily. It avoids unconditional cooperation (which is exploitable) and avoids harsh “grim” strategies that punish forever for a one-off defect. It defects in the final round (r) to avoid being exploited at the end.
- Adaptive: thresholds, window length and probabilistic acts make it responsive to different opponent mixes (all-cooperators, many defectors, noisy cooperators, strategic exploiters).

Recommended parameter tuning
- If the tournament environment is noisy (random mistakes), increase L to 8 and decrease θ to 0.5 for greater forgiveness.
- If opponents tend to use sophisticated reputation-systems, increase θ to 0.7 to avoid being exploited by sporadic cooperators.
- If group size n is very large, scale P_base up modestly (to 3) so that mass defections can be countered effectively.

Summary (short)
- Round 1: cooperate.
- Rounds 2..r−1: compute which players have been reliably cooperating in the recent L rounds. Cooperate when a sufficiently large fraction appears reliable; otherwise defect, but (a) if many defected last round do a short proportional punishment; (b) if borderline, cooperate with a moderate probability to be generous yet guarded.
- Round r: defect.
This produces a cooperative, forgiving, reputation-based, and robust strategy suitable for tournament play in repeated multi-player public–goods games.
'''

description_COOPERATIVE_50 = '''
Strategy name: Adaptive Conditional Cooperator (ACC) — reciprocal, forgiving, endgame-aware

Intuition (short)
- Start by signalling willingness to cooperate. Cooperate when recent evidence shows a majority (adjusted by efficiency k) of the other players have been cooperating.  
- If the group’s cooperation falls below a threshold, switch to defection to avoid exploitation. Use a short recent-history window so the strategy is responsive.  
- Include limited forgiveness (probabilistic return to cooperation when the group recovers) to tolerate noise and avoid permanent breakdown.  
- Defect in the final round (no future to reward cooperation). Be explicit about the first and last rounds and how to handle short games.

This strategy only uses the game parameters (n, k, r) and the observed history of contributions (who played C or D in earlier rounds). It does not rely on communication or external coordination.

Parameters (computed from game parameters)
- window w = min(5, r - 1). (Number of most recent rounds used to estimate others’ cooperativeness. If r is small, w shrinks automatically.)
- endgame_rounds m_end = 1. (Defect in the final round because there is no future to reward cooperation. This is minimal endgame adjustment.)
- forgiveness_prob f = 0.10. (Probability to forgive and cooperate even when short-term statistics are slightly unfavorable — prevents lock-in.)
- group_threshold theta = max(0.5, 1 - k/n). Rationale: when the public good is less efficient (k/n small) require stronger recent group cooperation to justify risking cooperation; when it’s more efficient, be more lenient (but always require at least a simple majority).

Core decision rule (natural language)
- First round: Cooperate (signal willingness).
- Last round (t = r): Defect.
- For rounds t in 2..r-1:
  - Compute recent_coop_rate = fraction of the other players’ moves that were C over the previous min(w, t-1) rounds.
    - Formally: recent_coop_rate = [Σ_{τ=t-w}^{t-1} Σ_{j≠me} c_j,τ] / [(n-1) * min(w, t-1)].
  - If recent_coop_rate >= theta → cooperate.
  - Else (recent_coop_rate < theta) → defect, unless a forgiveness draw occurs:
    - If recent_coop_rate >= (theta - 0.10) and a uniform random number in [0,1) < f, then cooperate (forgive).
    - Otherwise defect.
- Recovery/back-to-cooperation rule: once you resume cooperating because recent_coop_rate >= theta (or via forgiveness), continue cooperating as long as recent_coop_rate stays within a small margin of theta (i.e., don’t flip every single round on tiny fluctuations). Practically this is achieved by using w > 1 and the threshold margin used in forgiveness.

Targeted exploitation handling (group-level)
- This is a public-goods environment with a single action for all opponents each round, so one cannot retaliate against individuals without harming everyone. ACC therefore judges the entire group’s behavior. If a minority of persistent defectors reduces the group’s recent_coop_rate below theta, ACC defects to avoid repeated exploitation. If the minority is small and the rest cooperate so recent_coop_rate stays above theta, ACC keeps cooperating and so does not overreact to a few defectors.

Edge cases
- Very short games:
  - If r = 2: ACC cooperates round 1, defects round 2 (first-round signal is preserved; no cooperation expected in the last round).
  - If r = 3: w = 2 (min), ACC cooperates round 1; in round 2 it uses the data from round 1 to decide; round 3 defect.
- Noise or accidental mis-presses: forgiveness_prob f enables recovery after accidental defections in the group instead of permanent collapse.
- n = 2: recent_coop_rate is just the other player’s recent C fraction; behavior reduces to a forgiving tit-for-tat-like pattern.
- k very close to n (high efficiency): theta = max(0.5, 1 - k/n) will be 0.5. ACC requires only a simple majority of others cooperating to continue cooperating. That is appropriate since the public good is valuable and cooperation is easier to sustain.
- k only slightly above 1 (low efficiency): theta increases; ACC is more conservative and requires stronger evidence of cooperation to avoid exploitation.

Pseudocode

Inputs: n, k, r, t (current round, 1-indexed), history matrix C[j][τ] for j=1..n, τ=1..t-1, where C[j][τ]=1 if j played C in round τ else 0. (Assume you are player i; treat C[i][τ] similarly)

Compute parameters:
  w = min(5, max(1, r-1))   // use at least 1 for safety
  m_end = 1
  f = 0.10
  theta = max(0.5, 1 - k/n)
  forgiveness_margin = 0.10   // one-sided margin to allow forgiveness when slightly below theta

Function decide_action(t, history):
  if t == 1:
    return C   // first-round cooperation to signal
  if t > r - m_end:
    return D   // last m_end rounds: defect
  // compute recent cooperation rate among others
  lookback = min(w, t - 1)
  total_other_C = 0
  for τ from t - lookback to t - 1:
    for each player j != me:
      total_other_C += C[j][τ]
  recent_coop_rate = total_other_C / ((n - 1) * lookback)
  if recent_coop_rate >= theta:
    return C
  else:
    if recent_coop_rate >= (theta - forgiveness_margin) and random_uniform(0,1) < f:
      return C   // probabilistic forgiveness
    else:
      return D

Comments on robustness and tournament play
- ACC is cooperative: it starts by cooperating, tries to sustain cooperation when the group reciprocates, and uses a forgiving threshold to avoid collapsing on occasional errors.
- ACC is adaptive: it uses a finite window of recent history so it responds quickly to changes in group behavior (short-term trends). The window w is small so ACC can detect exploitation and stop cooperating quickly.
- ACC is robust: it does not assume partner norms or sophisticated coordination signals. It punishes by refusing to cooperate when the group’s cooperation falls below a (k-aware) threshold, which prevents persistent exploitation. Forgiveness prevents permanent lockout after accidental defections.
- Endgame handling: ACC defects in the last round (no future incentive to cooperate) and thus avoids being exploited in the final stage. This is minimal and transparent; it also reduces incentives for others to defect earlier just because of the final round.

Tunable knobs (implementer may tune by tournament)
- window w (5 is a reasonable default; increase if opponents are noisy and you want smoother behavior).
- forgiveness_prob f and forgiveness_margin (increase to tolerate more noise; decrease to be harsher).
- m_end (increase if you want conservative retreat in the final few rounds).

Why this is a good tournament strategy
- It rewards cooperation early (promoting mutual gains) and punishes consistent group-level defection to avoid exploitation.
- It is simple and interpretable (important when many opponents are unknown) and uses only observable history.
- It’s resistant to constant defectors (it will quickly stop cooperating) and tolerant of mostly-cooperative groups where a few players occasionally defect.
- The threshold theta depends on k and n in a simple way so ACC becomes stricter when individual contributions are less effective (you need stronger evidence that others reciprocate before you risk cooperating).

You can implement the pseudocode directly. The constants (w, f, forgiveness_margin, m_end) can be adjusted if you learn from competition dynamics; the core idea — cooperate when recent group cooperation ≥ threshold(k,n), defect otherwise, with forgiveness and last-round defect — should remain.
'''

description_COOPERATIVE_51 = '''
Name: Adaptive Forgiving Conditional Cooperator (AFCC)

Short description
- Start cooperatively to try to establish cooperation.
- Condition future cooperation on observed cooperation by others in recent history.
- If exploited, punish by defecting for a limited, parameter-dependent number of rounds (punishment length increases when the private gain from defecting is large), but forgive early if cooperation is restored.
- Always defect in the final round (to avoid unilateral exploitation in a one-shot) and progressively reduce willingness to risk long punishments near the end.

All decisions depend only on game parameters (n, k, r) and the publicly observed history of actions.

Derived internal parameters (computed from n, k, r)
- memory m = min(5, r - 1)  // how many past rounds we use to estimate recent behavior (adjustable)
- punish_length P = min(r, max(1, ceil(1 / (1 - k/n))))  // punishment duration scaled to private incentive to defect
  - Rationale: immediate private gain from defecting is g = 1 - k/n. We choose P roughly proportional to 1/g so that future loss can plausibly outweigh the one-shot gain.
- coop_threshold τ = 0.5  // require at least a simple majority cooperating (in recent history) before cooperating ourselves
  - This is conservative and robust; τ may be tuned toward 0.3 (more generous) or towards 0.6 (stricter) depending on the tournament but 0.5 is a robust default.
- endgame window E = P  // when remaining rounds ≤ E, punishment becomes ineffective; we reduce willingness to keep punishing

State variables maintained by the strategy
- punish_counter (initially 0) — how many remaining punishment rounds we will defect
- history list of observed fractions f_t = (number of other players who cooperated in round t) / (n - 1) for past rounds t

Decision rules — natural-language then concise pseudocode

Natural-language rules
1. First round: cooperate (C). This tests the population and gives cooperative strategies a chance to establish mutual cooperation.
2. Last round (t = r): defect (D). One-shot dominant action; cooperating in last round is exploitable and cannot be sustained by future punishment.
3. If punish_counter > 0: play D (we are currently punishing). After making this play, decrement punish_counter by 1.
   - Exception (forgiveness): if in the immediately preceding round every other player cooperated (f_{t-1} = 1.0), clear punish_counter and play C instead (forgive immediately when others fully reciprocate).
4. Otherwise (not currently punishing and not final round): estimate avg_coop = average of f_{t'} over the most recent up to m rounds (use available history if fewer rounds so far).
   - If avg_coop ≥ τ, play C.
   - If avg_coop < τ, play D.
5. Triggering a punishment: if in the previous round you cooperated and the fraction of other players cooperating in that round f_{t-1} < τ (i.e., you were effectively exploited relative to your expectation), set punish_counter = P (start a punishment window) after observing that round.
6. Endgame behavior: if remaining rounds rem = r - t + 1 ≤ E, set a higher bar for starting new punishments (do not start a new full-length punishment unless the recent exploitation was severe — e.g., f_{t-1} ≤ τ - 0.25). This avoids lengthy punishments that cannot be made credible when few rounds remain.

Pseudocode (deterministic; can be implemented directly)

Initialize:
  punish_counter = 0
  history = []  // will store f_t for each completed round

On each round t (1..r), before choosing action:
  if t == 1:
    choose C
    // After the round we will observe others and append to history
    return

  // Final round
  if t == r:
    choose D
    return

  // If we are in punishment mode
  if punish_counter > 0:
    // check forgiveness condition using last round
    if history and history[-1] == 1.0:
      punish_counter = 0
      // allow cooperation
    else:
      choose D
      punish_counter = punish_counter - 1
      return

  // Not currently punishing
  // Compute avg_coop over up to m most recent rounds
  use_count = min(m, len(history))
  if use_count == 0:
    avg_coop = 1.0 if t == 2 else 1.0  // after first cooperative move, treat as cooperative until we get data
  else:
    avg_coop = mean( history[-use_count:] )

  // If recent cooperation meets threshold, cooperate
  if avg_coop >= τ:
    choose C
    return
  else:
    choose D
    return

After each round completes (we observe actions of all players), update state:
  let others_coop = (number of cooperators this round minus our own action C?1:D?0)
    // if simultaneous, we observe after we act
  f = others_coop / (n - 1)
  append f to history

  // If we cooperated in that round and others cooperated less than threshold, start punishment
  if our_action_in_that_round == C and f < τ:
    rem = r - current_round_index
    if rem > E:
      punish_counter = P
    else:
      // near end - only start punishment if exploitation severe
      if f <= max(0, τ - 0.25):
        punish_counter = P
      else:
        // do not start a long punishment when few rounds left
        punish_counter = 0

Rationale and robustness properties
- Cooperative-first: starting C gives cooperating opponents the chance to establish mutual cooperation and achieve high joint payoffs.
- Reciprocity: cooperating only when a simple majority (τ = 0.5) has cooperated recently prevents being a naive sucker.
- Proportional punishment: the punishment length P is scaled by the private one-shot gain from defection (1 - k/n). When defecting gives a large immediate reward, punishments are longer, making the cost of defection for others potentially exceed the immediate gain.
- Forgiving: immediate forgiveness if everyone else fully cooperates in the previous round; also allows early abortion of punishment if cooperation is restored. This prevents long cycles of retaliation and supports re-establishing cooperation.
- Endgame awareness: reduces starting new long punishments when few rounds remain (punishments would be ineffective then), and always defects in the last round (standard safeguard against last-round exploitation).
- Memory-bounded and adaptive: uses only recent m rounds (default m ≤ 5) to be responsive to changing populations; history-based, deterministic, and only depends on observed actions and parameters.
- Resistant to exploitation: will stop cooperating if the population is largely defecting; will punish exploiters in a way tied to how attractive defection is.
- Works without coordination, pre-commitment, or common strategy knowledge; it leverages observed behavior to adapt.

Tuning suggestions (optional)
- If the tournament environment tends to be highly cooperative, lower τ to 0.3 to be more generous.
- If many exploiters appear, increase τ to 0.6 or increase P to punish longer.
- Increase memory m to smooth noisy environments; decrease m to react faster to strategic shifts.

Edge cases
- r = 2: Play C in round 1, D in round 2. This is the best cooperative attempt given the small horizon.
- Very small r: memory m reduces automatically; P is bounded by r to avoid impossible punishments.
- If n = 2 (pairwise PD style): this reduces to a forgiving WSLS-like conditional cooperator.
- If everyone is always C, AFCC will keep cooperating until end except last round (where it defects).
- If everyone always D, AFCC quickly switches to always D after observing insufficient cooperation.

Conclusion
AFCC is simple, history-based, forgiving, and parameterized by (n, k, r). It aims to sustain cooperation when a sizable fraction of the population reciprocates, deters exploitation with punishments proportional to the temptation to defect, and forgives rapidly when cooperation is restored. It is robust to a wide range of opponent behaviors and can be tuned if a different trade-off between generosity and defensiveness is desired.
'''

description_COOPERATIVE_52 = '''
Strategy name: Forgiving Proportional Trigger (FPT)

High-level intuition
- Begin by signaling cooperation. Cooperate while others are cooperating.
- If one or more players defect, punish in proportion to the number of defectors to make defection costly. Keep punishments short and forgiving so cooperation can resume quickly after errors or experiments.
- Make punishments no longer than the remaining horizon (so they remain credible in a finite game). In the final round, avoid being exploited by defaulting to defection unless cooperation has been reliably sustained.
- The strategy depends only on the game parameters (n, r, k) and observed history (who cooperated each past round); it does not rely on any coordination or communication.

Parameter choices (interpretable and implementable)
- gamma (punishment scale): 1. Punishment length = ceil(gamma × number_of_defectors). Gamma = 1 gives 1-round punishment per defector.
- P_max: optional cap on punishment length (suggest P_max = max(1, min(5, floor(r/3)))) — prevents overly long retaliation in long games.
- Forgiveness rule: after a punishment period, immediately try to return to cooperation (one-shot forgiveness).
- Last-round cooperation rule: default to defect in the final round unless cooperation has been unanimous for a small recent window (see rules below).

These parameters are simple and can be tuned in tournaments. Defaults above are robust across many n, r, k.

Decision rules (natural language)
1. First round (t = 1): play C (cooperate) to start cooperation.
2. After each round t, observe the number of cooperators M_t (including yourself). Compute number_of_defectors = n - M_t and remaining_rounds = r - t.
3. Maintain a punishment timer p_timer initialized to 0 and a counter consec_unanimous initialized to 0:
   - If M_t == n (everyone cooperated this round), increment consec_unanimous by 1.
   - If M_t < n, set consec_unanimous = 0 and set p_timer = min(remaining_rounds, min(P_max, ceil(gamma * number_of_defectors))). (If p_timer already > 0, you can extend it to the newly computed value if larger.)
4. Action selection for next round (t+1):
   - If remaining_rounds == 0 (i.e., we are at or beyond r): no action (game over).
   - If p_timer > 0: play D (defect) and decrement p_timer by 1 after the round. (This is the punishment phase.)
   - Else (p_timer == 0): normally play C (cooperate).
   - Exception for the final round (t+1 == r): play D by default to avoid one-shot exploitation, unless consec_unanimous >= min(2, r-1) (i.e., cooperation has been unanimous for at least two recent rounds or as many as available). If that unanimity condition holds, play C in the final round.
5. After punishment expires, the strategy immediately returns to cooperating (forgiving), unless a new defection triggers a new punishment.

Pseudocode
(Variables: p_timer = 0, consec_unanimous = 0)
Parameters: gamma = 1, P_max = max(1, min(5, floor(r/3)))

On round 1:
  play C

For t = 1 to r-1 do:
  observe M_t = total cooperators this round (including self)
  defec_count = n - M_t
  remaining_rounds = r - t

  if M_t == n:
    consec_unanimous += 1
  else:
    consec_unanimous = 0
    new_p = min(remaining_rounds, min(P_max, ceil(gamma * defec_count)))
    p_timer = max(p_timer, new_p)   # extend if already punishing but new offense is worse

  # choose action for round t+1
  if p_timer > 0:
    action = D
  else:
    if (t+1 == r):  # next round is final round
      if consec_unanimous >= min(2, r-1):
        action = C
      else:
        action = D
    else:
      action = C

  # after selecting action and observing the actual next round outcome, decrement p_timer if >0
  if p_timer > 0:
    p_timer -= 1

Why this design is cooperative and robust
- Cooperative bias: Always attempts cooperation unless a measured failure/misbehavior by the group justifies punishment. This signals willingness to maintain high collective payoff (full cooperation gives each player higher total payoff over repeated rounds when others reciprocate).
- Proportional punishment: Punishment length scales with the number of defectors, so large defections get stronger responses; small slips get short responses. This deters mass defections without incentivizing long vendettas.
- Forgiveness: Punishments are short and the strategy immediately tries to return to cooperation. That makes the strategy robust to one-off mistakes and to strategies that occasionally explore.
- Credibility in finite horizon: Punishment lengths are capped by remaining_rounds, so punishments are feasible and do not try to punish beyond the end of the game. The last-round default-to-defect avoids being exploited on the final move unless cooperation has been strongly and recently stable.
- Simplicity and observability: The rules use only observable totals per round and simple counters. This makes the strategy easy to implement and to reason about by other agents in a tournament (which helps sustain cooperation).
- Adaptivity: The strategy adapts to the observed level of cooperation (via consec_unanimous and defector counts) and scales response to severity of the deviation.

Edge cases and special notes
- Very short games (small r): punishment is automatically constrained by remaining rounds. In r = 2, you will cooperate on round 1, but if anyone defects round 1 you will defect on round 2 (punishment = remaining_rounds = 1); this is unavoidable in short horizons.
- Repeated identical defection: If opponents keep defecting, FPT will keep punishing (and thus also defecting), avoiding sustained exploitation. If opponents resume cooperation, FPT forgives quickly.
- Noise / accidental defection: Single or rare accidental defectors cause short punishments, but cooperation is resumed quickly, increasing resilience to noise.
- Parameters tuning: Tournament entrants can tune gamma and P_max. Larger gamma or P_max produce harsher punishments (more deterrence, less forgiveness); smaller values favor quicker restoration of cooperation.

Summary (short)
- Start cooperating.
- If any players defect, set a punishment timer equal to (roughly) the number of defectors (capped by remaining rounds and P_max) and defect while the timer runs.
- After punishment ends, return immediately to cooperating.
- In the final round, defect by default unless there has been unanimous cooperation for a short, recent run.
- This yields a cooperative, proportional, forgiving, and adaptive playstyle that is robust against exploitation, mistakes, and a wide range of opponent behaviors.
'''

description_COOPERATIVE_53 = '''
Name: Aspiration-Based Reciprocal (ABR) with Gradual Punishment and Endgame Softening

Summary intuition
- Start friendly to seed cooperation.
- Use a simple Win-Stay / Lose-Shift (Pavlov) rule based on a running aspiration level (my recent payoff). Pavlov-style rules are adaptive and robust in multi-player settings: they keep cooperating when cooperation pays, switch when it doesn't.
- Combine Pavlov with a short-term group check (how many others cooperated recently) so the strategy aligns with a cooperating majority and avoids being repeatedly exploited by many defectors.
- Punish proportionally and temporarily (gradual punishment), and forgive when the group returns to cooperative behavior.
- In the known-finite-horizon endgame (last rounds) bias toward defection by default (to avoid being exploited in a last-round collapse), except when the historical cooperation record is very strong (so cooperating is mutually beneficial even at the end).

Parameters derived from the game
- n, r, k are known to the strategy.
- Internal tunables (recommended defaults):
  - m = min(5, r-1) — recent-history window (how many prior rounds to summarize group behavior)
  - alpha = 0.20 — aspiration learning rate (how fast aspiration adapts)
  - endgame_window = min(2, r-1) — last rounds to treat specially
  - coop_high = 0.75 — fraction of others cooperating in recent window that signals a cooperative majority
  - coop_low = 0.25 — fraction of others cooperating in recent window that signals persistent defection
  - hist_coop_threshold = 0.95 — long-run cooperation threshold to override endgame default

State the strategy maintains
- A: current aspiration (numeric)
- prev_action: my action in the previous round (C or D)
- history of individual actions for all players (available per spec)
- my last payoff (π_last)

Initialization (before round 1)
- Set A := (1 + k) / 2 (midpoint between the all-defect per-round payoff = 1 and the all-cooperate payoff = k)
- prev_action := C
- Play C in round 1

Main decision rules (for each round t ≥ 2)
1. If t is in the endgame (t > r - endgame_window):
   - Compute long-run cooperation rate among others:
       hist_coop := (total cooperations by other players across all past rounds) / ((t-1) * (n-1))
   - If hist_coop ≥ hist_coop_threshold AND my prev_action == C:
       - Cooperate (we have a very reliable cooperating population; sustain it)
     Else:
       - Defect (default to safe play near the end)

2. Otherwise (normal rounds):
   a. Update aspiration A:
       A := (1 - alpha) * A + alpha * π_last
   b. Compute group_coop_recent:
       group_coop_recent := (total cooperations by other players in the previous m rounds) / (m * (n - 1))
       (If fewer than m rounds have occurred, use all available prior rounds.)
   c. Pavlov (Win-Stay / Lose-Shift) baseline:
       - If π_last ≥ A:
           candidate_action := prev_action    // stay with what worked
         Else:
           candidate_action := switch(prev_action)   // switch when payoff was below aspiration
   d. Group override (align with short-term majority or avoid exploitation):
       - If group_coop_recent ≥ coop_high and candidate_action == D:
           action := C   // join cooperative majority to restore mutual benefit
       - Else if group_coop_recent ≤ coop_low and candidate_action == C:
           action := D   // avoid being repeatedly exploited by a defecting majority
       - Else:
           action := candidate_action
   e. Gradual-punishment and forgiveness (implicit in Pavlov + group override):
       - Pavlov punishes short-lived shocks automatically by switching after a bad payoff but returns to cooperation if payoffs recover (forgiveness).
       - Because thresholds are moderate, punishment is not permanent and scales with how sustained group defection is.

3. Update state:
   - prev_action := action
   - After the round observe π_last and others’ actions, update history and A next turn.

Pseudocode (concise)
- See variables and defaults above.

Round 1:
  play C
  prev_action := C
  A := (1 + k) / 2

For each round t = 2..r:
  observe last payoff π_last and others’ actions in round t-1
  if t > r - endgame_window:
    hist_coop := total_cooperations_by_others_so_far / ((t-1)*(n-1))
    if hist_coop >= hist_coop_threshold and prev_action == C:
      action := C
    else:
      action := D
  else:
    A := (1 - alpha)*A + alpha*π_last
    m_eff := min(m, t-1)
    group_coop_recent := (cooperations_by_others_in_last_m_eff_rounds) / (m_eff*(n-1))
    if π_last >= A:
      candidate := prev_action
    else:
      candidate := switch(prev_action)
    if group_coop_recent >= coop_high and candidate == D:
      action := C
    else if group_coop_recent <= coop_low and candidate == C:
      action := D
    else:
      action := candidate
  play action
  prev_action := action
  // next loop will update A using this round’s payoff

Rationale and robustness
- Cooperative mindset: The strategy starts by cooperating and is biased to join cooperative majorities (group_coop_recent ≥ coop_high). This helps to establish and sustain high-payoff mutual cooperation when others reciprocate.
- Defense against exploitation: Pavlov switches when payoffs fall below aspiration. Combined with the group override (defect when most others defect), ABR avoids repeatedly contributing to a defecting group.
- Gradual punishment and forgiveness: Pavlov is neither an all-or-nothing grim trigger nor permanently unforgiving. It punishes by switching, but will return to cooperation as payoffs recover — this avoids long, costly vendettas and allows cooperation to reemerge.
- Endgame softening: Defaulting to defection in the last 1–2 rounds prevents trivial last-round exploitation; yet the hist_coop override lets the strategy sustain cooperation in the end if the group has been historically extremely cooperative (so cooperation still benefits everyone).
- Adaptive: aspiration A tracks experienced payoffs; the strategy adapts to opponents who change behavior, to groups of cooperators/defectors, and to noise or variable population mixes without assuming any common norm.
- Parameter sensitivity: Defaults above are conservative and generally robust. Tournament implementers can tune alpha, m, and thresholds to the tournament environment (e.g., increase coop_high and hist_coop_threshold in populations known to be less noisy).

Edge cases
- Small r: endgame_window is clamped to not exceed r-1; for very short games the strategy still starts cooperative and then reacts quickly.
- n = 2: ABR reduces to an adaptive Pavlov/Tit-for-Tat-like behavior suited to 2-player PD.
- Very noisy opponents: Pavlov with a modest alpha and small m gives forgiveness and adapts aspiration so the strategy does not lock into permanent punishment from occasional random defections.
- Very exploitative population: ABR will quickly defect in groups where recent cooperation is low, protecting itself from sustained exploitation.

Why this is practical for implementation
- The algorithm uses only the game parameters and observed history (allowed by the spec).
- Memory and computation are modest: track past m rounds per-player cooperations and running totals; update a single numeric aspiration.
- Decisions are deterministic given state (except the fact that group checks use averages); optional randomization can be added (small probability of exploratory cooperation) if desired to break deadlocks.

Conclusion
ABR is a cooperative-but-prudent strategy: it tries to create and sustain mutual cooperation, punishes observed defection proportionally, forgives when cooperation returns, and avoids predictable endgame exploitation. Its aspiration-based Pavlov core makes it adaptive and robust across many opponent behaviors in repeated public-goods tournaments.
'''

description_COOPERATIVE_54 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

Summary (high-level)
- Signal cooperation: cooperate in the first round to try to start mutual cooperation.
- Condition on the recent group cooperation level: cooperate in a round if recent history shows a substantial fraction of the group has been cooperating; otherwise withhold cooperation.
- Use a short, finite, forgiving withdrawal of cooperation (automatic by the condition) rather than harsh permanent punishment.
- Always defect in the final round (backward induction), and progressively reduce cooperation as the horizon approaches.
- All decisions depend only on game parameters (n, r, k) and the observable history of contributions; no assumptions about opponents' norms or explicit coordination.

Motivation (intuitive)
- Public-goods deviations are individually tempting in any single round, so sustainment of cooperation must be enforced by future rewards/losses. ACC rewards groups that cooperate by continuing to cooperate, and withdraws cooperation when group cooperation drops. The withdrawal is finite and forgiving (based on a moving average), which is robust to one-off mistakes and to a variety of opponent behaviours, including noise and opportunistic defectors. The last-round defection rule avoids being exploited by backward induction.

Parameters used by the strategy (computed from game inputs)
- Window W = min(5, r-1). (Use up to the last 5 rounds to form a short-term estimate of group cooperativeness; if r is small use fewer rounds.)
- Cooperation threshold α = max(0.5, 1 - 1/k). (Requires at least half the group cooperating, or the stronger threshold 1 - 1/k when k is large — this calibrates how many cooperators are needed before it is rational to restore cooperation.)
- Endgame horizon H = min(3, r-1). (When only H or fewer rounds remain, be progressively less likely to cooperate: in the last round defect always; in the H−1 earlier rounds apply the standard rule but bias toward defection as rem rounds shrink.)
These choices are simple, transparent and depend only on n, r, k.

Decision rules — exact behavior

Let t be the index of the current round (1 ≤ t ≤ r).
Let rem = r − t + 1 (rounds remaining, including current).
Let history provide for each prior round s the total contributions C_s (number of players who played C in round s).

1) Last round (rem = 1)
   - Play D (defect). (Backward induction: no future to reward/punish.)

2) Early rounds (t = 1)
   - Play C (cooperate) to signal willingness to sustain cooperation.

3) Intermediate rounds (2 ≤ t ≤ r−1)
   - Compute W_t = min(W, t−1) (number of prior rounds available up to W).
   - Compute recent average cooperation fraction:
       f = (1 / (W_t * n)) * Σ_{s=t−W_t}^{t−1} C_s
     (this is fraction of players who cooperated in the recent window).
   - Apply endgame bias:
       if rem ≤ H:
         require a slightly stronger signal to cooperate. Concretely, use threshold α' = α + 0.1*(H − rem + 1).
         (This makes cooperation progressively harder in the final few rounds; α' is capped at 1.)
       else:
         α' = α.
   - Cooperate if f ≥ α'; otherwise defect.

4) Forgiveness and re-entry
   - No explicit multi-round punitive state is stored. Withdrawal of cooperation is automatic: after one or more rounds with low f, the moving average will be low and ACC will defect; as soon as a sufficient fraction of players resume cooperating in recent rounds (f ≥ α'), ACC returns to cooperation. Using a moving window ensures finite and forgiving punishment.

5) Tie-breaking / deterministic choice
   - If f is exactly equal to α' (rare with integer counts), cooperate (favor forgiveness).

Pseudocode

Given parameters n, r, k and history of total cooperators C_s for past rounds s:

initialize:
  W = min(5, r-1)
  α = max(0.5, 1 - 1/k)
  H = min(3, r-1)

function choose_action(t, history):
  rem = r - t + 1
  if rem == 1:
    return D
  if t == 1:
    return C
  W_t = min(W, t-1)
  sumC = sum of C_s for s = t-W_t to t-1
  f = sumC / (W_t * n)    # recent cooperation fraction
  if rem <= H:
    αp = min(1.0, α + 0.1*(H - rem + 1))
  else:
    αp = α
  if f >= αp:
    return C
  else:
    return D

Notes and edge cases
- Very short games: if r = 2, W = 1, H = 1. Behavior: round 1 cooperate, round 2 defect. This is simple and transparent.
- No cooperators ever: ACC will start by cooperating once; if others never reciprocate, the recent f will be low and ACC will defect from then on.
- A single noisy defection: Because ACC uses a windowed average and favours cooperation on ties, a single defection is unlikely to permanently collapse cooperation (forgiveness).
- Exploiters: If many players free-ride but the group never reaches f ≥ α, ACC will stop cooperating and avoid giving free lunches. This protects ACC from long-term exploitation.
- Sensitivity to k: α depends on k so ACC requires a stronger cooperation signal when k is larger relative to 1. That reflects that sustaining cooperation with high public-multiplier requires near-universal cooperation to be worthwhile; conversely when k is small (close to 1) ACC requires only a majority.

Why this is cooperative and robust
- Cooperative: ACC begins with cooperation, rewards cooperative groups by continuing to cooperate, and returns to cooperation once cooperation resumes. It thus supports mutually beneficial, sustained cooperation when a substantial fraction of the group cooperates.
- Punishing-but-forgiving: ACC withdraws cooperation when group cooperation falls below the calibrated threshold, reducing the exploitable benefit supplied to defectors. Because withdrawal is based on a moving average, ACC does not engage in permanent harsh punishment — it is forgiving and will restore cooperation when the group’s behaviour improves.
- Adaptive: uses short-term empirical frequency (windowed average) so it adapts to a wide range of behaviors (steady cooperators, oscillators, conditional cooperators, occasional noise).
- Parameter-free relative to opponents: the only inputs are n, r, k and the observed history; ACC does not rely on coordinated schedules or shared norms.

Implementation notes
- The window W, threshold α and endgame horizon H are tunable constants. The specific values (W=5, α=max(0.5,1−1/k), H=3) are conservative choices that balance forgiveness and deterrence; tournament designers may tune them for particular populations.
- The strategy is deterministic given the history; a randomized tie-breaker could be used instead of the tie => cooperate rule.

Example (n=6, k=2, r=10)
- α = max(0.5, 1 − 1/2) = 0.5 ⇒ require at least 3 cooperators (on average) in the recent window to cooperate.
- Start with C. If in the recent window at least 3 out of 6 cooperated on average, ACC stays cooperative; otherwise it defects until the recent fraction recovers.

Conclusion
ACC is simple to implement, makes decisions only from parameters and observed history, promotes cooperation when many players cooperate, protects itself from persistent exploitation, and is forgiving enough to recover from mistakes — properties that make it robust in heterogeneous tournaments of independent agents.
'''

description_COOPERATIVE_55 = '''
Name: Threshold Forgiving Conditional Cooperator (TFCC)

Short summary
- Start by cooperating.
- Use the recent history (a short sliding window) to estimate how cooperative the group is.
- Cooperate while the group’s recent cooperation rate exceeds a threshold.
- If the group falls below the threshold, switch to a short, measured punishment (defect) but allow probabilistic forgiveness tests to re-establish cooperation.
- Always defect in the final round (no future to incentivize cooperation).
- Parameters (window size, threshold, punishment length, forgiveness probability) are functions of the game parameters and history; defaults are given below.

Rationale
- Encourages and stabilizes mutually beneficial full-group cooperation by rewarding groups that are mostly cooperative.
- Punishes persistent free-riding to avoid being exploited.
- Uses a windowed statistic to avoid overreacting to one-off mistakes.
- Uses occasional forgiveness tests so cooperation can be rebuilt after mistakes or noise.
- Defecting in the last round aligns with the single-shot dominant strategy and prevents being exploited at the end.

Default parameters (recommended, adjustable)
- window_m = min(3, r-1) — use last up to 3 rounds (fewer if r small)
- threshold θ = 0.6 — require >60% of other players cooperating in the window to be “satisfied”
- punishment_length P = window_m — punish for up to the window length
- forgiveness_prob q = 0.15 — 15% chance to cooperate during a punishment round to probe for return to cooperation
- endgame_rounds E = 1 — always defect in final round

All of these can be tuned by the tournament implementer; they are chosen to balance stability, responsiveness and robustness.

Definitions used below
- r: total rounds
- t: current round number (1..r)
- H: full history up to round t−1 (for each past round we know exactly who played C or D)
- coop_count_round(s): number of players who played C in round s
- coop_fraction_others_round(s): fraction of the other (n−1) players who cooperated in round s (exclude self)
- recent_window = rounds max(1, t−window_m) ... t−1

Decision rules (natural language)
1. First round (t = 1): play C.
2. Final round (t = r): play D (no future to incentivize reciprocation).
3. For intermediate rounds (1 < t < r):
   a. Compute f = average over the recent_window of coop_fraction_others_round(s). (If recent_window is empty because t=2, then use the single previous round.)
   b. If currently not in a punishment period and f ≥ θ, play C (stay cooperative).
   c. If f < θ, enter a punishment period of up to P rounds: play D for that punishment period, but during each punishment round cooperate with independent probability q (a forgiveness probe). If the recent_window shows f ≥ θ at any check during/after punishment, end punishment and return to cooperating.
   d. While in punishment, if the last P rounds (the full punishment length) have not produced sufficient repentance (f < θ), remain in punishment; if they show f ≥ θ, exit punishment.
4. If the group achieves near-unanimous cooperation for window_m consecutive rounds (e.g., f ≥ 0.95), optionally move into a “stable cooperation” mode where you cooperate deterministically until you see a sign of repeated breakdown (this is just an optimization to reduce stochastic probes).

Pseudocode
(Uses history H, parameters window_m, θ, P, q, E)

Initialize:
  punishment_remaining = 0
  cooperate_mode = True

For each round t = 1..r:
  if t == 1:
    action = C
    record action and continue
  else if t == r:               # last round
    action = D
    record action and continue
  else:
    # compute recent average fraction of others cooperating
    start = max(1, t - window_m)
    window_rounds = [s for s in range(start, t)]
    if window_rounds is empty:
      f = 0   # conservative default (shouldn't happen given t>1)
    else:
      sum_frac = 0
      for s in window_rounds:
        others_coop = coop_count_round(s) - (1 if you cooperated in s else 0)
        sum_frac += others_coop / (n - 1)
      f = sum_frac / len(window_rounds)

    # If currently punishing, decrement counter and possibly forgive
    if punishment_remaining > 0:
      # With small probability, probe by cooperating
      if random() < q:
        action = C   # forgiveness probe
      else:
        action = D
      punishment_remaining -= 1

      # After this round is played and observed next round, the computed f will be updated;
      # If at the next decision point f >= theta, we will exit punishment (see below).
    else:
      # not currently punishing
      if f >= θ:
        action = C
        cooperate_mode = True
      else:
        # start punishment
        punishment_remaining = P - 1   # we'll take one punishment action now and P-1 after
        action = D
        cooperate_mode = False

  play action; append to history; continue

Recovery rule (implicit in above):
  At each decision point after punishment, recompute f using the most recent window; if f >= θ, set punishment_remaining = 0 and resume cooperating.

Notes on implementation details
- The strategy requires access to full public history (available in the tournament).
- The random() call indicates independent uniform sampling on [0,1]; on deterministic-only platforms set q to 0 and implement deterministic periodic probing instead (e.g., cooperate once every P rounds while punishing).
- Because the strategy uses a small window and moderate threshold it is tolerant of occasional mistakes yet responds to persistent or coordinated free-riding.
- You can safely reduce q if opponents are known to be noisy (less probing) or increase q to be more generous.

Why this is cooperative and robust
- Cooperative: starts by cooperating, rewards groups that sustain cooperation, and returns to cooperation when others show they will cooperate again.
- Robust to exploitation: defects in response to sub-threshold group cooperation, punishing defectors so persistent free-riders do not reap long-term benefits from your contributions.
- Forgiving and adaptive: uses a time window so one-off lapses do not collapse cooperation, and explicit forgiveness probes allow recovery from mistakes without requiring unrealistic coordination.
- Endgame-aware: defects in the final round to avoid endgame exploitation while still fostering cooperation in earlier rounds where future payoffs create leverage.

Tuning guidance (practical)
- If opponents are noisy (random mistakes), increase window_m and/or reduce θ so you tolerate more one-offs.
- If many opponents are stubborn defectors, reduce window_m and increase θ to avoid being exploited.
- If k is close to n (public good is very valuable relative to private), you may want a lower threshold θ (since cooperating is more valuable collectively), but because the per-round individual incentive is still to defect, thresholds above 0.5 are still reasonable.
- For very short games (r <= 3), be more conservative: consider cooperating only until t = r-1 and defecting earlier if you observe any defection.

This strategy is simple to implement, depends only on parameters and observed history, and provides a balanced, forgiving but firm path to sustain high cooperation where possible while defending against exploitation.
'''

description_COOPERATIVE_56 = '''
Name: Adaptive Win‑Stay Lose‑Shift for Public Goods (AWSLS‑PG)

Short summary
- Start cooperatively. On each subsequent round use a win‑stay/lose‑shift rule: compare the payoff you actually received last round with the payoff you would have received had you played the opposite action (given the others' observed actions). If your actual payoff was at least as good, repeat your previous action; otherwise switch.  
- In the final round play the myopic best response given the most recent observed cooperation by others (break ties toward cooperation).  
This strategy is simple, fully history‑dependent, forgiving, and adaptive: it rewards mutual cooperation, punishes exploitation, but quickly returns to cooperation when others do.

Why this design
- Win‑stay/lose‑shift is well known to sustain cooperation in reciprocal settings while avoiding endless retaliation. It is robust to a wide range of opponent behaviours because decisions are driven by realized payoffs and observed contributions rather than assumptions about opponents’ motives or future agreements.  
- Using the counterfactual payoff (what you would have gotten if you had played the opposite action given the others' actions) makes the decision locally optimal and interpretable.  
- A myopic final‑round rule avoids being easily exploited in the endgame while still cooperating when immediate returns make it attractive.

Decision rules (natural language + precise formulas)

Notation
- n, r, k as given.
- For round t, let c_i,t ∈ {0,1} be your action last round (1 for C, 0 for D).
- Let M_t = total number of cooperators in round t (including you if you cooperated).
- Let m_others,t = M_t − c_i,t = number of cooperators among the other n−1 players in round t.
- Payoffs in any round t: if you play C: π_C(m_others) = (k/n) × (m_others + 1). If you play D: π_D(m_others) = 1 + (k/n) × m_others.

Round 1
- Play C (cooperate).

Rounds 2 through r−1 (intermediate rounds)
- Observe last round's M_{t−1} and thus m_others,{t−1} and your last action c_i,{t−1}.
- Compute last_round_payoff = if c_i,{t−1} = 1 then π_C(m_others,{t−1}) else π_D(m_others,{t−1}).
- Compute alt_payoff = payoff if you had played the opposite action last round:
  - If c_i,{t−1} = 1 then alt_payoff = π_D(m_others,{t−1})
  - If c_i,{t−1} = 0 then alt_payoff = π_C(m_others,{t−1})
- Decision:
  - If last_round_payoff >= alt_payoff: repeat your previous action (play c_i,t = c_i,{t−1}).
  - Else: switch action (play c_i,t = 1 − c_i,{t−1}).
- Tie‑breaking: if last_round_payoff equals alt_payoff, choose to cooperate (c_i,t = 1). This tie rule biases slightly toward cooperation.

Round r (final round)
- Use a myopic best response based on the most recent information (e.g., m_others,{r−1}). Compute:
  - π_C_est = π_C(m_others,{r−1})
  - π_D_est = π_D(m_others,{r−1})
- Play C if π_C_est >= π_D_est (tie → cooperate). Otherwise play D.

Edge cases and implementation notes
- If history is empty or incomplete for any reason, default to C.
- If r = 2: Round 1 play C; round 2 is final round so play the myopic best response using round 1 observations.
- If an opponent produces strange oscillatory or adversarial patterns, AWSLS‑PG responds using realized payoffs, so it will switch to D when exploited and will return to C if cooperation resumes. This prevents being held in long punishments while still deterring repeat exploitation.
- The algorithm requires only the last round's M (or you can use a short moving window, e.g., last 2 rounds, if you want smoother responses); the basic rule above only needs the immediate last round.
- The rule is deterministic (except for tie breaks) and depends only on parameters and observable history.

Pseudocode

Initialize:
  action[1] = C

For t = 2 .. r−1:
  Observe M = total cooperators in round t−1
  m_others = M − action[t−1]  // number of other players who cooperated last round
  if action[t−1] == C:
    last_payoff = (k/n) * (m_others + 1)
    alt_payoff  = 1 + (k/n) * m_others  // what you'd have gotten if you'd defected
  else: // last action was D
    last_payoff = 1 + (k/n) * m_others
    alt_payoff  = (k/n) * (m_others + 1)  // what you'd have gotten if you'd cooperated
  if last_payoff >= alt_payoff:
    action[t] = action[t−1]   // win-stay (tie -> stay -> tie-break cooperativeness)
  else:
    action[t] = opposite(action[t−1])  // lose-shift

Final round t = r:
  Observe M = total cooperators in round r−1
  m_others = M − action[r−1]
  if (k/n) * (m_others + 1) >= 1 + (k/n) * m_others:
    action[r] = C
  else:
    action[r] = D

Behavioral examples
- Mutual cooperation group: If everyone cooperates, last_payoff (π_C) > alt_payoff (π_D) only if k/n * (n) >= 1 + k/n*(n−1) which simplifies to tie in payoffs? But with tie-break preference to cooperate, AWSLS‑PG will stay cooperating and maintain full cooperation.
- Single defector appears: Suppose you cooperated but many others cooperated except one. If that defection reduces your last payoff below the counterfactual, you will switch to D next round (punishment). If others resume cooperation subsequently, you will return to C (forgiveness).
- Exploiter present: A purely exploitative always‑D player will cause AWSLS‑PG to defect when exploited, so it limits the exploiter’s advantage while still cooperating with reciprocators.

Why this is cooperative and robust
- Promotes cooperation: starts cooperative and rewards cooperation among others (it stays C when cooperation gives at least as good an outcome as defecting). That aligns the strategy with the cooperative mindset.
- Limits exploitation: when a defection actually reduces your payoff versus the alternative, the strategy defects next round (punishes), reducing exploiter gains.
- Forgiving and adaptive: punishment is automatic but not permanent — once the group returns to producing better payoffs by cooperating, AWSLS‑PG returns to cooperation. This prevents mutual retaliation spirals and improves group payoffs over many plausible opponent mixes.
- Low informational and computational needs: uses only the observed aggregate cooperators (or equivalently individual actions from previous round) and simple arithmetic; suitable for tournaments where you cannot rely on communication or shared conventions.

Optional tuning
- If you want more inertia toward cooperation, you can compare last_payoff + epsilon >= alt_payoff (with small epsilon > 0) to be slightly more forgiving. If you want to be stricter against exploitation, require last_payoff > alt_payoff (no tie favoring C) or use a short memory averaging m_others over the last 2 rounds before comparing.

This strategy is easy to code, respects the game information limits, is explicitly history dependent, and balances cooperative intent with robustness against opportunistic or arbitrary opponents.
'''

description_COOPERATIVE_57 = '''
Name: Adaptive Threshold with Forgiving Reciprocity (ATFR)

Summary (one line)
- Start cooperative, condition future cooperation on recent group cooperation (majority-style threshold), punish defections proportionally and temporarily, then forgive and probe to re-establish cooperation; behave cautiously in the endgame.

Intuition and design goals
- Be cooperative: initiate cooperation and prefer to keep cooperation if other players reciprocate.
- Be robust: avoid being repeatedly exploited by defectors by switching to temporary punishment when the group contribution drops below a reasonable threshold.
- Be adaptive: adjust punishment length to the severity of the drop in cooperation, allow forgiveness, and use occasional probes to discover if others have become cooperative.
- Handle endgame: avoid being exploited in the last rounds while still rewarding clear recent cooperation.

Parameters derived from game inputs (deterministic functions of n, k, r)
- threshold_fraction q = 0.50
  - Cooperate if at least q fraction of the other (n-1) players cooperated last round.
- max_punish_fraction α = 0.20
  - Maximum punishment length = ceil(α × r) (at least 1 if r > 1).
- forgiveness_probability pf = 0.80
  - When a punishment period ends, resume cooperation with probability pf (a generous, but not naïve, re-entry).
- exploration_probability ε = min(0.05, 1/r)
  - Small probability while defecting to cooperate to probe whether opponents have become cooperative.
These constants are simple, transparent, and scale with r when relevant. They can be tuned by tournament performance but are defined only from n, k, r (no extra information).

State variables (maintained from history)
- t: current round (1..r)
- rounds_remaining = r − t + 1
- punish_until: round index until which this strategy will play D (0 if not punishing)
- last_round_cooperators m_last: total cooperators in previous round (including or excluding self — we use total; code can subtract own previous action if needed)

High-level decision rules (natural language)
1. Opening move
   - Round 1: Cooperate.

2. If currently in a punishment period
   - If t ≤ punish_until: Defect.
   - While defecting during punishment, with small probability ε cooperate (probe) to check whether others are returning to cooperation; treat those probes as defects for punishment accounting (i.e., probing does not reset punishments).

3. If not punishing (t > punish_until)
   - Compute fraction_of_others_cooperating = (m_last − indicator(if you cooperated last round)) / (n − 1), or simply (m_last)/(n) depending on bookkeeping — the decision is based on the fraction of players who cooperated last round excluding yourself.
   - If fraction_of_others_cooperating ≥ q (i.e., at least half of the others cooperated last round):
       - Cooperate this round.
   - Else (group cooperation last round was below threshold):
       - Start a punishment: set punish_length P proportional to the shortfall:
         - shortfall = (n − m_last)  (the number of defectors last round)
         - P = min(max_punish, max(1, ceil(shortfall × max_punish / n)))
         - punish_until = t + P − 1
       - Defect this round (first round of punishment).
       - (Note: if already punishing, follow punishment rule above.)

4. Forgiveness and probing
   - When punish_until expires, do not automatically punish forever. On the first round after punish_until:
     - Cooperate with probability pf (forgiving re-entry).
     - If that cooperate is met by sufficient cooperation next round, stay cooperative; otherwise, re-initiate punishment with the same rule.
   - During non-punishing defecting phases, use exploration probability ε to occasionally cooperate to test group response.

5. Endgame handling
   - If rounds_remaining = 1 (final round):
     - Default to Defect (standard finite-horizon endgame precaution), except:
       - If the previous round showed near-universal cooperation (m_last = n or m_last ≥ n − 1) and you are not in punishment, Cooperate this final round to obtain mutual higher payoff.
   - If rounds_remaining = 2 (second-to-last round):
     - Be slightly cautious: cooperate only if last-round cooperation was strong (m_last ≥ ceil(q × (n − 1)) + 1), otherwise behave as above (may trigger short punishment but ensure not to start punishments that would extend into the final round usefully).
   - When computing punish_until ensure punishments do not extend past r − 1, i.e., avoid punishing into the very final round in ways that make short-term exploitation certain; cap punish_until ≤ r − 1.

Pseudocode (concise, implementable)
- Initialize punish_until = 0
- For each round t = 1..r:
  - rounds_remaining = r − t + 1
  - If t == 1:
      action = C
  - Else if t ≤ punish_until:
      With probability ε: action = C (probe)
      Else: action = D
  - Else:  // not currently punishing
      compute m_last = number of cooperators in round t − 1 (total)
      compute others_coop_frac = (m_last − indicator(self_cooperated in last round)) / (n − 1)
      If rounds_remaining == 1:  // final round
          If m_last ≥ n − 1 and t > punish_until: action = C
          Else action = D
      Else if rounds_remaining == 2:
          If m_last ≥ ceil(q * (n − 1)) + 1: action = C
          Else:
             // trigger normal punishment logic below
             shortfall = n − m_last
             P = min(max_punish, max(1, ceil(shortfall * max_punish / n)))
             punish_until = min(r − 1, t + P − 1)
             action = D
      Else:  // general rounds
          If others_coop_frac ≥ q:
             action = C
          Else:
             shortfall = n − m_last
             P = min(max_punish, max(1, ceil(shortfall * max_punish / n)))
             punish_until = min(r − 1, t + P − 1)
             action = D
  - If this is the first round after punish_until expired (t == punish_until + 1 and punish_until > 0):
      With probability pf: action = C else action = D  (this overrides action decided above for the first post-punishment round)
  - Execute action; update history.

Concrete choices (recommended defaults)
- q = 0.50
- max_punish = max(1, ceil(0.20 × r))
- pf = 0.80
- ε = min(0.05, 1/r)

Why this strategy encourages cooperation but resists exploitation
- Starts cooperative to signal willingness to cooperate.
- Requires a simple, transparent social proof (majority of others cooperated last round) before continuing cooperation — prevents being exploited when only a small minority cooperates.
- Punishment is proportional to the severity of defection (shortfall) and limited in length, so it penalizes offenders but allows opponents to return to cooperation without permanent exclusion.
- Forgiveness (high pf) restores cooperation quickly when others reciprocate, preventing endless punishment cycles (which would waste payoff).
- Small probing probability allows detecting a shift back to cooperation even when you currently defect.
- Endgame caution reduces guaranteed exploitation in final rounds while still rewarding clear cooperation shortly before the end.

Edge cases
- Very short games (r = 2 or 3): the algorithm scales punishments and exploration with r, will still start cooperating but be cautious in last round(s).
- Noisy environments or mixed opponents: forgiveness + probing prevents permanent collapse from transient defection.
- Many defectors: thresholding and punishment avoid being repeatedly exploited — if the group is largely defecting the strategy will defect mostly, which is optimal against persistent defectors.
- All-cooperators: the strategy will stay cooperative and obtain the socially efficient outcome.
- Single defector in otherwise cooperative group: the proportional punishment is small (short P) and the strategy forgives quickly, restoring full cooperation.

Notes about implementation
- Keep bookkeeping simple: m_last is the total cooperating count in the previous round; subtract own previous action if desired for exact “others” fraction.
- The small stochastic elements (pf, ε) can be implemented deterministically if required by the tournament (e.g., treat pf as a deterministic schedule like “cooperate first post-punishment round, but if punished again then only cooperate every other attempt”), but randomness helps escape cycles and discover cooperative opponents.
- All parameters (q, α, pf, ε) are fixed functions of n, k, r (no external coordination nor ad hoc signals).

Behavioral summary by scenario
- Mostly cooperative group: cooperate, maintain cooperation.
- Occasional defectors: proportionate punishment, quick forgiveness, cooperation restored.
- Persistent defectors or mostly-defecting group: defect to avoid exploitation, probe occasionally.
- Endgame: default to defect except when very recent behavior indicates near-universal cooperation.

This strategy is simple to implement, interpretable, and designed to produce high cooperative payoff when opponents are conditionally cooperative, while avoiding sustained exploitation by defectors.
'''

description_COOPERATIVE_58 = '''
Name: Forgiving Conditional Cooperator (FCC)

High-level idea
- Start by signalling cooperation.
- In each non-final round, decide whether to cooperate based on the recent observed cooperation level of the other players. Cooperate when the group appears sufficiently cooperative; otherwise apply a short, proportional punishment (defection) and then forgive if cooperation resumes.
- Always defect in the final round (no future to enforce reciprocity).
This strategy is explicitly cooperative (it prefers to sustain high contributions) but protects itself from persistent exploitation by short, targeted punishments and by being forgiving to occasional lapses.

Parameters computed from game inputs (deterministic functions of n, k, r)
- L (memory window) = min(3, r-1). (Look at up to the last 3 rounds; fewer if game is short.)
- grace g = max(0, round(0.10 × (n-1))). (Number of simultaneous defectors among the others tolerated without immediate punishment.)
- frac_th (cooperation threshold) = clamp(0.5 − 0.25 × ((k−1)/(n−1)), 0.2, 0.8).
  - Intuition: when public-good productivity k is larger the strategy is more forgiving (lower threshold), because cooperation yields larger group benefit.
- P (punishment length) = min(r-1, 1 + floor((n−k)/2), 3). (Short punishment; longer if k is small (= cooperation harder to sustain).)
- T_forgive = 1 (forgiveness window): require 1 consecutive round of observed sufficient cooperation to exit punishment (this is small to favor recovery).

Notes:
- clamp(x,a,b) bounds x into [a,b].
- All these are deterministic functions of (n,k,r) — no hidden extra tuning.

State variables (tracked during the game)
- punishment_remaining (integer ≥ 0), initially 0.
- history of actions of all players (visible per specification).

Decision rules (natural language + pseudocode)

Round t = 1
- Play C (cooperate). Rationale: signal willingness to cooperate and attempt to start mutual cooperation.

Rounds 2 ≤ t ≤ r−1
1. Compute frac_coop_recent:
   - For each of the last up to L rounds, compute the fraction of the other (n−1) players who played C.
   - frac_coop_recent = average of those fractions over the L rounds.
   - Also get last_round_cooperators = number of other players who cooperated in round t−1 (an integer 0..n−1).

2. If punishment_remaining > 0:
   - Play D.
   - Decrement punishment_remaining by 1.
   - Exception (fast forgiveness): If last_round_cooperators == n−1 (i.e., all others cooperated in the last round), set punishment_remaining = 0 and play C this round instead (immediate forgiveness when everyone returns to full cooperation).

3. Else (not currently punishing):
   - If frac_coop_recent ≥ frac_th OR last_round_cooperators ≥ (n−1 − g):
       - Play C.
     (Rationale: either the recent average cooperation meets threshold or most players cooperated in the immediately preceding round — in both cases continue to cooperate.)
   - Else:
       - Start punishment: set punishment_remaining = P − 1 (we defect this round and then P−1 more rounds), play D this round.
       - (If P = 1, this is a one-round “tit-for-tat style” response; larger P gives a longer penalty.)

Round t = r (final round)
- Play D. (No future rounds to enforce cooperation; defect to avoid being exploited in final round.)

Extra implementation notes and small refinements
- Tie-breaking: when frac_coop_recent equals frac_th, cooperate (bias toward cooperation).
- Noise robustness: the grace parameter g and averaging across L rounds make the strategy forgiving of occasional accidental defections.
- Punishment proportionality: P is capped and small (≤3), preventing long, costly mutual punishment cycles.
- Fast recovery: immediate forgiveness if the entire group returns to full cooperation, enabling quick restart of mutually beneficial play.
- Bookkeeping: all decisions depend only on (n,k,r) and the public history of actions; payoffs are not required but can be used equivalently.

Pseudocode (compact)

Initialize:
  punishment_remaining = 0
  L = min(3, r-1)
  g = max(0, round(0.10*(n-1)))
  frac_th = clamp(0.5 - 0.25 * ((k-1)/(n-1)), 0.2, 0.8)
  P = min(r-1, 1 + floor((n-k)/2), 3)

For round t = 1..r:
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue

  compute last_L_rounds (up to L) fractions f_s = (#others who played C in round s)/(n-1)
  frac_coop_recent = average(f_s over those rounds)
  last_round_cooperators = f_{t-1} * (n-1)  // integer

  if punishment_remaining > 0:
    if last_round_cooperators == n-1:
      punishment_remaining = 0
      play C
    else:
      play D
      punishment_remaining -= 1
  else:
    if frac_coop_recent >= frac_th OR last_round_cooperators >= (n-1 - g):
      play C
    else:
      punishment_remaining = max(0, P - 1)
      play D

Rationale and expected behavior
- Pro-cooperation: The strategy starts cooperative and cooperates whenever recent collective behavior seems cooperative. This supports and rewards other cooperative strategies, helping maintain high contribution levels when others reciprocate.
- Robustness: Thresholds, averaging, and a small grace parameter prevent overreacting to single defections and provide tolerance for transient mistakes or greedy players occasionally defecting.
- Deterrence: Short, predictable punishments deter persistent free-riders: if someone defects repeatedly, the strategy will respond with several defections, reducing exploitability.
- Forgiveness and fast recovery: Punishment is limited in length and canceled immediately if everyone returns to full cooperation, preventing endless mutual punishment and enabling cooperation to restart.
- Endgame safety: Defection in the last round avoids being exploited when no future enforcement is possible.

Why this is adaptive and tournament-robust
- It adapts to observed cooperation rates rather than assuming fixed schedules or norms.
- It uses parameter functions of (n,k,r) so behavior changes sensibly with game size, productivity, and horizon (e.g., more forgiveness when k is larger and short, capped punishments when r is short).
- It balances cooperation with protection: it will score well when others reciprocate and will not be an easy target for exploiters because it punishes persistent defection.
- Simple, transparent rules make it predictable to other learning strategies (promoting stable cooperation) while resisting exploitation.

If you want, I can also provide alternative tunings (more aggressive or more forgiving variants), or translate this specification into code (Python or pseudocode ready for implementation).
'''

description_COOPERATIVE_59 = '''
Name: Adaptive Conditional Cooperator (ACC)

Goal (cooperative mindset)
- Start and try to establish mutual cooperation.
- Sustain cooperation when others reciprocate.
- Respond to persistent free-riding with short, proportional punishments to deter exploitation.
- Be forgiving and restart cooperation when others return to cooperating.
- Be robust to many opponent types (exploiters, conditional cooperators, noisy strategies) by using modest punishment, forgiveness and occasional probing.

Main ideas (intuitive)
- Signal cooperation on round 1.
- Track recent fraction of others who cooperated and use it to decide. Cooperate when the group’s recent behaviour shows a clear cooperative tendency; defect when it does not.
- If you are (effectively) exploited (you cooperated but few others did), punish for a short, limited period rather than use permanent “grim trigger”.
- Forgive early if the group moves back to cooperation.
- Always defect in the very final round (to avoid being exploited by last-round free-riders), and be conservative in the last few rounds.

Parameters (derived from game parameters and history)
- r: number of rounds (given)
- n: number of players (given)
- k: multiplication factor (given)
- W: observation window length for recent behaviour = max(3, min(ceil(sqrt(r)), r-1)). (Use up to the last W rounds to estimate others’ cooperation; smaller windows make the strategy responsive, larger windows make it stable.)
- theta: cooperation threshold = 0.60 (cooperate when recent fraction of others cooperating ≥ theta). (A conservative majority threshold; can be adjusted with experience — 0.5–0.7 are reasonable.)
- phi_min: exploitation threshold = 0.45 (if others’ cooperation fraction falls below phi_min while you cooperated, treat that as exploitation).
- P_base: base punishment length = max(1, ceil(r/20)) (short punishment proportional to the length of the tournament; typically 1–3 rounds).
- P_scale: scale of punishment proportional to shortfall =  (optional) use P = P_base + ceil((theta - phi) * 10) when triggered; otherwise use P_base.
- G: endgame horizon = 1 (in the final round always defect). Optionally treat the penultimate round more conservatively (see rules below).
- epsilon: small probing probability = 0.02 (occasionally cooperate while in defect mode to test whether opponents are returning to cooperation).

State variables maintained
- punish_counter: remaining rounds to punish (initially 0).
- last_action: your action in previous round (C or D).
- history: full observable history of actions of all players (including yourself).

Decision rules (natural language)
1. First round (t = 1): Cooperate. (Signal cooperative intent.)
2. Final round (t = r): Defect. (Avoid last-round exploitation.)
3. Penultimate round (t = r-1): Be cautious. Cooperate only if the recent cooperation rate among others is very strong (>= theta_high), where theta_high = max(0.75, theta). Otherwise defect. (Optional stronger caution in the very late game.)
4. For any round t (2 <= t < r):
   a. Compute phi = average over the last min(W, t-1) rounds of the fraction of others (n-1 players) who cooperated in each round. Concretely:
      - For each of the last m = min(W, t-1) completed rounds, compute fraction f_s = (# of other players who played C in round s) / (n-1).
      - phi = (1/m) * sum_s f_s.
   b. If punish_counter > 0:
        - Defect this round.
        - Decrement punish_counter by 1.
        - However: if phi >= theta (others are clearly cooperating again), set punish_counter = 0 and cooperate next round (contrition/forgiveness).
   c. Else (not currently punishing):
        - If phi >= theta: Cooperate.
        - Else if (last_action == C) AND (phi < phi_min):
            * You were (practically) exploited recently. Set punish_counter = P (computed as P_base + ceil((theta - phi)*10) clipped to a small max, e.g., 3–5) and Defect this round.
        - Else:
            * Default to Defect, but with probability epsilon play Cooperate (probing move to test whether opponents have become cooperative).
5. Re-initialize last_action with your chosen action for use next round.

Pseudocode

Initialize:
  punish_counter = 0
  last_action = None

On round t (1..r):
  if t == 1:
    action = C
    last_action = action
    return action

  if t == r:
    action = D
    last_action = action
    return action

  if t == r-1:  # penultimate caution
    phi = compute_phi(last W rounds)
    theta_high = max(0.75, theta)
    if phi >= theta_high:
      action = C
    else:
      action = D
    last_action = action
    return action

  # general rounds 2 <= t < r-1
  phi = compute_phi(last W rounds)  # fraction of others cooperating averaged
  if punish_counter > 0:
    if phi >= theta:
      punish_counter = 0
      action = C
    else:
      action = D
      punish_counter -= 1
  else:
    if phi >= theta:
      action = C
    else:
      if last_action == C and phi < phi_min:
        # trigger short punishment proportional to shortfall
        shortfall = max(0, theta - phi)
        P = P_base + ceil(shortfall * 10)
        P = min(P, 4)  # cap punishment to avoid prolonged cycles
        punish_counter = P - 1  # we will defect this round and P-1 further rounds
        action = D
      else:
        # default defect with small probe
        if random() < epsilon:
          action = C
        else:
          action = D

  last_action = action
  return action

Notes and rationale
- Start-cooperate: provides a clear cooperative signal so reciprocating conditional cooperators will cooperate and produce mutual gains.
- Threshold-based conditional cooperation: using an observed recent fraction phi of others cooperating is a simple, robust rule in multi-player settings (generalizes TFT’s “copy the opponent” idea). Cooperate when the group is reciprocating.
- Short, proportional punishments: when exploitation is detected (you cooperated but most others did not), punish for a short, bounded time. This deters persistent free-riders but avoids permanent collapse of cooperation. Punishment length scales with tournament length r but is capped so it does not spiral.
- Forgiveness: if the group shows cooperative behavior again (phi >= theta), immediately stop punishing. Quick forgiveness is essential to re-establish cooperation after mistakes or after punishments had effect.
- Probing epsilon: ensures the strategy can test whether opponents have returned to cooperation even from long mutual defection states, avoiding lock-in.
- Endgame caution: always defect in the last round (rational last-round defection is unavoidable in finite horizons). In the penultimate round, be stricter about cooperating to reduce risk of last-round exploitation cascades.
- Parameter choices: theta = 0.60, phi_min = 0.45, small epsilon ~ 0.02, and modest punishment lengths are empirically sensible defaults. They can be tuned for a particular tournament environment but are set here to balance deterrence and forgiveness.

Why this is robust
- Works well versus unconditional cooperators (you will cooperate and get high payoffs).
- Deters pure defectors and persistent free-riders by limited retaliation that reduces their advantage.
- Tolerates occasional defects and noise because punishments are short and the strategy forgives quickly when cooperation returns.
- Probing allows recovery from mass defection without being exploitable.
- Threshold and window choices make it responsive to changes but not overly reactive to one-shot deviations.

Implementation notes
- compute_phi uses public history only; it does not require private signals.
- Randomness only used for small probes; the core is deterministic, simple and fast to evaluate.
- All thresholds are specified so an implementer can convert to exact numeric checks.

This ACC strategy is intentionally conservative (i.e., not blindly forgiving) but constructive: it aims to establish and preserve mutual cooperation where possible, penalize persistent defectors enough to be discouraging, and return to cooperation quickly when others reciprocate.
'''

description_COOPERATIVE_60 = '''
Name: Adaptive Win‑Stay Lose‑Shift with Soft Forgiveness (AWSLS‑SF)

Intuition (short)
- Start cooperative to signal willingness.
- Use a Win‑Stay Lose‑Shift (Pavlov) rule: repeat your previous move when you did at least as well as the counterfactual you would have obtained by switching; otherwise switch. This keeps mutual cooperation stable and punishes exploitation.
- Add short memory‑based forgiveness and a horizon check for the final round so the strategy is not trapped in long punitive cycles and is rational at the endgame.
- No assumptions about opponents beyond observed history; only uses parameters (n, k, r) and the observed history of actions each round.

Detailed decision rules

Notation
- t ∈ {1,...,r} is the current round.
- c_i,t ∈ {0,1} is your contribution in round t (1 for C, 0 for D).
- Cnt_t = total cooperators in round t = Σ_j c_j,t (includes you).
- Others_t = Cnt_t − c_i,t = number of other cooperators in round t.
- payoff(c_i, Others) = (1 − c_i) + (k/n) × (Others + c_i).
- History available: for each past round s < t we observe Cnt_s (hence others for each s) and we know our own past c_i,s.

Fixed internal parameters (simple, deterministic choices)
- Memory window M = min(5, max(1, r − 1)). (Short window gives responsiveness while being robust for many r.)
- Forgiveness window F = 2 (after switching because of being exploited we allow quick re‑testing of cooperation).
- Small tolerance ε = 0 (or a tiny positive number if implementation desires numerical tolerance).

Rule by rule

1) First round (t = 1)
- Play C (cooperate). Rationale: signal cooperation and try to reach the mutually better all‑C outcome.

2) For rounds t = 2,...,r − 1 (non‑final rounds)
a. Compute last round payoff:
   let last_c = c_i,t−1 (your action in t−1).
   let others_last = Others_{t−1}.
   payoff_last = payoff(last_c, others_last).

b. Compute counterfactual payoff if you had played the opposite action in t−1:
   If last_c == C (1): counterfactual = payoff(0, others_last) = 1 + (k/n) × others_last.
   If last_c == D (0): counterfactual = payoff(1, others_last) = (k/n) × (others_last + 1).

c. Win‑Stay / Lose‑Shift decision:
   - If payoff_last >= counterfactual − ε: repeat last action (c_i,t = last_c).
     (You did at least as well as you would have if you had switched, so stay.)
   - Else (payoff_last < counterfactual): switch action (c_i,t = 1 − last_c) — this is the immediate retaliatory / corrective move.

d. Soft forgiveness: if you switched in step (c) because you were exploited (i.e., you cooperated last round and defecting would have yielded strictly higher payoff), then for the next F rounds do:
   - In each of those rounds, cooperate if the average fraction of cooperators among others over the last M rounds (excluding your hypothetical future) ≥ threshold θ, otherwise defect.
   - Choose θ = 0.5 (i.e., require a majority of others to have cooperated recently to resume cooperation). Implementation note: compute avg_others = (1/M) Σ_{s=t−1 down to t−M} (Others_s / (n − 1)). If avg_others ≥ θ then cooperate; else defect.
   Rationale: after punishment we probe for restored reciprocity rather than entering permanent defection. Using an M‑round average avoids overreacting to one noisy round.

3) Final round (t = r)
- Use one‑shot rationality given the most recent information. Estimate what others will do in the final round using recent behavior:
   - Estimate expected Others_r as the average Others over the last M rounds (or if no history, use others_last).
   - Compute immediate payoff_if_C = payoff(1, expected_Others).
   - Compute immediate payoff_if_D = payoff(0, expected_Others).
- Play the action with the higher immediate payoff. If equal, choose C (prefer cooperation tie‑break).

Implementation pseudocode (compact)

Initialize: c_i,1 = 1.

For each round t = 2..r:
  last_c = c_i,t-1
  others_last = Others_{t-1}    # observed
  payoff_last = payoff(last_c, others_last)
  if last_c == 1:
    counterfactual = payoff(0, others_last)
  else:
    counterfactual = payoff(1, others_last)

  if t == r:
    # final round decision: estimate expected others
    M_eff = min(M, t-1)
    expected_Others = (1/M_eff) * sum_{s=t-1 down to t-M_eff} Others_s
    if payoff(1, expected_Others) >= payoff(0, expected_Others):
      c_i,t = 1
    else:
      c_i,t = 0
  else:
    if payoff_last >= counterfactual - ε:
      c_i,t = last_c
    else:
      c_i,t = 1 - last_c
      # mark that we entered a short punishment/probe phase for next F rounds:
      set P = F  # remaining forgiveness probes

  # If in a forgiveness probe phase (P > 0), override above decision with probing rule:
  if (exists P > 0):
    M_eff = min(M, t-1)
    avg_frac_others = (1/M_eff) * sum_{s=t-1 down to t-M_eff} (Others_s / (n-1))
    if avg_frac_others >= 0.5:
      c_i,t = 1
    else:
      c_i,t = 0
    P = max(0, P - 1)

Why this is cooperative and robust
- Cooperative alignment: starts by cooperating and prefers cooperation whenever it is reciprocated because repeating a mutual‑C yields higher payoffs for all.
- Stability of cooperation: WSLS keeps mutual cooperation stable — if all cooperate, no one has an incentive to switch because defecting would give a worse counterfactual when many others cooperated.
- Punishment for exploitation: if you are exploited (cooperate while many defect), WSLS switches and thereby increases the cost to exploiters.
- Forgiveness and short memory: the temporary probe/forgiveness phase prevents permanent mutual retaliation triggered by single deviations or opportunistic strategies, allowing cooperation to re‑emerge quickly.
- Adaptivity: decisions use observed counts of cooperators each round, so you adapt to different group sizes, different opponent styles, and changing behavior during the match.
- Endgame awareness: final round uses immediate payoff maximization based on recent behavior, avoiding being predictably exploited in the last round while still preferring cooperation when it is reasonable.
- Simple, deterministic, easy to implement: only needs counts per round and small memory M.

Edge cases handled explicitly
- r = 2 or small r: M and F are constrained by r − 1, and final‑round rule resolves endgame.
- Repeated single defector(s): WSLS will switch and respond, preventing long continuous exploitation by defectors.
- Temporary mistakes / one‑off defection by otherwise cooperative players: forgiveness window F and averaging over M rounds allow recovery to cooperation.
- No cooperation observed historically: avg_frac_others < 0.5 will make the strategy defect during probes, avoiding being a perpetual sucker.

Tunable options (implementation)
- M (memory window): min(5, r − 1) is a conservative default. For longer games you can increase M (e.g., 10 or sqrt(r)) to smooth more history.
- θ (forgiveness threshold): 0.5 default (requires majority cooperation among others to resume cooperative probing). Lower θ makes the strategy more forgiving; higher θ more cautious.
- F (forgiveness probe rounds): 2 default; increasing F lets you test for restored cooperation more persistently.
- ε: small positive if there is numerical noise; otherwise 0.

Summary
AWSLS‑SF is a simple, parameter‑driven strategy that signals cooperation, stabilizes mutual cooperation (Win‑Stay), punishes exploitation (Lose‑Shift), quickly forgives and probes for resumed reciprocity, and behaves sensibly in the final round. It requires only the game parameters and the observed history of cooperators per round and is robust against a wide range of opponent behaviors.
'''

description_COOPERATIVE_61 = '''
Strategy name: Adaptive Proportional Reciprocity (APR)

Goal
- Sustain near-universal cooperation when opponents are willing, but avoid long exploitation by defectors. 
- Use simple, observable statistics from history (who cooperated each round) only; no communication or private signals required.  
- Forgiving and proportional: punish defections enough to deter them, then return to cooperation quickly if others respond.

Main idea (in one sentence)
- Start cooperatively; in each non-final round cooperate if recent partner cooperation is high enough, otherwise defect for a short, size-proportional punishment period; always allow a clear re‑entry path to cooperation (forgiveness); be cautious in the final round unless there is very strong evidence of stable mutual cooperation.

Parameters (computed from game inputs; can be tuned)
- n, r, k: game inputs (given).
- window L = min(5, max(1, floor(r/5))) — number of recent rounds to measure cooperation (use as much recent history as available up to 5 rounds).
- cooperation threshold α = 0.55 by default. Optionally adapt α upward when the public good is very valuable: α := clamp(0.5, 0.9, 0.55 + 0.3 * ((k/n) - 0.2)) — (this makes the agent slightly more demanding when cooperation yields greater group benefit). (This is a practical default; implementation may use fixed 0.6 if preferred.)
- max punishment length Pmax = min(4, r) — cap on how long we punish to avoid long vendettas.
- punishment scaling: for observed fraction f of cooperators in the reference window, punishment length P = min(Pmax, ceil((1 - f) * (r_remaining + 1))) where r_remaining = r - t (rounds left including current after decision). This makes punishment longer when defection was widespread and when more future rounds remain.
- forgiveness threshold α_forgive = α - 0.15 (i.e., require a modest recovery of cooperation to resume).
- final-round strictness: in round t = r (final round), default to Defect unless very strong recent cooperation evidence: all players cooperated in at least min(3, L) of last L rounds (or average cooperation among others in window ≥ 0.95). This avoids being a final-round sucker but allows mutual last-round gains if trust clearly exists.

Decision rules (plain English)
1. First round (t = 1): Cooperate. (Signal willingness to cooperate, build reputation.)
2. For any round t with 1 < t < r (not final round):
   a. Compute recent cooperation rate f among the other n − 1 players, using the last L rounds (if fewer than L rounds exist, use available rounds). For each round in the window count how many of the other players cooperated; take the average fraction per round, or equivalently compute total cooperations by others divided by ((n−1) × number_of_rounds_used). (You may also use the last round only for responsiveness; the window adds noise robustness.)
   b. If we are currently in a punishment phase (see below), defect until that phase ends (punishment phases are announced by entering them based on past f).
   c. Otherwise, if f ≥ α, cooperate.
   d. If f < α, enter a punishment phase: defect for P rounds where P is calculated by punishment scaling above. The punishment starts immediately (we defect in this round) and counts down each round. The punishment applies to all players (we cannot target a single player), so it inflicts costs on defectors as well.
   e. After punishment ends: switch to “probation/cooperate” mode — cooperate for at least one round if the recent cooperation rate in the immediate post-punishment window meets α_forgive; otherwise, if others continue to defect, re-enter punishment (with P recomputed from latest f).
3. Final round (t = r):
   - Default: Defect.
   - Exception (cooperate in final round): If there is clear, strong evidence of stable cooperation (e.g., all players cooperated in at least min(3, L) of the last L rounds, or average cooperation among others in window ≥ 0.95), then cooperate in the final round. This gains the mutual-benefit payoff if everyone is reliably cooperative; otherwise avoid being exploited.

Handling mistakes and noise (forgiveness / contrition)
- Because one mistake (random noise or misimplementation) should not trigger endless punishment, punishment lengths are capped (Pmax) and forgiveness requires recovery of cooperation above α_forgive.
- If I mistakenly defected earlier and others retaliate, the short forgiveness window allows me to re-establish cooperation by cooperating during probation: after a punishment phase I will be willing to cooperate if others’ cooperation recovers, so contrition is built in.
- If opponents punish me while I behaved cooperatively (they misinterpreted), the forgiveness mechanism allows returning to cooperation quickly if they resume cooperating.

Pseudocode (concise)
- Inputs: n, r, k; history H where H[t][j] ∈ {C,D} for rounds t<current and players j.
- Initialize: punishment_timer = 0.
- Compute L, α, α_forgive, Pmax as above.

For round t = 1..r:
  if t == 1:
    play C; continue
  if t == r:
    if strong_cooperation_in_window(H, L): play C else play D; continue
  if punishment_timer > 0:
    play D; punishment_timer -= 1; continue
  compute f = average fraction of other players that cooperated over last up to L rounds
  if f ≥ α:
    play C
  else:
    P = min(Pmax, ceil((1 - f) * (r - t + 1)))
    punishment_timer = P - 1  // we are defecting this round, then P-1 more rounds
    play D

Function strong_cooperation_in_window(H, L):
  use the last up to L rounds available;
  return (average cooperation among others ≥ 0.95) OR (count of rounds in which all players cooperated ≥ min(3, L))

Rationale / commentary
- Start cooperatively: signaling is cheap and necessary to elicit reciprocal cooperation from conditional cooperators.
- Thresholding (α) implements conditional cooperation: we cooperate when others mostly cooperate; when cooperation is low we switch to punishment to reduce the benefit defectors get from the public good.
- Proportional punishment length (P proportional to the observed defection rate and remaining rounds) is adaptive: it deters heavy defection when there are enough future rounds to make punishment credible, but avoids open-ended vendettas near the end of the game.
- Forgiveness (α_forgive) prevents cycles of retaliation and recovers cooperation after mistakes.
- Final-round caution protects against guaranteed exploitation in that round but still allows cooperation if there is clear evidence of stable cooperation (so mutual gains are harvested when trust exists).
- This strategy uses only the public history and game parameters, is robust to many opponent types (pure defectors, conditional cooperators, randoms), and is designed to do well in tournaments that value sustained cooperation but penalize excessive exploitation or permanent vengefulness.

Tuning notes (implementation)
- L = 1 (last round only) makes the strategy very responsive; larger L increases noise resistance.
- α can be tuned between 0.5 (cooperate if a bare majority cooperates) and 0.8+ (demand near-unanimity). A mid-range value (0.55–0.65) is robust in diverse tournaments.
- Pmax = 3–4 is usually effective: enough to deter, not long enough to collapse mutual cooperation permanently.
- If you know opponent population is highly rational and forward-looking, shorter punishments may be sufficient; if many naive cooperators exist, one may afford to be more forgiving.

This strategy is cooperative-minded (it begins with cooperation, rewards cooperation, and quickly returns to cooperation after transients), but it is also robust (it punishes defections proportionally and refuses to be a final-round sucker).
'''

description_COOPERATIVE_62 = '''
Strategy name: Adaptive Proportional Conditional Cooperator (APCC)

High-level idea
- Start by offering cooperation. Maintain cooperation as long as a large-enough fraction of the other players have been cooperating recently.
- When group cooperation falls substantially below the target, impose a proportional, limited punishment (short defecting phase) whose length scales with how large the shortfall is and how many rounds remain. After punishment, try to re-establish cooperation with a short probation (forgiveness) rather than permanent retaliation.
- Be endgame-aware: defect in the final round (no future to enforce reciprocity).
- The strategy uses only the public history of actions and the known game parameters (n, k, r). No assumptions about other players’ norms or communication.

Notation used below
- t: current round (1..r)
- history H: for each past round s < t we observe vector of contributions c_j,s (0/1), or equivalently number_of_cooperators_s = Σ_j c_j,s.
- S_{-i,s} = number of cooperators other than me in round s (observed)
- frac_recent = average over a recent window of rounds of S_{-i,s} / (n-1) (fraction of other players cooperating)
- r_remain = r - t + 1 (rounds remaining including current)

Core parameter choices (derived from n,k,r; implementer may tune constants)
- window length w = min(5, t-1) (use last up to 5 rounds to judge recent behavior)
- baseline_target_coop T_coop = clamp(0.5 + 0.3 * ((k-1)/(n-1)), 0.5, 0.9)
  - intuition: require at least a modest majority; when k is larger relative to n (public return stronger), be more demanding because cooperation is more valuable.
- tolerance epsilon = 0.05 (a small slack to avoid reacting to tiny fluctuations)
- max_punish_fraction = 0.2 (cap punishments to at most 20% of remaining rounds)
- forgiveness_delta = 0.05 (we treat small improvements as meaningful)
- All clamps and ceilings are integer where rounds/lengths are required.

Decision rules (natural-language then pseudocode)

Natural-language summary
1. Last round: defect. (No future to enforce cooperation.)
2. First round: cooperate. (Offer to build cooperation.)
3. At any round t>1, compute frac_recent = average fraction of other players cooperating over the last w rounds.
4. If currently in a punishment phase (set by previous rule), continue defecting until that punishment phase expires.
5. Otherwise:
   - If frac_recent >= T_coop - epsilon, cooperate (C).
   - If frac_recent < T_coop - epsilon, enter a punishment phase:
     - Compute shortfall s = T_coop - frac_recent (in [0,1]).
     - Set punishment length P = min( ceil(s * r_remain), max(1, ceil(max_punish_fraction * r)) ). (Punishment proportional to shortfall but bounded.)
     - Defect for P consecutive rounds (including this one).
6. After a punishment phase ends: perform a one-round probationary cooperation (cooperate one round and observe). If other players’ cooperation in that probation round (or averaged over the last probation+post-punish window) improves by at least forgiveness_delta over the frac_recent that triggered punishment, return to the normal cooperative regime. If not, resume a short punishment (P'=1 or recompute P from current shortfall) — keep punishments bounded so they do not continue indefinitely.
7. Always treat single accidental deviations gently (small shortfall leads to small punishment). Repeated or large shortfalls trigger longer punishments but still bounded and followed by probation.

Pseudocode

Inputs: n, k, r, history H up to round t-1
State variables (maintained across rounds): punish_until_round (initially 0), last_trigger_frac (for assessing recovery)

Function frac_recent(t, H):
  w = min(5, t-1)
  if w == 0: return 1.0   # treat "no history" as neutral (first round handled separately)
  sum_frac = 0
  for s = t - w to t - 1:
    sum_frac += (S_{-i,s} / (n-1))   # S_{-i,s} = number of cooperators excluding me in round s
  return sum_frac / w

At start of each round t:

1. If t == r:
     action = D   # final round defect
     return action

2. If t == 1:
     action = C
     return action

3. If t <= punish_until_round:
     action = D
     return action

4. Otherwise:
     T_coop = clamp(0.5 + 0.3 * ((k-1)/(n-1)), 0.5, 0.9)
     epsilon = 0.05
     frac = frac_recent(t, H)
     if frac >= T_coop - epsilon:
         action = C
         return action
     else:
         # Enter proportional punishment
         s = T_coop - frac              # shortfall in [0,1]
         r_remain = r - t + 1
         # punishment proportional to shortfall, but at least 1 and capped
         P_raw = ceil(s * r_remain)
         P_cap = max(1, ceil(max_punish_fraction * r))   # e.g. 20% of total rounds max
         P = min(max(1, P_raw), P_cap)
         punish_until_round = t + P - 1
         last_trigger_frac = frac
         action = D
         return action

After a punishment expires (i.e. on the first round t > punish_until_round that we would otherwise cooperate):
  - Instead of immediately returning to full cooperation, cooperate for one probation round and observe others’ cooperation in that probation round (or the average of the probation round and the previous round).
  - If observed_frac_post >= last_trigger_frac + forgiveness_delta (i.e. cooperation has meaningfully improved), resume normal cooperative regime.
  - Else, resume a small punishment of length 1 (or recompute P). Keep this process bounded (punishments never exceed the P_cap and overall we reduce P as rounds run out).

Notes on implementation details and variants
- Window w = min(5, t-1) is conservative; implementers may use larger windows (e.g. sqrt(t)) for longer games. Using a short window makes the strategy responsive to recent changes.
- T_coop formula ties the required cooperation threshold to k: as k grows relative to n, the public good is more valuable and APCC demands slightly higher recent cooperation to keep cooperating. The clamp ensures we never require implausibly strict or lax thresholds.
- The punishment length is proportional to the shortfall s and to the remaining rounds: if a deviation happens early and is large, punishment will be longer (proportionally); if it happens near the end, punishments are short (endgame-aware). The P_cap prevents unbounded grim-trigger style retaliation.
- Forgiveness and the probation stage prevent permanent breakdown after a single or small number of defections and reduce exploitation by occasional noisy or opportunistic defectors.
- Always defect in final round because there is no future enforcement.

Why this is cooperative and robust
- Cooperative: APCC starts cooperatively and returns to cooperation automatically when the group shows sufficient cooperative behavior. It aims for the socially optimal outcome (full cooperation) when the group reciprocates.
- Robust to exploitation: APCC punishes shortfalls; the punishment length is proportional to how badly the group deviated. That makes defection costly for sustained defectors and deters exploitation. But punishments are limited and followed by probation, preventing permanent deadlocks.
- Adaptive: The strategy uses recent observed fractions to adapt to mixed populations. If a stable subset of players cooperates and others defect persistently, APCC will adapt by requiring a higher recent fraction to keep cooperating — so it avoids being exploited by defectors while still cooperating when enough others do.
- Endgame-aware and forgiving: Short punishments and explicit probation make the strategy resilient to accidental deviations and to noisy behavior while still providing credible deterrence against repeated defection.

Edge cases handled explicitly
- First round: always cooperate (offers cooperation unilaterally to try to seed mutual cooperation).
- Last round: always defect (no future payoff to enforce cooperation).
- Short games (small r): P_cap formula uses r so punishments scale sensibly; near the end punishments are necessarily short.
- Single isolated defection: small shortfall produces P_raw small (possibly 1), so a brief punishment then probation — not catastrophic.
- Persistent many defectors: the strategy will reduce cooperation if frac_recent remains below threshold; punishments will not be infinite and the strategy will eventually stop wasting rounds on futile punishment because punishments are bounded and the threshold is applied each round. That reduces losses against hopelessly non-cooperative groups.

Implementation checklist for programmers
- Maintain punish_until_round and last_trigger_frac across rounds.
- Maintain action history H (observed vector of cooperators each past round).
- Compute S_{-i,s} from observed actions each round.
- Use the exact formulas given to compute T_coop, P, etc.
- Ensure integer rounding for P, punish_until_round, window indices.

Tuning guidance
- If you expect many noisy mistakes, increase epsilon and/or decrease T_coop and increase forgiveness_delta to be more forgiving.
- If you want to be stricter against defectors, decrease epsilon, increase T_coop baseline (raise clamp ceiling), or increase max_punish_fraction.
- In very long tournaments, consider larger window w (e.g., min(10, t-1) or sqrt(t)) to stabilize estimates of others’ behavior.

Final remarks
APCC is designed to be simple, interpretable, and to balance being cooperative (seed and restore mutual cooperation) with robustness (proportional punishment, forgiveness, endgame awareness). It relies only on the game parameters (n,k,r) and the public action history, making it suitable for a tournament against independently designed AIs with unknown motives.
'''

description_COOPERATIVE_63 = '''
Name: Adaptive Conditional Cooperation with Proportional Punishment (ACCPP)

Short description
- Start cooperative to signal willingness.
- Monitor recent group cooperation (a short sliding window).
- Continue cooperating while recent group cooperation is high (generous conditional cooperation).
- If cooperation falls below the windowed threshold, apply a finite, proportionate punishment (defect for a limited number of rounds) whose length scales with the shortfall.
- Forgive quickly if the group returns to cooperating; never punish in the final round.

Intuition / goals
- Be cooperative: open with cooperation and prefer to cooperate when others reciprocate.
- Be robust: don’t collapse to permanent grim-trigger on a single mistake; punish proportionally to deter free-riding while allowing recovery.
- Be adaptive: use recent history to gauge cooperation; scale punishment to the observed shortfall so we respond strongly to repeated/large defections and lightly to isolated slips.
- Handle endgame: in the strictly final round, defect because there is no future to enforce cooperation.

Parameters derived from game inputs
- n, k, r are given by the tournament.
- Memory window M = min(4, r-1). Use up to the four most recent past rounds (fewer when r small).
- Cooperation threshold tau = 0.6 (requires a clear majority of others cooperating in recent history). (You may tune 0.6; it is intentionally generous.)
- Maximum punishment fraction beta = 0.25 of remaining rounds (so punishment cannot consume all future opportunities).
- Minimum punishment length P_min = 1 (punish at least one round when triggered).

State variables
- punishment_counter: integer >= 0 (remaining punishment rounds we will defect).
- history: the full observed history of all players’ actions by round (we observe everyone each round).

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This establishes a cooperative signal.
2. Last round (t = r): Defect (D) — no future to enforce cooperation.
3. If punishment_counter > 0: play D this round and decrement punishment_counter by 1 (unless we see full cooperation during a punishment round — see forgiveness rule below).
4. Otherwise (not currently punishing):
   a. Compute recent average cooperation level across the last M rounds (or all past rounds if fewer than M exist). Specifically, for each recent round compute fraction_cooperators = (# cooperators in that round) / n, then average those fractions to get avg_coop_fraction.
   b. If avg_coop_fraction >= tau: play C (continue cooperation).
   c. Else (avg_coop_fraction < tau): begin a proportional punishment this round (play D) and set punishment_counter to a finite length proportional to the shortfall (see formula below). The punishment begins immediately and runs for punishment_counter rounds unless ended early by full cooperation (forgiveness).
5. Forgiveness during punishment: If while we are punishing we observe a round with full (or sufficiently high) cooperation (e.g., fraction_cooperators >= tau), reset punishment_counter = 0 and cooperate next round. In other words: end punishment early if the group demonstrates cooperative recovery.
6. Noise protection (lenience): Because we use a windowed average, a single accidental defection will usually not drop avg_coop_fraction below tau if cooperation was previously high; thus we avoid disproportionate reactions to single errors.

Punishment length formula (proportional)
- Let shortfall = max(0, 1 - avg_coop_fraction). This lies in [0,1].
- Remaining rounds after current round = R_remain = r - t (if counting after current play); to avoid punishing in the final round we ensure punishment_counter ≤ R_remain - 1 when possible.
- Set raw_P = ceil(shortfall * n)  (i.e., punish for roughly as many rounds as the shortfall scaled by group size — more defectors → longer punishment).
- Set cap_P = max(1, floor(beta * (r - t + 1)))  (cap punishment to a fraction beta of the remaining game to avoid runaway punishment).
- punishment_counter = min(max(raw_P, P_min), cap_P, max(0, R_remain)).

Concrete pseudocode

Inputs: n, k, r, history (list of rounds; each round is list/array of n actions in {C,D})
State: punishment_counter = 0

On each decision for round t (1-based):
  if t == 1:
    action = C
    return action

  if t == r:
    action = D
    return action

  if punishment_counter > 0:
    # We are punishing now
    # But observe the last round: if the group has already returned (forgiveness), cancel punishment
    last_round = history[-1]  # last observed round actions
    last_frac = (number of C in last_round) / n
    if last_frac >= tau:
      punishment_counter = 0
      action = C
      return action
    else:
      action = D
      punishment_counter -= 1
      return action

  # Not currently punishing
  # Compute avg cooperation fraction over last M rounds (or all past rounds if fewer)
  lookback = min(M, len(history))
  if lookback == 0:
    avg_coop_fraction = 0  # should never happen except t==1 which is already handled
  else:
    sum_frac = 0
    for round_actions in history[-lookback:]:
      sum_frac += (number of C in round_actions) / n
    avg_coop_fraction = sum_frac / lookback

  if avg_coop_fraction >= tau:
    action = C
    return action

  # Trigger proportional punishment
  shortfall = max(0, 1 - avg_coop_fraction)
  raw_P = ceil(shortfall * n)
  R_remain = r - t
  cap_P = max(1, floor(beta * (r - t + 1)))
  P = min(max(raw_P, P_min), cap_P, max(0, R_remain))
  punishment_counter = P - 1   # we will play D this round (counts as the first punished round)
  action = D
  return action

Parameter defaults and rationale
- M = min(4, r-1): short memory gives quick detection and recovery; 4 rounds is often enough to see patterns while staying responsive.
- tau = 0.6: slightly generous; cooperation continues when a clear majority (or at least strong minority if n small) cooperate. This avoids punishing for small fluctuations.
- beta = 0.25: punishment consumes at most 25% of remaining rounds — strong enough to create a future cost but not so long that recovery becomes impossible.
- P_min = 1 ensures punishments are non-zero when triggered.

Why this strategy is cooperative and robust
- Cooperation-first: it opens with C and prefers C when recent group behavior has been cooperative.
- Proportionate punishment: length of punishment scales with observed shortfall; a single defection typically produces a small shortfall and therefore a small or no punishment (lenience), while repeated or large-scale defections lead to longer punishment to create deterrence.
- Forgiving: punishment is cancellable early if others resume cooperation.
- Adaptive: uses n and r implicitly through M and punishment caps; the punishment cap ties to remaining rounds so the strategy is credible (it doesn’t promise punishments longer than the remaining game) and avoids wasting all remaining interactions on punishment.
- Endgame-aware: defects in the final round, consistent with backward induction and preventing being exploited when no future enforcement is available.

Edge cases
- Very short games (r = 2 or 3): M is reduced accordingly; punishment cap ensures we do not require more punishment rounds than remain. For r=2: t=1 we cooperate; t=2 we defect (last round).
- Single accidental defection by another: the sliding window and tau prevent immediate harsh punishment; generally this yields forgiveness and continued cooperation.
- Multiple simultaneous defectors: shortfall is large → longer punishment, deterring coordinated free-riding.
- Perfect-cooperation opponents: we cooperate every round (except last), obtaining the high mutual payoff.
- Uncooperative opponents (always defect): we quickly switch to defect and minimize exploitation; because we start cooperative we give opportunistic strategies a chance to reciprocate.

Tuning suggestions (optional)
- Increase tau toward 0.7 to be less tolerant of defections (more demanding reciprocity); lower tau toward 0.5 to be more forgiving.
- Increase beta to punish more heavily (longer punishments) if opponents are likely to respond to severe sanctions; decrease beta to prioritize quicker recovery.
- Increase M if opponents exhibit slow-changing behavior and you want smoother averages; decrease M to be more reactive.

Final note
This strategy depends only on the game parameters (n,r,k) for setting M and penalty caps, and on public history for all decisions. It balances being cooperative (first move and continued cooperation when reciprocated), robust (forgiveness, window averaging), and adaptive (proportional punishment scaled to observed shortfalls and remaining rounds). It is ready to be implemented as an algorithm with the pseudocode above.
'''

description_COOPERATIVE_64 = '''
Strategy name: Forgiving Proportional Reciprocity (FPR)

Short description / intuition
- Start by cooperating to signal willingness to sustain public good.
- Condition future cooperation on observed cooperation by others (recent and medium-term), so you reward cooperators and withhold cooperation from persistent defectors.
- Punish defections only briefly and proportionally (limited-length group punishment), then forgive so cooperation can be re-established.
- Always defect in the final round (no credible future punishment there). Be explicit about short games and avoid useless long punishments near the end.

The strategy depends only on the game parameters (n, r, k) and observed history of all players' actions. It is deterministic, easy to implement, and designed to be robust across many opponent behaviours (unconditional defectors, conditional cooperators, mixed strategies).

High-level decision rules
1. First round: Cooperate.
2. Final round (t = r): Defect.
3. Intermediate rounds (1 < t < r):
   - Compute:
     - c_last = number of cooperators in previous round (including yourself).
     - p_last_others = fraction of other players who cooperated in the previous round = (c_last - my_last_action) / (n-1).
     - For memory window L = min(5, r-1), compute p_avg_others = average fraction of other players who cooperated over the last L rounds (sliding window).
   - Compute a cooperation threshold theta that adapts modestly to how valuable the public good is:
       theta = clamp(0.3, 0.7, 0.5 - 0.4 * (k/n - 0.5))
       (Interpretation: when k/n is high (public good returns large), be more willing to cooperate; when k/n is low, be stricter.)
   - Use a short punishment timer pun (initially 0). If pun > 0, defect this round and decrement pun. Punishment length P = 2 by default (see edge-case adjustment below).
   - Otherwise:
     - If p_last_others >= theta: Cooperate (recent behavior is good).
     - Else if p_avg_others >= theta and there are at least 2 full rounds remaining (i.e., t <= r-2): Cooperate (generosity to help re-establish cooperation).
     - Else: Defect, and if the last round had a substantial drop in cooperation (e.g., c_last <= floor(n/2)), set pun = min(P, remaining_rounds - 1) to start a limited group punishment (do not start a punishment that cannot fit before the end of the game).

Rationale for these choices
- Cooperating first signals cooperative intent (helps coordinate with other conditional cooperators).
- Threshold-based conditioning (theta) makes cooperation contingent on others’ recent behavior (so you reward cooperation and avoid being exploited).
- Using both a very recent statistic (p_last_others) and a short moving average (p_avg_others over L rounds) balances responsiveness and robustness: you react to sharp changes but don’t overreact to a single isolated defection.
- Limited punishment (small P) is strong enough to deter exploitive strategies while being forgiving so cooperation can resume. Punishments are group-wide (you cannot target one player), so they are proportional and temporary.
- Always defect in final round: with finite horizon and no binding agreements, there is no credible future punishment after the last round, so cooperating in the final round is dominated.

Edge cases and implementation details
- Very short games:
  - r = 2: t=1 cooperate, t=2 defect (final round).
  - r = 3: t=1 cooperate, t=2 follow normal intermediate rules, t=3 defect.
  - When remaining_rounds <= P: reduce effective punishment length to fit (pun = min(pun, remaining_rounds - 1)). Do not start punishments you cannot complete before the game ends.
- Memory L: use L = min(5, r-1). If r-1 < 1, L = 0 (no past data) and you fall back to cooperating on t=1 and defecting on last round.
- Threshold theta: the formula above is a simple, interpretable adaptation to k and n. It keeps the strategy cooperative for higher public-good returns (higher k/n). The clamp ensures thresholds stay in a reasonable range (between 0.3 and 0.7).
- Punishment trigger: I recommend triggering a short punishment when the last-round cooperation count falls to half or below (c_last <= floor(n/2)), or when you detect multiple players defecting persistently (e.g., some players have defected in all of the last L rounds). This avoids reacting to small fluctuations.
- Forgiveness: after punishment expires, the normal conditional rules apply again (p_last_others and p_avg_others). This allows the group to return to cooperation.
- Deterministic or probabilistic: the above is deterministic. A small randomized element (e.g., cooperate with small probability epsilon when p_last_others is just below theta) can increase robustness against adversarial exploiters; include this only if allowed in the tournament.

Pseudocode (concise)
Inputs: n, r, k
Internal params: L = min(5, r-1), P = 2, pun = 0 (punishment timer), history of actions (matrix rounds × players)

Function decision(t, history, my_id):
  if t == 1:
    return C
  if t == r:
    return D
  remaining = r - t + 1
  # update history stats
  my_last_action = 1 if I cooperated in round t-1 else 0
  c_last = number_of_cooperators_in_round(t-1)
  p_last_others = (c_last - my_last_action) / (n - 1)
  # p_avg_others over last L rounds (or available rounds)
  start_round = max(1, t - L)
  total_other_coops = sum_over_rounds(start_round..t-1) (cooperators_in_round - my_action_in_that_round)
  window_length = (t - start_round)
  if window_length > 0:
    p_avg_others = total_other_coops / (window_length * (n - 1))
  else:
    p_avg_others = p_last_others
  # adaptive threshold
  theta = 0.5 - 0.4 * (k/n - 0.5)
  theta = min(max(theta, 0.3), 0.7)
  # punishment handling
  if pun > 0:
    pun = pun - 1
    return D
  # main decision
  if p_last_others >= theta:
    return C
  if p_avg_others >= theta and remaining >= 2:
    return C
  # otherwise defect and possibly start punishment
  if c_last <= floor(n / 2) and remaining > 2:
    pun = min(P, remaining - 2)  # leave at least one round after punishment if possible
  return D

Notes about tournament performance and robustness
- Strengths:
  - Starts cooperative and returns to cooperation after temporary punishments — compatible with many cooperative strategies.
  - Limited punishments reduce the chance of long vendettas and cascading collapse of cooperation.
  - Uses both short-term and medium-term signals so it can distinguish isolated mistakes from systematic defection.
  - Adapts modestly to the payoff environment via theta(k/n).
- Weaknesses / trade-offs:
  - Can be exploited briefly by sophisticated opportunists who defect only when you will not punish strongly; but limited punishments combined with conditional cooperation reduce long-term gain for exploiters in many settings.
  - In strictly adversarial environments (many unconditional defectors) the strategy will eventually defect and limit losses, but cannot force cooperation where no one reciprocates.
  - Because the game is finitely repeated, there is no guaranteed enforcement of cooperation in the last rounds; this strategy accepts that and focuses on maximizing cooperation earlier.

Conclusion
FPR is a cooperative, history-dependent strategy that starts by cooperating, conditions on recent group behaviour with an adaptive threshold tied to k/n, punishes briefly and proportionally when cooperation collapses, and forgives so cooperation can resume. It is simple to implement, robust across a wide range of opponent types, and tailored to the public-goods multi-player setup.
'''

description_COOPERATIVE_65 = '''
Name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Start by cooperating and try to sustain high group contributions by conditional reciprocity.
- Cooperate when past history gives good reason to expect others will reciprocate; defect to punish low group cooperation but punish for only a short, calibrated time and then forgive (with occasional probabilistic tests) so cooperation can be re-established.
- Always treat the final round as a one-shot (defect) and be cautious in the immediate endgame.
- Use only the game parameters (n, k, r) and observed history of moves; no communication or outside coordination.

Derived tuning parameters (computed at initialization from n, k, r)
- window m for short-term statistics: m = min(5, max(1, floor(r/5)))
- punishment length P: P = min(3, max(1, floor(r/10)))  (short, so punishments are costly and reversible)
- forgiveness-test probability p_test = 0.10 (10% occasional cooperation during punishment to probe)
- endgame: last_round = r; always defect in round r. Optionally treat the final G rounds cautiously: G = min(2, floor(r/10)). (See edge-handling.)
- adaptive cooperation threshold T ∈ [0.5, 0.75]: set
    benefit_gap = 1 - k/n   (how attractive a single-round defection is)
    norm = 1 - 1/n
    T = 0.5 + 0.25 * (benefit_gap / norm)
  Intuition: when defection gives a large single-round advantage (k/n small) require a higher recent-group-cooperation fraction to keep cooperating; when k/n is large (k close to n), be more forgiving.

State the strategy maintains
- history of rounds: for each past round t store number_of_cooperators[t] and who cooperated (so per-player frequencies can be computed).
- current punishment_counter (int) — number of remaining rounds to continue punishment (0 if not punishing).

Decision rules (natural language)
1. First round (t = 1): cooperate.
2. Last round (t = r): defect (dominant one-shot).
3. If currently in an active punishment (punishment_counter > 0):
   - With probability p_test cooperate (probe).
   - Otherwise defect.
   - After the action decrement punishment_counter by 1, but if the most recent observed round (the one you just saw) shows group cooperation fraction >= T then clear punishment_counter = 0 (forgive early).
4. If not in punishment:
   - Compute recent group cooperation fraction f_recent as the average over the last min(m, t-1) rounds of (number_of_cooperators / n). (Include all players, including yourself in past rounds.)
   - If f_recent >= T:
       - Cooperate (reward sustained cooperation).
   - Else (f_recent < T):
       - Begin punishment: set punishment_counter = P (or set to P-1 after this round depending on implementation)
       - Defect this round (but with probability p_test you may cooperate to probe).
5. Special-case handling for "clear long-term cooperators":
   - If a substantial subset of players (≥ 1) has cooperated in ≥ 80% of last m rounds, treat the group as more cooperative: lower threshold for cooperating to max(0.4, T - 0.05). This rewards stable cooperators even if a temporary dip shows up.
6. Endgame refinement:
   - Always defect in round r.
   - For rounds t where r - t < G (final G rounds), be more conservative: require f_recent >= (T + 0.05) to cooperate; otherwise defect (still allow p_test probes occasionally). This reduces exploitation risk close to the end.

Pseudocode (clear algorithmic form)

Initialize:
  m = min(5, max(1, floor(r/5)))
  P = min(3, max(1, floor(r/10)))
  p_test = 0.10
  benefit_gap = 1 - k/n
  norm = 1 - 1/n
  T = 0.5 + 0.25 * (benefit_gap / norm)
  G = min(2, floor(r/10))
  punishment_counter = 0
  history = empty list of past rounds (store cooperators set or counts per round)

For each round t = 1..r:
  if t == 1:
    play C
    record action and observe others
    continue
  if t == r:
    play D
    record and exit
  // compute f_recent
  recent_rounds = last min(m, len(history)) rounds from history
  if recent_rounds not empty:
    f_recent = average over recent_rounds of (num_cooperators / n)
  else:
    f_recent = 1.0  // if no history, optimistic (but t==1 handled)
  // reward known long-term cooperators
  compute per-player cooperation rate over recent_rounds
  if any player_coop_rate >= 0.8:
    T_eff = max(0.4, T - 0.05)
  else:
    T_eff = T
  if (r - t) < G:
    T_eff = T_eff + 0.05  // be a bit more conservative near the end
  if punishment_counter > 0:
    with probability p_test:
      play C
    otherwise:
      play D
    // After observing others' actions for this round, update punishment_counter:
    if fraction_cooperators_in_just_observed_round >= T_eff:
      punishment_counter = 0  // forgive early
    else:
      punishment_counter = punishment_counter - 1
    record history and continue
  else:  // not in punishment
    if f_recent >= T_eff:
      play C
      record and continue
    else:
      // start a short punishment
      punishment_counter = P
      with probability p_test:
        play C
      otherwise:
        play D
      record and continue

Why this is cooperative and robust
- It starts cooperatively and rewards sustained group cooperation (so it produces high joint payoff when many others are cooperative).
- It punishes defections quickly but for a short, bounded time to avoid long retaliatory wars that destroy payoffs for everyone.
- It includes probabilistic probing (p_test) so cooperation can be re-tested and re-established after noise or mistaken defections.
- It adapts the cooperation threshold to how tempting defection is (via k and n) — when defection gives a large one-round advantage the strategy becomes more demanding; when k is large (cooperation is cheaper) the strategy is more lenient.
- It protects itself in the final round (defect) and becomes a bit more cautious approaching the endgame so it is not exploited by opportunistic strategies that only defect near the finish.
- It uses only observable history and game parameters; no external coordination required.

Edge cases and implementation notes
- If r is very small (e.g., r = 2 or 3), m and P collapse to small values; the algorithm will still start cooperating and then quickly adapt. For r = 2 the behavior is: cooperate round 1, defect round 2 (standard one-shot endgame).
- If many players are persistent defectors (group cooperation rate very low for many rounds), ACC will stop cooperating (minimizes exploitation). If group becomes more cooperative later, ACC will resume cooperation after short punishments and probes.
- The numeric constants (m, P, p_test, 0.8, 0.05 shifts) are choices intended to balance responsiveness, forgiveness, and robustness. They are conservative and easy to implement; tournament tuning can adjust them but the overall structure (start-cooperate, conditional reciprocity, short punishment, forgiveness probes, endgame caution) is the essential algorithmic policy.

Summary
- ACC is a practical, parameterized conditional-cooperation strategy: cooperate initially, reward sustained cooperation, punish brief deviations, forgive with probing, adjust threshold according to n and k, and defect in the final round. It is adaptive, resistant to long vendettas, and designed to achieve high group payoff against reciprocating opponents while avoiding exploitation by persistent defectors.
'''

description_COOPERATIVE_66 = '''
Name: Adaptive Generous Win‑Stay–Lose‑Shift (AG‑WSLS)

Intuition (short)
- Start by being nice (cooperate).
- After each round, compare the payoff you actually received to the payoff you would have received if you had done the opposite that round (you can compute this from observed actions). If your choice was at least as good as the alternative, repeat it (stay). If it was strictly worse, switch (shift).
- Allow quick recovery to cooperation when the group shows strong cooperative behavior (forgiveness).
- Defect in the final round (no future to protect).

This approach is simple, uses only game parameters and observed history, punishes exploiters, and forgives so cooperation can be re‑established.

Detailed decision rules

Notation
- n, r, k: game parameters.
- t: current round index (1..r).
- For each past round s we observe S_s = Σ_j c_j,s (total cooperators in round s) and we observe our action c_i,s ∈ {0,1} (0 = D, 1 = C) and our realized payoff π_i,s.
- For convenience define last round s = t − 1 when t > 1.
- Define COUNTS: S_s ∈ {0,...,n}.

Rule set

1) First round (t = 1)
- Play C (cooperate). Rationale: be nice to seed cooperation.

2) Last round (t = r)
- Play D (defect). Rationale: single‑shot defection strictly dominates given no future.

3) Intermediate rounds (2 ≤ t ≤ r−1)
Compute from last round s = t−1:
- Observed S = S_s (number of cooperators last round).
- Let my previous action c_prev = c_i,s ∈ {0,1}.
- Compute my actual last payoff π_prev (observed).
- Compute the counterfactual payoff last round had I played the opposite action:

If c_prev == 1 (I cooperated last round):
  π_alt = 1 + (k/n) * (S − 1)    // payoff if I had defected instead

If c_prev == 0 (I defected last round):
  π_alt = 0 + (k/n) * (S + 1)    // payoff if I had cooperated instead

Decision:
- If π_prev >= π_alt: repeat previous action (c_i,t = c_prev).
  - Tie-breaker: if π_prev == π_alt prefer cooperation (choose C).
- If π_prev < π_alt: switch action (c_i,t = 1 − c_prev). This is the "lose-shift" move.

Forgiveness / group recovery override
- If we would switch to D because π_prev < π_alt but the group last round showed strong cooperation (S ≥ ceil(α · n)), then instead return to cooperation (c_i,t = 1).
  - Use α = 0.75 (i.e., 75% cooperation) as default; this parameter can be tuned. The idea: if a large majority cooperated and you just lost slightly by being cooperative, prefer to stay cooperative to preserve high collective payoffs.
  - If S is extremely low (e.g., S ≤ floor(β · n) with β = 0.25) and you are switching to C (because π_prev < π_alt and c_prev was 0), prefer to defect (do not forgive when almost everyone defects).

Temporal punishment length
- The basic rule switches immediately; to avoid cycles of alternating punishments among players, you may optionally treat a "switch to D" as a one‑round punishment: after switching to D, in the next round apply the WSLS rule normally (the rule will restore cooperation if the defection produced a "win" for you or if group cooperation rebounds). No additional multi‑round punishment is enforced.

Summary pseudocode

Input: n, r, k, history of rounds 1..t−1 (S_s and my c_i,s, π_i,s)
If t == 1:
  play C
Else if t == r:
  play D
Else:
  S = S_{t-1}
  c_prev = my action in round t-1
  π_prev = my payoff in round t-1
  if c_prev == 1:
    π_alt = 1 + (k/n) * (S - 1)
  else:
    π_alt = (k/n) * (S + 1)    // equivalently 0 + (k/n)*(S+1)
  if π_prev > π_alt:
    play c_prev
  else if π_prev == π_alt:
    play C    // tie → cooperate
  else: // π_prev < π_alt
    // switching would be "rational" for that round; consider forgiveness
    if S >= ceil(0.75 * n):
      play C    // forgive because group strongly cooperative
    else if S <= floor(0.25 * n) and c_prev == 0:
      play D    // no forgiveness when almost everyone defects
    else:
      play 1 - c_prev

Notes and rationale (concise)
- The WSLS comparison uses full knowledge of past actions to assess whether your previous move was better than the alternative. This gives a principled way to reward strategies that help you (stay) and punish those that hurt you (shift).
- Forgiveness via a high‑cooperation threshold avoids collapse of cooperation after small losses (for example, when many cooperators exist but one defector slightly reduced your payoff).
- The strategy is adaptive: it responds to what actually happened rather than assuming types or using fixed schedules. It is robust: it punishes defectors but allows cooperation to recover quickly.
- The final‑round defection is rational and prevents being exploited in the endgame. Choosing C on ties favors cooperation when indifferent.

Tunable parameters
- α (forgiveness threshold fraction, default 0.75). Higher α → stricter forgiveness requirement; lower α → more forgiveness.
- β (no‑forgiveness lower bound, default 0.25). Prevents being exploited when nearly everyone defects.
These can be tuned for a given tournament environment.

Why this should perform well in tournaments
- Begins cooperating (encourages mutually beneficial cooperation).
- Punishes exploitive behavior quickly (switching reduces exploiter payoff).
- Forgives and returns to cooperation once cooperation recovers (prevents permanent breakdown).
- Uses only verifiable history and game parameters; no reliance on communication or conventions.
- Works for any n and lever k (works even as k/n changes because decisions are based on actual payoffs and counterfactuals computed from observed S).

Implementation is straightforward given the history (counts and own past actions/payoffs).
'''

description_COOPERATIVE_67 = '''
Name: Generous Conditional Cooperator (GCC)

Idea in one sentence
- Start cooperative and try to sustain high group cooperation by cooperating whenever a clear majority of players cooperated recently; if cooperation drops, punish proportionally but briefly, use occasional probes and forgiveness so the group can return to cooperation, and withdraw (stop trusting) if repeatedly exploited.

Rationale (brief)
- In the repeated public-goods setting defection is the one-shot dominant action, so to sustain cooperation we must condition our moves on the observed willingness of others to cooperate. Majority-based conditional cooperation encourages and stabilises group-level cooperation, proportional short punishments deter large defections without locking into endless mutual punishment, and small-probability probing + forgiveness prevents collapse from occasional mistakes. A final-round exception avoids naïvely cooperating when no future enforcement is possible.

Parameters the strategy uses (derived from game parameters n, r)
- coop_threshold = ceil(n/2)   // majority threshold to consider the group "cooperating"
- punish_max = 3               // maximum punishment length (in rounds)
- probe_prob = 0.05            // small probability to probe (cooperate) during punishment
- forgive_window W = min(5, r) // window length for recent-history statistics
- exploit_epsilon = 0.02      // small margin when testing whether we are being exploited
(These are tunable; they only depend on n and r through coop_threshold and W. They are moderate choices that balance responsiveness and stability.)

State variables (maintained through the game)
- round t (1..r)
- history of contributions per round H = [(c_1,1,...,c_n,1), (c_1,2,...), ...] (we only need counts)
- punishment_timer (integer ≥ 0): rounds left to punish (defect) before trying cooperation again
- withdraw_flag (boolean): if true, we stop trusting and mostly defect for the remainder (except occasional probes)
- my_last_action (C/D)

High-level decision rules (natural language)
1. First round (t = 1)
   - Cooperate.

2. After each observed round t, compute:
   - M_t := number of cooperators in round t (0..n)
   - others_cooperated := M_t - (1 if I cooperated in t else 0)
   - rounds_remaining := r - t

3. Exploitation detection (after every round, using last W rounds)
   - Let my_avg_payoff_window be my average payoff over the last W rounds (or all past rounds if < W).
   - Let baseline_defect_payoff = 1 (the payoff from always defecting in a single round when no public good is produced).
   - If my_avg_payoff_window ≤ baseline_defect_payoff + exploit_epsilon AND over that window I cooperated more often than I defected, set withdraw_flag = true.
   - Once withdraw_flag = true, we will largely defect for the remainder (see probing below).

4. Punishment update (immediately after observing round t)
   - If punishment_timer > 0: decrement punishment_timer by 1 (we are in punishment; decision for next round follows the punishment rule below).
   - Else (not currently punishing):
       - If others_cooperated ≥ coop_threshold - 1 (i.e. a majority of others cooperated, counting ourselves would give majority), then remain cooperative (punishment_timer stays 0).
       - Else set punishment_timer = min(punish_max, max(1, coop_threshold - others_cooperated), rounds_remaining). Intuition: punishment length grows with the shortfall in cooperating players, but is capped and cannot exceed remaining rounds.

5. Action choice for next round t+1
   - If rounds_remaining = 0: game over; no action.
   - If rounds_remaining = 1 (next round is the final round):
       - Cooperate only if (a) we are not in withdraw_flag and (b) in the last round others_cooperated ≥ coop_threshold - 1 and we cooperated in the last round. Otherwise defect. (Do not rely on punishment in the final round — punishment cannot be enforced afterwards.)
   - Else if withdraw_flag = true:
       - Defect by default.
       - With small probe probability probe_prob, cooperate for one round to test whether others return cooperation. If the probe gets reciprocated by a majority, clear withdraw_flag and set punishment_timer = 0 (rejoin cooperation).
   - Else if punishment_timer > 0:
       - Defect this round (enact punishment), but with small independent probability probe_prob cooperate (a probe inside punishment) to give group a chance to recover from noise/faults. If probe is reciprocated by majority next observed round, clear punishment_timer and resume cooperating.
   - Else (not in punishment and not withdrawn):
       - Cooperate.

6. Forgiveness rule
   - After any punishment_timer expires (hits 0) we immediately try cooperation on the following round (unless withdraw_flag is set). This allows return to cooperation quickly for small mistakes.

7. Bookkeeping for probe responses
   - If we probe (cooperate while punishment_timer>0 or withdraw_flag), observe M_next. If a majority reciprocated (others_cooperated_next ≥ coop_threshold - 1), we treat the probe as successful: clear punishment_timer and withdraw_flag and resume normal cooperative mode.
   - If the probe fails (majority defects), continue punishment/withdraw path (possibly extend punishment via the punish update mechanism on the next observed round).

Pseudocode (concise)

Initialize:
  punishment_timer = 0
  withdraw_flag = false
  H = empty list
  my_last_action = None

For round t = 1..r:
  if t == 1:
    action = C
  else:
    M_prev = number of cooperators in round t-1 (from H)
    others_prev = M_prev - (1 if my_last_action == C else 0)
    rounds_remaining = r - (t-1)

    // Exploitation detection over window
    compute my_avg_payoff_window over last W rounds
    compute my_coop_count_window over last W rounds
    if my_avg_payoff_window <= 1 + exploit_epsilon and my_coop_count_window > W/2:
      withdraw_flag = true

    // Update punishment_timer if not currently punishing (we set it based on observed shortfall)
    if punishment_timer == 0 and not withdraw_flag:
      if others_prev < (coop_threshold - 1):
        shortfall = (coop_threshold - 1) - others_prev
        punishment_timer = min(punish_max, max(1, shortfall), rounds_remaining)

    // Decide action
    if rounds_remaining == 1:
      if not withdraw_flag and others_prev >= (coop_threshold - 1) and my_last_action == C:
        action = C
      else:
        action = D
    else if withdraw_flag:
      action = D with probability (1 - probe_prob), C with probability probe_prob
    else if punishment_timer > 0:
      punishment_timer = punishment_timer - 1  // we will defect this round (unless probe)
      action = D with probability (1 - probe_prob), C with probability probe_prob
    else:
      action = C

  Play action, observe full round outcome (M_t and payoffs), append to H, set my_last_action = action
  // Probe response handling (if we probed)
  if (we probed in this round) and (M_t - (1 if my_last_action == C else 0) >= coop_threshold - 1):
    // majority reciprocated probe
    punishment_timer = 0
    withdraw_flag = false

Behavioral summary (short)
- Cooperate by default.
- If majority of others cooperated last round, keep cooperating.
- If fewer than a majority cooperated, punish by defecting for a short, proportional number of rounds (up to punish_max).
- While punishing or withdrawn, occasionally probe (cooperate with small probability) to test recovery; if a probe is reciprocated by a majority, immediately resume cooperation.
- In final round, cooperate only if recent cooperation was strong and you were already cooperating.
- If you detect that you are being systematically exploited (your recent payoff is close to the one-shot defect baseline despite cooperating often), withdraw to protect yourself (but continue to probe occasionally in case conditions change).

Why this is cooperative and robust
- Cooperative: It starts cooperative, returns to cooperation after punishments, and only defects to discourage others from free-riding; it tries to re-establish cooperation quickly after mistakes via forgiveness and probing.
- Robust: Punishments are proportional and finite to avoid long mutual-defection traps; probing and forgiveness reduce lock-ins caused by noise or misunderstandings; exploit-detection prevents endless exploitation by purely selfish strategies.
- Parameter dependence: coop_threshold uses n (majority) so the strategy scales with group size; the window W uses r; punish lengths are capped by remaining rounds so the algorithm respects the finite horizon.

Notes and possible tweaks
- coop_threshold can be made more stringent (e.g., 60% rather than 50%) when k is large and the benefit of full cooperation is high; or less stringent when k barely exceeds 1 and cooperation is fragile.
- probe_prob and punish_max are tunable: raise probe_prob if many strategies are error-prone; raise punish_max if opponents are slow to learn.
- If you expect many irrational defectors, you can lower forgiveness and increase withdrawal sensitivity.

This description provides a fully specified, history-dependent, implementable algorithm that aims to sustain cooperation where feasible while defending against exploitation.
'''

description_COOPERATIVE_68 = '''
Name: Adaptive Conditional Cooperator with Proportional Punishment (ACCP-P)

Intuition (short)
- Start cooperative and try to sustain mutual cooperation when the group reciprocates.
- Use a short moving-window estimate of how many others are cooperating; cooperate when the estimate is sufficiently high, defect when it falls below a threshold.
- When others undercut cooperation, punish for a short, proportionate number of rounds to make defection unattractive, but forgive quickly so cooperation can be re-established.
- Always defect in the last round (no future to enforce reciprocity).

This makes the strategy cooperative (it seeks and sustains cooperation when reciprocated), adaptive (it reacts to observed cooperation rates), and robust (limited, proportional punishment prevents long exploitation and allows recovery).

Parameters (internal, computable from game parameters)
- MPCR = k / n (marginal per-capita return from one contribution).
- window size w = min(5, t−1) (look at up to the last 5 rounds; smaller early in the game).
- target_coop ∈ [0.5, 0.9] — the required fraction of others cooperating to cooperate yourself. Compute
  target_coop = clamp(0.5 + 0.4 * (1 − MPCR), 0.5, 0.9).
  Rationale: when MPCR is small (cooperation is individually costly), require a higher observed cooperation rate; when MPCR is closer to 1, be more tolerant.
- punishment scale S = 5 (converts deficit in cooperation into punishment length).
- forgiveness tolerance eps = 0.05 (if cooperation rate slightly below target, be generous).
- max_punish = 3 (max consecutive punitive defections).

Decision rules (deterministic pseudocode)

State tracked between rounds:
- rounds_left_to_punish (initially 0)

At round t (1-indexed), given n, r, k and history H of previous rounds (for each prior round a vector of n actions C/D, and you know your own action history):

1. Endgame:
   - If t == r (last round): play D. (No future to motivate reciprocity; defect to avoid exploitation.)

2. First round:
   - If t == 1: play C. (Open with cooperation.)

3. Compute cooperative behaviour among others:
   - Let w = min(5, t−1).
   - Over the most recent w rounds, compute others_coop_count = total number of C actions by other players (exclude your own action) across those w rounds.
   - others_coop_rate = others_coop_count / (w * (n − 1)). This is the fraction of other-player actions that were C in the window.

4. If rounds_left_to_punish > 0:
   - Play D this round.
   - Decrement rounds_left_to_punish by 1.
   - End.

5. Compare observed cooperation to threshold:
   - If others_coop_rate >= target_coop − eps:
       - Play C (cooperate). (This includes generous forgiveness when others are only slightly below target.)
   - Else (others_coop_rate < target_coop − eps):
       - Initiate punishment: compute deficit = target_coop − others_coop_rate.
         punishment_length = min(max_punish, max(1, round(deficit * S))).
         rounds_left_to_punish = min(punishment_length, r − t)  (do not overrun end of game).
       - Play D this round.
       - End.

Notes and rationale (detailed)
- Starting cooperative: a cooperative opening can trigger mutual cooperation if many others are conditional cooperators.
- Using a short window (w up to 5) makes the strategy responsive to recent behaviour without overreacting to an isolated deviation.
- The target_coop scales with MPCR: if k/n is small (cooperation yields little private return), require a high observed cooperation rate to risk cooperating; if k/n is close to 1, be more trusting because cooperation is individually less costly.
- Punishment is proportional and limited: punishment_length scales with how far observed cooperation is below the target but is capped (max_punish). This deters sustained exploitation but is forgiving so cooperation can re-form quickly.
- Forgiveness tolerance eps prevents tiny stochastic or occasional mistakes from triggering punishment.
- Defecting in the last round is standard for finitely-repeated games because there is no future to enforce reciprocity; it prevents being exploited in the endgame.
- The strategy is purely history-dependent (uses only observed past actions), requires only game parameters and history, and is deterministic (easily implementable).

Behavioral summary against archetypes
- Against persistent cooperators: ACCP-P cooperates and gains high group payoff.
- Against unconditional defectors: ACCP-P quickly switches to defect to avoid continuous exploitation.
- Against conditional cooperators / tit-for-tat-like players: ACCP-P will coordinate on cooperation because it rewards high recent cooperation.
- Against noisy or mixed strategies: ACCP-P tolerates occasional defections (eps) and limits punishment, allowing recovery.
- Against exploiters who defect a lot but sometimes cooperate: punishments are proportional and limited, reducing long-run exploitation while allowing restoration of cooperation if behavior improves.

Implementation notes for tournament
- All parameters (S, max_punish, eps, window cap) are chosen to balance responsiveness and forgiveness; they can be tuned if you want a more aggressive (shorter forgiveness, larger punishments) or more generous (larger eps, smaller punishments) variant.
- Ensure the history excludes your own actions when computing others_coop_rate.
- rounds_left_to_punish is persistent across rounds so multi-round punishments proceed deterministically.

Example (n=6, r=10, k=2):
- MPCR = 0.333, target_coop = clamp(0.5 + 0.4*(1−0.333)=0.5+0.267=0.767, => 0.767).
- If in the last 5 rounds others cooperated on average 80% of the time → others_coop_rate=0.8 ≥ target → keep cooperating.
- If others_coop_rate=0.4 → deficit ≈ 0.367 → punishment_length ≈ round(0.367*5)=2 (play D for 2 rounds, then reassess).

This strategy is cooperative by design (it seeks mutual cooperation), adaptive to observed behaviour, and robust because it punishes proportionately and forgives quickly so groups can re-establish cooperation.
'''

description_COOPERATIVE_69 = '''
Strategy name: Generous Group TFT with Adaptive Punishment (GGTFT-AP)

High-level idea
- Start cooperatively and try to sustain wide-group cooperation by reciprocating group-level behavior.
- Tolerate a small number of defectors (noise or occasional selfishness), punish sustained or large-scale defections in a graded way, and forgive quickly when the group returns to cooperating.
- Protect yourself in the final few rounds (where retaliation has little effect) by switching to a cautious myopic rule.
- All decisions use only the game parameters (n, r, k) and observed history (who cooperated in past rounds).

Rationale
- Because one-shot defection strictly dominates cooperation (k/n < 1 given 1 < k < n), sustaining cooperation requires credible conditional cooperation and calibrated punishment.
- The strategy balances: (a) willingness to cooperate when the group is cooperating, (b) forgiveness to recover from noise, (c) proportional punishment to deter exploitation, and (d) caution near the end of the finite horizon.

Parameters (computed from game inputs)
- window w = min(5, r) — number of recent rounds used to smooth noise.
- forgiveness_tolerance f = max(1, round(0.2 * n)) — number of defectors tolerated in a round without immediate punishment.
- base_punishment P0 = 2 — minimum punishment length in rounds.
- max_punishment Pmax = max(1, min(ceil(r/10), r)) — upper bound on punishment length (scales with horizon).
- endgame_guard g = min(3, max(1, floor(r/10))) — final rounds where play becomes cautious.

These are simple, deterministic formulas so the strategy depends only on (n, r) and the history.

State variables the player maintains
- punishment_counter (integer, initially 0) — rounds remaining in an active punishment phase.
- last_action (C or D; initially C).

Decision rules (natural-language)
1. First round:
   - Cooperate.

2. If currently in a punishment phase (punishment_counter > 0):
   - Play D (defect).
   - Decrement punishment_counter by 1.
   - After punishment ends, check group behavior in the most recent round(s) — if the group has returned to cooperation (see forgiveness rule below), resume cooperating.

3. Endgame (if current round t > r - g):
   - Play cautiously because retaliation has little remaining power:
     - Cooperate only if in the previous round at least (n - 1) players cooperated (i.e., everyone except possibly you cooperated).
     - Otherwise defect for the rest of the game (no further punishment phases needed because game ends soon).

4. Normal rounds (not in punishment phase and not in endgame):
   - Let m_prev = number of cooperators observed in the previous round (including yourself if you cooperated).
   - If m_prev >= n - f (the number of defectors in last round ≤ f - 1):
       - Play C (cooperate). This is “generous”: tolerate a small number of defectors, keep cooperation to stabilize the group.
   - Else (many defectors last round: m_prev < n - f):
       - Enter a graded punishment:
         - Compute severity = n - m_prev (how many defectors there were last round).
         - Set punishment_counter = min(Pmax, P0 + ceil(severity / 2)).
         - Play D this round and continue defecting while punishment_counter > 0.
       - After the punishment phase, if the group shows recovery (next observed m_prev >= n - f) then forgive immediately and resume cooperating.

5. Forgiveness / reset:
   - The strategy is forgiving: after a punishment phase, if observed group cooperation meets the tolerance threshold (m_prev >= n - f) then reset punishment_counter to 0 and resume cooperating. If not, continue punishments as triggered by rule 4 whenever applicable.

Pseudocode

Initialize:
  punishment_counter = 0
  last_action = C
  set w, f, P0, Pmax, g as above

For each round t = 1..r:
  observe history up to round t-1; let m_prev = number of cooperators in round t-1 (if t=1 treat m_prev = n)
  if t == 1:
    action = C
    last_action = C
    continue to next round
  if punishment_counter > 0:
    action = D
    punishment_counter = punishment_counter - 1
    last_action = D
    continue
  if t > r - g:   # endgame window
    if m_prev >= n - 1:
      action = C
    else:
      action = D
    last_action = action
    continue
  # Normal round
  if m_prev >= n - f:
    action = C
  else:
    severity = n - m_prev
    punishment_counter = min(Pmax, P0 + ceil(severity / 2))
    # we defect this round and will keep defecting while punishment_counter > 0
    action = D
  last_action = action

Notes and choices explained
- Why start with C? Starting cooperative gives a chance to establish mutual cooperation quickly and is robust in tournaments where many strategies begin with cooperation.
- Why tolerate f defectors? In N-player settings single defections could be noise or a single selfish player; tolerating a small number avoids needless cycles of retaliation. The 20% rule is simple and scales with group size.
- Why graded punishment? Punishing proportionally to the number of defectors discourages large-scale exploitation while avoiding persistent mutual defection when defections are minor.
- Why forgiveness? Quick recovery prevents long wars of retaliation and allows cooperation to re-emerge, which produces higher total payoffs when opponents respond to punishment.
- Why endgame caution? In the last few rounds punishment/counter-punishment has limited impact; being cautious reduces the chance of being exploited in a finite-horizon environment.
- Parameter choices (w, f, P0, Pmax, g) are conservative defaults that can be tuned if necessary; they are deterministic functions of (n, r) so they adhere to the requirement of depending only on parameters and history.

Behavioral examples (intuition)
- If nearly everyone cooperates every round, the strategy will keep cooperating.
- If a single player defects once (occasional mistake), the strategy will usually tolerate it (if defectors ≤ f) and keep cooperating.
- If many players defect in a round, the strategy defects for a small number of rounds proportional to how many defected, signaling punishment; if cooperation returns afterward, it forgives and resumes cooperation.
- Near the end of the game the strategy only cooperates if the group is effectively unanimous, guarding against last-round exploitation.

Why this is robust and cooperative
- Robust: tolerates noise, punishes clear exploitation, scales punishment with group-level severity, and protects in the endgame. It does not rely on explicit agreements or on expecting particular patterns from others.
- Cooperative: biased toward re-establishing wide-group cooperation whenever the observed group behavior warrants it; seeks mutually higher payoffs rather than permanent retaliation.

Implementation note
- The algorithm only needs to track the number of cooperators each round and a small internal punishment counter. It is straightforward to implement and deterministic given (n, r, k) and observed history.
'''

description_COOPERATIVE_70 = '''
Name: Adaptive Conditional Cooperator with Limited Punishment (ACCLP)

Short description
- Start by signalling cooperation. Sustain cooperation when others show substantial cooperation. If many opponents defect, respond with a short, proportional punishment (temporary defection) that is enough to deter exploitation but limited so cooperation can be restored. Always defect in the final round (no future to enforce cooperation). The strategy adapts how lenient/forgiving it is based on game parameters (n, k, r) and recent history.

Intuition / design goals
- Cooperative: try to sustain high total contributions when opponents reciprocate.
- Robust: do not allow persistent exploitation — punish defectors to create future costs.
- Forgiving: punishments are short and cancelable if others return to cooperation.
- Adaptive: take account of recent history (not just last round) and adjust sensitivity using k and n (when k is large, public good is more valuable; be more forgiving).
- Endgame-aware: no costly cooperation in the final round where punishments cannot be enforced.

Notation used below
- n, k, r: game parameters.
- t: current round index (1..r).
- history: list of past rounds; each round record includes the total number of cooperators C_tau and your action a_tau ∈ {C,D}.
- remaining = r - t + 1 (rounds remaining including current).
- m = min(3, t-1) (memory window size; use up to 3 most recent rounds if available).
- For a past round τ, opponents_coop_frac_τ = (C_τ - indicator(that you cooperated in τ)) / (n-1).
- s_recent = average of opponents_coop_frac_τ over last m rounds (if m=0, treat s_recent = 1 as optimistic initial default).
- gamma = (k - 1) / (n - 1) ∈ (0,1) — scaled public-good strength; higher gamma -> the public good is more valuable relative to private payoff.
- coop_threshold = max(0.1, 0.5 - 0.25 * gamma). This is the fraction of opponents cooperating (recent average) that we require to keep cooperating. It falls between 0.1 and 0.5; when k is large (gamma closer to 1) the threshold is lower (we are more willing to cooperate).
- P_max = min(3, max(1, r-2)) — maximum punishment length (never longer than 3 rounds; also never longer than available rounds for punishment).
- punishment_counter: a nonnegative integer state that records how many rounds of punishment remain. Initially 0.

High-level decision rules (natural language)
1. Round 1: Cooperate (signal willingness to build the public good).
2. Last round (t == r): Defect (no future to enforce cooperation).
3. If currently in punishment (punishment_counter > 0): Defect this round, decrement punishment_counter by 1. However, if the most recent round(s) show clear return to cooperation (s_recent >= coop_threshold), cancel remaining punishment early (set punishment_counter = 0) and cooperate next round.
4. Otherwise (not in punishment, not first or last round):
   - Compute s_recent (average fraction of opponents who cooperated in recent history).
   - If s_recent >= coop_threshold: Cooperate.
   - If s_recent < coop_threshold: Start a short punishment:
       - Set punishment_length = min(P_max, 1 + floor( (coop_threshold - s_recent) / max(0.01, coop_threshold) * P_max ))
         (this makes punishment length proportional to how far below the threshold the recent cooperation is; at least 1 round).
       - Set punishment_counter = punishment_length - 1 (we defect this round and will defect the next punishment_counter rounds), and defect this round.
5. Ties / borderline cases: if s_recent exactly equals coop_threshold, cooperate (be slightly generous).
6. Recovery: after punishment ends, require s_recent >= coop_threshold in at least one of the next m rounds to resume long-term cooperation; if defecting persists, punishment will be re-imposed.

Pseudocode

Initialize:
  punishment_counter = 0

At start of round t:
  remaining = r - t + 1
  if t == 1:
    action = C
    record and continue
  else if remaining == 1:  # last round
    action = D
    record and continue

  # compute memory and recent cooperation fraction
  m = min(3, t-1)
  if m == 0:
    s_recent = 1.0
  else:
    sum_frac = 0
    for τ in (t-m) .. (t-1):
      opp_frac = (C_τ - I_you_cooperated_in_τ) / (n-1)
      sum_frac += opp_frac
    s_recent = sum_frac / m

  gamma = (k - 1) / (n - 1)
  coop_threshold = max(0.1, 0.5 - 0.25 * gamma)
  P_max = min(3, max(1, r-2))

  if punishment_counter > 0:
    # allow early forgiveness if opponents returned to cooperating
    if s_recent >= coop_threshold:
      punishment_counter = 0
      action = C
    else:
      action = D
      punishment_counter = punishment_counter - 1
    record and continue

  # not currently punishing
  if s_recent >= coop_threshold:
    action = C
  else:
    # start a short, proportional punishment
    delta = coop_threshold - s_recent
    # scale punishment to delta; ensure at least 1
    raw_len = 1 + floor( (delta / max(0.01, coop_threshold)) * P_max )
    punishment_length = min(P_max, raw_len)
    punishment_counter = punishment_length - 1  # we defect now + punishment_counter more rounds
    action = D

  record and continue

Edge cases and small-r rules
- r = 2: Cooperate in round 1, defect round 2 (last round).
- If P_max would be 0 because r small, ensure P_max >= 1 so we always allow at least a single-round penalty when needed.
- If history is empty (t = 1) we cooperate.
- If some opponents never cooperate but are stubborn, the strategy will settle into a pattern of defecting while still forgiving occasionally; this limits exploitation while allowing recovery opportunities.

Why this is cooperative and robust
- Cooperates by default and keeps cooperating whenever recent opponent behavior shows a majority-like cooperation (according to coop_threshold). This directly supports mutually beneficial full-cooperation outcomes when others are willing.
- Punishments are short and proportional to the observed shortfall of cooperation. Short, limited punishments deter exploitation but avoid permanently destroying cooperation and avoid getting trapped in long vendettas.
- Forgiveness (cancelling punishment early when opponents return to cooperating) helps re-establish cooperation quickly and is robust to isolated mistakes or single-round deviations.
- Endgame defection avoids being exploited in the final round where future punishments are impossible.
- Adaptation to k: coop_threshold reduces when k is larger (public good is more valuable), making the strategy more willing to tolerate a modest number of defectors and sustain cooperation when the social return is high.
- Memory of up to 3 rounds gives resistance to noise-like behavior (alternating or accidental defections) while still reacting responsively to persistent defection.

Implementation notes & tuning knobs
- The constants (memory m=3, coop_threshold baseline 0.5, P_max=3, forgiveness scaling) were chosen for robustness in a wide variety of opponent mixes. A tournament implementer can tune them if desired.
- If you expect noisy environments (random mistakes), increase m and increase forgiveness (raise P_max or lower coop_threshold). If you expect purely rational exploiters, decrease coop_threshold or increase punishment length but risk less restoration of cooperation.
- The design deliberately avoids “grim” (permanent) punishments and avoids unconditional cooperation — both of which are poor in mixed tournaments.

Summary: ACCLP starts cooperative, cooperates while recent opponent cooperation is above a threshold that is relaxed when the public-good return k is high, punishes proportional to the observed shortfall but only for a short limited time, forgives early when opponents return to cooperating, and defects in the final round. This yields a balance between promoting group payoffs and avoiding exploitation in a tournament of unknown strategies.
'''

description_COOPERATIVE_71 = '''
Name: Generous Conditional Cooperator with Finite Punishment (GCC‑FP)

Summary (one line)
- Start cooperative, keep cooperating while the group’s recent cooperation is high; if a meaningful drop or a repeat free‑rider appears, punish by defecting for a short, fixed number of rounds, then forgive and resume cooperation. Always defect in the final round.

Rationale
- Cooperation is socially efficient (total payoff up when people contribute) but individually costly each round. Repeated play allows use of reciprocity and short, credible punishments to sustain cooperation. To be robust we (a) use simple group-level signals so we do not assume special coordination, (b) punish only when deviations are significant or persistent so we are not exploited by noise, and (c) use finite (forgiving) punishments so we avoid permanent war and can recover cooperation.

Parameters (derived from game parameters)
- n: number of players (given)
- r: rounds (given)
- k: multiplication factor (given)

Internal strategy parameters (computed from n, r, k)
- punishment_length P = max(1, min(4, floor(r/10)))  
  - Short fixed punishment; scaled so small tournaments still punish at least one round, and large tournaments have modest punishments.
- memory_window L = min(5, max(1, floor(r/5)))  
  - Number of recent rounds used to form an expectation of cooperation.
- generosity_threshold q (fraction) = clamp(0.3, 0.5 - 0.2 * ((k - 1)/(n - 1)), 0.7)  
  - If k is high relative to 1, cooperation is more valuable; become slightly more tolerant. Clamp between 0.3 and 0.7.
- significant_drop d = max(1, ceil(0.15 * n))  
  - A drop in number of cooperators this round compared with recent average that is treated as meaningful.

State variables maintained by the strategy
- round t (current round)
- punish_timer (integer ≥ 0): rounds remaining in active global punishment (initial 0)
- recent_cooperators[]: circular buffer of length up to L storing the number of cooperators observed in each of last L rounds (excluding current round)
- freq_j (for each other player j): exponential moving average of j’s cooperation (initially 1.0, i.e., optimistic); update weight w = 0.5

Top-level decision rule (natural-language + pseudocode)
- First round (t = 1): cooperate.
- Last round (t = r): defect (no future to enforce cooperation).
- Otherwise (1 < t < r):
  1. Update per-player frequencies freq_j using last round’s observed actions:
     - freq_j ← w * freq_j + (1 - w) * c_j_last  (c_j_last = 1 if j cooperated last round, else 0)
  2. Update recent_cooperators buffer with the count m_prev of cooperators observed in last round (including or excluding yourself is fine; define consistently – I use including all players).
  3. Compute:
     - m_prev = number of cooperators observed last round (0..n)
     - m_avg = mean(recent_cooperators) over available entries (if none, treat m_avg = n-1 (optimistic)).
     - group_freq = average_j freq_j (average estimated cooperation probability among others).
  4. Trigger detection:
     - Persistent free‑rider condition: any player j with freq_j < 0.2 and who defected last round → mark as a persistent defector.
     - Significant group drop: (m_prev < m_avg - d) OR (m_prev < ceil(q * n)).
     - If persistent defector exists OR significant group drop → set punish_timer ← P (start punishment now).
  5. Action:
     - If punish_timer > 0 → defect this round; punish_timer ← punish_timer - 1.
     - Else if group_freq ≥ q (the group looks cooperative) → cooperate.
     - Else if m_prev ≥ ceil(q * n) → cooperate (generous tie-break: if exactly at threshold, cooperate).
     - Else → defect and set punish_timer ← P - 1 (start punishment; if P=1 this defects only this round).

Pseudocode (compact)
- Initialize freq_j = 1.0 for all j ≠ me; recent_cooperators empty; punish_timer = 0
- For t in 1..r:
  - if t == 1: play C; continue
  - if t == r: play D; continue
  - Update freq_j with last round actions (freq_j = w*freq_j + (1-w)*c_j_last)
  - Append m_prev to recent_cooperators (keep last L entries); compute m_avg
  - group_freq = average(freq_j)
  - persistent_defector = exists j with freq_j < 0.2 and c_j_last == 0
  - significant_drop = (m_prev < m_avg - d) or (m_prev < ceil(q*n))
  - if persistent_defector or significant_drop: punish_timer = P
  - if punish_timer > 0:
      play D
      punish_timer -= 1
    else:
      if group_freq >= q or m_prev >= ceil(q*n):
        play C
      else:
        play D
        punish_timer = P - 1  (start punish immediately)

Behavioral notes and tuning rationale
- Generosity: If in doubt (ties at the threshold), we cooperate. Generosity helps establish cooperation early and recovers faster after isolated mistakes.
- Forgiving finite punishment: Punish for P rounds (typically 1–4), then resume cooperation. This avoids permanent collapse while still imposing a short, credible cost on defectors.
- Localized detection of persistent free‑riders: If one player consistently defects while others cooperate, we detect them via freq_j and will start punishing quickly; this discourages exploiters who attempt to free-ride repeatedly.
- Use of both short-run (m_prev) and recent average (m_avg) avoids overreacting to single noisy rounds while still responding to real shifts in behavior.
- Last-round defection: Because no future retaliations are possible, defecting in the final round avoids being exploited in the last stage. (This follows standard game-theoretic logic for finite known horizon.)
- Parameter choices are conservative and intentionally modest so the strategy is robust across many tournaments. Implementers may tune P, L, q, d to the expected environment (e.g., larger P if opponents react strongly to punishment).

Examples (expected responses)
- All others start cooperating and keep cooperating → group_freq stays high ≥ q → we always cooperate (high group payoff).
- One opponent defects once (an error) while the rest cooperate → small one‑off drop: if m_prev is not below m_avg - d and group_freq stays high we forgive and continue cooperating (no punishment).
- Several players defect together or cooperation drops significantly → significant_drop triggers a P-round global punishment (we defect P rounds) to reduce defectors’ payoff and signal we will not tolerate sustained drops; afterwards we attempt to resume cooperation.
- A single player consistently defects every round → freq_j falls below 0.2; persistent_defector condition triggers punishments; after repeated punishments the defector’s payoff is lowered and other cooperators may choose to match punishment, encouraging return to cooperation or isolating the defector.

Edge cases
- r ≤ P: P is clamped so punishments still work; but last round is always defect so effective cooperation will be only in earlier rounds.
- Very small n (n=2): Strategy reduces to forgiving tit‑for‑tat style behavior because freq_j tracks the single opponent.
- Noisy environments: The combination of m_avg, the drop threshold d, and exponential smoothing of freq_j makes the policy tolerant to isolated mistakes while still responsive to persistent defections.
- Determinism: The above description is deterministic. If implementers prefer stochastic behavior for extra robustness (e.g., to avoid synchronized punishments), they can replace deterministic cooperate/defect in marginal cases with cooperating with probability p = group_freq (or add small randomness when near thresholds).

Why this is cooperative and robust
- Cooperative: The strategy prefers cooperation when group evidence supports it; it is generous on ties and does not punish for one-off slips. This helps form and sustain high-cooperation equilibria.
- Robust: It detects persistent defectors and responds with short, predictable punishments—enough to make defection costly but not so long that the group is locked into mutual defection. The use of per-player frequencies and group statistics makes the strategy resilient to many opponent types (always-defectors, conditional cooperators, noisy players, random players).

Implementation notes
- Use consistent conventions: define whether recent_cooperators includes yourself; in the above text I included all players when counting cooperators observed (that is simplest).
- w (smoothing weight) suggested = 0.5; adjust down for slower adaptation or up for quicker reaction.
- Ensure punish_timer and freq_j persist across rounds.

This strategy is intentionally simple to implement, interpretable, and tuned to promote cooperation while avoiding exploitation. It should perform well in mixed tournaments: it forms cooperation with similar cooperators, punishes exploiters, forgives noise, and avoids permanent collapse.
'''

description_COOPERATIVE_72 = '''
Name: Forgiving Conditional Majority (FCM)

Summary (one line)
- Start cooperative, keep cooperating when a clear majority cooperates, punish short and proportionally when defections are substantial, forgive occasional lapses, and always defect in the final round.

Rationale
- The public-goods stage payoff makes one-shot defection individually attractive, so cooperation must be sustained by reciprocity across rounds. FCM signals cooperative intent by starting with C, then conditions cooperation on observed group behaviour (majority rule) so it is robust to diverse opponents. Punishments are finite and short to limit loss to the group while still making exploitation costly; a forgiveness rule allows recovery from mistakes or noise. The last round is treated as a one-shot (defect) to avoid being exploited in the endgame.

Parameters computed from game inputs (deterministic, no external tuning)
- majority_threshold = ceil(n/2)      // require at least a majority of players to have cooperated to trust cooperation
- small_defection_cut = floor(n/4)    // very small number of cooperators (many defectors) -> clear breakdown
- lookback L = min(3, t-1)            // how many past rounds to smooth short-term noise (used when t>1)
- max_punish = 3                      // maximum punishment length (in rounds)
- forgiveness_rate = 0.8              // if a player’s personal cooperation rate ≥ this, treat occasional defection as likely noise

State to maintain
- history H of all past rounds (who played C/D each round)
- punishment_timer (integer, initially 0); when >0 the strategy defects and decrements timer each round
- r_remaining = r - (t-1) at start of round t

Decision rules (natural language)
1. First round (t = 1): Cooperate (C). This signals cooperative intent and creates opportunity for mutual cooperation.

2. Final round (t = r): Defect (D). With common knowledge of the horizon, last-round cooperation is exploitable; defecting prevents endgame exploitation.

3. If punishment_timer > 0: play D in this round and decrement punishment_timer by 1. (This implements a short, finite punishment phase.)

4. Otherwise (general rounds 1 < t < r), compute:
   - coop_last = number of players who cooperated in round t-1 (from history H).
   - For each other player j, compute coop_rate_j = (# rounds j cooperated in history) / (t-1).
   - effective_coop_last = coop_last adjusted for forgiving rare lapses:
       - For every player j who defected in t-1 but coop_rate_j >= forgiveness_rate, ignore that player's defection when counting effective_coop_last (treat it as if they cooperated). (This prevents punishing when mostly-cooperative players make occasional mistakes.)
   - smoothed_group_coop = average (over the last L rounds) of coop_counts (each round’s number of cooperators, after applying the same forgiveness adjustment per round).

   Apply rules:
   a) If coop_last (after forgiveness adjustment) ≥ majority_threshold: play C. (Group shows clear recent majority cooperation → reciprocate.)
   b) Else if coop_last ≤ small_defection_cut: play D and set punishment_timer = min(max_punish, floor(r_remaining/3)). (Very low cooperation → trigger punishment for a limited period scaled to remaining time.)
   c) Else (intermediate case): if smoothed_group_coop ≥ majority_threshold, play C (transient dips forgiven); otherwise play D and set punishment_timer = min(max_punish, 1 + floor(r_remaining/4)). (Moderate punishment for ambiguous situations; allow recovery if group returns to majority cooperation.)

Notes on the forgiveness adjustment
- Forgiveness is targeted: if a player has cooperated frequently in past rounds (coop_rate_j ≥ forgiveness_rate), an isolated defection by them in the most recent round is treated as likely a mistake and does not count against the group cooperation measure. This reduces cycles of mutual punishment triggered by accidental defections and increases robustness to noisy opponents.

Pseudocode (concise)
- Initialize: punishment_timer = 0
- For each round t = 1..r:
  - r_remaining = r - (t-1)
  - If t == 1: play C; continue
  - If t == r: play D; continue
  - If punishment_timer > 0: play D; punishment_timer -= 1; continue
  - Compute coop_count_last (number of C in round t-1)
  - For each player j compute coop_rate_j over rounds 1..t-1
  - Adjust cooperators in last round by treating defectors with coop_rate_j >= forgiveness_rate as cooperators → effective_coop_last
  - Compute smoothed_group_coop = average of adjusted coop_counts over last L rounds
  - If effective_coop_last >= majority_threshold: play C
  - Else if effective_coop_last <= small_defection_cut:
      play D
      punishment_timer = min(max_punish, max(1, floor(r_remaining/3)))
  - Else:
      If smoothed_group_coop >= majority_threshold: play C
      Else:
        play D
        punishment_timer = min(max_punish, max(1, 1 + floor(r_remaining/4)))

Why this is cooperative and robust
- Cooperative: starts by cooperating and only defects in response to substantial or persistent defection by the group. When a majority cooperates, FCM reciprocates, enabling mutually beneficial all-C outcomes.
- Robust: majority-rule and short smoothing windows let the strategy adapt quickly to changes while filtering occasional noise. Targeted forgiveness protects against accidental defections and reduces destructive retaliation loops.
- Deterrent: finite punishments are long enough to impose a cost on persistent defectors but short enough to limit mutual damage and allow recovery.
- Endgame-awareness: defects in the final round to avoid last-round exploitation.

Edge cases
- Very small r (e.g., r = 2): t=1 -> C, t=2 -> D (this follows the general first/last rules).
- Very small n (n = 2): majority_threshold = 1 (cooperate if the other cooperated last round), which reduces to a conventional two-player conditional cooperator with forgiveness.
- All opponents always defect: FCM will quickly switch to defection and impose short punishments (which have little effect against pure defectors) — this avoids wasting payoff on being exploited.
- A single occasional deviator among many cooperators: forgiveness prevents long punishments if that player is otherwise cooperative.

Implementation notes
- All computations rely only on known parameters (n, r, k) and observable history (which players cooperated each past round).
- The constants (lookback L = 3, max_punish = 3, forgiveness_rate = 0.8, threshold fractions) are chosen to balance deterrence and forgiveness; they can be adjusted if tournament data suggests different noise levels or opponent behaviours. The algorithm remains parameter-free in the sense that it derives thresholds from n and r and uses only a few small fixed integers for punishment/forgiveness control.

Final comment
- FCM is a pragmatic, easily implemented conditional cooperation strategy that favors mutual cooperation with a majority, tolerates occasional lapses, and uses short, proportional punishments to discourage exploitation. It is adaptive to history, robust across opponents, and simple to implement for tournament play.
'''

description_COOPERATIVE_73 = '''
Strategy name: Adaptive Generous Conditional Cooperator (AGCC)

High-level goal
- Signal cooperation, sustain high mutual cooperation when a substantial fraction of players reciprocate, punish deviations in a proportional and targeted way, and forgive quickly so cooperation can be re-established. Do not cooperate in the final round (no future to enforce reciprocity).

Design principles
- Start cooperative to signal willingness.
- Use simple aggregate statistics (number of cooperators last rounds and per-player recent cooperation rates) to detect whether the group environment is reciprocal.
- If reciprocity is strong, cooperate; if reciprocity collapses, punish for a short, bounded time proportional to the game length.
- Punish proportionally and target persistent defectors when possible.
- Forgive rapidly after punishment or after evidence of renewed cooperation to avoid long mutual breakdowns.
- Always defect in the last round.

Parameters derived from game inputs (computed once at start)
- n (players), r (rounds), k (multiplier) are given.
- Memory window W = min(5, r - 1). (We look back up to 5 past rounds; if r is small, look back fewer.)
- Target fraction alpha = max(0.5, k / n). Rationale: require at least half the group to show cooperation, except when k/n > 0.5 (when the value of public good is high relative to private return) raise the bar to k/n.
- Target count g = ceil(alpha * n). (If last-round cooperators ≥ g, treat the environment as “cooperative”.)
- Punishment length P = max(1, min(3, floor(r / 10))). (Short, bounded punishments: 1 for short games, up to 3 for long games.)
- Forgiveness requirement S = 2 consecutive “good” rounds (cooperators ≥ g) before returning from punishment to sustained cooperation.
- Persistent-defector window and threshold: detect a persistent defector if in the last W rounds some player j defected in at least tau = ceil(0.6 * W) rounds while the group (excluding j) was cooperative in those rounds. (This identifies free-riders who defect even while others are cooperating.)

State maintained across rounds
- phase ∈ {COOP, PUNISH, TARGETED_PUNISH}
- if in PUNISH: punishment_rounds_remaining (integer)
- if in TARGETED_PUNISH: target set T of suspected persistent defectors (subset of players)
- recent history of actions (who cooperated each round) for last W rounds (available by game rules)
- consecutive_good_rounds counter (counts consecutive rounds with cooperators ≥ g)

Action rules (for round t, 1-based index)
1) Final round
- If t == r: play D (defect). No future to enforce cooperation.

2) First round
- If t == 1: play C (cooperate) and initialize phase = COOP, consecutive_good_rounds = 0.

3) For intermediate rounds 1 < t < r
- Compute m_last = number of cooperators in round t - 1 (including yourself).
- Update per-player cooperation counts over last W rounds and check for persistent defectors as defined above.
- If a persistent defector set T is non-empty, set/maintain TARGETED_PUNISH phase with T.

Phase handling:
A. If phase == COOP:
   - If m_last ≥ g:
       - Play C (cooperate).
       - consecutive_good_rounds = consecutive_good_rounds + 1 (cap at S).
       - If consecutive_good_rounds ≥ S: remain in COOP.
   - Else (m_last < g):
       - Enter punishment: set phase = PUNISH, punishment_rounds_remaining = P, consecutive_good_rounds = 0.
       - Play D (the first punishment round is immediate).
B. If phase == PUNISH:
   - Play D.
   - Decrease punishment_rounds_remaining by 1.
   - If punishment_rounds_remaining == 0:
       - Move to a probation check: require S consecutive "good" rounds to re-enter COOP.
       - Set phase = COOP but set consecutive_good_rounds = 0 (we will only cooperate if we see S good rounds in a row).
C. If phase == TARGETED_PUNISH:
   - If the set T of persistent defectors is non-empty:
       - If you can identify defectors individually (you can observe identities), you enact targeted punishment by defecting until each j ∈ T improves their recent cooperation count above a threshold (for example, cooperated at least ceil(0.4 * W) of last W rounds); in practice you:
           - Play D every round while T ≠ ∅ (targeted punishment).
           - Update T each round: remove any player whose recent cooperation count rises above the recovery threshold.
       - Once T == ∅, set phase = PUNISH with punishment_rounds_remaining = P to re-establish mutual discipline before returning to COOP.

Generosity / probabilistic forgiveness (optional to avoid lock-in)
- If you prefer an element of generosity (helps against random noise and “trembling hands”), implement: when m_last == g - 1 and you personally cooperated in t - 1, play C with probability p = 0.7 (otherwise D). This lets the system recover from small one-off shortfalls without full punishment.

Pseudocode (compact)
(assume history structure actions[t][i] where i indexes players)
Initialize:
  W = min(5, r-1)
  alpha = max(0.5, k/n)
  g = ceil(alpha * n)
  P = max(1, min(3, floor(r/10)))
  S = 2
  phase = COOP
  punishment_rounds_remaining = 0
  consecutive_good_rounds = 0

For each round t:
  if t == r: action = D; continue
  if t == 1: action = C; continue

  compute m_last = count_cooperators(t-1)
  update per-player cooperation counts over last W rounds
  detect persistent defector set T (players with cooperation_count ≤ W - tau under cooperative contexts)

  if T non-empty and phase != TARGETED_PUNISH:
    phase = TARGETED_PUNISH

  if phase == COOP:
    if m_last >= g:
      action = C
      consecutive_good_rounds += 1
      if consecutive_good_rounds >= S: consecutive_good_rounds = S
    else:
      phase = PUNISH
      punishment_rounds_remaining = P
      consecutive_good_rounds = 0
      action = D

  else if phase == PUNISH:
    action = D
    punishment_rounds_remaining -= 1
    if punishment_rounds_remaining <= 0:
      phase = COOP
      consecutive_good_rounds = 0   // require S consecutive good rounds to resume full trust

  else if phase == TARGETED_PUNISH:
    if T is empty:
      phase = PUNISH
      punishment_rounds_remaining = P
      action = D
    else:
      action = D
      // Update T next round based on latest W-window cooperation

Notes and rationale
- Start cooperatively to show willingness and to test whether others reciprocate.
- The target threshold g uses k/n (the marginal per-player return of each contributed unit) as a guide but enforces at least a majority. This balances two facts: a cooperative public good is easier to sustain when k is larger, but relying on unanimity is fragile. Requiring a majority avoids being exploited by a small minority of defectors.
- Short bounded punishments (P small) avoid long destructive downturns; they are long enough to change incentives in repeated environments but short enough to be forgiving.
- Targeted punishment tries to identify persistent free-riders and focuses retaliation on rounds where their behavior is abnormal (they defect while others cooperate) instead of permanently collapsing into universal defection.
- Forgiveness (probabilistic cooperation when near the threshold) prevents chains of mutual retaliation for minor slips or random defections.
- Always defect in the final round (standard last-round logic).

Edge cases
- Very short games (r = 2 or 3): W will be small; P will be 1; the behavior reduces to “cooperate first round, defect last round.” For r = 2 that is the only sensible cooperative signal possible.
- If the whole population never cooperates, AGCC quickly switches to PUNISH and stays defecting (minimizes exploitation).
- If other players use robust reciprocity, AGCC will lock into sustained cooperation (it begins cooperative, rewards cooperative rounds, forgives small slips, and uses short punishments when exploitation occurs).
- If opponents are random or malicious, AGCC limits losses by shortening punishments and by defecting in non-cooperative environments.

Why this is robust and cooperative
- Robust: reacts to observed behavior, not to any coordination assumption. Uses short punishments and forgiveness to adapt to many opponent types (grim, tit-for-tat, random, exploitative).
- Cooperative: it signals cooperation and returns to cooperation as soon as others reciprocate for S rounds; therefore it supports mutually beneficial sustained cooperation when opponents are conditional cooperators.
- Adaptive: parameters depend on game size (n), public-good multiplier (k) and total rounds (r), so behavior scales sensibly with the game environment.

Implementation notes for tournament
- The pseudocode requires only the history of player actions and current round index — all available by the specification.
- Tune numeric choices (W, P, S, alpha) if you expect particular opponent classes. The provided defaults are conservative and should work well across a wide range of unknown opponent behaviors.
'''

description_COOPERATIVE_74 = '''
High-level idea (summary)
- Try to establish and sustain group cooperation, but avoid long, costly punishments and be forgiving so cooperation can recover after mistakes.  
- Cooperate by default; condition future cooperation on observed cooperation by others in a recent window.  
- Use short, proportional punishments when the group’s cooperation rate falls below a threshold, then forgive.  
- In the known final rounds (“endgame”) stop trying to induce future reciprocity and defect to avoid being exploited.

This strategy depends only on n, r, k and the public history of contributions; it is adaptive to opponents’ behaviour (through measured cooperation rates) and robust to varied opponents (forgiving, short punishments, parameterized by k and n).

Notation and state
- t = current round (1..r).  
- For each past round s < t we observe c_j,s ∈ {0,1} for each player j. Let c_−i,s = Σ_{j≠i} c_j,s (others’ cooperations in round s).  
- history = list of vectors (c_1,s,...,c_n,s) for s = 1..t-1.
- Parameters the strategy uses (fixed rules you will implement):
  - window m = min(10, t−1) (the number of most recent past rounds used to estimate others’ behaviour; if t=1 then m=0).
  - cooperation threshold T_coop = clamp(0.4, 0.6 − 0.2*(k/n), 0.7). (Lower threshold when k/n is larger, because cooperation is more valuable.)
  - punishment length P = 2 (punish by defecting for P rounds if group cooperation drops below threshold).
  - forgiveness probability g = 0.2 (occasional probabilistic forgiveness during/after punishment to test recovery).
  - endgame length L = min(r−1, max(1, ceil((1 − k/n) * 5))). (Shorter endgame when k/n is large; longer endgame if individual contribution has small marginal benefit.)
  - small epsilon eps = 1e−9 for comparisons.

Decision rules (exact)
1. Endgame: If t > r − L then defect. (In the final L rounds the strategy defects; no further attempts to enforce future cooperation.)
2. First round (t = 1): Cooperate. (Signalling cooperative intent.)
3. If t > 1 and not in endgame:
   a. Compute m = min(10, t−1). Let S = sum over the last m rounds of c_−i,s (sum of others’ cooperations in those rounds). Compute O_rate = S / (m*(n−1)) — the fraction of other-cooperation in the recent window. If m = 0 set O_rate = 1 (so first move is cooperate).
   b. If O_rate ≥ T_coop then cooperate. (Group cooperates enough — continue cooperating.)
   c. If O_rate < T_coop then enter punishment mode:
      - Defect for P consecutive rounds (or until the end of the game or endgame begins).
      - During punishment, each round independently with probability g play Cooperate instead of Defect (probabilistic forgiveness / testing).
      - After P rounds of punishment, stop punishment and re-evaluate O_rate over the most recent m rounds; if O_rate ≥ T_coop resume cooperation, else repeat another P-round punishment cycle.
   d. Special-case “immediate tit-for-tat” rule to avoid dangling defections: if last round (t−1) had unanimous cooperation by others (c_−i,t−1 = n−1) then cooperate this round regardless of O_rate. (This gives fast response to sudden recovery.)
4. Tie-breaking:
   - If on the exact round you would both punish and be in endgame (t > r − L), endgame rule overrides: defect.
   - If probabilities (forgiveness) require a random draw use a fresh uniform random sample per round.

Pseudocode (compact)
- Input: n, r, k, history
- Compute t = |history| + 1
- Compute L = min(r−1, max(1, ceil((1 − k/n) * 5)))
- If t > r − L: return D
- If t == 1: return C
- Set m = min(10, t−1)
- If m == 0: O_rate = 1 else O_rate = (sum_{s=t−m}^{t−1} sum_{j≠i} c_j,s) / (m*(n−1))
- T_coop = clamp(0.4, 0.6 − 0.2*(k/n), 0.7)
- If (sum_{j≠i} c_j,t−1 == n−1) return C   // immediate reward for unanimous recent cooperation
- If O_rate ≥ T_coop: return C
- Else: // punishment segment
    - If within an ongoing P-round punishment segment (count rounds since punishment start < P): 
        - With probability g return C else return D
    - Else: // start new punishment segment
        - Begin punishment counter = 1; With probability g return C else return D

Handling edge cases & implementation notes
- t = 1: Cooperate. That signals intent and seeds cooperation.  
- t = r (last round): Always defect (because no future to incentivize reciprocity). The endgame rule already forces defection for last L≥1 rounds.  
- If r is very small (r = 2 or 3): the endgame L will be clipped to r−1, so the strategy will defect in the last round(s) as required; but still cooperate at t=1 to test cooperation.  
- If n is large and k/n is very small (marginal benefit tiny): T_coop becomes higher; the strategy becomes more reluctant to cooperate, which protects against exploitation when a single contribution is almost worthless to the contributor.  
- If opponents produce noisy histories (random mistakes), the short punishment length P and forgiveness probability g allow recovery; the window m smooths transient shocks.  
- If opponents unilaterally defect consistently, the strategy will keep punishing in short cycles rather than inflicting indefinite mutual destruction (so the strategy limits how much it self-harms).

Why this is cooperative and robust
- Cooperative: starts cooperatively and cooperates whenever others have cooperated sufficiently recently. If your play encourages others to cooperate, the strategy reciprocates, producing high group payoffs when others are willing. It rewards unanimous or very strong cooperation quickly (immediate cooperation when the last round showed unanimous cooperation by others).
- Discourages exploitation: if many opponents defect persistently, the strategy defects (punishes) rather than continuing to subsidize defectors. Punishments are short and proportional so you do not inflict or sustain unbounded mutual losses.
- Forgiving and adaptive: forgiveness probability g and windowed rate O_rate allow recovery from errors and noisy opponents; the threshold T_coop depends on k/n, so the strategy accounts for how valuable cooperation is in the given parameterization.
- Endgame-safe: in the known-finiter-horizon game you cannot credibly threaten infinite punishments; the explicit endgame (last L rounds are defect) prevents being exploited late when future leverage is small.

Tunable parameters and variants
- P, m, g, T_coop formula and L are chosen as conservative defaults. In implementation you can tune them (e.g., P=1 or 3, m up to 20) to match tournament conditions (more aggressive enforcement vs more lenient forgiveness).  
- If you expect noisier opponents, increase g and m. If you want stronger deterrence of defectors, increase P and raise T_coop.  
- You can also replace probabilistic forgiveness with an “observational test”: after one punishment round, cooperate once to test whether others return to cooperation; punish again only if exploitation repeats.

Final note
This strategy is simple to implement, uses only public history and parameters (n,r,k), balances cooperation with anti-exploitation measures, includes forgiveness to recover from mistakes, and avoids the doomed attempt to sustain cooperation in the final rounds by stopping cooperation in a k/n-dependent endgame window. It is robust across a wide class of opponent behaviours (cooperators, defectors, random/noisy players, grudgers, generous players).
'''

description_COOPERATIVE_75 = '''
Summary (high level)
- Start by cooperating to signal cooperative intent.
- In each non-terminal round decide based on the fraction of players who cooperated in the immediately preceding round.
- If the previous round was overwhelmingly cooperative, cooperate; if it was badly non‑cooperative, punish by defecting for a short, calibrated number of rounds; then forgive and try to re‑establish cooperation.
- In the final round cooperate only if the group has been perfectly cooperative in all earlier rounds (reward perfect history), otherwise defect (protects against last‑round exploitation).
- Punishment length is computed from the game parameters so a single profitable one‑shot deviation would be made unattractive relative to the expected future loss if everyone enforces the punishment.

Rationale (why this is cooperative and robust)
- Cooperative: the strategy starts cooperatively, returns to cooperation whenever there is evidence the group is cooperating, and rewards perfect cooperation in the last round. That maximizes group payoffs when opponents reciprocate.
- Robust: it punishes non‑cooperation sufficiently to make a one‑shot deviation unattractive (punishment length uses the k and n parameters). Punishments are limited and forgiving so the strategy does not lock into permanent mutual defection in response to occasional or accidental defection.
- Adaptive: decisions use observed history (fraction of cooperators last round and a small memory for punishment state) and adapt punishment severity to the observed level of defection.

Precise decision rules (natural language)
1. Initialization
   - Compute a minimum punishment length P0 := ceil( (1 - k/n) / (k - 1) ). (Because a single defection against otherwise full cooperation yields an immediate gain of 1 - k/n, and cooperation yields future gains of k−1 per cooperative round; P0 is the number of lost cooperative rounds required to offset the immediate gain.)
   - Ensure P0 ≥ 1 (set P0 = 1 if formula gives 0).
   - Maintain state variables:
     - punishing_until (round index) — initially 0 (not punishing).
     - last_round_all_cooperate_flag (true if every previous round had all players cooperate) — initially true.

2. First round (t = 1)
   - Play C (cooperate). This signals cooperative intent.

3. Any round t with 1 < t < r (neither first nor final round)
   - Observe the vector of actions in round t−1. Let f_prev = (number of players who played C in round t−1) / n (fraction cooperating).
   - Update last_round_all_cooperate_flag := last_round_all_cooperate_flag AND (f_prev == 1).
   - If t ≤ punishing_until:
     - Play D (continue punishment).
   - Else (not in an active punishment window):
     - If f_prev == 1:
       - Play C (everyone cooperated last round; reward).
     - Else if f_prev ≥ 0.5:
       - Play C (generous reciprocity: majority cooperated last round, so continue cooperation).
     - Else (f_prev < 0.5):
       - Initiate a calibrated punishment: compute a punishment length P = min(r - t, max(P0, ceil( (1 − f_prev) * (r − t + 1) * 0.25 ))).
         - Intuition: P is at least P0 (the theoretically needed minimal punishment), and scales up with how many players defected (1 − f_prev). The factor 0.25 is a small scale to keep punishments finite and allow forgiveness; implementers can tune it (see notes below).
       - Set punishing_until := t + P − 1.
       - Play D this round (start the punishment).
   - After deciding, if a future round shows sustained cooperation (f_prev ≥ 0.5 for several rounds or f_prev==1), the strategy naturally resumes cooperation after punishing_until is passed.

4. Final round (t = r)
   - If last_round_all_cooperate_flag is true (i.e., in every earlier round all n players cooperated), play C (reward perfect history).
   - Else play D (protect against last‑round exploitation).

Pseudocode
(assume rounds are numbered t = 1..r)

initialize:
  P0 = ceil( (1 - k/n) / (k - 1) )
  if P0 < 1 then P0 = 1
  punishing_until = 0
  last_round_all_cooperate_flag = true

for each round t = 1 .. r:
  if t == 1:
    action = C
  else:
    // observe previous round
    f_prev = (#C in round t-1) / n
    last_round_all_cooperate_flag = last_round_all_cooperate_flag AND (f_prev == 1)

    if t == r:
      if last_round_all_cooperate_flag:
        action = C
      else:
        action = D
    else:
      if t <= punishing_until:
        action = D
      else:
        if f_prev == 1:
          action = C
        else if f_prev >= 0.5:
          action = C
        else:
          // compute punishment length
          remaining_rounds = r - t + 1
          P = max(P0, ceil((1 - f_prev) * remaining_rounds * 0.25))
          P = min(P, r - t)   // do not punish in the last round (use last-round rule instead)
          punishing_until = t + P - 1
          action = D

  play action

Implementation notes and parameter guidance
- The core design choices are (a) P0 (the theoretical minimum punishment length based on n and k) and (b) the generosity threshold (here 0.5) and the scale of punishment growth (the 0.25 factor). These are tunable:
  - P0 must use n and k as given. It enforces that a one-shot deviation against otherwise full cooperation is not profitable if opponents would impose a punishment of length ≥ P0.
  - A generosity threshold of 0.5 is simple and robust: if a majority cooperated last round, try to cooperate. You can increase generosity (e.g., threshold 0.4) to favor cooperation more, or decrease it to protect against exploitation.
  - The scale factor for punishment ramping controls how strongly the strategy responds to many defectors. The chosen 0.25 yields modest punishments beyond P0; increase toward 1 to punish more harshly, or reduce toward 0 to be more forgiving.
- Punishment is finite and followed by forgiveness. This prevents lock‑in to mutual defection and allows re‑establishment of cooperation with adaptive opponents.
- The last‑round rule is conservative but cooperative: only reward perfect prior cooperation with a final‑round cooperation. This avoids surrendering a safe payoff in the final round to opportunistic defectors while still granting the full mutual‑cooperation surplus to a unanimously cooperative history.
- If the tournament environment contains lots of purely naive cooperators who will keep cooperating on the last round even after some defections, you can relax the last‑round rule (e.g., cooperate in the last round if f_prev ≥ 0.8). The conservative rule above protects against exploitation; choose according to how adversarial you expect the field to be.

Why this will perform well in mixed tournaments
- Against cooperators/conditional cooperators it will sustain high cooperation because it starts cooperative, returns to cooperation after punishments, and rewards full cooperation.
- Against exploiters/defectors it limits losses by defecting after observing poor cooperation and punishing for a punishment window that makes single-shot deviation unattractive (P0).
- Against unpredictable strategies it is forgiving enough to re-establish cooperation but not so forgiving that a small number of defectors can repeatedly exploit it.

If you want a stricter (harder) strategy
- Increase the generosity threshold (e.g., require f_prev ≥ 0.7) and increase the punishment scale factor toward 1. That yields stronger deterrence but risks slower re‑establishment of cooperation.

If you want a softer (more cooperative) strategy
- Lower the generosity threshold (e.g., 0.4), reduce the punishment scale factor, and consider cooperating in the last round if the previous round was cooperative (f_prev near 1).

This strategy is fully specified by the game parameters n,k,r and the observed history (past full action vectors); it does not require communication, precommitment, or any external coordination.
'''

description_COOPERATIVE_76 = '''
Name: Adaptive Win‑Stay / Lose‑Shift with Forgiveness and Endgame Safety (AWSLS)

Idea in one sentence
- Start cooperative, keep cooperating while your recent payoff meets an adaptive aspiration level, switch (punish or try to restore cooperation) when your payoff falls below aspiration, forgive automatically after a short recovery, and defect on the known final round to avoid end‑game exploitation.

Why this is appropriate
- Win‑stay/lose‑shift is simple, robust and performs well in many repeated social dilemmas. The adaptive aspiration makes the strategy responsive to the actual environment (so it tolerates occasional mistakes or noisy opponents and does not over‑punish). Explicit forgiveness prevents long grudges that reduce group payoff. Endgame safety (defect on the last round) protects against last‑round exploitation when punishment is impossible.

Parameters computed from game parameters
- n, k, r are given.
- Initial aspiration A0 = (1 + k) / 2 (midway between mutual defection payoff 1 and mutual cooperation payoff k).
  - Rationale: this balances ambition (aim above mutual defection) and realism.
- Aspiration update smoothing λ = 0.9 (exponential smoothing weight).
- Very low‑cooperation cutoff ε = 1/n (to detect rounds with essentially no cooperation).
- (Optional tuning) You can make λ depend on r (e.g., λ = max(0.8, 1 - 1/r)).

Decision rules (natural language)
1. Round 1: Cooperate.
2. Last round (t = r): Defect. (No future to enforce reciprocity; safe default.)
3. For each intermediate round t (1 < t < r):
   a. Observe your payoff π_{i,t-1} from the previous round and your previous action a_{i,t-1}.
   b. If the number of cooperators in the previous round ≤ 1 (i.e., almost nobody cooperated), play D (defect). This avoids being the lone contributor to a dying cooperation pool.
   c. Otherwise compare π_{i,t-1} to the current aspiration A:
      - If π_{i,t-1} ≥ A: repeat your previous action (win‑stay).
      - If π_{i,t-1} < A: switch your action (lose‑shift).
         - If you had cooperated previously, switching means defecting (a punishment/defensive response).
         - If you had defected previously, switching means cooperating (an attempt to restore cooperation).
   d. After choosing your action for the current round, update the aspiration level:
      A ← λ·A + (1 − λ)·π_{i,t-1}.
4. Forgiveness built in: because aspiration is smoothed and because a previous defection will be switched to cooperation whenever that defection produces payoff below aspiration, the strategy naturally forgives and tries to reestablish cooperation instead of punishing forever.

Pseudocode (concise)
- Inputs: n, k, r, history of rounds 1..t-1 (for each round we know every player's action and payoffs)
- Initialize:
    A := (1 + k) / 2
    λ := 0.9
- For each round t = 1..r:
    if t == 1:
        action := C
    else if t == r:
        action := D
    else:
        last_my_action := my action at t-1
        last_my_payoff := π_{i,t-1}
        last_total_coops := number of players who played C at t-1
        if last_total_coops <= 1:
            action := D
        else if last_my_payoff >= A:
            action := last_my_action
        else:
            action := switch(last_my_action)  // C → D, D → C
        // update aspiration after observing last payoff
        A := λ * A + (1 - λ) * last_my_payoff

Comments and edge cases
- Short games (r = 2): The rules give C on round 1, D on round 2. This is safe because punishment on round 2 is impossible; cooperating in round 1 can still raise collective payoff if opponents are cooperative.
- If many opponents are unconditional cooperators, AWSLS will sustain cooperation because cooperative rounds yield payoffs ≥ A and the agent will continue cooperating.
- If opponents frequently defect or exploit, your payoff will drop below aspiration and you will switch to defect: this defends you from sustained exploitation.
- The strategy is deterministic (except for optional randomized forgiveness if implementer prefers stochastic forgiveness) and depends only on game parameters and observed history.
- Adaptivity: aspiration updates let the strategy calibrate to population norms (if average payoffs are low it lowers expectations gradually), preventing overreaction to occasional bad rounds while still reacting to persistent exploitation.
- Forgiveness: because switching from D to C happens when your payoff from defection is below aspiration, you do not stay locked in defection unnecessarily — you give others chances to cooperate again.
- Endgame safety: defecting on the final round avoids being exploited when future punishment is impossible. Implementers may optionally relax this (cooperate on the last round) if the tournament environment rewards risky last‑round cooperation among reciprocators; but the safe default is D.

Why this is cooperative and robust
- Cooperative: the strategy opens with cooperation, continues cooperation whenever it yields reasonably good payoffs, and actively tries to restore cooperation (D → C switches) when defection hasn’t paid off.
- Robust: it punishes persistent exploitation by switching to defection, so it won’t be milked indefinitely. The adaptive aspiration prevents over‑punishment from rare deviations and supports recovery when opponents respond.
- Simple and implementable: rules are few, local (based on own payoff and observed cooperators), and require only standard history information provided by the tournament.

Optional refinements (implementer choices)
- Use average group payoff instead of own payoff to update aspiration — that makes the rule more group‑oriented but slightly less defensive versus targeted exploitation.
- Use a smaller λ (faster learning) in short games so aspiration tracks more quickly.
- Allow randomized forgiveness: after a punishment switch to D, cooperate again with small probability p to probe restoration of cooperation.
- Make last‑round behavior conditional: cooperate in the last round if the previous round had high cooperation fraction (≥ some high threshold), else defect.

This AWSLS strategy balances attempting and sustaining cooperation, adapting to opponent behaviour, defending against exploitation, forgiving to regain cooperation, and protecting itself near endgame.
'''

description_COOPERATIVE_77 = '''
Strategy name: Adaptive Forgiving Conditional Cooperator (AFCC)

High-level idea
- Signal cooperation: start by cooperating.
- Reward cooperation: keep cooperating when the group shows reasonably high cooperation.
- Proportionally punish defections: punish only as much as the observed defections warrant and for a limited time.
- Forgive and rebuild: let trust decay back up when others return to cooperating.
- Safe endgame: defect in the final round (one-shot Nash) to avoid being exploited there.

AFCC is simple to implement, only needs the game parameters (n,k,r) and the full history of who cooperated each previous round. It is adaptive (uses a running “trust” score to smooth noisy histories) and robust (punishes proportionally and temporarily so it is not driven to permanent mutual defection).

Parameters (recommended defaults)
- λ (memory decay for trust): 0.6 — how strongly past rounds influence trust (0 < λ < 1).
- τ (trust threshold to cooperate): 0.55 — if trust ≥ τ, cooperate.
- τ_high (strong cooperation threshold): 0.80 — if last round was strongly cooperative (fraction cooperators ≥ τ_high), immediately cooperate.
- forgiveness_floor: 0.2 — if last-round cooperation is below this, treat as low.
- δ (sharp-drop detection): 0.15 — if cooperation fraction dropped by more than δ vs the previous round, trigger a short punishment.
- γ (punishment scale): 0.5 — how long to punish relative to remaining rounds and magnitude of drop.
- P_max (maximum punishment length): max(1, ceil(0.2 * r)) — cap on number of consecutive punishment rounds.
You may tune these (especially λ, τ, τ_high) for the tournament environment; the defaults are intentionally conservative and forgiving.

State variables maintained
- T: trust score in [0,1]. Initialize T = 1 (full trust).
- punish_timer: integer ≥ 0. Number of rounds left in an active punishment period. Initialize punish_timer = 0.
- history: list of previous rounds’ cooperation counts (or fraction cooperators).

Pseudocode (natural-language style)

Initialize:
- T ← 1.0
- punish_timer ← 0
- store history of past rounds (initially empty)

For each round t = 1..r:
  if t == 1:
    play C (cooperate)  // open with cooperation to signal niceness
    continue to next round

  if t == r:
    play D (defect)  // safe endgame: one-shot defect in final round
    continue

  // observe last round(s)
  f_last ← fraction of players (including yourself) who cooperated in round t-1  // in [0,1]
  f_prev ← fraction of cooperators in round t-2 (if exists; else set f_prev = f_last)
  r_remaining ← r - t + 1

  // update trust (exponentially-weighted average of recent cooperation)
  T ← λ * T + (1 - λ) * f_last

  // If we are in an active punishment period, continue to punish (defect)
  if punish_timer > 0:
    play D
    punish_timer ← punish_timer - 1
    // after punishment ends, do not reset T to 0 — keep trust as updated so forgiveness can operate
    continue

  // Strong reward: if last round was strongly cooperative, cooperate
  if f_last ≥ τ_high:
    play C
    continue

  // If trust is high enough, cooperate
  if T ≥ τ:
    play C
    continue

  // Detect a sharp drop in cooperation (mass defection event)
  drop ← max(0, f_prev - f_last)
  if drop ≥ δ:
    // Set punishment length proportional to drop and remaining rounds, capped
    p_len ← min(P_max, max(1, ceil(γ * r_remaining * drop)))
    punish_timer ← p_len - 1  // we'll use one punishment round now, remaining p_len-1 in future
    play D
    continue

  // Forgiving fallback: cooperate if last-round cooperation is at or above the forgiveness floor,
  // otherwise defect. This gives a chance to rebuild cooperation when the group shows modest cooperation.
  if f_last ≥ forgiveness_floor:
    play C
  else:
    play D

End for

Rationale and properties

1. Niceness: AFCC never defects first. Starting with C signals cooperative intent so reciprocal cooperators can lock into full cooperation.

2. Rewarding cooperation quickly: If the last round showed strong cooperation (fraction ≥ τ_high), AFCC immediately cooperates to lock in cooperation.

3. Smooth, robust assessment (trust score T): Instead of reacting only to the last round, T is a smoothed statistic (EWMA) of recent cooperation. This makes AFCC robust to transient noise or single defections.

4. Proportional, bounded punishment: If cooperation sharply drops, AFCC punishes for a short, proportional number of rounds. Punishment is limited by P_max and scaled by γ and the magnitude of the drop. This deters persistent free-riding but avoids eternal mutual defection (unlike a grim trigger).

5. Forgiveness: Once punishment ends, the trust score continues to rise if the group recovers. The forgiveness_floor ensures AFCC is willing to cooperate even when cooperation is not perfect, fostering re-establishment of cooperation.

6. Endgame safety: AFCC defects on the last round to avoid guaranteed unilateral exploitation in the one-shot final round.

Why this is cooperative and robust
- AFCC is cooperative-minded: it prefers cooperation whenever the group behavior supports it and rewards cooperative groups quickly.
- It is adaptive: it uses an EWMA of past cooperation so it can detect trends, not just single events.
- It is robust to exploiters: it punishes proportional to the observed defection and the remaining rounds (so punishments are meaningful), deterring strategies that habitually defect.
- It tolerates occasional mistakes and noise: punishment is temporary and trust recovers, so two cooperative AFCC-like players will converge to sustained cooperation.
- It requires no external coordination or communication — only each round’s observable cooperators.

Edge cases and clarifications
- First round: Cooperate (signal niceness).
- Last round: Defect (one-shot Nash).
- If the history is very short (t=2), trusts and f_prev are defined so the logic still works (use f_prev = f_last when t=2).
- If nearly everyone defects for an extended period, AFCC will defect too, but because punishments are limited it will resume cooperation if others begin to cooperate again.
- Parameter tuning: if you expect harsher environments (many exploiters), increase δ or lower τ so the strategy defects sooner; if you expect cooperative opponents, increase τ_high or reduce λ to react faster.

Variants and optional probabilistic element
- If the tournament environment supports mixed strategies, you may replace the deterministic forgiveness fallback (when f_last ≥ forgiveness_floor) with a cooperation probability proportional to f_last. That can make AFCC less exploitable by adversarial patterns.
- Another optional change is to set τ as a function of k/n: if k is large (public good very valuable), you may raise τ slightly to be stricter about rewarding cooperation; if k is just above 1, lower τ to be more forgiving.

Summary
AFCC is a simple, easy-to-implement conditional cooperator that:
- begins by cooperating,
- uses a smoothed trust score to assess the group,
- rewards strong cooperation immediately,
- punishes defections proportionally and temporarily,
- forgives and rebuilds cooperation when the group improves,
- and defects in the final round to avoid endgame exploitation.

This mix of reciprocity, proportional punishment, and forgiveness is well-suited for N-player repeated public goods tournaments where opponents’ behaviors are unknown and varied.
'''

description_COOPERATIVE_78 = '''
Strategy name: Generous Gradual Reciprocity (GGR)

High-level idea
- Start by signaling cooperation.
- Cooperate as long as the group cooperates sufficiently.
- When others under-contribute, respond with a short, graded punishment whose length scales with how many players defected. Be forgiving and allow quick return to cooperation after recovery.
- In the last round always defect (rational endgame). In the final few rounds reduce punishment lengths because there is less future to enforce cooperation.
- Include a small, intentional forgiveness probability to recover from noise and to probe opponents.

This strategy depends only on game parameters (n, r, k) and observed history (past contribution counts per round). It does not rely on any external agreements.

Key constants (derived from parameters; can be tuned)
- W (lookback window) = min(5, r-1). Use recent history up to W rounds.
- T_base (cooperation threshold) = 0.5 (cooperate if at least half cooperated last round). Optionally raise T_base when k is larger (see note below).
- gamma (punishment scale) = 1.0 (punishment length per missing cooperator).
- p_probe (probability to cooperate while punishing / to test restoration) = 0.05 (5%).
- Endgame_safe (number of final rounds with special behavior) = 1 (last round); optionally set to 2 if r is small.

Rationale for constants
- T_base = 0.5 is a robust midpoint used in public-goods settings: it tolerates occasional free-riders but requires a majority to sustain cooperation.
- gamma = 1.0 makes punishments proportional to how many players fell short of T_base × n rather than over-reacting to a single mistake.
- Small probing probability enables recovery from noise and avoids permanent collapse from single mistakes.

Decision rules (natural language)
1. First round: Cooperate. (Signal cooperative intent.)
2. Every non-terminal round (t < r):
   a. Compute observed_cooperators = number of players who played C in round t-1 (0 if t=1).
   b. If currently in a punishment episode with remaining_punish_rounds > 0:
      - With probability p_probe cooperate this round (probe for restoration).
      - Otherwise defect.
      - If observed_cooperators in the most recent round (or in the last two rounds) has risen to ≥ T_base × n for two consecutive rounds, cancel remaining punishment early and return to "good" mode.
   c. If not in punishment ("good" mode):
      - If observed_cooperators (last round) ≥ T_base × n, cooperate.
      - If observed_cooperators < T_base × n, initiate punishment:
         • deficit = max(0, ceil(T_base × n) − observed_cooperators)
         • remaining_punish_rounds = min( r − t, 1 + ceil(gamma × deficit) )
         • Defect this round (first of punishment).
3. Last round (t = r): Defect. (Rational endgame: no future to enforce cooperation.)
4. Small-game adjustment: If r ≤ 3, be more conservative: reduce initial cooperation probability after round 1 (or set p_probe lower) because short horizons make exploitation more likely. A simple rule: for r ≤ 3, if more than one player defected in round 1, switch to permanent defection. (This prevents exploitable generosity in tiny games.)
5. Error tolerance and forgiveness: punishments are short, graded, and cancellable on observed recovery. This prevents long retaliatory wars due to mistakes.

Pseudocode

Initialize:
  state = "GOOD"
  remaining_punish = 0
  consecutive_recovery_rounds = 0

Constants:
  W = min(5, r-1)
  T = T_base (default 0.5)
  gamma = 1.0
  p_probe = 0.05
  Endgame_safe = 1  // last round special case

On each round t (1..r):
  if t == r:
    play D ; // last round defect
    continue

  if t == 1:
    play C ; // open cooperatively
    continue

  // observe last round
  observed_cooperators = count_C_in_round(t-1)

  // recovery bookkeeping
  if observed_cooperators >= ceil(T * n):
    consecutive_recovery_rounds += 1
  else:
    consecutive_recovery_rounds = 0

  // If we are in punishment mode
  if remaining_punish > 0:
    // early cancellation if recovery is sustained
    if consecutive_recovery_rounds >= 2:
      remaining_punish = 0
      state = "GOOD"
      // treat as GOOD below
    else:
      // probe occasionally
      if rand() < p_probe:
        play C
      else:
        play D
      remaining_punish = remaining_punish - 1
      continue

  // In GOOD mode
  if observed_cooperators >= ceil(T * n):
    play C
  else:
    // Initiate graded punishment
    deficit = max(0, ceil(T * n) - observed_cooperators)
    remaining_punish = min(r - t, 1 + ceil(gamma * deficit))
    state = "PUNISH"
    // defect this round (start punishment immediately)
    play D

Notes and refinements
- Choice of T as a function of k: if k is relatively large (k close to n), cooperation produces stronger group returns and is easier to sustain; you may raise T toward 0.6–0.8. A simple automatic adjustment:
   T = clamp( 0.5 + 0.3 * (k - 1) / (n - 1), 0.5, 0.8 )
  This raises threshold when k is large.
- If empirical play shows many unconditional defectors, the strategy will revert to mostly defecting (because punishments do not elicit recovery). That protects from exploitation.
- Against unconditional cooperators the strategy cooperates almost always and gains full mutual-cooperation payoff.
- Against conditional cooperators (TFT-like or generous TFT) the strategy coordinates to mutual cooperation because both reward cooperation and respond to defection with limited punishment and forgiveness.
- Against random players the strategy adapts: frequent defection triggers punishment; occasional cooperation yields cooperation; the probing probability allows discovering stable cooperation pockets.
- Against extortionate or persistent defectors the strategy punishes proportionally and then resumes attempts to cooperate; you are not permanently exploited.

Edge cases
- First round: always cooperate to signal willingness and give opponents a chance to reciprocate.
- Last round: always defect. If you absolutely want to trust continued cooperation and the group has been perfectly cooperative for many rounds, you could optionally cooperate in round r if you observed unanimity of cooperation in r-1 and you know opponents never use backward induction; but default is defect to avoid guaranteed exploitation.
- Very small r (2 or 3): treat more conservatively; punishments are short and proportional because threats have little credibility in short horizons.
- Noisy mistakes: short punishments with early cancellation on 2 consecutive good rounds plus p_probe prevent long retaliation cycles.

Why this is cooperative and robust
- Cooperative: it signals cooperation, rewards cooperation by cooperating when a majority cooperates, and seeks to re-establish cooperation quickly after provocation.
- Robust: graded punishments proportional to shortfalls avoid overreaction and prevent exploitation by persistent defectors; probing and forgiveness allow recovery from errors; last-round conservatism avoids being exploited by backward induction strategies.
- Adaptive: thresholds can be adjusted by k (if desired), and the algorithm adapts automatically to observed cooperation levels in the group.

Implementation notes
- The strategy only needs to track: remaining_punish, consecutive_recovery_rounds, and the last W rounds’ cooperation counts for possible extensions.
- All parameters (T_base, gamma, p_probe, W) are tunable. The defaults above aim for a good balance of cooperation and protection in heterogeneous tournaments.

Summary (short)
- Cooperate initially. Cooperate whenever last-round cooperators ≥ threshold T (default 50%). If the group under-contributes, defect for a short, graded punishment whose length scales with how many players fell short; be forgiving (early cancellation after 2 good rounds) and occasionally probe by cooperating while punishing. Always defect in the last round. This balances cooperative intent, protection against exploitation, and recovery from mistakes.
'''

description_COOPERATIVE_79 = '''
Name: Adaptive Forgiving Conditional Cooperator (AFCC)

Short description
- AFCC is a history-dependent conditional-cooperation rule that (1) opens with cooperation, (2) punishes short and proportionate to observed shortfalls in group cooperation, (3) forgives quickly to re-establish cooperation, and (4) defects in the final round (no future to enforce reciprocity). It uses only the game parameters (n, r, k) and observable history (who cooperated each past round and payoffs).

Design goals
- Be clearly cooperative: try to achieve and sustain high mutual cooperation when others reciprocate.
- Be robust: avoid indefinite punishments that lock the group in mutual defection; punish proportionally to deter exploitation; test for recovery; limit punishments near the end of the game.
- Be implementable: decision rules depend only on past actions/payoffs and known parameters.

Parameters (internal to the strategy; can be tuned)
- window w = min(3, r) : number of most recent rounds used to estimate current group cooperation (default 3).
- threshold γ = 0.60 : fraction of other players cooperating in the recent window that we treat as “sufficient cooperation.”
- punishment cap Pmax = 3 : maximum number of consecutive punishment rounds.
- punishment scale s = 2/(n-1) : scales punishment length to the magnitude of the shortfall and group size.
- forgiveness-test probability p_test = 0.25 : probability of trying a cooperative “test” after punishment to check for recovery.
- aspiration smoothing α = 0.2 : how fast our aspiration level adapts (for a WSLS-like backup rule).
- endgame_horizon E = 1 : number of last rounds where onward punishments are not initiated (we always defect in final round).

You may tune these in tournaments; defaults work well across a wide set of environments.

High-level decision rules (natural language)
1. First round: Cooperate (signal cooperativeness).
2. Last round (t = r): Defect (no future to enforce reciprocity).
3. In rounds before the last:
   a. Estimate recent cooperation among the other n−1 players using the most recent w rounds (or all past rounds if fewer than w exist).
   b. If the recent cooperation fraction among others ≥ γ, play C (we interpret group as cooperative).
   c. If recent cooperation fraction < γ:
      - Enter a short, proportionate punishment: defect for T rounds, where T = min(Pmax, ceil(s × (γ − observed_fraction) × (n−1))). (This makes punishment longer when the shortfall is larger and gives larger groups proportionate response.)
      - After the punishment period ends, with probability p_test play C once to test whether the group has reverted to cooperative behavior. If the test sees improved cooperation, continue cooperating; otherwise resume punishment or defect until recovery.
4. Special handling for self-caused deviations and aspiration (contrition & WSLS backup):
   - Maintain a smoothed aspiration level A (init A between 1 and k, e.g., A0 = (1 + k)/2). Update A after each round: A ← (1−α)A + α × (my payoff that round).
   - If neither rule (3) nor punishment mode gives a clear prescription, use WSLS-like fallback: if my last-round payoff ≥ A then repeat my last action; else switch. This helps stabilize mutually beneficial patterns and repairs after accidental defections.
5. Endgame tolerance: Do not start a new punishment whose scheduled length would push into the final round(s) counted as inside E. (We still defect in the final round but avoid launching long punishments that would be meaningless there and generate useless retaliations.)

Pseudocode

(Note: rounds are indexed t = 1..r. history[t][j] ∈ {0,1} is player j’s contribution in round t; my_history is history of my own actions; payoff_history stores my round payoffs.)

Initialize:
  w = min(3, r)
  γ = 0.60
  Pmax = 3
  s = 2 / (n - 1)     # punishment scale
  p_test = 0.25
  α = 0.2
  E = 1
  in_punishment = false
  punishment_left = 0
  last_action = C
  A = (1 + k) / 2     # aspiration initial value

For each round t = 1..r:
  if t == 1:
    play_action = C
    last_action = C
    continue to next round after observing results

  if t == r:
    play_action = D   # last round: defect
    last_action = D
    continue

  # compute observed cooperation among others using last min(w, t-1) rounds
  W = min(w, t-1)
  sum_others = 0
  for τ from t-W to t-1:
    sum_others += sum_{j ≠ me} history[τ][j]
  observed_frac = sum_others / (W * (n - 1))

  # update aspiration from previous round payoff (if available)
  if t > 1:
    A = (1 - α) * A + α * payoff_history[t-1]

  # If currently punishing, continue punishment
  if in_punishment:
    if punishment_left > 0:
      play_action = D
      punishment_left -= 1
      if punishment_left == 0:
        in_punishment = false
      last_action = D
      continue
    else:
      in_punishment = false

  # If observed cooperation is high enough, cooperate
  if observed_frac >= γ:
    play_action = C
    last_action = C
    continue

  # Otherwise, observed_frac < γ -> decide punishment length unless too close to end
  rounds_remaining = r - t + 1   # including current
  # do not initiate punishment that would extend into the game end (E)
  max_punishable_rounds = max(0, rounds_remaining - E)
  if max_punishable_rounds <= 0:
    # no effective future to punish; prefer immediate payoff
    # fall back to WSLS-like rule
    if payoff_history[t-1] >= A:
      play_action = last_action
    else:
      play_action = (C if last_action == D else D)
    last_action = play_action
    continue

  # compute proportional punishment length
  deficit = max(0.0, γ - observed_frac)
  raw_T = ceil(s * deficit * (n - 1))
  T = min(Pmax, max(1, raw_T))
  # but do not exceed max_punishable_rounds
  T = min(T, max_punishable_rounds)

  # start punishment
  in_punishment = true
  punishment_left = T
  play_action = D
  last_action = D
  continue

After each round, when observing the round outcome:
  - record history[t][j] and payoff_history[t]
  - if we just ended a punishment period (punishment_left == 0 and in_punishment toggled false):
      with probability p_test:
        in_test = true
        next round play C as a test; if others respond with increased cooperation (observed_frac in next measurement ≥ γ), resume cooperating; otherwise resume punishments as above.

Rationale / explanation of components
- Opening with C: provides a cooperative signal and gives AFCC a chance to establish mutual cooperation. In many tournaments a cooperative opening yields higher long-run returns when reciprocators exist.
- Threshold γ: a short-run majority-like threshold that indicates the group is sufficiently cooperative; if so, keep cooperating.
- Proportionate punishment: short, bounded punishment discourages exploitation but avoids spiraling into long mutual defection. Punishment length scales with the magnitude of the observed shortfall, so a single rogue defector yields a short response, while systemic defection yields stronger short response.
- Forgiveness and tests: after punishment we intentionally try a cooperative trial (with some probability) to allow recovery. This avoids endless retaliation cycles and helps re-establish efficient cooperation.
- WSLS fallback and aspiration: WSLS-style behavior stabilizes patterns of mutual benefit and repairs mistakes (contrition). The aspiration level adapts slowly, so the strategy is not overly reactive to single anomalous payoffs.
- Endgame: defect in the final round (Nash logic). Avoid initiating punishments that would only run into the last E round(s) because they are ineffective and cause unnecessary rounds of mutual defection.

Edge cases handled
- First round: always cooperate.
- Last round: always defect (no future to enforce cooperation).
- Very short games (small r): window w reduces accordingly; punishment lengths are capped by remaining rounds so you don’t attempt meaningless long punishments.
- If the strategy itself accidentally defects (tracked via my_history), the WSLS fallback and the test-after-punishment rule allow quick contrition and recovery.
- If many players are persistent defectors, AFCC will defect after a short series of punishments and will not endlessly try to feed cooperators; proportional punishment and the WSLS fallback prevent being exploited indefinitely.
- If many players are conditional cooperators, AFCC’s combination of cooperation-first, measured punishment, and forgiveness quickly converges to high cooperation.

Why this is robust in a heterogeneous tournament
- It cooperates with cooperative opponents, punishes defectors in proportion to their deviation (deterring exploitation), but does not punish indefinitely (forgives and tests). That combination tends to gain high total payoff against a mixture: it scores well when many others are cooperative and is not the lowest-scoring when opponents are mostly defectors. The WSLS component adds stability and error-correction against noise or accidental defections.

Implementation notes for algorithm designers
- The constants (γ, w, Pmax, p_test, α) are tunable. If you expect very large groups (large n) you may want smaller γ (e.g., 0.55) because a single defector is proportionally less impactful; in very small groups you may want slightly stricter γ.
- The strategy can be made deterministic by setting p_test to 1/0 instead of probabilistic testing; probabilistic testing helps prevent lock-step cycles against other deterministic punishers.
- All internal bookkeeping uses only the public history and known parameters n, r, k.

Summary
AFCC is a cooperative-minded, adaptive, forgiving conditional cooperator: it opens with cooperation, uses short proportionate punishments to deter exploitation, tests for recovery, stabilizes cooperative equilibria via WSLS-like aspiration adaptation, and defects in the final round. It balances deterrence and forgiveness to perform well across a wide range of opponent behaviours in repeated public-goods tournaments.
'''

description_COOPERATIVE_80 = '''
Strategy name: Threshold Reciprocity with Proportional Punishment and Forgiveness (TR-PPF)

Summary (one line)
- Signal cooperation, sustain cooperation when a clear majority cooperates, punish defections proportionally and briefly, and always forgive to allow recovery; defect in the final round.

Intuition
- Start by cooperating to signal willingness.
- Continue cooperating while other players largely cooperate (majority), because mutual cooperation raises group payoff.
- If cooperation falls, punish in proportion to how many players defected so that defectors lose short-term incentive; but do not punish forever — return to cooperation so cooperation can be re-established.
- Forgive early if you observe real improvement in others’ cooperation.
- Defect in the final round (one-shot Nash behavior) to avoid being exploited in the known endgame.

Parameters derived from game inputs
- n (players), r (rounds), k (multiplier) are given.
- Threshold for “satisfactory cooperation” (T): T = ceil(n/2). (Majority rule.)
- Maximum punishment length (P_max): choose P_max = max(1, floor(log2(r))) or a small constant (e.g., 3). This bounds how long you punish and prevents long mutual collapse. (Implementation may set P_max = min(3, r-1).)
- Punishment length formula (when triggered): L = min(P_max, max(1, ceil((n - m_prev)/2))) where m_prev is number of cooperators in previous round (including you if you cooperated then). Thus more defectors => longer punishment, but capped.

State (kept between rounds)
- last_action ∈ {C, D} (what you played last round)
- punish_timer ∈ {0,1,2,...} (how many rounds of punishment remain; 0 = not currently punishing)
- trigger_defectors (optional): number of defectors that triggered current punishment (for diagnostics)

Decision rules (deterministic)
1. Final round rule:
   - If current round t = r (final round): play D.
2. If punish_timer > 0:
   - Default action: play D (continue punishment).
   - Update rule: after observing the current round’s cooperation count, reduce punish_timer by 1 at the end of the round unless forgiveness condition triggers early termination (see Forgiveness).
3. If not punishing (punish_timer = 0):
   - If t = 1 (first round): play C (initial signal).
   - Else (t > 1):
     - Let m_prev be number of cooperators in round t-1 (0..n).
     - If m_prev ≥ T (majority cooperated last round): play C (reward cooperation).
     - Else (m_prev < T): enter punishment:
         - Compute L = min(P_max, max(1, ceil((n - m_prev)/2))).
         - Set punish_timer := L.
         - Set trigger_defectors := n - m_prev.
         - Play D this round (begin punishment).

Forgiveness and early recovery
- While punish_timer > 0, monitor subsequent rounds’ cooperation levels.
- If, during punishment, you observe a clear improvement such that the number of cooperators in the most recent observed round ≥ T, then immediately set punish_timer := 0 and resume cooperation next round. (This lets the strategy forgive quickly when others respond positively.)
- Optional extra forgiveness: if the cooperation count is increasing steadily across two rounds (monotone improvement), consider ending punishment early.

Pseudocode

Initialize:
  last_action := C  (preparing to play C in round 1)
  punish_timer := 0

For each round t = 1..r:
  if t == r:
    action := D
  else if punish_timer > 0:
    action := D
  else if t == 1:
    action := C
  else:
    m_prev := observed number of cooperators in round t-1
    if m_prev >= ceil(n/2):
      action := C
    else:
      L := min(P_max, max(1, ceil((n - m_prev)/2)))
      punish_timer := L
      trigger_defectors := n - m_prev
      action := D

  play(action)
  observe m_this := number of cooperators this round (after play)

  # Update punishment timer and forgiveness
  if t != r:   # final round ends anyway
    if punish_timer > 0:
      if m_this >= ceil(n/2):  # quick forgiveness when majority returns
        punish_timer := 0
      else:
        punish_timer := punish_timer - 1

  last_action := action

Design choices and rationale
- Majority threshold (T = ceil(n/2)): simple, robust. If at least half the group cooperated last round, there is clear momentum toward cooperation; joining it helps sustain high group payoffs. This threshold is forgiving to partial cooperation while discouraging small clusters of cooperators from being exploited continually.
- Proportional punishment length: the number of defectors in the previous round determines punishment length (capped). That punishes more when defection is widespread, and punishes less (or just one round) for occasional deviations. Short punishments avoid long mutual defection cycles and allow rapid recovery.
- Immediate forgiveness on majority return: if others respond by restoring cooperation, you stop punishing immediately to re-establish mutual cooperation.
- First-round cooperation: signals cooperative intent to the field and helps form cooperative clusters.
- Last-round defection: necessary because the dominant one-shot incentive is to defect in the final round.
- Bounded (short) punishments and immediate forgiveness make the strategy robust to noise and occasional mistakes and avoid exploitation by highly retaliatory opponents (like grim) producing mutual long-run losses.
- Deterministic, memory-light, and only depends on game parameters and public history as required.

Handling edge cases
- r = 2: First round you cooperate; second (final) round you defect. This yields the maximal safe outcome given endgame logic.
- Very short tournaments (small r): P_max will be small; punishments will be short or impossible in last rounds due to min with r.
- If everyone defects from the start: punishments will trigger but are short and periodically the strategy will attempt to re-cooperate when it sees any improvement (so it will not be stuck forever).
- If some opponents randomly switch: the majority rule plus forgiveness stabilizes cooperation if a clear cooperative majority forms, and tolerates occasional randomness.
- If many opponents are unconditional defectors: the strategy will quickly settle into defection against them but still cooperate with cooperative opponents when a majority forms.
- If opponents are extortionate (get high payoff by exploiting cooperators): proportional punishment removes short-term benefit from defection clusters and returns to cooperation when they stop.

Tuning suggestions (for implementer)
- P_max: 2 or 3 is usually enough. Larger P_max increases deterrence but risks prolonged mutual defection. A logarithmic function of r (e.g., floor(log2(r))) is a pragmatic automatic setting.
- Threshold T: if you expect small-group cooperation to be valuable, you can lower T toward ceil(n/3) to be more willing to cooperate with minority cooperators; lowering T invites exploitation unless punishments are strong.

Why this is cooperative and robust
- Cooperative: gives others the opportunity to obtain and keep the mutually superior cooperative payoff by cooperating; rewards clear cooperative behavior.
- Robust: punishes defections proportionally rather than catastrophically, so it resists exploitation but avoids permanent breakdown. Forgiveness enables recovery after mistakes or strategic experimentation.
- Adaptive: reacts to observed cooperation levels and adjusts punishment lengths and forgiveness accordingly; works across a broad spectrum of opponent behaviors (full defectors, conditional cooperators, noisy players, etc.).

Example behaviors
- All others always cooperate: you cooperate every round except the last → mutual high payoff.
- A few defect occasionally: you punish briefly, then forgive if cooperation recovers → short-term cost but long-term recovery.
- Many defect persistently: you defect (since majority < T), minimizing exploitation.
- Mixed population where cooperation becomes majority: you join and help stabilize that majority.

This specification is fully defined by n, r, k and the public history and is straightforward to implement.
'''

description_COOPERATIVE_81 = '''
Overview (goal)
- Be cooperative: try to sustain high group cooperation when others reciprocate.
- Be robust: punish defections so we are not relentlessly exploited, but punish only temporarily and forgive so cooperation can be restored.
- Be adaptive: decide from recent observed behavior (history) and game parameters; tolerate occasional mistakes; escalate only if defection is persistent.
- Be pragmatic near the end: avoid automatic exploitation in the final round(s) but allow cooperating in the final round when there is strong evidence opponents will also cooperate.

Key ideas
- Start by cooperating to signal cooperation.
- Use a short moving window of recent rounds to estimate how cooperative the group (other players) has been.
- Cooperate when recent cooperation by others is above a threshold.
- If recent cooperation falls below the threshold, respond with a limited punishment (defect for L rounds), with L proportional to how bad the drop was.
- If cooperation resumes, forgive immediately (stop punishment and resume cooperating).
- In the final round we normally defect (no future to enforce cooperation), but we will cooperate in the final round if recent evidence strongly suggests near-certain mutual cooperation (to avoid missing mutual gains when opponents also consistently cooperate).

Notation used below
- t: current round index (1..r)
- r: total rounds
- n: number of players
- history: for each past round s < t we observe the actions of all players including how many cooperated
- coop_others(s): number of other players (excluding me) who cooperated in round s
- H: window size for recent history (default H = min(3, t-1))
- avg_coop = (1/H) × sum_{s = t-H to t-1} [coop_others(s) / (n-1)] — fraction of other players who cooperated on average in last H rounds
- punishment_counter: rounds of punishment remaining (integer >= 0)
- parameters (defaults recommended): theta = 0.6 (cooperation threshold), P_scale = 3 (scales punishment length), P_max = 5 (max punishment length), final_round_confidence = 0.95 (threshold to cooperate in last round)

Decision rules (natural language)
1. Initialization
   - punishment_counter := 0.
   - Round 1: Cooperate (C). This signals willingness to cooperate.

2. For any round t (2 ≤ t ≤ r):
   - Compute H = min(3, t-1) and avg_coop over the last H rounds as defined above.
   - If punishment_counter > 0:
       - Play D (defect) this round and decrement punishment_counter by 1.
       - After this round re-check avg_coop next round to decide whether to continue punishing or to forgive.
   - Else (punishment_counter == 0):
       - If t == r (final round):
           - Cooperate (C) only if avg_coop ≥ final_round_confidence (evidence others will almost surely reciprocate).
           - Otherwise play D (defect) in the final round (safe default).
       - Else (not final round):
           - If avg_coop ≥ theta: Play C (cooperate).
           - Else (avg_coop < theta): Enter a short punishment:
               - Compute L = min(P_max, max(1, ceil((theta - avg_coop) * P_scale * (n-1)))).
                 (Explanation: punishment length increases with how far avg_coop is below theta; scaling by (n-1) makes length sensitive to proportion drop in terms of people.)
               - Set punishment_counter := L (this round counts as first punishment round).
               - Play D (defect) this round.

3. Forgiveness and reset
   - If at the start of a round avg_coop ≥ theta and punishment_counter == 0, cooperate.
   - If after punishment rounds the recent avg_coop returns to >= theta, reset punishment_counter to 0 and resume cooperation.

4. Long-term collapse protection
   - If avg_coop stays extremely low (e.g., avg_coop < 0.05) for T_long consecutive rounds (e.g., T_long = 4), switch to permanent defection for remaining rounds (because continued cooperation is being exploited and punishment is ineffective). This prevents wasting the rest of the game trying to re-establish cooperation with unresponsive opponents.

Pseudocode (concise)
- Parameters: H_window = 3, theta = 0.6, P_scale = 3, P_max = 5, final_round_confidence = 0.95, T_long = 4, long_defect_flag = false
- State variables: punishment_counter = 0, consecutive_low = 0

Round t each step:
1. If long_defect_flag: play D; continue.
2. Let H = min(H_window, t-1). If H == 0 set avg_coop := 1 (so first round cooperates).
   Else avg_coop := average over last H rounds of [coop_others(s)/(n-1)].
3. If avg_coop < 0.05 then consecutive_low += 1 else consecutive_low := 0
   If consecutive_low ≥ T_long then long_defect_flag := true; play D; continue.
4. If punishment_counter > 0:
     play D; punishment_counter -= 1; continue.
5. If t == r:
     If avg_coop >= final_round_confidence then play C else play D; continue.
6. If avg_coop >= theta:
     play C; continue.
   Else:
     L = min(P_max, max(1, ceil((theta - avg_coop) * P_scale * (n-1))))
     punishment_counter := L - 1   // we use one punishment round now; store remaining
     play D; continue.

Parameter choices and rationale
- First-round cooperation: builds goodwill and allows mutual cooperators to benefit early.
- theta = 0.6: requires a clear majority of others to be cooperating recently before trusting them; this balances being cooperative and not naive.
- H = 3: short memory makes the strategy responsive and robust to occasional mistakes or one-off defections.
- Proportional limited punishment (L): punishment increases with how bad the drop is, making punishment credible while avoiding permanent breakdown. A cap P_max prevents endless vendettas and helps reestablish cooperation.
- Forgiveness: immediate return to cooperation once others behave. Prevents long mutual-defection cycles that are costly for everyone.
- Final-round rule: default to defect for strategic safety, but allow cooperation if we observe near-certain cooperation historically (final_round_confidence). This captures pragmatic deviation from strict backward induction when opponents are very reliable cooperators.
- Long-term collapse protection: if everyone is unresponsive and near-zero cooperation for several rounds, switch to permanent defection to avoid being a continual loser.

Why this is adaptive and robust
- Adaptive: decisions use only recent observed behavior; punishments scale with the severity of defection; the strategy responds quickly to improvements or declines in group cooperation.
- Robust: limited, proportional punishments deter opportunistic defectors but avoid permanent retaliation that would cost mutual cooperators; forgiveness allows re-creation of cooperative equilibria.
- Cooperative mindset: The baseline behavior is cooperative, and the strategy will rejoin cooperation whenever others demonstrate cooperative behavior. It avoids needless defections once cooperation resumes.
- Exploit-resistance: The use of punishment and the option to permanently defect against persistent non-cooperation protect the strategy from long-term exploitation.

Optional tuning for tournaments
- If opponents are known to be highly forgiving/noisy, lower theta (e.g., 0.5) and shorten punishments to increase mutual gains.
- If opponents are often exploitative, increase theta, increase P_scale and P_max to make punishment stiffer.
- One can adapt theta over time using the observed global cooperation level across the population, but the baseline policy above is a simple robust choice.

Summary
- Cooperate first; cooperate while recent group cooperation is >= theta; punish proportionally for a few rounds when cooperation drops; forgive immediately once cooperation resumes; defect in the last round unless near-certain cooperation; if cooperation collapses persistently, switch to permanent defection. This balances being reliably cooperative with protection against exploitation and quickly restores cooperation when opponents reciprocate.
'''

description_COOPERATIVE_82 = '''
Summary strategy name: Adaptive Conditional Cooperator with Gradual Punishment and Forgiveness (ACC-GPF)

Design goals
- Try to sustain high cooperation when the group is mostly cooperative.
- Avoid long-term exploitation by defectors via proportional, temporary punishments.
- Forgive and restore cooperation quickly after punishment so accidental defections don’t collapse cooperation.
- Be adaptive (use recent history and overall cooperation level) and robust (don’t rely on shared conventions).
- Avoid being exploited in the known final round(s).

High-level rule
- Start by cooperating.
- Cooperate when recent group behavior indicates sufficient cooperation.
- When others defect below a threshold, respond with a short, proportional punishment (defect for a few rounds). Escalate punishments only if defections continue.
- After punishment, require a short run of sufficiently cooperative rounds to return to cooperating.
- Always switch to safe (defect) mode in the final round; optionally extend safe mode to the last few rounds if desired.

Required inputs available to the strategy
- n, r, k (game parameters)
- Full action history of all players up to round t − 1 (who cooperated each prior round)
- The strategy’s own internal state (punishment counter, escalation level, forgiveness counter)

Tunable internal parameters (implementer may choose defaults)
- w: smoothing window size for recent history (default w = min(5, r−1))
- q: cooperation threshold for deciding whether others are “cooperative enough” (default q = 0.6)
- P_base: base punishment length in rounds when triggered (default 1)
- Escalation rule: increase punishment length if defections persist (default: +1 per repeated offense)
- S: required consecutive “good” rounds after punishment to reset escalation (default S = 2)
- Endgame horizon M: number of final rounds in which we switch to safe mode (default M = 1; i.e., defect in last round)

Rationale for defaults
- q < 1 (e.g., 0.6) makes the strategy forgiving of occasional isolated defections while still demanding substantial cooperation to continue cooperating.
- Small base punishment (P_base = 1) prevents long, destructive wars of defection while still making defection costly.
- Short window w ensures adaptation to recent changes in opponent behavior.
- Defecting in the last round (M = 1) avoids being exploited when no future punishment is possible.

Pseudocode (per round t)
State variables:
- pc: punishment counter (integer ≥ 0). When > 0, we play D and decrement each round.
- escalation: integer ≥ 0 counting how many punishment escalations have occurred (used to lengthen punishments for repeated defections).
- good_run: consecutive rounds of sufficiently cooperative behavior since last punishment (used to reset escalation).

Initialize:
- pc = 0, escalation = 0, good_run = 0

On entering round t (1..r):
1. If t == 1:
   - Play C. (Start cooperatively.)

2. If (r − t + 1) ≤ M:
   - Play D. (Endgame safe mode: defect in last M rounds.)
   - Update internal bookkeeping if needed (but do not count endgame defections as “offenses” for escalation).

3. Else if pc > 0:
   - Play D.
   - pc := pc − 1
   - (Do not increase escalation here; this is executing an outstanding punishment.)

4. Else (no active punishment, not in endgame, t > 1):
   - Compute for the most recent m = min(w, t−1) rounds:
     - For each of the last m rounds, compute fraction of other players who cooperated in that round:
         f_s = (# cooperators among others on round s) / (n − 1)
     - Let f_hist = average_s f_s  (average fraction of others who cooperated over the window)
     - Let f_last = f_{t−1} (fraction of others cooperating in immediate previous round)

   - Decide whether the group is “cooperative enough”:
     - If f_hist ≥ q (recent average cooperation strong) OR f_last ≥ q (most recent round was sufficiently cooperative):
         - Play C.
         - good_run := good_run + 1
         - If good_run ≥ S:
             - escalation := 0   (reset escalation after S consecutive good rounds)
       Else (group not cooperative enough):
         - This is treated as an offense triggering punishment:
         - Offense severity measure: s = max(0, q − f_last)  (how far below threshold last round was)
         - Set punishment length L:
             - L := P_base + escalation
             - Optionally scale by severity: L := min(L + ceil( (n − 1) * s ), r − t)  (cap at remaining rounds)
         - Set pc := L − 1  (we will play D this round and then have pc rounds of D to execute)
         - escalation := escalation + 1
         - good_run := 0
         - Play D now (punishment starts immediately)

Notes on implementation details
- Using both f_last and f_hist makes the strategy responsive to sudden defections (f_last) and robust to noise via smoothing (f_hist).
- Escalation is small and gradual; if opponents keep defecting, punishments grow so sustained exploitation becomes unattractive.
- Forgiveness is explicit: after S consecutive satisfactory rounds the escalation counter resets, and the strategy resumes normal cooperation behavior.
- The severity scaling (optional) makes punishments larger when many players defect in a single round. Implementers may omit severity scaling for simplicity.

Edge cases
- Very small r: if r is so small that w or S exceed r, use w = r − 1, S = 1.
- If n = 2: fraction f_last is either 0 or 1. Choose q in (0,1) so single defections trigger punishment; escalation behaves similarly but will be felt immediately.
- If many opponents are pure defectors: the strategy will quickly detect low f_hist and spend most rounds defecting, avoiding persistent exploitation.
- If occasional single-player mistakes occur: single low f_last but f_hist may stay ≥ q, so the strategy may remain cooperative (forgiving).
- If an opponent uses last-round defection: our M = 1 prevents being exploited in the final round.

Why this strategy is cooperative and robust
- Cooperative: it begins cooperating, cooperates whenever the group shows sufficient cooperation, and rewards cooperative behavior by returning cooperation after punishments. When the group cooperates, the strategy preserves and reinforces that outcome.
- Robust: it does not assume shared norms or precommitments. It reacts only to observed actions. It punishes defectors in proportion to their continued misbehavior, deterring exploitation while not initiating long destructive mutual defection wars.
- Adaptive: the smoothing window and escalation let it adapt to transient noise, coordinated waves of defection, or persistent exploitation.
- Endgame-safe: defecting in the last round prevents guaranteed exploitation when future punishment is impossible.

Example default parameter choices for implementation
- w = min(5, r−1)
- q = 0.6
- P_base = 1
- S = 2
- M = 1

These defaults are conservative: reasonably forgiving, quick to punish repeated defection, and safe in the final round.

Closing remarks
This strategy is implementable directly from the game parameters and observed history. It balances the need to promote cooperation (start C, reward cooperative groups) with protection against exploiters (proportional, escalating punishment, endgame defections). Tuning the internal parameters (q, w, P_base, S, M) trades off generosity vs. safety depending on your appetite for risk in the tournament environment.
'''

description_COOPERATIVE_83 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

Intuition (short)
- Start by offering cooperation. Reward groups that show substantial cooperation, punish (temporarily) when the group is mostly defecting, and forgive promptly if cooperation returns. Use a short recent-history window so the strategy adapts to changing populations and is robust to a variety of opponents. Be cautious in the final round unless cooperation has been stable.

Key design choices
- Uses only game parameters (n, k, r) and observed history (who cooperated each past round).
- Works without communication or coordination and copes with mixed populations.
- Balances cooperation (to achieve high group payoffs) and self-protection (to avoid being persistently exploited).
- Uses thresholds that scale with k and n so the strategy is stricter when the public good returns are small and more permissive when returns are large.

Parameters computed from the game (deterministic functions of n, k, r)
- L = min(5, r-1)  // look-back window (use up to last 5 rounds)
- thr_base = 1 - k/(2n)    // central threshold: higher when k is small relative to n
- thr_high = clamp(thr_base, 0.5, 0.95)    // require this fraction (of other players cooperating) to reliably cooperate
- thr_low = clamp(thr_high - 0.20, 0.10, 0.5)  // below this fraction we defect
- forgive_rounds = 1    // number of rounds of good behavior required to resume cooperation immediately
- stable_rounds_for_last = min(3, r-1) // number of recent rounds used to decide last-round cooperation

Notes on these choices:
- thr_high increases (becomes stricter) when k is small; if k is close to n, thr_high moves toward 0.5 (more willing to cooperate).
- The clamp bounds keep thresholds sensible for extreme parameter values.

Notation used below
- t: current round (1..r)
- history_s: number of cooperators (including you) in round s
- my_action_s: your action in round s (C or D)
- coop_others_s = (history_s - my_action_s) / (n-1)  // fraction of other players who cooperated in round s

Decision rules (natural language)
1. First round (t = 1):
   - Cooperate (C). This is a conciliatory opening that enables cooperation if others are willing.

2. For rounds 2 ≤ t < r (not last round):
   - Compute coop_rate = average of coop_others_s over s = max(1, t-L) .. t-1 (the recent fraction of other players cooperating).
   - If coop_rate ≥ thr_high:
       - Cooperate. The group is cooperating enough to justify cooperation.
   - Else if coop_rate ≤ thr_low:
       - Defect. The group is largely defecting; avoid being exploited.
   - Else (coop_rate in (thr_low, thr_high)):
       - Probe with a graded response: cooperate with probability
         p = (coop_rate - thr_low) / (thr_high - thr_low).
         (This is a soft transition: more cooperation observed ⇒ higher chance to cooperate.)
   - Additional safety/punishment rule:
       - If in some recent round you cooperated while fewer than thr_low fraction of others cooperated (i.e., you were likely exploited), then punish by defecting next round(s) until you observe at least forgive_rounds rounds where coop_rate ≥ thr_high (immediate forgiveness after short good behavior).

3. Last round (t = r):
   - Cooperate only if the last stable_rounds_for_last rounds (or as many as exist) had coop_rate ≥ thr_high in each of those rounds and there has been no recent exploitation of you (see punishment rule). Otherwise defect.
   - Rationale: in finite games the last round is vulnerable to pure defection; we allow cooperating in the very last round only if cooperation has been stable and mutual so far (a strong signal that others will not opportunistically defect).

Pseudocode (concise)

Initialize:
  L = min(5, r-1)
  thr_base = 1 - k/(2n)
  thr_high = clamp(thr_base, 0.5, 0.95)
  thr_low = clamp(thr_high - 0.20, 0.10, 0.5)
  forgive_rounds = 1
  stable_rounds_for_last = min(3, r-1)

Function fraction_others_cooperated(round s):
  return (history_s - my_action_s) / (n - 1)

For each round t = 1..r:
  if t == 1:
    play C
    continue

  compute coop_rate = average_{s = max(1, t-L)}^{t-1} fraction_others_cooperated(s)

  // check whether we were recently exploited
  exploited_recently = exists s in {max(1, t-2)..t-1} such that (my_action_s == C and fraction_others_cooperated(s) < thr_low)

  if t == r:  // last round
    if (for all s in last stable_rounds_for_last rounds coop_rate_s >= thr_high) and (not exploited_recently):
      play C
    else:
      play D
    continue

  // intermediate rounds
  if exploited_recently:
    // punish until group shows enough cooperation (forgiveness condition)
    play D
    continue

  if coop_rate >= thr_high:
    play C
  else if coop_rate <= thr_low:
    play D
  else:
    p = (coop_rate - thr_low) / (thr_high - thr_low)
    play C with probability p, else play D

Behavioral comments and robustness
- Starts friendly to allow mutual cooperation to form.
- Uses recent group-level statistics (not trying to track or identify individuals) so it scales to large n and is robust to noise or random players.
- The graded/probabilistic transition zone avoids sudden collapse of cooperation when small fluctuations appear, and it also probes to discover if apparent cooperation is stable.
- Punishment is local and short: when exploited, ACC defects until it sees a short run of clear cooperation, then returns to cooperation (forgiveness). This deters persistent exploiters but does not lock into permanent grim-trigger, which performs poorly in mixed populations.
- The last-round caution avoids easy exploitation in known finite-horizon games but allows mutual last-round cooperation when cooperation has been stable.
- Thresholds depend on k and n: when k is small relative to n (public good yields low per-person benefit), thr_high is high → the strategy demands strong group cooperation before cooperating. When k is larger, the strategy is more permissive.

Tuning notes (for implementers)
- L (look-back window) can be increased for slower-changing populations; small L makes ACC more reactive.
- The width of transition (thr_high - thr_low) controls how smooth the strategy is between clear cooperation/defection regimes.
- forgive_rounds controls how quickly the strategy forgives: increasing it makes ACC more cautious about returning to cooperation.

Why this is a good tournament entrant
- It is cooperative-minded (opens with cooperation, rewards collective cooperation) and yet defends against exploitation through short punishments and last-round caution.
- It adapts to the observed mix of strategies (pure defectors, conditional cooperators, random players) using only the recent cooperation rate, so it performs well against many kinds of opponents.
- Probabilistic probing discovers whether apparent cooperation is stable without committing to long punishments that reduce overall score.

You can implement the pseudocode above directly; its state is simply recent rounds’ actions and computed thresholds (all deterministic given n, k, r).
'''

description_COOPERATIVE_84 = '''
Name: Adaptive Forgiving Conditional Cooperator (AFCC)

Short description
- AFCC is a simple, parameterized conditional-cooperation strategy designed to be cooperative but robust. It starts by cooperating, continues to cooperate when the group shows reliable cooperation, punishes defections proportionally but briefly, and forgives to restore cooperation rather than escalate. Near the end of a finite game, it becomes cautious and cooperates only when there is strong historical evidence of trustworthy partners.

Key design goals
- Cooperative: prefers and tries to sustain mutually beneficial contributions when others reciprocate.
- Adaptive: uses recent history to detect trends and calibrate punishment length and willingness to forgive.
- Robust: short, proportional punishments prevent long tit-for-tat wars and limit exploitation by persistent defectors; the strategy defects when the group cooperates too little to avoid wasting endowments.
- Parameterized by (n, k, r) and the observed history only.

Notation used below
- t: current round (1..r)
- history: matrix of past contributions c_j,t' for each player j and prior rounds t' < t (c = 1 if contributed, 0 if not). We can also track total cooperators per round M_t' = Σ_j c_j,t'.
- me: index of this player
- others_coop_count_in_round(t') = M_t' - c_me,t' (if t' < t we know c_me,t')
- W := min(5, r) (recent-window length)
- recent_coop_rate := average over last W rounds (excluding current) of fraction of other players who cooperated (i.e., average of others_coop_count_in_round / (n-1))
- last_coop_count := others_coop_count_in_round(t-1) (0..n-1)
- punish_until: round index until which we are currently punishing (initially 0)
- coop_history_fraction_of_others := fraction of rounds so far in which at least a majority (or a chosen threshold) of others cooperated — used for endgame decision

Tunable internal constants (set as default rules below; they depend on parameters but are simple to compute)
- base_major_frac := 0.60. This is a conservative "majority" baseline.
- major_frac := a slight adjustment of base_major_frac toward easier cooperation when k is larger:
    major_frac := clamp(base_major_frac - 0.15 * (k-1)/(n-1), lower=0.45, upper=0.75)
  Intuition: when k is larger (public good is more productive) AFCC is willing to cooperate with a slightly lower fraction of cooperating others.
- punishment_base := 2 (minimum punishment length in rounds)
- forgiveness_prob := 0.80 (when returning from punishment, AFCC will probabilistically resume cooperation to break cycles)
- endgame_rounds := min(3, max(1, floor(r*0.08))) (a small number of last rounds where AFCC becomes cautious)
- endgame_trust_threshold := 0.75 (to cooperate in final rounds requires strong past evidence of reciprocation)

Decision rules (natural language)
1. First round (t == 1)
   - Cooperate (C). This signals willingness to coordinate.

2. If currently in a punishment phase (t <= punish_until)
   - Defect (D).

3. Otherwise (not punishing)
   a. Compute recent_coop_rate over the last W rounds (excluding current). Also look at last_coop_count (others who cooperated in round t-1).
   b. If recent_coop_rate >= major_frac:
        - Cooperate.
        Rationale: the group has been reliably cooperative recently; continue cooperating to sustain mutual gains.
   c. Else if last_coop_count >= ceil((n-1)*0.5) (a simple majority of others cooperated last round):
        - Cooperate.
        Rationale: a single recent round with a majority cooperating is taken as a signal to try cooperation again.
   d. Else if we observe a sharp drop in cooperation:
        - Define drop := max(0, previous_round_others_coop_count - last_coop_count), where previous_round_others_coop_count is others_coop_count_in_round(t-2) if t>2, else 0.
        - If last_coop_count is substantially below expectation (e.g., last_coop_count < ceil((n-1)*major_frac)) and drop >= 1:
            - Enter a proportional punishment: set punish_length := punishment_base + ceil(drop/1.5). Set punish_until := min(r, t + punish_length - 1). Defect this round (and during punish_until).
            Rationale: respond to defections promptly, with punishment length proportional to magnitude of the recent breakdown; short punishments avoid long mutual losses.
   e. Else (no clear signal to cooperate and no immediate big drop):
        - Defect for this round (conserve endowment) but with a built-in path back to cooperation:
           * If we are just exiting a punishment phase and recent_coop_rate >= 0.30, resume cooperation with probability forgiveness_prob (0.8). Otherwise, stay defecting until recent_coop_rate improves.

4. Endgame (t in last endgame_rounds)
   - Be cautious because backward induction makes cooperation hard to sustain in final rounds.
   - Compute long_run_trust := fraction of completed rounds in which at least ceil((n-1)*major_frac) of others cooperated.
   - If long_run_trust >= endgame_trust_threshold:
        - Cooperate (the group has been trustworthy overall).
   - Else:
        - Defect (avoid being exploited with no future to enforce reciprocity).

5. Quick resumption policy (after punishment)
   - Once the punish_until is passed, AFCC prefers to resume cooperation if recent_coop_rate has recovered. To avoid oscillations, the resumption is probabilistic (forgiveness_prob) unless recent_coop_rate >= major_frac, in which case resume deterministically.

Edge cases and clarifications
- If r is very small (e.g., r=2 or r=3):
  * W becomes small; use the same rules but give more weight to the last round observation. The first-round cooperation remains to encourage possible mutual cooperation.
  * For t==r (last round) AFCC uses endgame rule.

- If many players defect persistently (group never reaches major_frac):
  * AFCC will stop cooperating and defect to avoid repeated exploitation. Because punishments are short and forgiveness likely, it will continue probing for improved behavior but will not be persistently exploited.

- If a single defector among many appears (a small drop in last_coop_count):
  * The strategy uses a short, proportional punishment rather than permanent grim trigger. This minimizes losses from occasional noise or mistakes and reduces the risk of long cycles of retaliation.

Pseudocode (compact)
Variables:
- punish_until := 0
- W := min(5, r)
- base_major_frac := 0.60
- major_frac := clamp(base_major_frac - 0.15*(k-1)/(n-1), 0.45, 0.75)
- punishment_base := 2
- forgiveness_prob := 0.8
- endgame_rounds := min(3, max(1, floor(r*0.08)))
- endgame_trust_threshold := 0.75

At round t:
1. if t == 1: play C; return
2. if t <= punish_until: play D; return
3. compute recent_coop_rate over last min(W,t-1) rounds, where for each prior round compute (others_coop_count)/(n-1)
   last_coop_count := others_coop_count_in_round(t-1)
4. if recent_coop_rate >= major_frac: play C; return
5. if last_coop_count >= ceil((n-1)*0.5): play C; return
6. if t > 2:
     prev := others_coop_count_in_round(t-2)
   else prev := last_coop_count
   drop := max(0, prev - last_coop_count)
   if last_coop_count < ceil((n-1)*major_frac) and drop >= 1:
       punish_length := punishment_base + ceil(drop/1.5)
       punish_until := min(r, t + punish_length - 1)
       play D; return
7. if t > r - endgame_rounds:
     compute long_run_trust := fraction of prior rounds where others_coop_count >= ceil((n-1)*major_frac)
     if long_run_trust >= endgame_trust_threshold: play C; else play D; return
8. // default cautious behavior:
   if recent_coop_rate >= 0.30 and random() < forgiveness_prob: play C; else play D

Why this strategy is suitable for tournaments
- It signals cooperation on first move and rewards cooperative groups, which helps earn high cumulative payoffs against like-minded reciprocal strategies.
- It punishes defections quickly but not forever, reducing the chance to be exploited by persistent defectors while avoiding long mutual losses.
- It adapts to k and n (major_frac depends on k,n) so it is more permissive about cooperating when the public good is more productive.
- Probabilistic forgiveness prevents lock-in into oscillatory cycles with other conditional strategies and gives opportunities to re-establish cooperation.
- Endgame caution avoids naive exploitation in the final rounds where one-shot incentives dominate.

Implementation notes
- The constants (base_major_frac, punishment_base, forgiveness_prob, endgame_trust_threshold, W) are tunable. Tournament meta-information (e.g., distribution of strategies) might motivate different tuning; the defaults above provide a good robustness/cooperation trade-off across many common environments.
- The strategy uses only the public history (actions of all players each round) and the known parameters (n,k,r), so it satisfies the requirement of depending only on parameters and history.

Summary
AFCC tries to establish and sustain cooperation when others reciprocate, responds to defections with short proportional punishments, quickly attempts to forgive to restore cooperation, and becomes cautious in the final rounds. It balances cooperative orientation with safeguards against exploitation and is practical to implement from the provided game history and parameters.
'''

description_COOPERATIVE_85 = '''
Strategy name: Generous Conditional Cooperator (GCC)

Goal: Promote and sustain high group cooperation across r rounds while being robust to exploiters. GCC starts cooperatively, conditions cooperation on observed group behaviour (majority-style rule), punishes defections proportionally and briefly, and forgives quickly so cooperation can resume.

High-level idea
- Cooperate initially to give cooperation a chance.
- In later rounds, cooperate when a large enough fraction of other players cooperated in the immediately preceding round.
- If cooperation falls below the threshold, defect for a short, proportional punishment period (so defectors are made worse off), then test to re-establish cooperation.
- In the final round, defect unless the group has been reliably cooperative recently (so we are not exploited in the one-shot endgame unless everyone really has been cooperating).

Parameters (derived from game parameters n, r, k)
- Threshold fraction T = 0.5 (cooperate if at least half of the other players cooperated in the previous round). Rationale: majority-based, simple, robust to noise in individual actions.
- Maximum punishment length P_max = min(3, max(1, floor(r/4))). (Short but meaningful punishments; scaled down for short games.)
- Minimum punishment length P_min = 1.
- Forgiveness rule: after a punishment period finishes, the strategy immediately returns to cooperation if the previous round shows enough cooperation (f_prev ≥ T). This allows quick recovery.
- Last-round cooperation condition: cooperate in round r only if the average fraction of cooperators among others over the last L = min(5, r-1) rounds ≥ 0.8; otherwise defect. (Prevents being exploited in the unavoidable one-shot endgame unless there has been strong sustained cooperation.)

State maintained
- pun_counter (integer ≥ 0): rounds of punishment remaining. Initialized 0.
- history of rounds: for each past round t, number of cooperators m_t (observed).

Decision rules (natural language)
1. Round 1: Cooperate.
2. For rounds 2..r-1:
   a. If pun_counter > 0:
      - Defect this round and decrement pun_counter by 1.
      - However, if the immediately previous round (t-1) shows group cooperation fraction f_prev ≥ T, clear pun_counter and cooperate this round (contrition/forgiveness check).
   b. Else (pun_counter == 0):
      - Compute f_prev = (number of other players who cooperated in t-1) / (n-1).
      - If f_prev ≥ T: Cooperate.
      - Else (f_prev < T): enter punishment:
         • Let severity = (T - f_prev) * (n-1). (This is the shortfall in cooperating others.)
         • Set pun_length = min(P_max, max(P_min, round(severity)))  — i.e., at least 1 round, up to P_max, scaled to how bad the shortfall was.
         • Set pun_counter = pun_length (you will defect this round and for pun_length-1 following rounds unless early forgiveness is triggered).
         • Defect this round.
3. Round r (last round):
   - If r == 1 this was handled in (1). For r > 1:
   - Compute recent_coop_fraction = average over last L = min(5, r-1) rounds of (number of other cooperators)/(n-1).
   - If recent_coop_fraction ≥ 0.8: Cooperate in round r.
   - Else: Defect in round r.
4. Small contrition/forgiveness detail:
   - If you are currently punishing but then observe that the other players achieved f_prev ≥ T in the immediately preceding round, stop punishing and cooperate immediately (this prevents punishment spirals and allows rapid return to cooperation).

Pseudocode (compact)
Note: rounds are 1..r; history[t] stores m_t = total cooperators (including you) in round t.

initialize pun_counter = 0

for t in 1..r:
  if t == 1:
    action = C
  else if t == r:
    L = min(5, r-1)
    recent_sum = sum_over_u=(r-L)..(r-1) of (history[u] - (your_action_at_u ? 1 : 0))
    recent_coop_fraction = recent_sum / (L*(n-1))
    if recent_coop_fraction >= 0.8:
      action = C
    else:
      action = D
  else:  # 1 < t < r
    if pun_counter > 0:
      # forgiveness check using previous round
      f_prev = (history[t-1] - (your_action_at_(t-1) ? 1 : 0)) / (n-1)
      if f_prev >= T:
        pun_counter = 0
        action = C
      else:
        action = D
        pun_counter = pun_counter - 1
    else:
      f_prev = (history[t-1] - (your_action_at_(t-1) ? 1 : 0)) / (n-1)
      if f_prev >= T:
        action = C
      else:
        severity = (T - f_prev) * (n-1)
        pun_length = min(P_max, max(P_min, round(severity)))
        pun_counter = pun_length
        action = D
  play(action)
  observe others and update history

Rationale and properties
- Cooperative: Starts with cooperation and cooperates whenever a substantial fraction (≥50%) of others cooperated in the most recent round. This rewards cooperative groups and promotes stable cooperation clusters.
- Adaptive: The threshold rule uses only immediate past group behaviour (easy to compute) and scales punishments to the observed shortfall. It reacts quickly to falling cooperation but does not over-punish.
- Robust: Short, proportional punishments make exploitation costly for defectors but limit long retaliatory cycles against transient or mistaken defections. The forgiveness checks allow quick recovery to cooperation when others reciprocate.
- Endgame-aware: Last-round defection protects you from the one-shot temptation unless the group has been reliably cooperative recently — in which case cooperating on the last round sustains the mutually beneficial payoff.
- No reliance on communication or shared norms; the rule uses public history only and depends only on (n, r) via P_max scaling and the last-round window.

Edge cases
- n = 2: T = 0.5 means cooperate if the other cooperated last round (equivalent to direct reciprocity). Punishment lengths remain small.
- Very short r: P_max computed from r ensures punishments are not longer than the game length; when r ≤ 4 punishment lengths are 1, so GCC becomes quick tit-for-tat-like in short games.
- Persistent exploiters: If many opponents always defect, GCC will defect after observing f_prev < T and punish for short periods; repeated exploitation will lead GCC to mostly defect (safe) but still periodically test for renewed cooperation (because after punishment ends it checks previous-round cooperation and will cooperate if others improved).
- Occasional noise or missteps: Forgiveness (ending punishment early if others resume cooperation) prevents spirals triggered by occasional deviations.

Implementation notes for tournament
- The strategy is deterministic and depends only on public history and the parameters n and r (and internal constants T, P_max etc.). It can be implemented efficiently by tracking the number of cooperators each round and a small pun_counter state.
- Tunable constants: T = 0.5 and last-round cooperativity threshold 0.8 are chosen for robustness and simplicity. If you expect especially forgiving opponents, raising T or lowering the last-round threshold can be considered; in unknown populations these defaults perform well across a wide range of opponents.

Summary
- GCC: Start cooperative, condition cooperation on whether at least half of others cooperated in the previous round, punish proportionally but briefly when cooperation drops, forgive quickly when cooperation returns, and avoid last-round exploitation unless cooperation has been reliably strong. This balances being cooperative and being robust to a wide range of opponent behaviors.
'''

description_COOPERATIVE_86 = '''
Strategy name: Adaptive Conditional Cooperator with Forgiveness (ACCF)

Goal (high level)
- Actively seek and sustain high group cooperation when others reciprocate, but avoid being exploited by persistent defectors.
- Use clear, observable signals (cooperating early and forgiving after short punishments) plus measured retaliation when exploitation is detected.
- Always defect in the final round (backward induction), and adapt behavior as the end approaches.

Intuition
- Start cooperative to signal willingness.
- In each round, condition on the observed fraction of cooperators among the other players in recent history.
- If many others cooperated last round, cooperate; if many defected, defect.
- If mixed signal, probabilistically cooperate (a generous “probe”) to re-establish cooperation but limit exploitation by punishment of limited length.
- Forgive quickly when group behavior improves.

Parameters (set from game parameters and design choices)
- n: number of players (given)
- r: number of rounds (given)
- window w: how many past rounds to consider when estimating others’ cooperation (recommended w = min(3, max(1, floor(r/10)))).
- punishment length L: number of rounds to defect after detecting exploitation (recommended L = 2).
- high threshold θ_high: cooperate if fraction of others who cooperated ≥ θ_high (recommended θ_high = 0.6).
- low threshold θ_low: defect if fraction ≤ θ_low (recommended θ_low = 0.3).
- generosity factor g: scales probe probability in mixed cases (recommended g = 1.1).
- probe floor p_min: minimum cooperation probability in mixed case (recommended p_min = 0.05).
- final-round rule: always defect in round r.

All of the above parameters are tunable; recommended defaults above are conservative and robust across a wide variety of opponents.

State variables derived from history
- For each past round t' we observe full action profile. Let coop_count_t' = number of players (including self) who played C in round t'.
- For decision at round t, let t_prev be the most recent completed rounds used (last w rounds or fewer if t is early).
- Compute f = average fraction of other players cooperating over the window:
  f = (1/w) * Σ_{t' in window} (coop_count_t' − I[self cooperated at t']) / (n − 1)
  (i.e., fraction among the OTHER players).
- Maintain `punish_until` flag indicating if we are currently in punishment phase (and when it ends).

Decision rules — concise
1. If t == r (final round): play D.
2. If t == 1: play C (signal cooperation).
3. If currently in punishment (t ≤ punish_until): play D.
4. Else compute f as above (fraction of others cooperating averaged over last w rounds):
   a. If f ≥ θ_high: play C.
   b. If f ≤ θ_low: play D.
   c. If θ_low < f < θ_high: play C with probability p = max(p_min, min(1, g * f)); else play D.
5. After observing the actions this round, if you detect exploitation (defined below), set punish_until = t + L (i.e., punish for L rounds starting next round).

Exploitation detection (when to punish)
- After any round t, if:
  - You cooperated in round t and your round payoff π_i,t is strictly below what you would have gotten by defecting given others’ actions that round (i.e., you were exploited), AND
  - The fraction of other cooperators that round (call f_t) ≤ θ_low_exploit (recommend θ_low_exploit = θ_low), then mark exploitation and punish for L rounds.
- Operationally: if you played C and your round payoff < median payoff of the group that round or < the payoff you would have gotten by switching to D (which is 1 higher than C minus k/n effect), treat as exploitation and punish.
- This ensures punishment is triggered only when cooperating was clearly punished by others’ defection.

Forgiveness / recovery
- Punishment is limited to length L (not permanent “grim”). After L rounds of punishment we stop punishing and resume the conditional rule (step 4). This allows cooperative opponents to recover and prevents long punish cycles and excessive self-harm.
- If during punishment we observe immediate improvement (f in a post-punishment round ≥ θ_high), stop punishing early (set punish_until = current round) — immediate forgiveness when others respond.

Edge cases and special considerations
- Small r (few rounds): for r = 2, the strategy cooperates first round and defects last round. For small r, w reduces to 1 so decisions are based on the single previous round.
- n = 2 (the two-player case): the same rules reduce to a generous tit-for-tat / WSLS-like behavior: cooperate first, mirror observed cooperation mostly, punish briefly if exploited, forgive quickly.
- Mixed and stochastic opponents: probabilistic probing in ambiguous situations (θ_low < f < θ_high) prevents the strategy from getting stuck in mutual defection and allows it to discover cooperative basins.
- Always-defectors: exploitation will trigger brief punishment and then persistent defection (because f will stay low). This avoids endless suck-up exploitation.
- Always-cooperators: the strategy will cooperate after observing high f, gaining mutual cooperation rewards.
- Sophisticated exploiters trying to burst-cooperate to avoid punishment: the use of a small window w and averaging reduces sensitivity to short bursts; punishment checks payoffs (not merely counts) to detect exploitation rather than brief manipulation.

Pseudocode (structured)
- Initialize punish_until = 0.
- For t from 1 to r:
  if t == r:
    play D
    observe actions; update history (no further rounds)
    break
  if t == 1:
    action = C
    play action; observe actions; update history
    if exploitation_detected(...): punish_until = t + L
    continue
  if t ≤ punish_until:
    action = D
    play action; observe actions; update history
    if immediate_recovery_detected(...): punish_until = t  # end punishment early
    continue
  compute f over last min(w, t−1) rounds (fraction among others who cooperated)
  if f ≥ θ_high:
    action = C
  else if f ≤ θ_low:
    action = D
  else:
    p = max(p_min, min(1, g * f))
    action = C with probability p else D
  play action; observe actions; update history
  if exploitation_detected(...):
    punish_until = t + L

Notes on exploitation_detected and payoff check
- After a round where you played C:
  - Your actual payoff π = (k/n)*coop_count (since you contributed 0 private funds)
  - If π + 1 <= payoff_if_defected (which is 1 + (k/n)*(coop_count − 1) ) — equivalent to finding that you would have gotten strictly more by defecting — flag exploitation. Numerically, this reduces to comparing whether others’ cooperations were low enough that your own cooperation strictly lowered your payoff. Use a margin epsilon = 1e-9 to avoid floating glitches.
- Optionally: treat group median payoff check as extra guard (if your payoff is significantly below group median when you cooperated, that’s evidence of exploitation).

Why this is cooperative yet robust
- Cooperative: signals cooperation early, cooperates when others clearly reciprocate (θ_high), and uses generosity/probing to re-establish cooperation from ambiguous situations.
- Robust: defects when others are mostly defecting, includes explicit exploitation detection to avoid being repeatedly exploited, and uses finite punishment (not infinite grim trigger) to avoid permanent collapse from misunderstandings or short defections.
- Adaptive: responses are based on observed cooperation frequency and realized payoffs; probabilistic probing prevents deterministic cycles and helps discover cooperative opponents.
- Tournament-minded: starts cooperative (good for many tournaments), avoids being suckered repeatedly (punishment), forgives (so it does well with conditional cooperators), and defects at the final round (necessary).

Tuning guidance for implementers
- Use the recommended defaults and test against a small suite of opponent archetypes (always-defect, always-cooperate, tit-for-tat analogues, random, mixed). Increase θ_high if you want to demand stronger reciprocation; increase L if you want harsher punishment (but that may lower average payoff with noisy opponents).
- If the tournament includes noise (random flips), increase forgiveness (shorter L, higher p_min) and increase window w to smooth noise.
- If opponents are mostly sophisticated and try to exploit initial cooperation, you may start with a short pre-play of probing instead of unconditional C in round 1 (e.g., probe with C with probability 0.9), but unconditional C in round 1 is generally better in mixed tournaments.

Summary (one-sentence)
Cooperate on round 1, defect on the final round; in between, condition on recent observed fraction of others’ cooperations—cooperate when a clear majority cooperated, defect when most defected, use generous probabilistic cooperation to probe in mixed situations, punish briefly when exploited, and forgive quickly to restore mutual cooperation.
'''

description_COOPERATIVE_87 = '''
Name: Adaptive Majority-Conditional Cooperator with Graduated Punishment (AMCGP)

Short description/intention
- Start by signaling cooperation, try to sustain group cooperation when a clear majority cooperates, forgive occasional mistakes, and punish sustained or systemic defection so as not to be exploited. Cooperate whenever there is evidence that a sizeable share of players are cooperating; defect otherwise. Always defect in the final round because no future enforcement is possible.

Intuition
- Public goods are socially improved by contributions but each round a lone cooperator loses relative to a defector. The strategy therefore uses reciprocity through history: it rewards rounds in which a substantial fraction of players cooperated and punishes when cooperation falls below a threshold. One-round forgiveness prevents breakdowns from single mistakes or noise; graduated (limited) punishment deters persistent defectors but permits recovery. The rule depends only on parameters and observed history (who cooperated in past rounds).

Parameters (derived from game parameters and history)
- n: number of players (given)
- r: total rounds (given)
- p* : group-cooperation threshold to treat the group as cooperating. Default p* = 0.5 (majority). This is simple and robust; it can be raised toward 0.6–0.7 if you expect many opportunists.
- forgiveness_slips = 1 (allow one below-threshold round without triggering punishment)
- max_punish = min(3, max(1, floor(r/6))) — maximum punishment length in rounds (scales mildly with horizon but bounded small)
- endgame: always defect in round r (last round)

State variables maintained during the game
- punish_remaining: integer ≥ 0, rounds of active punishment left (initial 0)
- slips_used: integer, counts forgiven below-threshold rounds since last “good” round (initial 0)
- last_round_cooperators m_{t-1} (observed each round)

Decision rules (natural language)
1. Round 1: Cooperate (signal willingness to cooperate).
2. For every round t = 1..r:
   - If t = r (last round): Defect.
   - Else if punish_remaining > 0:
       - Defect this round.
       - After observing the round outcome, decrement punish_remaining by 1 (and check for early recovery - see recovery rule below).
   - Else (no active punishment):
       - Compute fraction_coop = (number of cooperators in previous round) / n. (For t=1 we already cooperated by rule.)
       - If fraction_coop ≥ p*:
           - Cooperate this round. Reset slips_used := 0.
       - Else (fraction_coop < p*):
           - If slips_used < forgiveness_slips:
               - Use forgiveness: cooperate this round and increment slips_used by 1.
           - Else:
               - Enter punishment: set punish_remaining := min(max_punish, r - t) and Defect this round.

Recovery rule (end punishment early)
- After each round, if punish_remaining > 0 but the group shows clear recovery (fraction_coop ≥ p* for two consecutive rounds observed while punish_remaining > 0), set punish_remaining := 0 and reset slips_used := 0. This allows the strategy to stop punishing once cooperation actually returns.

Per-player targeting (optional enhancement, keeps the same global action)
- Keep counts of each other player’s cooperation rate over a recent window (window length H = min(5, r-1)). If a small set of players persistently defect while most cooperate, the group fraction condition will still allow cooperation; if defectors form a blocking minority and keep group fraction below p*, the punishment will be triggered and persist until defection subsides. This reduces exploitation from a persistent minority relative to unconditional cooperation.

Pseudocode (concise)
- Initialize: punish_remaining := 0; slips_used := 0; Observe nothing yet.
- For t in 1..r:
    if t == 1:
        action := C
    else if t == r:
        action := D
    else if punish_remaining > 0:
        action := D
    else:
        fraction_coop := (cooperators in round t-1) / n
        if fraction_coop >= p*:
            action := C
        else if slips_used < forgiveness_slips:
            action := C
            slips_used := slips_used + 1
        else:
            punish_remaining := min(max_punish, r - t)
            action := D
    Play action.
    Observe cooperators in round t.
    If punish_remaining > 0:
        If (fraction_coop >= p* for two consecutive most recent rounds):
            punish_remaining := 0
            slips_used := 0
        else:
            punish_remaining := max(0, punish_remaining - 1)
    If action was C and (cooperators in round t)/n >= p*:
        slips_used := 0

Edge cases / clarifications
- r = 2: t=1 cooperate, t=2 defect (matches the need to avoid last-round exploitation while still signaling).
- Short games: max_punish uses r so you never schedule punishment beyond remaining rounds.
- No history available (t=1): strategy cooperates.
- If many players are stubborn defectors (group fraction never reaches p*), the strategy defects after initial forgiveness(s), protecting you from repeated exploitation.
- If opponents are conditional cooperators, the initial cooperative signal and one-round forgiveness allow a path to reach sustained cooperation; the majority threshold keeps you cooperating only when a clear portion reciprocates.
- Parameter tuning: p* controls how strict you are. p* ≈ 0.5 is robust in heterogeneous environments. Increasing p* biases toward cooperating only when more players reciprocate, reducing exploitation risk at the cost of possible lost cooperative gains.

Why this is cooperative and robust
- Cooperativeness: starts cooperating and favors cooperation whenever a clear majority is cooperating; it aims to maintain or re-establish mutual cooperation rather than giving up at the first sign of a lapse.
- Robustness: forgives one slip (avoids collapse from a single mistake), employs finite punishments (not permanent grim-trigger) so recovery is possible, and defects when being exploited (protects payoff).
- Adaptivity: uses observed fraction of cooperators to respond to collective behavior rather than assuming uniform types; scales punishment to remaining horizon so it won’t overcommit late in the game.

Implementation notes for a tournament
- Implement the simple state variables (punish_remaining, slips_used, short history of last two rounds to check consecutive recoveries, optionally per-player counts). The rule uses only observed past rounds and given parameters; it does not rely on cross-agent agreements or off-path signals.
- If desired, tune p* and max_punish via preliminary simulations against likely opponent mixes (e.g., raise p* if many pure defectors are expected, lower p* if many conditional cooperators that trust minority signals are expected).

This strategy balances cooperative ambition with defensive measures so it can both create high-payoff cooperative episodes and avoid sustained exploitation in varied opponent pools.
'''

description_COOPERATIVE_88 = '''
Strategy name: Adaptive Generous Trigger (AGT)

High-level idea
- Signal cooperation immediately, sustain mutual cooperation when a clear majority of other players reciprocate, but defend against exploitation with short, calibrated punishments and an explicit forgiveness / reconciliation rule. Make forgiveness more generous when the public-good multiplier k is large (cooperation is more socially valuable), and shorten punishments near the end of the r-round horizon so you do not waste rounds on futile long punishments.

Summary of behavior
- Round 1: Cooperate (C).
- Last round (t = r): Defect (D).
- In other rounds: cooperate if recent others’ cooperation is above a threshold τ (threshold depends on k and n); otherwise defect and punish for a short number of rounds P. After a punishment episode, resume cooperation only after observing sustained improvement in others’ behavior for S consecutive rounds (reconciliation). Punishments are short, proportional to how badly others have deviated, and bounded to avoid permanent war-of-attrition.

Parameters derived from game parameters
- n, k, r: given by the game.
- window w = min(5, r) — number of recent rounds to average others’ behavior. (Small fixed window makes the strategy responsive.)
- generosity factor g = (k - 1) / (n - 1). This normalizes how socially valuable cooperation is (0 < g < 1). Larger g → be more forgiving.
- base threshold τ0 = 0.5 (baseline: require roughly a majority of others cooperating).
- threshold τ = clamp( τ0 - 0.2 * g, 0.2, 0.6 ). (If k is large, reduce τ to be more forgiving; τ always stays between 0.2 and 0.6.)
- max punishment P_max = min(4, max(1, floor(r/10))). (Punishments are short and bounded; scale a little with horizon.)
- reconciliation window S_req = min(3, r) — number of consecutive good rounds required to resume cooperation after punishment (bounded by remaining rounds).

State variables maintained by the strategy
- PunishCounter (integer ≥0): how many rounds of punishment remain. Initialized 0.
- InReconciliation (integer ≥0): how many good consecutive rounds of others we've observed since punishment ended. Initialized 0.

Interpretation of history
- In any round t you can observe full past vector of contributions. For round t consider others’ cooperation in rounds t-1, t-2, ... etc.
- For prior round s, let others_coop_rate_s = (sum_{j ≠ i} c_j,s) / (n - 1).
- For current decision at round t (t>1) compute recent_coop_rate = average of others_coop_rate_s over the last w rounds that exist (s = max(1,t-w) ... t-1).

Decision rules (natural language + pseudocode)
- If t = 1: choose C.
- Else if t = r: choose D. (No credible punishment left; defect on final round.)
- Else (1 < t < r):
  1. If PunishCounter > 0:
     - Play D.
     - Decrement PunishCounter by 1.
     - Set InReconciliation = 0 (punishment resets reconciliation).
  2. Else (not currently punishing):
     - Compute recent_coop_rate over last w rounds (others only).
     - If recent_coop_rate >= τ:
         - If InReconciliation > 0 or InReconciliation == 0: play C and increment InReconciliation by 1 (cap at S_req).
         - If InReconciliation reaches S_req, clear any flags (fully reconciled); continue cooperating until/unless punished again.
     - Else (recent_coop_rate < τ):
         - Play D now (punish).
         - Let severity = τ - recent_coop_rate (how far below threshold others are).
         - Set PunishCounter = clamp( 1 + floor( severity / τ * P_max ), 1, P_max ).
           (This makes P at least 1 and larger if others’ cooperation is far below τ, but bounded.)
         - Set InReconciliation = 0.

Pseudocode
(Note: c_history[t][j] is 1/0 contribution of player j in round t; i is you.)

initialize:
  PunishCounter = 0
  InReconciliation = 0
  w = min(5, r)
  g = (k - 1) / (n - 1)
  τ = clamp(0.5 - 0.2*g, 0.2, 0.6)
  P_max = min(4, max(1, floor(r/10)))
  S_req = min(3, r)

for round t = 1..r:
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue

  if PunishCounter > 0:
    play D
    PunishCounter -= 1
    InReconciliation = 0
    continue

  compute recent_coop_rate = average over s = max(1, t-w) .. t-1 of ((sum_{j != i} c_history[s][j]) / (n - 1))

  if recent_coop_rate >= τ:
    play C
    InReconciliation = min(S_req, InReconciliation + 1)
    continue
  else:
    # trigger punishment
    play D
    severity = τ - recent_coop_rate   # ∈ (0, τ]
    PunishCounter = clamp(1 + floor((severity / τ) * P_max), 1, P_max)
    InReconciliation = 0
    continue

Why this is cooperative and robust
- Cooperative mindset: AGT begins by cooperating and cooperates whenever a clear and sustained majority of others cooperated recently. If reciprocation exists, AGT uses a short reconciliation requirement (S_req) so cooperation is re-established quickly after small, possibly accidental, defections.
- Robustness: AGT refuses to be exploited: when others systematically defect (recent_coop_rate < τ), it defects and executes a short, predictable punishment. Punishment lengths depend on how badly the group deviated and on the remaining horizon (P_max scales modestly with r), preventing infinite punish wars and limiting losses if opponents never respond.
- Forgiveness: Threshold τ is lowered when k is larger (greater social gains), meaning AGT is more forgiving and more likely to sustain cooperation when mutual cooperation produces big group payoffs.
- Endgame safety: AGT defects on the final round to avoid being exploited when no punishment is possible. It also avoids long punishments very late in the game by bounding P_max and S_req relative to r.
- Public monitoring and deterministic rules: The strategy uses only public history (others’ contributions), no communication, and deterministic rules so it can be implemented in tournaments reliably. (Optional: implement a probabilistic generosity step instead of a strict cutoff if randomization is allowed and desired.)

Tuning notes (suggested defaults)
- w = 5 is responsive but smooths one-off mistakes.
- τ default 0.5 (adjusted downward by g) balances intolerance of free-riding with willingness to forgive when cooperation is highly valuable.
- P_max = 3 or 4 keeps punishments short and avoids long mutual defection wars.
- S_req = 2 or 3 requires small but clear signs of goodwill before resuming cooperation.

Edge cases explicitly handled
- First round: Cooperate to signal intent.
- Single accidental defection by one player: recent_coop_rate likely remains high; AGT forgives and continues cooperating.
- Multiple simultaneous defections: If the majority of others defect, AGT punishes for P rounds; if others resume cooperation it will rapidly reconcile.
- Very short games (r small): All parameters clamp appropriately (P_max, S_req, w) so AGT doesn't waste the few rounds available on long punishments.
- Final round: Always defect (no credible future punishment).
- Never-cooperating opponents: AGT will switch to defect and keep losses low rather than being continuously exploited.

Why this is a good tournament entry
- Starts cooperatively to enable mutually high payoffs.
- Uses public information to detect when cooperation is reciprocated vs when exploitation occurs.
- Calibrates punishment and forgiveness so it is neither too vengeful (which loses mutual gains) nor too gullible (which gets exploited).
- Adapts its tolerance to k (more forgiving when group returns are larger).
- Implements predictable and bounded punishments so other strategies can learn to reciprocate rather than escalate.

If desired, small variations can be used:
- Make τ, w, P_max functions of n, k, r with different sensitivities;
- Replace deterministic reconciliation with probabilistic cooperation to be even more forgiving;
- Escalate punishment length on repeated offenses (add a recidivism counter) if you observe persistent exploitation.

This description provides a complete, implementable algorithm that depends only on game parameters and observed history, is cooperative in spirit, and balances exploitation defense with forgiveness to foster stable cooperation in diverse tournaments.
'''

description_COOPERATIVE_89 = '''
Name: Forgiving Threshold Conditional Cooperator (FTCC)

Goal:
- Try to sustain high group cooperation across the repeated public-goods game.
- Be cooperative and forgiving so cooperation can re-emerge after mistakes, but punish in a way that deters persistent exploitation.
- Adapt tolerance to the strength of the public good (k) and the group size (n).
- Use only game parameters (n, k, r) and observable history of past moves.

Plain-language decision rules (summary)
1. Start cooperatively (play C in round 1).
2. In each round t (t < r), cooperate if recent group behaviour indicates cooperation is the norm (past fraction of cooperators is at or above a threshold α, or only slightly below it). If the group fell significantly below α in the previous round, switch to a short, proportional punishment phase (play D for a small number of rounds).
3. If punished or seeing low cooperation, defect until either (a) the punishment period expires or (b) the group shows a clear recovery (last observed fraction ≥ α), at which point return to cooperating.
4. Always defect in the final round t = r (endgame), because that round cannot be reciprocated.
5. Use short punishments and bounded forgiveness to avoid cascades of mutual defection and to be robust to occasional mistakes.

How α (cooperation threshold) is computed (adaptive to n and k)
- Let α = 1 − 0.5 × (k / n).
  - Intuition: when per-person public benefit (k/n) is large, the strategy is more willing to cooperate even if fewer players cooperated previously; when k/n is small it requires stronger evidence of cooperation before contributing.
  - This produces α in (0.5, close-to-1) for allowed k and n, so we demand a majority or stronger for low-return settings and are more permissive when the public good is strong.

Forgiveness window and punishment length
- Let δ = 0.15 (a small forgiveness margin).
  - If the previous-round cooperation fraction f_prev is in [α − δ, α), we cooperate (forgiveness) to avoid punishing for small/isolated slips.
- If f_prev < α − δ, start punishment:
  - Punishment length P = min(P_max, max(1, ceil((α − f_prev) × r)))
  - Choose P_max = min(5, max(1, floor(r/4))) — a small bounded cap so punishments are meaningful but not ruinous.
  - Intuition: deeper shortfalls produce longer but still bounded punishments proportional to remaining horizon r; this gives deterrence while allowing recovery.

Edge-case rules
- First round (t = 1): Cooperate (C).
- Last round (t = r): Defect (D).
- If punishment is active in late rounds, punish only up to remaining rounds − 1 (never punish into the last-round decision logic — last round is always D).
- If the group shows clear recovery (observed f_prev ≥ α) at any decision point while in punishment mode, end punishment and resume cooperating immediately.

Detailed pseudocode (uses only parameters and history)
- Inputs: n, k, r; history is list of previous rounds, each round storing contributions by all players (1 for C, 0 for D).
- State variables (kept across rounds): punish_remaining (integer, initially 0).

Initialize:
  punish_remaining ← 0
  α ← 1 − 0.5 × (k / n)
  δ ← 0.15
  P_max ← min(5, max(1, floor(r/4)))

For each round t = 1..r do:
  if t == r:
    action ← D
    output action and continue
  end if

  if t == 1:
    action ← C
    output action and continue
  end if

  // compute last-round cooperation fraction
  let last_round = history[t−1]   // array of n values 0/1
  f_prev ← (sum(last_round)) / n

  if punish_remaining > 0:
    // we are in punishment mode
    if f_prev ≥ α:
      // group recovered, stop punishment and cooperate
      punish_remaining ← 0
      action ← C
    else:
      // continue punishment
      action ← D
      // decrement punishment counter, but not below 0
      punish_remaining ← max(0, punish_remaining − 1)
    end if
    output action and continue
  end if

  // not in punishment mode
  if f_prev ≥ α:
    action ← C
    output action and continue
  end if

  if f_prev ≥ α − δ:
    // slight slip — forgive
    action ← C
    output action and continue
  end if

  // significant slip: start punishment
  // compute punishment length proportional to shortfall
  shortfall ← α − f_prev           // positive
  P ← min(P_max, max(1, ceil(shortfall × r)))
  punish_remaining ← P − 1         // we will play D this round and have P−1 rounds left
  action ← D
  output action

Notes about probabilistic variants and noise
- The deterministic rules above are intentionally simple and robust. If you expect noisy actions (implementation errors) you can:
  - Increase δ to tolerate more slips.
  - Or implement probabilistic cooperation when f_prev is in [α − δ, α): cooperate with probability (f_prev − (α − δ)) / δ (linearly increasing forgiveness).
- Both modifications preserve the same high-level behaviour (cooperate when group cooperates, punish short but sufficiently, forgive small slips).

Why this strategy is cooperative and robust
- Cooperative: starts by cooperating and always attempts to return to cooperation when evidence of group recovery appears. It tolerates small deviations to avoid punishing single mistakes.
- Adaptive: α depends on k and n, so tolerance scales with how valuable the public good is per person. Punishment length scales with the magnitude of the shortfall and with the horizon r (but is bounded).
- Defensible: short, proportional punishments deter persistent defectors; immediate defection in the final round prevents giving free exploitation in a round that cannot be punished later.
- Simple and implementable: uses only the last-round fraction of cooperators and a small internal counter. This keeps the strategy computationally light while giving good empirical performance in tournaments of diverse opponents.

Variants (optional, switchable when implementing)
- More memory: use running average of last k rounds instead of a single-round f_prev for more stability.
- Stronger deterrent: increase P_max to punish longer if tournament shows many exploiters.
- Probabilistic forgiveness: trade robustness for exploration if you want to discover whether opponents can be coaxed to cooperate.

Final remark
This strategy aims to produce high mutual payoffs when others are willing to cooperate, to resist exploitation via observable punishments, and to recover quickly from errors. It adapts automatically to the strength of the public good (k) and the group size (n), uses only history (no communication), and treats the final round conservatively to avoid guaranteed exploitation.
'''

description_COOPERATIVE_90 = '''
Name: Adaptive Forgiving Majority (AFM)

Intuition (short)
- Start by cooperating to signal willingness.
- Use the recent observed cooperation level of the group to decide: cooperate when a clear majority of others has been cooperating; defect when they have not.
- Punish defections only briefly (limited-length punishment) and include probabilistic forgiveness to recover cooperation after mistakes or occasional exploitation.
- Always defect in the final round (no future to enforce reciprocity).
- Parameters adapt modestly to the game parameters (k,n) so generosity scales with how valuable the public good is.

This keeps a cooperative mindset (seek and sustain high group cooperation) while being robust to persistent defectors and not locking into permanent retaliation.

Decision rules (natural language)
1. First round (t = 1): Cooperate. This opens opportunities for mutual cooperation.

2. Last round (t = r): Defect. There is no future to enforce reciprocation, so defect to avoid guaranteed exploitation.

3. Intermediate rounds (1 < t < r):
   - Compute the recent cooperation rate of the other players over a short window W of recent rounds (W = min(5, t-1)):
        frac = (total number of cooperations by opponents in the last W rounds) / (W * (n-1))
   - Use a majority threshold θ = 0.50 as the default:
        - If frac >= θ: Cooperate (we observe substantial cooperation by others).
        - If frac < θ: Defect (others are not cooperating enough).
   - But be forgiving and adaptive:
        - Generosity: with small probability g we cooperate even when frac < θ. g increases with the value of the public good k (because re-establishing cooperation is more valuable when k is larger). Concretely:
            g = clamp((k - 1) / k, 0.05, 0.50)
        - Limited-length punishment: if you have been defecting for several consecutive rounds already (ConsecutiveD >= P_max), avoid permanent defection — after P_max consecutive punish rounds switch to attempting forgiveness (cooperate with probability g). Use P_max = 2 (punish briefly, then try to restore cooperation).
        - Quick reward: if the cooperation level improved from the previous window to the current window (i.e., opponents are cooperating more now than previously), cooperate to reward recovery.

4. Small adaptive tweak based on historical cooperation:
   - If overall historical cooperation rate across all players (all rounds so far) is high (e.g., >= 0.75), be slightly more trusting: lower θ to 0.40 and increase g by +0.10 (capped at 0.50). This accelerates cooperation when the group is already generally cooperative.

Pseudocode

Inputs:
- n, r, k
- history: list of past rounds; each round t' gives a vector actions[t'] of length n where actions[t'][j] ∈ {C,D}
- me = my player index

Parameters:
- W_default = 5
- θ_default = 0.50
- P_max = 2            # maximum consecutive punishment rounds
- g_min = 0.05
- g_max = 0.50

Function decide(t, history):
    if t == 1:
        return C
    if t == r:
        return D

    # Window for recent behavior
    W = min(W_default, t - 1)

    # Count opponent cooperations in last W rounds
    coop_count = 0
    for round_idx in (t - W) .. (t - 1):
        for j in 1..n:
            if j == me: continue
            if history[round_idx][j] == C:
                coop_count += 1
    frac = coop_count / (W * (n - 1))   # fraction of opponent cooperations recently

    # Overall historical cooperation rate (all players, all past rounds)
    total_coop_all = sum over past rounds and players of indicator(action == C)
    total_actions_all = (t - 1) * n
    hist_coop_rate = total_coop_all / total_actions_all

    # Adjust threshold and generosity slightly if history is very cooperative
    theta = θ_default
    if hist_coop_rate >= 0.75:
        theta = 0.40

    # generosity based on k
    g = clamp((k - 1) / k, g_min, g_max)
    if hist_coop_rate >= 0.75:
        g = min(g + 0.10, g_max)

    # Count my consecutive recent defections up to previous round
    my_consec_D = 0
    for round_idx in (t - 1) downto 1:
        if history[round_idx][me] == D:
            my_consec_D += 1
        else:
            break

    # Quick improvement check: compare cooperation in last W rounds vs the W before that (if available)
    improvement = False
    if (t - 1) >= 2 * W:
        coop_prev_window = count cooperations by opponents in rounds (t - 2W) .. (t - W - 1)
        frac_prev = coop_prev_window / (W * (n - 1))
        if frac > frac_prev:
            improvement = True

    # Decision logic
    if frac >= theta:
        return C
    else:
        # If we have punished for P_max or more rounds, attempt forgiveness with probability g
        if my_consec_D >= P_max:
            with probability g: return C
            else: return D
        # If cooperation has recently improved, return C to reward the recovery
        if improvement:
            return C
        # Otherwise defect, but with small generosity probability attempt cooperation anyway
        with probability g: return C
        else: return D

Rationale and properties
- Starts cooperatively to give others a chance to reciprocate.
- Uses only observed actions (common-knowledge history) and the game parameters (n, r, k).
- Majority-based conditional cooperation (frac >= 0.5) is simple and robust: if most others cooperate, contribute; if most defect, protect yourself.
- Forgiveness (probabilistic cooperation after punishment and brief punishment length P_max) prevents permanent breakdown after single mistakes or short-term exploitation, allowing fast restoration of cooperation.
- Generosity g scales with k: larger public-good multipliers make restoring cooperation more valuable, so the strategy is more willing to risk a short-term loss to regain long-term gains.
- The last-round defection is unavoidable under backward induction; cooperating earlier rounds preserves group welfare while still guarding against persistent defectors.
- The limited punishment length prevents sink states of mutual defection and avoids being exploited by strategies that expect indefinite retaliation.

Edge cases
- Very small r (r=2): behavior: cooperate on t=1, defect on t=2 per rules.
- No prior rounds (t=1): cooperate.
- If all others always defect: this strategy will learn that quickly and mostly defect (protecting individual payoffs).
- If some agents occasionally defect/noise: probabilistic forgiveness and short punish lengths allow recovery.
- If the population is nearly all cooperators, the strategy becomes slightly more trusting (lower theta, higher g) to maximize social payoff.

Tuning notes (for implementers)
- W_default = 5 and P_max = 2 are conservative, practical choices. They can be adjusted (wider windows slow adaptivity but stabilize estimates; longer punishments increase deterrence against persistent defectors but risk permanent breakdown).
- The generosity function g = clamp((k-1)/k, 0.05, 0.50) is simple and ties generosity to the public-good return. Implementers may tune bounds for the tournament environment.

Summary
AFM is a straightforward, history-dependent, and parameter-aware conditional-cooperation rule: open with cooperation, condition cooperation on a recent majority of others cooperating, punish briefly but forgive probabilistically (more forgiveness when the group historically cooperates or k is large), and defect in the final round. It encourages and sustains cooperation when others are willing but protects against sustained exploitation.
'''

description_COOPERATIVE_91 = '''
Strategy name: Conditional Cooperate with Proportional Punishment (CCPP)

Summary (goal)
- Be openly cooperative to try to reach and sustain Pareto‑superior mutual cooperation (each round payoff = k when all cooperate).
- Deter and limit exploitation by defectors using short, proportional punishments.
- Forgive and restore cooperation quickly when others resume cooperating.
- Use only public history (counts of C/D each round), the known parameters (n, k, r), and no private signals.

High-level idea
- Cooperate initially to signal willingness to cooperate.
- In subsequent rounds use the previous round’s observed cooperation rate to decide: if a clear majority cooperated last round, cooperate; if cooperation fell below threshold, defect and impose a short punishment that grows with how many players defected.
- After punishment end, revert to cooperation if recent history shows cooperation returning (forgiveness window).
- Always defect in the final round (no future to enforce reciprocity).

Concrete decision rules

Internal strategy parameters (fixed inside the policy; can be tuned):
- p_threshold = 0.5 (cooperate if at least a majority cooperated last round).
- Lmin = 1 (minimum punishment length in rounds).
- Lmax = 3 (maximum punishment length in rounds).
- F = 3 (forgiveness/lookback window in rounds).
- These are conservative defaults; p_threshold and Lmax can be adjusted using k (see notes below).

State kept by the strategy:
- punishment_counter (integer ≥ 0): remaining rounds to punish. Initially 0.
- history: public history of contributions (we only need counts per round).

Action selection for round t (1-indexed), given known n, k, r and observed history up to round t−1:

1) Last round:
- If t == r: play D (defect). Rationale: no future rounds to punish, so cooperating can be exploited.

2) First round:
- If t == 1: play C (cooperate) and do not start any punishment (punishment_counter = 0). Rationale: give cooperation a chance.

3) Intermediate rounds (2 ≤ t < r):
- If punishment_counter > 0:
  - Play D.
  - Decrement punishment_counter by 1.
  - Continue to next round.
- Else (no active punishment):
  - Let coop_last = number of players who played C in round t−1 (observed); let frac = coop_last / n.
  - If frac ≥ p_threshold:
    - Play C (cooperate).
  - Else (frac < p_threshold):
    - Compute fraction_defectors = 1 - frac.
    - Compute punishment length L as:
        L = ceil( Lmin + fraction_defectors * (Lmax - Lmin) )
      (So: if many defected last round, punish for more rounds; if only one or few defected, punish briefly.)
    - Cap L so that punishment does not extend beyond remaining rounds: L = min(L, r - t).
    - Set punishment_counter = L (we will defect this round and the next L−1 rounds).
    - Play D this round.

4) Forgiveness check (after a punishment run completes):
- Whenever punishment_counter hits 0 (i.e., last punishment round just finished), before deciding the next round:
  - Compute avg_frac = average of (coop counts / n) over the last up to F rounds (use available rounds if fewer than F).
  - If avg_frac ≥ p_threshold: clear punishment and resume cooperation as normal.
  - Else: if avg_frac < p_threshold, immediately start a new punishment with L computed from the most recent round as above.

Pseudocode (compact)

Initialize:
  punishment_counter = 0

For each round t = 1..r:
  if t == r:
    action = D
    continue

  if t == 1:
    action = C
    continue

  if punishment_counter > 0:
    action = D
    punishment_counter -= 1
    if punishment_counter == 0:
      perform forgiveness check (see below) before next round
    continue

  // no active punishment
  coop_last = observed_cooperators_in_round(t-1)
  frac = coop_last / n
  if frac >= p_threshold:
    action = C
  else:
    fraction_defectors = 1 - frac
    L = ceil( Lmin + fraction_defectors * (Lmax - Lmin) )
    L = min(L, r - t)   // do not punish past final round
    punishment_counter = L
    action = D
    // we will decrement the counter at the start of subsequent rounds

Forgiveness check (when punishment_counter reaches 0):
  avg_frac = average of (cooperators / n) over last up to F rounds
  if avg_frac >= p_threshold:
    leave punishment cleared
  else:
    // start a new punishment determined by most recent round
    fraction_defectors = 1 - (cooperators in last round / n)
    L = ceil( Lmin + fraction_defectors * (Lmax - Lmin) )
    L = min(L, remaining_rounds)
    punishment_counter = L

Rationale and properties

- Cooperative by default: starts cooperative and cooperates whenever a clear majority cooperated last round. This encourages mutual cooperation if other strategies reciprocate.
- Proportional punishment: punishment length scales with how widespread defection was. One-off or few accidental defections lead to short punishments; mass defections get longer punishments. This reduces over-reaction to noise while still deterring repeated free-riding.
- Forgiving: after punishment ends, the strategy checks recent cooperation and returns to cooperation if others have improved; this prevents permanent breakdowns from single mistakes.
- Last-round safety: defects in the final round to avoid being exploited when punishment is impossible.
- Simple and local: uses only the observable count of cooperators each round, not any out-of-band coordination or signaling.
- Robust to many opponent types: it punishes persistent defectors, tolerates occasional mistakes, and rewards reciprocal cooperators; works against unconditional defectors, conditional cooperators, tit-for-tat variants, and sophisticated mixed strategies.

Tuning notes (use k and n if desired)
- If k is large (k close to n), mutual cooperation is very valuable; it can be worthwhile to be more forgiving and lower p_threshold (e.g., 0.4) and/or increase Lmax to sustain cooperation more strongly.
- If k is close to 1, mutual cooperation gives only small gains over defection; be stricter (raise p_threshold above 0.5) and keep Lmax small.
- The defaults (p_threshold = 0.5, Lmax = 3, F = 3) are conservative and should perform robustly across many n, k, and opponent behaviors in tournaments.

Why this is appropriate for a tournament
- It invites cooperation (good score against cooperators), penalizes and reduces exploitation (limits gains of defectors), is forgiving so it can re-establish cooperation with conditional cooperators, and avoids suicidal long-term punishments that hurt overall score. It is simple to implement and depends only on the legally available public history and the parameters n, k, r.
'''

description_COOPERATIVE_92 = '''
Strategy name: Reciprocal Forgiver (RF)

Goal
- Achieve and sustain near-universal cooperation when others are willing, but resist and punish sustained free-riding.
- Forgive occasional defections so cooperation can recover.
- Be explicit about endgame so the strategy is internally consistent with known r.

Intuition (short)
- Start by cooperating to signal willingness.
- Track each player's recent cooperation rate. Cooperate when a large fraction of players has been reliably cooperating; otherwise withhold cooperation.
- If there is a clear breakdown of cooperation, enter a short, proportional punishment (collective defection) whose length depends on severity and remaining rounds.
- After punishment, offer a fast, conditional reconciliation (one cooperative round) and resume cooperation if others reciprocate.
- Always defect in the final round (no future to enforce reciprocity).

Parameters derived from game inputs (deterministic functions of n, r, k)
- Window H for recent-history statistics: H = max(2, min(6, floor(r/4))) — a short sliding window for measuring recent reliability.
- Reliability threshold ρ = 0.7 (player considered “reliable” if cooperation rate ≥ ρ in window H).
- Cooperation-majority threshold Q_coop = ceil(0.75 × n) (sufficiently many reliable players).
- Minimal-majority threshold Q_min = ceil(0.5 × n) (used to detect breakdown).
- Base punishment length P_base = max(1, floor(r/10)). Actual punishments are min(P_base × severity, remaining_rounds − 1).
- Forgiveness probe: after punishments, try one cooperative round as a probe.

These choices are constant functions of n and r and do not require tuning to opponents.

Decision rules (high-level)
1. First round (t = 1): Cooperate.
2. Last round (t = r): Defect (no future rounds to enforce cooperation).
3. For rounds 1 < t < r:
   - Maintain for every player i the count C_i = number of times player i cooperated in the last up-to-H rounds (if fewer than H past rounds exist, use available rounds).
   - Define reliability_i = C_i / min(H, t − 1).
   - Let R = number of players with reliability_i ≥ ρ.
   - If currently in a punishment episode: continue defecting until punishment timer expires.
   - Else:
     a) If the previous round had near-universal cooperation (total_cooperators_prev ≥ n − 1) then cooperate.
     b) Else if R ≥ Q_coop then cooperate (many players have been reliably cooperating).
     c) Else if R < Q_min then defect and (if not already punishing) trigger a punishment episode whose length increases with severity (see "Punishment trigger" below).
     d) Else (Q_min ≤ R < Q_coop): be forgiving but cautious — cooperate if either (i) the most recent round’s cooperators increased relative to the prior round, or (ii) we just finished a punishment and are in the single-round probe; otherwise defect.
4. Punishment trigger and length:
   - Trigger a punishment episode when the group shows clear breakdown: total_cooperators_prev < Q_min, or one or more players defected repeatedly (e.g., defected in two or more of last H rounds) while others had cooperated.
   - Severity = max(1, number_of_persistent_defectors) where persistent_defectors = players with defect count ≥ 2 in last H rounds.
   - Punishment length P = min(remaining_rounds − 1, P_base × severity). During punishment the strategy defects each round (collective defection is the punishment).
   - Punishment is collective (you defect); it is short and scaled by severity so you avoid permanently destroying cooperation.
5. Reconciliation (forgiveness):
   - After the punishment timer expires, perform one cooperative probe round (cooperate).
   - If probe round is met by a meaningful return to cooperation (total_cooperators_probe ≥ Q_min or R after the probe ≥ Q_coop), resume normal cooperation rules.
   - If probe is exploited (many still defect), escalate with a longer punishment (increase P by factor 2 but still bounded by remaining_rounds − 1).
6. Tie-breaking / safety:
   - If in doubt, prefer cooperating when many others have been cooperative recently; prefer defecting if cooperation appears unreliable or we are near the end of the game.
   - Always ensure punishments leave at least one round after them (never punish into the last round).

Pseudocode (compact)

Initialize:
  For all players i: C_i = 0
  punishment_timer = 0
  just_probed = false

For each round t = 1..r:
  Observe actions of all players in previous rounds; update C_i using last up-to-H rounds

  If t == 1:
    action = C
    just_probed = false
    continue to next round

  If t == r:
    action = D
    continue

  remaining = r − t + 1  // rounds left including this one

  If punishment_timer > 0:
    action = D
    punishment_timer -= 1
    just_probed = false
    continue

  Compute reliability_i = C_i / min(H, t-1) for each i
  R = number of i with reliability_i ≥ ρ
  total_cooperators_prev = number of cooperators in round t−1

  // Immediate cooperative signal
  If total_cooperators_prev ≥ n − 1:
    action = C
    just_probed = false
    continue

  // Clear cooperation majority
  If R ≥ Q_coop:
    action = C
    just_probed = false
    continue

  // Clear breakdown -> punish
  If R < Q_min:
    severity = max(1, number of players with defect count≥2 in last H rounds)
    P = min(remaining − 1, P_base × severity)
    If P <= 0:  // safety
      action = D
      just_probed = false
    Else:
      punishment_timer = P − 1  // we will defect this round and P−1 further rounds
      action = D
      just_probed = false
    continue

  // Intermediate zone: try to be forgiving but cautious
  // If cooperation is improving or we are in probe phase, cooperate; else defect
  recent_trend = (total_cooperators_prev > number of cooperators in round t−2) if t>=3 else True
  If just_probed:
    // We cooperated one round as probe last time; if others responded, cooperate; else defect
    If total_cooperators_prev ≥ Q_min or R ≥ Q_coop:
      action = C
      just_probed = false
    Else:
      // escalate punishments next
      severity = max(1, number of players with defect count≥2 in last H rounds)
      P = min(remaining − 1, 2 * P_base * severity)
      punishment_timer = P − 1
      action = D
      just_probed = false
  Else If recent_trend:
    action = C
    just_probed = false
  Else:
    // be cautious: defect to signal disapproval
    action = D
    just_probed = false

  // After a punishment expires, on the following round set just_probed = true and cooperate (handled implicitly by above logic:
  // when punishment_timer becomes 0, next iteration falls through intermediate zone, and we should set action = C and just_probed = true.
  // Implementation detail: set just_probed = true when you choose C immediately after punishment_timer reached 0.)

Notes and rationale
- Starting with cooperation signals willingness and enables mutual cooperation with other responsive strategies.
- Using short sliding window H gives sensitivity to recent behavior and prevents being locked into old history; it adapts if opponents change.
- Reliability threshold ρ and majority thresholds Q_coop/Q_min mean we only cooperate when many others are demonstrably reliable; this defends against many defectors while still cooperating when most are good.
- Punishments are collective but short and scaled to the number of persistent defectors; proportional punishments are less likely to destroy cooperation altogether and more likely to deter repeat offenders.
- Explicit reconciliation (a cooperative probe) is crucial: it tests whether cooperation can resume. Immediate and unconditional return to cooperation would be exploitable; never punishing forever reduces incentives to cooperate.
- Last-round defection avoids being exploited in a round where future reciprocity is impossible. (If you want to be even more cooperative in settings where many opponents are known to be “nice” even in last round, you could make last-round behavior conditional; but in a tournament with unknown agents, last-round defection is safer.)

Edge cases
- Very short r (e.g., r = 2): behavior is consistent: cooperate round 1, defect round 2.
- Small n (n = 2): thresholds adapt (Q_coop = ceil(0.75*2)=2, Q_min=1); strategy behaves like a generous conditional reciprocator in pairwise PD-like public good.
- If history is noisy (occasional accidental defection), short punishments and forgiveness allow recovery.
- If opponents never reciprocate (all defectors), RF quickly defects and avoids being repeatedly exploited.
- If a minority of players free-ride while majority reciprocates, RF continues cooperating as long as reliability of majority stays high; punishments target breakdowns only when too many players are unreliable.

Why robust and cooperative
- RF is cooperative-seeking: it begins by cooperating, cooperates whenever a strong majority is reliably cooperative, and uses reconciliation to re-establish cooperation after punishment.
- RF is robust: it defends against exploitation by punishing clear breakdowns; punishments are proportional and limited so RF can still regain cooperation.
- RF depends only on game parameters (n, r, k) to set internal windows and punishment lengths, and only on the public history of actions to make decisions — exactly as required.

Implementation note
- All quantities used (C_i, reliability_i, total_cooperators_prev, punishment_timer) are computable from the publicly observable history and the known parameters n, r, k.
- The strategy is deterministic given these rules (except possible deterministic tie-breaking choices); a probabilistic variant (e.g., cooperate with some probability in intermediate zone) can be added for extra generosity if desired.
'''

description_COOPERATIVE_93 = '''
Name: Adaptive Conditional Cooperator with Proportional Punishment (ACPP)

Intuition (short):
- Signal cooperation by cooperating early.
- Condition future cooperation on recent group cooperation levels (so we reciprocate cooperators and discourage free-riders).
- If the group falls below a cooperation threshold, punish by defecting for a short, proportional number of rounds (so punishments are meaningful but limited).
- Forgive quickly when the group shows restored cooperation.
- Defect in the final round (standard finite-horizon logic).

This strategy depends only on the game parameters (n, r, k) and the observable history of past rounds (who cooperated each round). It is adaptive, aims to sustain mutual cooperation where possible, and is robust to a wide range of opponent behaviors (including stubborn defectors and noisy reciprocators).

Parameters used by the strategy (computed from game inputs):
- W (window length for detecting trends): W = min(5, max(1, floor(r/10))) — a short recent-history window.
- T (cooperation threshold, fraction): T = 0.60 (i.e., require roughly a majority cooperating recently). This is a fixed, simple threshold that biases toward cooperation but demands a clear sign of group reciprocity. (One may tune T in implementations; 0.6 is robust.)
- P_max (maximum punishment length): P_max = min(3, max(1, floor(r/6))) — punishments are short and capped.
- All thresholds that count players are applied to integers via ceil, e.g., required_cooperators = ceil(T * n).

State variables maintained by the strategy:
- punish_counter (integer >= 0): how many rounds of punishment (defection) remain; initially 0.
- history: full observed history of past rounds (actions of all players).

Decision rules (plain language):
1. Last round (t = r): Defect. (No future to gain from cooperating.)
2. If punish_counter > 0:
   - If the most recent observed round (the one immediately before this decision) shows group cooperation recovered (i.e., number of cooperators >= required_cooperators), immediately cancel punishment (set punish_counter = 0) and cooperate this round (reconciliation).
   - Otherwise, decrement punish_counter and defect this round.
3. If not in punishment:
   - Round 1: Cooperate (signal goodwill).
   - For t > 1:
     a. Let s = number of cooperators in the most recent past round (includes all players).
     b. If s >= required_cooperators: Cooperate (rewarding continuing group cooperation).
     c. Otherwise (group cooperation fell below threshold):
        - Compute shortfall = max(0, required_cooperators - s).
        - Set punish_counter = min(P_max, 1 + shortfall). Immediately defect this round (start a proportional, limited punishment).
4. Forgiveness/Reconciliation rule (built into punish logic): if, while punishing, the group returns to meeting the cooperation threshold in any round, stop punishment immediately and cooperate next round.

Additional safety rules to avoid being repeatedly exploited:
- If you find yourself as the sole cooperator (s == 1 including you) for two consecutive observed rounds, switch to persistent defection (i.e., set punish_counter = r — t) until you observe at least required_cooperators in a single round. This prevents being a repeated sucker.
- Punishments are limited by P_max to avoid long mutual retaliation — this makes recovery of cooperation likely against reciprocators and reduces payoff loss if many opponents are spiteful.

Why this works and design rationale:
- Initial cooperation signals openness to mutual gains and can start cooperation with reciprocators.
- Requiring a majority-like threshold (T) avoids being exploited by a few cooperators among many defectors: we only cooperate when we see a clear group reciprocation signal.
- Proportional punishment (punishment length grows with how far cooperation fell short) discourages opportunistic one-off defections while not provoking endless wars of attrition.
- Immediate reconciliation: once the group demonstrates restored cooperation, we stop punishing immediately — this favors rapid return to mutual cooperation and gives others an incentive to repair.
- Limited punishment length and forgiveness reduce the risk of collapse from accidental or strategic miscoordination and make the strategy robust in a diverse tournament.
- Final-round defection is consistent with finite-horizon incentives; we accept the unavoidable last-round behavior but maximize cooperation in earlier rounds.

Pseudocode

Inputs: n, r, k, history (list of past rounds; each round is list of n actions {C or D})
Init:
  W = min(5, max(1, floor(r/10)))
  T = 0.60
  required_cooperators = ceil(T * n)
  P_max = min(3, max(1, floor(r/6)))
  punish_counter = 0

For round t = 1..r do:
  if t == r:
    action = D
    record action and continue
  if punish_counter > 0:
    s = number_of_cooperators_in_round(history.last)   # last observed round
    if s >= required_cooperators:
      punish_counter = 0
      action = C   # reconciliation
    else:
      punish_counter = punish_counter - 1
      action = D
    record action and continue
  # not currently punishing
  if t == 1:
    action = C
    record action and continue
  # t > 1 and punish_counter == 0
  s = number_of_cooperators_in_round(history.last)
  if s >= required_cooperators:
    action = C
  else:
    shortfall = required_cooperators - s
    punish_counter = min(P_max, 1 + shortfall)
    action = D
    # Special-case anti-sucker rule:
    if you_were_cooperator_in_last_two_rounds_and_were_only_cooperator_both_rounds:
      punish_counter = r - t   # move to persistent defection until recovery signal
  record action and continue

Remarks for implementers:
- All comparisons of coop counts are deterministic (no randomness required), but a small randomized element could be added for tie-breaking or to reduce predictability (e.g., when s == required_cooperators you might cooperate with probability 0.95). In tournaments deterministic strategies often fare well, but slight randomization can help against pure exploiters.
- The constants (T = 0.60, P_max up to 3, W small) are chosen to balance generosity and defensiveness. They are conservative and robust across a broad range of n, r, k; they can be tuned depending on tournament meta.
- The strategy uses only public information (past rounds' full actions) and game parameters, per the restrictions.

This strategy is cooperative in spirit (opens with cooperation, rewards group cooperation, and quickly forgives) while robust to defectors (proportional punishments and anti-sucker protection). It adapts to the observed behavior of the group and keeps punishments short so cooperation can recover.
'''

description_COOPERATIVE_94 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Start by cooperating and try to sustain high group cooperation by reciprocating cooperation and punishing defections in a measured, forgiving way.
- Use short memory windows to estimate how cooperative others are, adapt punishment length to severity, allow recovery when others improve, and always defect in the final round.
- Include occasional low-probability probing to detect changes in opponents’ behaviour so the strategy can re-open cooperation with previously defecting populations.

Why this approach
- Public-goods payoffs make one-shot defection individually attractive, so cooperation must be sustained by credible contingent responses. ACC provides a simple, interpretable set of rules that:
  - Reward cooperative groups,
  - Punish defections proportionally (not permanently unless necessary),
  - Forgive and re-establish cooperation when others return to cooperative behaviour,
  - Avoid being persistently exploited by switching to defection when the group is largely uncooperative,
  - Is robust to a wide range of opponent behaviours (no reliance on shared conventions) and to occasional mistakes because of forgiveness and probing.

Notation used below
- n, r, k: game parameters (players, rounds, multiplication factor).
- t: current round (1..r).
- history: full log of previous rounds; for each past round s we know actions a_j,s ∈ {C,D} for each player j and the total cooperators totalC_s.
- self_action_{t-1}: my action in previous round (if t>1).
- totalC_{t-1}: number of cooperators in previous round (including me if I cooperated).
- coop_rate_j(M): fraction of rounds (most recent M) in which player j cooperated (observed).
- P_others(M): average cooperation rate among the other n-1 players over most recent M rounds, expressed as fraction in [0,1].
- punish_until: round index; if t ≤ punish_until I am in a punishment phase and will defect.
- suspect[j]: a running score for each other player that increases with recent defections by j (optional per-player tracking).
- Parameters derived from game parameters below (M, window sizes, thresholds).

Parameter choices (computed at start)
- M = min(5, r) — memory window for short-term statistics (use last M rounds). This keeps responsiveness while being robust to noise. If r is small, M shrinks appropriately.
- L_base = 2 — base punishment length (in rounds). Final punishment length L scales with observed severity (see below).
- suspicion_window = min(10, r) — longer window for detecting persistent defectors.
- probe_rate ε = min(0.05, 1/r) — small probability to probe (cooperate) when otherwise defecting, to test whether others have become cooperative again.
- c_threshold = clamp(0.5 - 0.15 * (k-1)/(n-1), 0.25, 0.75)
  - Interpreted: required recent average cooperation among others to justify cooperating now.
  - Rationale: as k increases (public good more valuable), ACC becomes slightly more willing to cooperate for a given observed cooperation rate; clamp keeps thresholds reasonable.
- suspicion_threshold = 0.20 — if average cooperation among others over suspicion_window is below this, treat the population as largely defecting and switch to safe mode (mostly defect).
- endgame: always defect on round r (last round). Optionally, be cautious in the final 1–2 rounds depending on r; main rule: t == r => D.

Decision rules (natural-language and a compact pseudocode)

Natural-language description
1. First round (t = 1): Cooperate. (Signal willingness to form cooperation.)
2. Last round (t = r): Defect. (Backward-induction endgame.)
3. If currently in a punishment phase (punish_until ≥ t): Defect.
4. Compute recent cooperation of others: P_others = average over last M rounds of (cooperators excluding me)/(n-1).
5. If long-run cooperation among others is very low (average over suspicion_window < suspicion_threshold):
   - Enter safe mode: defect this round and continue defecting until the suspicion-window average rises (or until a rare probe succeeds).
6. If not in punishment and not in safe mode:
   - If P_others ≥ c_threshold: Cooperate. (Reward reciprocation/participation.)
   - Else (P_others < c_threshold): Defect, but with small probability ε cooperate as a probe.
7. Punishment trigger and length:
   - If I cooperated in t-1 and in round t-1 some other players defected (i.e., totalC_{t-1} < n), start a punishment phase:
     - severity = fraction_defectors = (n - totalC_{t-1}) / n.
     - L = L_base + round( (n-1) * severity / 3 )  (so if many defected, punish longer)
     - punish_until = min(r-1, t-1 + L)  (never punish into final round; final round is always D anyway)
     - Immediately defect while in punishment.
   - During punishment, monitor whether cooperation among others (P_others over most recent M) recovers ≥ c_threshold for consecutive recover_window rounds (recover_window = 2). If recovered, clear punishment early (punish_until = 0) and resume cooperation as rules above permit.
8. Forgiveness and re-entry:
   - After punish_until expires, do not permanently lock out cooperation. Resume rule-based behaviour (cooperate if P_others ≥ c_threshold), possibly aided by an occasional probe ε to re-establish cooperation if others have changed.
9. Individual-targeting (optional refinement):
   - Keep suspect[j] scores for each player j; increase suspect[j] when j defects while I cooperated, decrease when j cooperates while I cooperate.
   - If certain players are persistent defectors (suspect[j] high over suspicion_window), treat them as defectors; condition some punishments on their behaviour (e.g., increase severity when persistent defectors are present). This reduces exploitation by persistent free-riders while still allowing group recovery if most players cooperate.

Compact pseudocode

Initialize:
  M = min(5, r)
  suspicion_window = min(10, r)
  L_base = 2
  ε = min(0.05, 1/r)
  c_threshold = clamp(0.5 - 0.15*(k-1)/(n-1), 0.25, 0.75)
  suspicion_threshold = 0.20
  punish_until = 0
  For each j ≠ me: suspect[j] = 0

On round t:
  If t == r:
    return D

  If t == 1:
    return C

  // compute statistics
  P_others_M = average over last M rounds of (cooperators_excluding_me)/(n-1)
  P_others_sus = average over last suspicion_window rounds of (cooperators_excluding_me)/(n-1)
  totalC_last = total cooperators in round t-1
  my_last = my action in t-1

  // update per-player suspicion (optional)
  For each j ≠ me:
    if j defected in t-1: suspect[j] += 1
    else: suspect[j] = max(0, suspect[j] - 0.5)

  // safe mode if group is mostly defecting
  if P_others_sus < suspicion_threshold:
    // safe mode: defect to avoid exploitation; but occasionally probe
    if random() < ε:
      return C  // probe
    else:
      return D

  // if in punishment phase, defect
  if t <= punish_until:
    return D

  // Punishment trigger (start new punishment if I cooperated last round and others defected)
  if my_last == C and totalC_last < n:
    severity = (n - totalC_last) / n    // fraction who defected
    L = L_base + round( (n-1) * severity / 3 )
    punish_until = min(r-1, t-1 + L)
    return D

  // Normal decision rule
  if P_others_M >= c_threshold:
    return C
  else:
    // defect unless probing
    if random() < ε:
      return C
    else:
      return D

Recovery rule (run every round):
  If currently in punishment and P_others_M >= c_threshold for recover_window consecutive rounds:
    punish_until = 0  // forgive and resume cooperation logic

Key design behaviours and rationale
- Start cooperatively: gives others incentive to reciprocate and establishes a cooperative baseline.
- Reward cooperation: if recent group cooperation is above threshold, cooperate to sustain mutually beneficial public good.
- Measured punishment: punish proportionally to how many others defected; short, predictable punishments encourage others to return to cooperation rather than lock into perpetual war.
- Forgiveness and probing: limited punishment and probing allow the strategy to recover from mistakes and to detect changes in opponents’ behaviour; this is crucial in tournaments with diverse strategies and possible stochasticity.
- Safety mode: if the group is persistently uncooperative, ACC switches to defect to avoid being exploited; this prevents giving away payoff to persistent free-riders.
- Final-round defection: in the last round we defect (no future to enforce cooperation); this is necessary against rational opponents—while most tournaments reward cooperation, the last-round defection prevents being exploited on the last move.

Edge cases
- Very small r (2–3): M and suspicion_window shrink automatically. With r=2 ACC cooperates in round 1 and defects in round 2 (standard).
- Noise / mistakes by opponents: short punishments, forgiveness rules and probing enable recovery after mistakes.
- One-shot defectors or occasional freeloaders: short, proportional punishments deter exploitation without collapsing cooperation across the whole tournament.
- Many cooperators but a few defectors: ACC punishes but allows recovery; the punishment is proportional so a single defector causes limited disruption.
- Extremely low k (near 1): cooperation is fragile; c_threshold formula makes ACC stricter (more reluctant) when public-good multiplier is low.
- Extremely high k (close to n): cooperation is more valuable; threshold reduces and ACC is more willing to cooperate.

Parameters tuning guidance
- M, L_base, c_threshold, ε are tunable. For more risk-averse (avoid exploitation) make c_threshold higher and L_base longer; for more cooperative orientation make c_threshold lower and ε larger.
- The chosen defaults aim for robust cooperative behaviour with moderate protection against exploitation.

Summary
ACC is a simple, adaptive conditional-cooperation strategy that:
- Opens with cooperation,
- Cooperates when recent others’ behaviour justifies it,
- Punishes defections in proportion to severity for a short predictable period,
- Forgives and probes to re-start cooperation,
- Avoids being exploited by switching to a safe defection mode when the group is persistently non-cooperative,
- Defects in the final round.

This specification is fully implementable from the game parameters and observed history alone, and should perform robustly against a wide range of opponent behaviours in tournaments.
'''

description_COOPERATIVE_95 = '''
Summary (strategy name: Conditional Limited-Punishment with Forgiveness — CLPF)
- Aim: Promote and sustain near-universal cooperation while avoiding long-run exploitation. Start cooperatively, forgive occasional mistakes, target and limit punishment of persistent defectors, and use finite, bounded punishments so cooperation can be re-established before the endgame. Always defect in the final round (standard finite-horizon reasoning).
- Uses only game parameters (n, r, k) and the observed history of who cooperated each round.

High-level intuition
- Signal cooperation by cooperating on round 1.
- Reward rounds with near-universal cooperation by cooperating next round.
- Treat isolated or one-off defections leniently (forgive with a substantial probability) so we avoid permanent retaliation cycles.
- If defections are repeated or many players defect at once, switch to deterministic short punishments proportional to the observed severity; then return to cooperation (limited punishment).
- Keep per-player reputations to identify persistent defectors and punish them until they demonstrate a short run of cooperative behavior.
- Always defect in round r (last round).

Concrete decision rules

State and derived parameters (computed once from n, r, k)
- W = min(5, r-1) — sliding window for detecting persistence.
- persistent_threshold = ceil(W/2) — defects within W to mark a player persistent.
- consecutive_reform = 2 — number of consecutive Cs required for a previously persistent player to be removed from persistent list.
- max_punish = max(1, floor(r/4)) — an upper bound on punishment length (never punishes for more than this).
- generosity g = clamp( (k-1)/(n-1), 0.1, 0.5 ) — determines forgiveness tendency:
  - clamp(x,a,b) = min(max(x,a),b). This maps the public-good benefit to a forgiveness probability between 0.1 and 0.5. If k is near 1 (public good weak) be less forgiving; if k closer to n (public good strong) be more forgiving.

Runtime state variables (kept throughout play)
- round t (1..r)
- for each player j: recent_defects_j = number of D actions by j in the last W rounds
- persistent_set = set of players marked persistent
- punish_until = 0 (round index until which this strategy will play D as punishment; 0 means not in punishment phase)

Core decision rules (apply each round t)

1) If t == 1:
   - Play C (cooperate). Rationale: open cooperatively to establish good norms.

2) If t == r (final round):
   - Play D. Rationale: last-round defection is dominant.

3) Otherwise (1 < t < r):
   - Update recent_defects_j and persistent_set from history up through round t-1.
     - For any j with recent_defects_j > persistent_threshold, add j to persistent_set.
     - For any j in persistent_set that has cooperated for consecutive_reform consecutive rounds, remove j from persistent_set.

   - If punish_until >= t:
     - We are in an active punishment phase: play D this round.
       (Punishment rounds are deterministic: defect while punish_until not passed.)

   - Else (not currently in punishment phase):
     - If persistent_set is non-empty:
       - Targeted punishment: play D until every persistent player has shown consecutive_reform cooperations.
         - Set punish_until = t + min(max(1, 1 + |persistent_set|), max_punish) - 1.
           (Punishment length equals 1 + number of persistent players, bounded by max_punish.)
         - Play D this round (first punishment round).
       - Rationale: punish persistent defectors but limit punishment length.

     - Else (no persistent defectors):
       - Let m_last = number of cooperators in the immediately previous round (round t-1).
       - If m_last == n (everyone cooperated last round): play C.
         - Rationale: reward full cooperation.
       - Else if m_last >= n-1 (one defector in last round):
         - Forgive small deviations: play C with probability p = g (generosity), otherwise play D.
           - If you choose D this round, optionally set a short punishment: set punish_until = t + 1 (one extra round).
         - Rationale: single defections may be mistakes; generous but not blindly so.
       - Else (m_last <= n-2: two or more defectors last round):
         - Group punishment: set punish_length = min( max(1, 1 + (n - m_last) ), max_punish ).
           (i.e., punish longer the more defectors there were; bounded by max_punish.)
         - Set punish_until = t + punish_length - 1.
         - Play D this round (start of punishment).
         - Rationale: deter coordinated or repeated mass defection.

Notes on randomness
- The only randomized choice is forgiveness after a single defector round (cooperate with probability g). Randomized forgiveness reduces the risk of endless retaliation cycles and reduces exploitation by strategies that try to trigger deterministic punishments.

Pseudocode

Initialize:
 W = min(5, r-1)
 persistent_threshold = ceil(W/2)
 consecutive_reform = 2
 max_punish = max(1, floor(r/4))
 g = clamp((k-1)/(n-1), 0.1, 0.5)
 punish_until = 0
 persistent_set = ∅
 for all j: recent_defects_j = 0

Each round t:
  if t == 1:
    action = C
    record action
    continue

  if t == r:
    action = D
    record action
    continue

  // Update recent_defects_j and persistent_set using history up to round t-1
  for each player j:
    recent_defects_j = number of D by j in rounds max(1,t-W) .. t-1
    if recent_defects_j > persistent_threshold:
      persistent_set.add(j)
    // check if in persistent_set and has reformed
    if j in persistent_set:
      if j cooperated in the last consecutive_reform rounds:
        persistent_set.remove(j)

  if punish_until >= t:
    action = D
    record action
    continue

  if persistent_set not empty:
    punish_length = min( max(1, 1 + size(persistent_set)), max_punish )
    punish_until = t + punish_length - 1
    action = D
    record action
    continue

  // No persistent defectors, look at last round aggregate
  m_last = number of cooperators in round t-1

  if m_last == n:
    action = C
    record action
    continue

  if m_last >= n-1:   // exactly one defector or none (none handled above)
    with probability g:
      action = C
    else:
      action = D
      punish_until = t + 1   // optional one-round punishment if we choose D
    record action
    continue

  // m_last <= n-2 : mass defections
  punish_length = min( max(1, 1 + (n - m_last) ), max_punish )
  punish_until = t + punish_length - 1
  action = D
  record action

Design rationale and robustness
- Cooperative signal: Playing C in round 1 invites cooperation and can yield mutual gain if opponents reciprocate.
- Last-round defection: avoids being exploited in the final stage (standard finite-horizon logic).
- Forgiveness (probabilistic) after an isolated defection avoids long tit-for-tat cycles and re-establishes cooperation after mistakes or one-offs.
- Targeted punishment for persistent defectors focuses deterrence at repeat defectors rather than punishing the whole group indefinitely; this reduces wasted mutual punishment when a single rogue is causing problems.
- Group punishment after mass defection deters coordinated free-riding.
- Punishments are finite and bounded (max_punish) so cooperation can be re-built before the horizon ends — important because unbounded punishments in finite games can cause collapse of cooperation.
- Parameters scale with n, r and k:
  - Forgiveness g increases with k (public good more valuable → more reason to be forgiving to sustain cooperation).
  - Punish lengths scale with severity (# defectors) but are bounded by r to avoid last-round unrecoverable collapse.

Edge cases and detailed behavior
- Very small r: W and max_punish are capped so the strategy behaves sensibly when only a few rounds exist. In extreme small r (r=2), first-round cooperate, second-round defect (as per t==r).
- If opponents always defect, strategy quickly settles to defect (limited-cost), avoiding heavy losses.
- If many opponents are conditional cooperators, the strategy will stabilize on cooperation because it rewards full cooperation and forgives occasional slips.
- If facing exploiters who defect always, the targeted punishment and eventual classification as persistent will make you defect (and thereby avoid being exploited repeatedly), while the strategy still attempts cooperative restart if those players begin cooperating.

Implementation note
- The strategy requires tracking each player's last W actions (publicly observable per assumption). It requires randomization only for the forgiveness decision; that can be implemented with a random number generator seeded per-match for reproducibility.

This strategy balances a cooperative starting stance and clear incentives (limited, proportional punishments and forgiveness) to sustain high levels of cooperation against a wide range of opponent behaviors while preventing unbounded exploitation.
'''

description_COOPERATIVE_96 = '''
Name: C3IF — Collective Conditional Cooperation with Individual Forgiveness

Intuition (short)
- Start by cooperating and try to sustain group cooperation when others reciprocate.
- Use the observed cooperation rate and simple per-player reputations to decide whether continuing to cooperate is likely to be rewarded in future rounds.
- If cooperation collapses, apply a short, proportional punishment (temporary defection) to discourage free-riding, but forgive when cooperation returns. In the last round defect to avoid being exploited with no future leverage.

Design goals
- Cooperative: begins cooperating and attempts to sustain high group cooperation.
- Adaptive: reacts to recent history and per-player behavior.
- Robust: limited, proportional punishment avoids permanent collapse; forgiveness prevents long retaliation spirals; simple rules work for many opponent types.
- Parameterized only by n, r, k and observed history.

Parameters used (derived from r, n)
- Memory window M = min(10, max(1, r-1)) — how many past rounds we use for reputations.
- High cooperation threshold H = 0.70 (fraction of other players cooperating in recent observation that signals a cooperative environment).
- Low cooperation threshold L = 0.40 (below this we consider cooperation to have broken down).
- Trust threshold T_trust = 0.60 (used when deciding to cooperate under borderline group rates).
- Punishment maximum length Pmax = min(3, max(1, floor(r/10))) — maximum rounds we will punish after a breakdown.
- Forgiveness window F = 2 (consecutive rounds of high cooperation required to stop punishment early).

State tracked (updated each round)
- For each opponent j ≠ i: trust_j = fraction of rounds (within last M) in which j cooperated.
- recent_coop_rate_prev = fraction of other players who cooperated in the immediately preceding round (or averaged over last few rounds; implementation should use the most recent round for reactivity).
- punishment_remaining (integer ≥ 0): how many rounds of punishment we still plan to carry out.

Decision rules (natural language then pseudocode)

Natural-language description
1. Round 1: Cooperate.
2. Last round (t = r): Defect (no future incentive to sacrifice private payoff).
3. For rounds 2 ≤ t ≤ r−1:
   a. Update trust_j for each other player using the last M rounds.
   b. Compute recent_coop_rate_prev = (# of other players who cooperated in round t−1) / (n−1).
   c. If currently in punishment (punishment_remaining > 0):
        - Defect this round.
        - Decrease punishment_remaining by 1.
        - If after observing actions this round you see cooperation at rate >= H for F consecutive rounds, set punishment_remaining = 0 (forgive early).
   d. Else (not currently punishing):
        - If recent_coop_rate_prev ≥ H: Cooperate (clear signal that group cooperation is strong).
        - Else if recent_coop_rate_prev ≥ L AND average_j(trust_j) ≥ T_trust: Cooperate (borderline group rate but individuals have decent reputations).
        - Else (recent_coop_rate_prev < L OR reputations too low): Defect this round and set punishment_remaining = min(Pmax, r − t) (initiate a short punishment to signal cost of defection).
4. Forgiveness: If a punishment is in progress, the strategy stops punishing early if the observed cooperation rate reaches ≥ H for F consecutive rounds.

Pseudocode (concise)
- Initialize: For all j ≠ i: trust_j = 0; punishment_remaining = 0.
- For each round t = 1..r:
    if t == 1:
        play C
        continue
    if t == r:
        play D
        continue
    // update trust_j using last M rounds (count cooperations by j / M)
    recent_coop_rate_prev = (# other players cooperated in round t-1) / (n-1)
    avg_trust = average_j≠i(trust_j)
    if punishment_remaining > 0:
        action = D
        // After observing current round actions, update punish/forgive:
        punishment_remaining -= 1
        if we observe cooperation rate >= H for F consecutive rounds:
            punishment_remaining = 0
    else: // not punishing
        if recent_coop_rate_prev >= H:
            action = C
        else if recent_coop_rate_prev >= L and avg_trust >= T_trust:
            action = C
        else:
            action = D
            punishment_remaining = min(Pmax, r - t)
    play action

Why these choices
- Start cooperatively to signal willingness to sustain high-payoff mutual cooperation.
- Use the immediate previous round’s cooperation rate because simultaneous moves and full observability make last-round behavior highly informative and reactive opponents will respond to it.
- Use per-player trust to distinguish a situation where a few volatile defectors caused a small dip vs. one where long-term exploitation is happening.
- Punishment is short and capped (Pmax) so we avoid long mutual-defection traps, but it's costly enough to be a credible deterrent to many strategies.
- Forgiveness (observed high cooperation sustained for a short window) restores cooperation quickly and prevents persistent retaliation.
- Last-round defection avoids being exploited where no future reward/punishment is possible.

Edge cases handled
- r very small: M = min(10, r−1) ensures we don’t require nonexistent history; punishment lengths scale down if r is small.
- n = 2 (repeated PD): trust_j becomes simply the partner’s cooperation frequency; the same conditional logic reduces to a forgiving TFT-like policy.
- If many opponents are always-defectors: recent_coop_rate_prev will stay low and the strategy will defect (minimizing exploitation).
- If many opponents are conditional cooperators: the strategy cooperates when a majority cooperates and thereby helps stabilize full cooperation.
- If a small number of opponents defect occasionally: trust_j lets us keep cooperating if the bulk of players are reliable; isolated defections yield short targeted punishment via group defection signal.
- If an opponent adopts a random strategy: the trust average will reflect randomness; the strategy will cooperate only when group signals are strong enough.

Behavioral summary
- Cooperative by default and seeks to sustain universal cooperation when others reciprocate.
- Punishes breakdowns briefly and proportionally, but forgives when cooperative behavior returns.
- Avoids being exploited in the final round.
- Simple, interpretable rules that are robust to heterogeneous opponent types and suitable for tournament play.

Implementation notes for programmers
- Use the last observed round’s actions for recent_coop_rate_prev for responsiveness; optionally smooth over last 2–3 rounds if opponents are noisy.
- Maintain trust_j as rolling count of cooperations in last M rounds divided by M (or EWMA if preferred).
- Track consecutive-forgiveness-counter to detect F consecutive high-cooperation rounds.
- Tune thresholds (H, L, T_trust, Pmax, M, F) if prior tournament data suggest a different environment; the defaults above are conservative and robust across many settings.
'''

description_COOPERATIVE_97 = '''
Name: Adaptive Conditional Cooperator with Proportional Punishment (ACCP)

High-level idea
- Aim to sustain high group cooperation by starting cooperative, rewarding groups that have cooperated recently, and applying proportional, short punishments when group cooperation drops.  
- Be forgiving (short punishments, quick return to cooperation) so a single round of defections (or stray defectors) does not permanently collapse cooperation.  
- Be deterrent enough (punishments scale with severity and duration) to discourage persistent free-riding.  
- Always defect in the known last round (no future to enforce cooperation).

Parameters used by the strategy (derived from game parameters and fixed defaults)
- n, r, k: game parameters (given).
- W = min(5, r-1) — monitoring window length (how many recent rounds we average).
- p_good = 0.60 — threshold fraction of cooperators that signals healthy cooperation.
- p_tolerate = 0.40 — lower threshold: if cooperation falls below this, we use stronger punishment.
- P_unit = 1 — basic punishment unit (one round).
These numerical values are fixed rules of the strategy (they do not rely on opponents). They are conservative and work across a wide range of n, k, r.

Decision rules (exact conditions when to Cooperate vs Defect)
Let t be the round about to be played (1-based). Let history provide, for each prior round s < t, m_s = number of cooperators observed in round s. Define f_s = m_s / n. Define F = average(f_s) over the last min(W, t-1) rounds (if t = 1 then no history).

1. Last-round rule (endgame)
- If t = r (final round), play D (defect). Rationale: no future to enforce cooperation.

2. First-round and initial cooperation
- If t = 1, play C (cooperate). This signals willingness to cooperate.

3. Normal rounds (1 < t < r)
Compute F (average recent fraction of cooperators) using available history:
- If F ≥ p_good: play C. (Group cooperating: continue cooperating.)
- If p_tolerate ≤ F < p_good: play D for one round (short, forgiving punishment), then return to cooperative mode (monitoring continues). In other words: single-round retaliation to nudge behavior.
- If F < p_tolerate: compute severity S = p_tolerate - F (how far below tolerance). Set punishment length P = min(r - t, P_unit + ceil( (S / p_tolerate) * (r/4) ) ). Then:
    - Defect for the next P rounds (or until end of game).
    - After those P rounds, return to cooperative mode (start monitoring again with fresh window).

Notes:
- If a punishment is in progress, it is carried out irrespective of occasional cooperation during the punishment window; after completion, resume the normal monitoring rules.
- If t is near the end and P would overrun the final round, the P is truncated so we never attempt to punish past r; note the final round remains defect.

Pseudocode (structured)
Initialize state:
- punishment_remaining = 0
- last_punish_end_round = 0

On each round t:
1. If t == r:
     action = D
     return action

2. If punishment_remaining > 0:
     action = D
     punishment_remaining -= 1
     return action

3. If t == 1:
     action = C
     return action

4. Compute W_actual = min(W, t-1)
   Compute F = mean( m_s / n for s in rounds t-W_actual ... t-1 )

5. If F >= p_good:
     action = C
     return action

6. Else if F >= p_tolerate:
     # short, single-round retaliation
     action = D
     # No extended punishment; next round monitoring resumes
     return action

7. Else:  # F < p_tolerate, stronger proportional punishment
     S = p_tolerate - F
     P = min( r - t, P_unit + ceil( (S / p_tolerate) * (r/4) ) )
     punishment_remaining = P - 1   # we'll defect this round and P-1 subsequent rounds
     action = D
     return action

Rationale and explanation of design choices
- Start cooperative to signal willingness to form cooperation clusters and to get good payoffs versus other cooperators.
- Use a short monitoring window (W up to 5 rounds) so the strategy adapts quickly to changing group behavior; this keeps it responsive in tournaments with diverse opponents.
- Two thresholds (p_good and p_tolerate) create three behavioral regimes:
  - High cooperation (F ≥ p_good): reward with continued cooperation.
  - Moderate decline (p_tolerate ≤ F < p_good): issue a single-round retaliation to discourage occasional free-riding but avoid overreaction; quick forgiveness reduces cascades of mutual defection.
  - Severe breakdown (F < p_tolerate): apply a proportional multi-round punishment so that persistent or large-group defection is costly; punishment length scales with severity S and with remaining horizon (via r/4 scaling), so it is substantial enough to deter but not endless (forgiveness resumes after punishment).
- Final-round defection is mandated because there is no future to change incentives; this is honest and predictable to other players.
- The algorithm never requires inter-player communication or coordination beyond observing actions, so it conforms to the game’s information constraints.
- The strategy is robust to many opponent classes: it exploits unconditional cooperators (continues cooperating with them), punishes persistent defectors, tolerates limited noise/experimentation, and is adaptive so it doesn’t lock into perpetual revenge after a single deviation.

Edge cases handled explicitly
- First round: cooperate.
- When less than W rounds of history exist (early rounds), F averages over available rounds.
- Punishments do not extend beyond final round (punishment length truncated).
- If a punishment is in progress and the group quickly returns to cooperative behavior, the punishment still finishes (this increases predictability), then the strategy immediately resumes cooperation if the post-punishment window meets p_good.
- If most players always defect (F stays low), ACCP will defect repeatedly (after proportional punishments) and avoid being exploited continually — it will not chase a sunk cause.

Practical tuning notes (for implementers)
- p_good and p_tolerate can be tuned for specific tournaments. p_good around 0.6–0.75 and p_tolerate around 0.3–0.45 are empirically sensible. Larger W makes the strategy slower to react, smaller W makes it more volatile.
- The punishment length formula uses r/4 so the strategy scales punishments with remaining horizon: in long games punishments can be longer (to keep deterrent strength).
- One can make the strategy slightly more generous by increasing p_tolerate or shortening P_unit; or make it harsher by lowering p_tolerate and increasing the punishment multiplier.

Why this is cooperative
- The strategy’s default is to cooperate and to reinstate cooperation quickly after punishments; it only defects to defend against exploitation and to re-establish incentives for group contribution. Short, proportional punishments combined with forgiveness are designed to stabilize high-cooperation outcomes against a broad spectrum of opponents without assuming shared norms or communication.
'''

description_COOPERATIVE_98 = '''
Name: Firm-but-Fair Conditional Cooperator (FFCC)

Intuition (short)
- Be cooperative and start by cooperating to build a cooperative trajectory.
- Use a short, finite, proportionate punishment window when any defection is observed to deter exploitation.
- Forgive after the punishment window so cooperation can resume.
- Stop attempting to enforce cooperation in the final handful of rounds where future punishments cannot deter profitable one-shot deviation (endgame).

These rules depend only on (n, k, r) and the observed history of C/D each round.

Key computed quantities
- Immediate one-shot gain from defecting against full cooperation:
  G = 1 − k/n  (positive because k < n)
- Per-round cooperative advantage that a future punishment can deny:
  B = k − 1  (positive because k > 1)
- Punishment length (finite, integer):
  P = max(1, ceil( G / B ))  // rounds of defection punishment after observing a defection
- Endgame length L = P  // last L rounds we stop trying to enforce cooperation (defect)

Rationale: If a deviator gains G immediately, denying them cooperative payoffs for P rounds imposes a loss ≈ B × P. Choosing P ≥ G/B makes the punishment able to outweigh the immediate gain (so the threat is a credible deterrent) while keeping punishment finite and forgiving. In a finite game, cooperation cannot be sustained in the final L rounds because remaining future punishment is too short to deter defection — so we defect in those rounds.

Decision rules (natural language)
- First round: Cooperate.
- If r ≤ P: defect every round (the game is too short to credibly deter defection; avoid being exploited by cooperating).
- For rounds t = 1 .. r:
  - If t > r − L (i.e., in the final L rounds): Defect.
  - Else (t ≤ r − L, the enforcement window):
    - Maintain a local punish_timer (integer ≥ 0), initially 0.
    - If punish_timer > 0:
      - Play D (defect) this round and decrement punish_timer by 1.
    - Else (punish_timer == 0):
      - If t == 1: Play C.
      - Else look at actions of all n players in round t−1:
        - If all n players cooperated in round t−1: Play C.
        - If at least one player defected in round t−1: set punish_timer = P − 1 and play D this round
          (i.e., start a P-round punishment including this round).
- After a punishment window ends, return to cooperation unless another defection is observed.

Notes on forgiveness and robustness
- Punishment is finite and immediately forgiven after P rounds; this prevents long permanent breakdowns and allows cooperation to resume if opponents change.
- Punishment triggers on any observed defection to deter free-riding; because punishment length P is small and finite, the strategy is not brittle.
- The strategy only uses observable history (full rounds of C/D) and parameters n,k,r; it does not rely on communication or shared conventions.
- In tournaments where many independent opponents punish defectors, the collective deterrence is stronger; if opponents do not punish, a single-player’s punishment still reduces that deviator’s future gain from cooperating groups they interact with later.

Edge cases and clarifications
- If the computed P = 0 (which can happen numerically only if G = 0), we set P = 1 to maintain a minimal deterrent.
- If r ≤ P, the algorithm defects every round because there is no way to credibly deter profitable one-shot deviation in such a short game.
- Punishment is not cumulative: observing new defections during an ongoing punishment does not extend the timer; it keeps punishment bounded and forgiving. If defections continue after the punishment ends, the strategy will re-start punishment.
- You observe full-action history and may use only the last round’s actions to trigger punishment; one may extend to use a sliding-window or fraction threshold, but the core FFCC above is simple, robust and parameter-based.

Pseudocode

Inputs: n, k, r
Compute:
  G = 1 - k/n
  B = k - 1
  P = max(1, ceil(G / B))
  L = P

State:
  punish_timer = 0

For each round t = 1..r:
  if r <= P:
    action = D
    continue to next round
  if t > r - L:
    action = D
    continue
  if punish_timer > 0:
    action = D
    punish_timer -= 1
    continue
  if t == 1:
    action = C
    continue
  // t > 1 and punish_timer == 0
  observe actions of all players in round t-1
  if (all n players played C in round t-1):
    action = C
  else:
    // at least one defection observed
    action = D
    punish_timer = P - 1  // we've used one round of punishment now

Why this is cooperative
- The strategy starts by cooperating and cooperates as long as the group does. It only defects to punish observed defections and does so for a short, precomputed, proportionate period. After punishment it forgives and returns to cooperation. This approach maximizes the chance of sustained mutual cooperation while making defection unprofitable given the finite punishment window and the known horizon.

Parameter examples
- Example n=6, k=2:
  G = 1 − 2/6 = 2/3, B = 1, P = ceil(2/3) = 1, L = 1.
  => Cooperate until the final round, defect in final round. Any observed defection is punished by defecting in the next round only (1-round punishment) then forgiving.

Implementation note
- The strategy requires only the last-round actions (and local punish_timer), but you can easily expand triggers to use more history (e.g., count defectors over last W rounds) if you implement variants for noise-tolerance. The above FFCC is intentionally simple, parameter-derived, and suited to tournaments where robustness and interpretability matter.
'''

description_COOPERATIVE_99 = '''
Name: Adaptive Conditional Cooperator (ACC)

Goal: sustain high group cooperation when others reciprocate, while avoiding long-term exploitation. The rule set is simple, deterministic, uses only the game parameters (n, k, r) and observed history, and is robust across a wide range of opponent behaviours.

Intuition (short)
- Start by signalling cooperative intent (cooperate in round 1).
- Cooperate when a recent majority (adjusted for k) of players have been cooperating; defect briefly if cooperation falls below threshold (a short, proportional punishment), then forgive if cooperation returns.
- If many players are persistent defectors, switch to permanent defection to avoid sustained exploitation.
- Defect in the final round (no future to incentivize cooperation).

Parameters used by the rule (fixed internally)
- W = min(3, r) : look-back window for recent behaviour (use last up to 3 rounds).
- f_long_min = 0.20 : if long-run average cooperation among others falls below this, switch to permanent defection.
- punishment_length = 1 : punish for one round when threshold violated (keeps punishment short and forgiving).
- f_thresh dynamic: a threshold fraction of players cooperating (see below).

Dynamic cooperation threshold f_thresh
- f_thresh is the fraction of players cooperating (among all players, including self if previous rounds are used) required in recent history to continue cooperating.
- Make f_thresh depend on k and n to reflect how socially valuable contributions are:
  f_base = 0.50 (start from simple majority)
  adjustment = 0.25 * (k - 1) / (n - 1)  (when k is larger, public good is more valuable so be a bit more willing to cooperate even if fewer others cooperate)
  f_thresh = clamp(f_base - adjustment, 0.25, 0.75)
  (clamp(x,a,b) limits x to [a,b].)
Rationale: higher k makes cooperation more socially effective; thus the agent is slightly more tolerant (lower threshold) when k is higher.

Decision rules (plain language)
1. First round (t = 1): Cooperate (C). This signals willingness to form cooperation.

2. Last round (t = r): Defect (D). There is no future to punish or reward, so defect to avoid being exploited.

3. For each intermediate round t (2 ≤ t ≤ r - 1):
   a. Compute recent cooperation fraction among all players over the last W rounds:
      frac_recent = (total cooperations by all players in rounds t-W ... t-1) / (n × W)
   b. Compute long-run cooperation fraction among other players (exclude myself) over rounds 1..t-1:
      frac_long_others = (total cooperations by other players in rounds 1..t-1) / ((n-1) × (t-1))
   c. If frac_long_others < f_long_min:
         Enter permanent defection mode: play D for all remaining rounds (including current) except still D in final round (consistent).
      (This avoids being exploited by a population of persistent defectors.)
   d. Otherwise:
        - If we are currently in a punishment period triggered last round (i.e., we punished last round because frac_recent_previous < f_thresh), then:
            * Cooperate this round if frac_recent (computed in 3a) >= f_thresh (forgiveness if cooperation returns).
            * Else continue to defect (but only up to punishment_length rounds total).
        - If we are not in a punishment period:
            * Cooperate (C) this round if frac_recent >= f_thresh.
            * Otherwise, defect (D) this round and start a 1-round punishment period (punish only one round, then allow recovery).

Notes on simultaneous actions: frac_recent and frac_long_others are computed from observed past rounds only (not from current simultaneous choices).

Pseudocode

Inputs: n, k, r
State maintained from history:
  history[t][i] ∈ {C,D} for rounds t = 1..(current-1) and players i = 1..n
Derived values:
  W = min(3, r)
  f_base = 0.5
  adjustment = 0.25 * (k - 1) / max(1, n - 1)
  f_thresh = clamp(f_base - adjustment, 0.25, 0.75)
  f_long_min = 0.20
  punishment_length = 1

Variables:
  punish_counter = 0  # remaining punishment rounds (0 means not punishing)
  permanent_defect = False

Procedure decide_action(t, my_index):
  if t == 1:
    return C

  if permanent_defect:
    return D

  if t == r:
    return D

  # compute frac_recent over last W rounds (all players)
  start = max(1, t - W)
  rounds_count = t - start
  total_coop_recent = sum_{s = start..t-1} sum_{j=1..n} [history[s][j] == C]
  frac_recent = total_coop_recent / (n * rounds_count)

  # compute long-run cooperation among others
  total_coop_others = sum_{s = 1..t-1} sum_{j=1..n, j != my_index} [history[s][j] == C]
  frac_long_others = total_coop_others / ((n - 1) * (t - 1))

  if frac_long_others < f_long_min:
    permanent_defect = True
    return D

  if punish_counter > 0:
    # we are in punishment; decide whether to stop punishing
    if frac_recent >= f_thresh:
      punish_counter = 0
      return C
    else:
      punish_counter = max(0, punish_counter - 1)
      return D

  # normal (not punishing)
  if frac_recent >= f_thresh:
    return C
  else:
    # start a one-round punishment
    punish_counter = punishment_length
    return D

Implementation notes and tuning
- W = 3 keeps the rule responsive but not jumpy. When r is small (e.g., r ≤ 3) window reduces appropriately.
- punishment_length = 1 (short, forgiving) prevents cascades of defect; experiments can increase to 2 to be stricter.
- f_long_min = 0.20 avoids permanent cooperation in highly defective populations; this number can be tuned upward if you want to be stricter.
- f_thresh depends on k: when k is larger the threshold is reduced a bit (makes the strategy more willing to cooperate even if fewer others cooperate).
- All thresholds are deterministic and simple to compute from history.

Why this is cooperative and robust
- Cooperative: begins by cooperating and continues to cooperate when others reciprocate (recent cooperation >= threshold). This enables high group payoffs if many opponents are reciprocators or conditional cooperators.
- Robust to exploitation: brief punishment deters casual exploitation but is forgiving so one noisy defection does not collapse cooperation. A permanent-defection fallback avoids being milked forever by a population of persistent defectors.
- Adaptive: uses both short-term (recent) and long-term (cumulated) statistics; adjusts willingness to cooperate according to the public good’s effectiveness via k.
- Edge-case safe: always defects in the final round (so it is not exploited when there is no future) and behaves sensibly for small r by shrinking the look-back window.

Examples (intuitive)
- If most players cooperate consistently, frac_recent >= f_thresh and ACC cooperates every round (except final) → high total payoff.
- If a minority defect occasionally, ACC punishes for one round then resumes cooperating if cooperation recovers → tolerates noise.
- If many players never cooperate, frac_long_others falls below f_long_min and ACC switches to permanent defection → prevents ongoing losses.

You can implement exactly the pseudocode above. The particular constants (W, f_long_min, punishment_length, clamp bounds) are conservative defaults; they can be tuned for a particular tournament but the policy logic (start cooperate, conditional cooperation on recent majority, short punishment + forgiveness, permanent defection if population persistently defects, defect final round) should be preserved.
'''

description_COOPERATIVE_100 = '''
Name: Adaptive Conditional Cooperator with Graded Punishment & Forgiveness (ACG)

Short description
- Start by signalling cooperation.
- Track each other player's recent cooperation with an exponential moving average.
- Cooperate when a majority of others are likely to cooperate (with a small generosity margin that increases when the public-good multiplier k is larger).
- If others defect, respond with a short, graded punishment (defect for a small number of rounds proportional to the observed defection severity), then forgive and return to cooperating if they resume cooperation.
- In the final round always defect (no future to influence), and switch to permanent defection only if the group is persistently noncooperative.

Why this strategy
- It is cooperative (starts cooperating, prefers cooperation when others reciprocate).
- It is adaptive (per-player scores, EMA weighting, threshold adapts with k).
- It is robust (only short graded punishments avoid long costly wars; forgiveness allows recovery; per-player tracking isolates persistent defectors).
- It depends only on game parameters (n, r, k) and public history.

Parameters (recommended defaults)
- α (memory weight for EMA): 0.7 (recent actions weighted 30%, past 70%)
- θ0 (base cooperation threshold): 0.50 (requires a simple majority of others to expect cooperation)
- generous_factor (reduces threshold when k is large): = min(0.25, (k-1)/(n-1) * 0.25)
- θ = θ0 - generous_factor  (final threshold; lower if k is larger)
- max_punish: 3 (cap on punishment length)
- permanent_defect_threshold: group EMA G < 0.10 for W_persist rounds (W_persist = 4) → switch to permanent defection
- forgiveness: after punishment ends, resume cooperation immediately if others show cooperation

(Implementers may tune these numerics. The strategy remains the same.)

State to maintain
- For each other player j ≠ i: score S_j ∈ [0,1], initialized at 0.5
- Group-level cooperation estimate G = (1/(n-1)) * Σ_j S_j
- punishment_counter (integer ≥ 0), initially 0
- permanent_defect_flag (bool), initially false

Pseudocode (round t = 1..r)
1. Pre-decision checks:
   - If permanent_defect_flag is true: play D (defect).
   - If t == r: play D (last round: defect).
   - If t == 1: play C (open with cooperation to signal intent).

2. If punishment_counter > 0:
   - action := D
   - punishment_counter := punishment_counter - 1
   - (Proceed to observe the round, then update EMAs; do not start a new punishment until this one finishes.)

3. Otherwise (no active punishment):
   - Compute predicted cooperating others: m_hat := Σ_j S_j  (sum of EMAs over other players)
   - If m_hat >= (n-1) * θ then action := C
     Else action := D

4. Observe the round's actions c_j for all j (including yourself). Update EMAs:
   - For each j ≠ i:
       S_j := α * S_j + (1 - α) * c_j
   - Update G := (1/(n-1)) * Σ_j S_j

5. Decide whether to start a punishment:
   - Let d := number of other players who defected this round (d = Σ_j≠i (1 - c_j)).
   - If you cooperated this round (your action was C) and d > 0:
       - Set punishment_counter := min(max_punish, 1 + d)  (punish for 1 + number of defectors; capped)
   - If you defected this round (either because predicted noncooperation or punishment), do not immediately start a new punishment (avoid escalation). Instead rely on EMA to react.

6. Check persistent noncooperation:
   - Maintain a short memory (e.g., last W_persist rounds) of G. If G < permanent_defect_threshold for W_persist consecutive rounds, set permanent_defect_flag := true.

Notes and rationale for components

- Start cooperatively: Signalling cooperation increases chance that reciprocating strategies will cooperate in subsequent rounds.
- EMA per-player S_j: identifying regular defectors (low S_j) allows you to estimate group composition and target punishment implicitly (since punishment length is proportional to observed defection severity). Per-player tracking also lets you be tolerant of occasional random defections by specific players.
- Threshold θ and generosity: demanding only a simple majority of others (θ ≈ 0.5) prevents you being exploited when the group is mostly defecting; reducing the threshold when k is larger rewards cooperation more readily when the public good returns are higher (k close to n makes cooperation more beneficial collectively).
- Graded punishment (1 + d, capped): punishing for more rounds when more players defect reduces exploitation incentives for multiple defectors while avoiding permanent collapse in cooperation that harsh, indefinite punishments produce.
- Forgiveness: after punishment, resume cooperation if others cooperate; this allows recovery from misunderstandings and avoids long vendettas.
- Final-round defection: rational endgame move (no future leverage). This prevents being exploited on the last round by long-term defectors. (If you absolutely prefer to maximize collective welfare regardless of exploit risk, you could choose to cooperate in final round — but that is easily exploited, and many tournament opponents will exploit consistent final-round cooperators.)
- Permanent defection fallback: if the group is persistently noncooperative, stop wasting contributions and defect permanently to protect own payoff.

Edge cases
- Small r (e.g., r = 2): t=1 cooperate, t=2 defect (last round). This keeps the opening cooperative signal but avoids last-round exploitation.
- 2-player game (n = 2): behaves like a generous tit-for-tat with graded punishments and forgiveness. EMA on one opponent will drive choices. Works robustly against many PD strategies.
- Noise-free vs noisy actions: the EMA and short punishment window make the strategy tolerant of sporadic defections. The EMA smooths isolated defections so occasional errors do not trigger permanent punishment.
- All-cooperators: if all players keep cooperating, S_j → 1 quickly, m_hat ≥ threshold, so you keep cooperating and achieve the efficient outcome.
- All-defectors: you will quickly detect low G and either defect each round or switch to permanent defection, limiting losses.
- Mixed population: The method adapts to the mix — cooperating with reciprocators, punishing defectors proportionally, forgiving when they return to cooperation.

Implementation checklist
- Use reliable floating point for S_j and G.
- Keep a small log (W_persist) for recent G values to check persistence of noncooperation.
- Parameter tuning: increase α (longer memory) when you want to be less reactive; decrease α to respond faster. Increase max_punish to punish more harshly at cost of higher short-term losses and risk of permanent mutual defection.

Summary
This strategy signals cooperation, cooperates when others are likely to reciprocate, responds to defections with short graded punishments (proportional to observed severity) and forgives, and protects itself from persistent exploitation by moving to permanent defection only when necessary. It is parameterized by n, r, k (the threshold θ uses k), and reacts only to public history — exactly what the tournament requires.
'''

description_COOPERATIVE_101 = '''
Summary (one-line): Cooperate initially and whenever recent group reciprocity is strong; respond to sustained drops in group cooperation with short, proportional punishments and quick forgiveness; in the final small “endgame” window reduce cooperation unless near-unanimous cooperation was just observed.

Intuition and goals
- Be cooperative: try to sustain high contributions when others reciprocate.
- Be robust: do not allow sustained exploitation by persistent defectors.
- Be adaptive: respond to the observed fraction of cooperators, to individual histories, and to the finite horizon.
- Be forgiving: punish briefly and proportionally, then probe to re-establish cooperation.

Parameters derived from game inputs (usable defaults)
- n (players), k (multiplication factor), r (rounds) are given.
- Window for short-run history: W = min(5, max(2, r-1)). (Uses up to last 5 rounds but at least 2 if possible.)
- Punishment length: P = min(3, max(1, floor(r/10))) (short proportional punishment).
- Endgame length: L_end = min(3, max(1, floor(r/10))). (Final rounds in which we reduce willingness to cooperate to avoid deep endgame exploitation.)
- Cooperation threshold (previous-round majority test): theta_base = 0.6. If k is relatively large (k > n/2) set theta = 0.5 (be more generous when public-good return is larger); otherwise theta = theta_base.
- Strong-unanimity threshold: high_theta = 0.9.
- Exploitation detector threshold (recent average): exploit_threshold = 0.5.
- Small probe probability during punishment: epsilon = 0.05 (occasionally cooperate to test recovery).
- Target low individual cooperation threshold: low_indiv = 0.25 (for flagging persistent defectors).

State variables your algorithm maintains
- punishment_timer (integer ≥ 0): rounds left to carry out punishment; 0 means not punishing.
- last_probe_round (integer): last round you cooperated as a probe during/after punishment (for controlled forgiveness).
- (Optional for statistics) For each player j keep cooperation counts over last W rounds to compute individual rates.

High-level decision rules (ordered priority)
1. First round (t = 1): Cooperate. (Signal willingness to cooperate.)
2. If punishment_timer > 0: normally defect (carry out punishment). With small probe probability epsilon, cooperate this round to test recovery (but only once per punishment cycle — track last_probe_round to avoid repeated probes).
3. If in the endgame (t > r - L_end):
   - If the previous round showed near-unanimous cooperation among others (fraction ≥ high_theta), cooperate (preserve near-consensus fairness).
   - Otherwise defect (reduce risk of being exploited in final rounds).
4. Otherwise (normal rounds):
   a. Compute f_prev = fraction of other players who cooperated in previous round = (#others who played C in t-1)/(n-1).
   b. Compute group_recent = average fraction of players who cooperated across the last W rounds (or average of others’ cooperation rates).
   c. If f_prev ≥ theta: Cooperate (reciprocate recent cooperation).
   d. Else if group_recent < exploit_threshold OR there exist one or more players whose individual cooperation rate over last W rounds ≤ low_indiv:
        - Start punishment: set punishment_timer = P and defect this round. (This is a short, proportional, and targeted response to sustained low cooperation.)
   e. Else (mild or one-off drop): play probabilistically to be forgiving: cooperate with probability p_coop = max(0.1, f_prev). (This avoids spirals from single mistakes; still penalizes big drops.)
5. After punishment_timer reaches 0: forgive-and-probe. Cooperate for one round (unless you already probed and failed recently) to test return to cooperation. If others respond by increasing cooperation, resume rule 4; otherwise resume punishment cycles as needed.

Pseudocode (concise)
Inputs: n, k, r. Observed history up to round t-1: actions a_j,t' for all players j and rounds t' < t.
Initialize: punishment_timer = 0; last_probe_round = -Inf.

On each round t:
  if t == 1:
    play C; return
  if punishment_timer > 0:
    if last_probe_round < punishment_cycle_start and random() < epsilon:
      play C; last_probe_round = t; return
    else:
      punishment_timer -= 1
      play D; return
  // Endgame check
  if t > r - L_end:
    compute f_prev
    if f_prev >= high_theta:
      play C
    else:
      play D
    return
  // Normal rounds
  compute f_prev = (number of other cooperators in t-1) / (n-1)
  compute group_recent = average cooperation fraction over last W rounds
  compute for each player j their coop_rate_j over last W rounds
  if f_prev >= theta:
    play C; return
  if group_recent < exploit_threshold OR (exists j with coop_rate_j <= low_indiv):
    punishment_timer = P
    punishment_cycle_start = t
    play D; return
  // mild drop: be forgiving but cautious
  p_coop = max(0.1, f_prev)
  if random() < p_coop:
    play C
  else:
    play D

Edge cases explained
- Very short games (r small): W, P, L_end scale down; if r = 2, the strategy cooperates first round and defects second (practical given finite horizon), but in slightly longer games it tries to build cooperation in early rounds.
- n = 2 (repeated PD): reduces to a generous tit-for-tat-like policy (cooperate initially, reciprocate cooperation, forgive occasional defections via probabilistic cooperation, brief punishments).
- k close to n (higher group returns): theta is lowered (be more generous); this increases willingness to cooperate when public good is especially valuable.
- Noise / mistakes by others: single or occasional defections do not trigger long punishments: we only punish when recent group cooperation is low or specific players are persistent defectors.
- Persistent defectors: the strategy triggers repeated short punishments targeted by the group_recent and low_indiv tests; punishments are short and followed by probes, increasing chance of recovery while making sustained exploitation unattractive.
- Cooperation collapse: if everyone defects, this strategy reduces losses by defecting; but it will probe occasionally and, if others reciprocate, re-establish cooperation.

Why this is cooperative and robust
- Cooperative: opens with cooperation and reciprocates when the group shows reciprocity.
- Proportional punishment: short punishments avoid endless mutual retaliation and reduce the incentive to exploit (they impose a cost on defectors while not crippling later cooperation).
- Forgiving: probabilistic cooperation and immediate probes after punishments let the group return to cooperation quickly if other players are willing.
- Adaptive to parameters: thresholds change slightly with k and windows scale with r; punishments are proportional to r, so the strategy does not over-commit near the end.
- Reasonable computational burden: only needs counts of cooperations in recent rounds.

Tuning suggestions for tournament implementation
- If tournament opponents are very exploitative, increase P slightly or lower exploit_threshold so punishment is triggered earlier.
- If opponents are error-prone (noise), increase W and be more forgiving (decrease exploit_threshold) and increase epsilon to probe more often.
- If public-good multiplier k is large, consider cooperating more often (lower theta), because the return to group cooperation increases the expected group payoff.

Final note
No strategy can guarantee cooperation against fully rational backward-inducing opponents in a known, finite-horizon game, but this strategy maximizes the chance to sustain high cooperation in realistic tournaments: it signals cooperation, rewards reciprocity, punishes sustained exploitation in a calibrated way, and forgives quickly to recover cooperation when others reciprocate.
'''

description_COOPERATIVE_102 = '''
Name: Adaptive Generous Tit‑for‑Tat with Proportional Punishment (AGTTPP)

Short description
- Start friendly (cooperate) and try to sustain mutually‑beneficial public‑good contributions.
- Be tolerant of a small number of defections (tolerance grows with k), punish defections in proportion to how many players defected, and forgive quickly so cooperation can recover.
- Protect against persistent free‑riding by switching to a short defensive phase when average cooperation among others is low.
- Make a conservative last‑round choice: cooperate only if recent history strongly supports it.

All decisions depend only on (n, r, k) and the history of observed actions; no communication or assumptions about others’ norms.

Parameters computed from game parameters
- τ (tolerance): τ = floor(k − 1). (If k < 2, τ = 0.) Intuition: higher k makes cooperation more valuable and worth tolerating small defection noise.
- min_majority := ceil(n/2).
- max_punish := min(5, r − 1). (Caps punishment length so retaliation is finite and returns to cooperation quickly.)
- lookback_L := min(5, r − 1). (Window for computing recent average cooperation.)
- protection_threshold := 0.5. (If average cooperation among others in lookback_L < 0.5, enter protective mode.)

State variables (maintained between rounds)
- punish_timer (integer ≥ 0), initially 0. While punish_timer > 0 the strategy defects and decrements punish_timer each round.
- protective_mode (boolean), initially false. When true, the strategy defects every round until condition to leave is met.

High‑level decision rules
1. Round 1:
   - Cooperate.

2. For each subsequent round t = 2..r:
   - Observe previous rounds’ actions of all players. Let for round s:
       m_s = total cooperators in round s (including self),
       d_s = total defectors in round s = n − m_s.
     Let d_others_{s} = number of defectors among others in round s = d_s − (1 if you defected in s else 0).
     Let avg_coop_others = average fraction of others cooperating over the last lookback_L rounds (if fewer than lookback_L have occurred, use all available past rounds).
   - Update protective_mode:
       protective_mode := (avg_coop_others < protection_threshold).
   - If protect ive_mode is true:
       - Defect this round. Recompute protective_mode next round from updated history.
       - (Rationale: persistent low cooperation suggests exploitation; protect own payoff until group behavior improves.)
   - Else if punish_timer > 0:
       - Defect this round; set punish_timer := punish_timer − 1.
       - (When punish_timer reaches 0 you return to normal checks next round.)
   - Else if t == r (last round):
       - Cooperate this (final) round only if both:
            a) In round t − 1 at least min_majority players cooperated (m_{t−1} ≥ min_majority),
            b) There were no “large” defections in round t − 1: d_{t−1} − (1 if you defected in t − 1 else 0) ≤ τ.
         Otherwise defect on the last round.
       - (Rationale: no future to punish, so only cooperate in the last round if strong recent cooperation exists.)
   - Else (normal round, not in protection and not currently punishing):
       - Let d_others_last := d_others_{t−1} (defectors among others last round).
       - If d_others_last ≤ τ:
           - Cooperate.
           - (Rationale: recent defections are within tolerance; reward cooperation to maintain group success.)
       - Else (d_others_last > τ):
           - Set punish_timer := min( max_punish, 1 + (d_others_last − τ) ).
             (This is a short proportional punishment: more defectors → slightly longer punishment, but capped.)
           - Defect this round (the first round of punishment).
           - (After punish_timer rounds finish, the strategy returns to cooperating if others resume cooperating.)

Edge‑cases and clarifications
- If τ ≥ n − 1 (happens only if k − 1 ≥ n − 1), then τ = n − 1 and the rule reduces to always cooperate (no tolerance limit). But the game assumes k < n so τ ≤ n − 2 in usual parameter ranges.
- punish_timer is capped by max_punish so retaliation cannot cascade indefinitely.
- protective_mode prevents repeated being exploited by strategies that never reciprocate. It is re‑evaluated each round from recent history; when avg_coop_others recovers above 0.5 the strategy returns to normal operation.
- No randomness is required; policy is deterministic given history. Randomization could be added (e.g., sometimes cooperating in punishment to encourage reconciliation), but determinism keeps the strategy transparent in tournaments.
- If r is small, max_punish will be small (≥1) so we do not try to sustain long punishments that would waste remaining rounds.

Pseudocode (compact)
- initialize punish_timer := 0; protective_mode := false
- compute τ := floor(k − 1), min_majority := ceil(n/2), max_punish := min(5, r − 1), lookback_L := min(5, r − 1)
- for each round t = 1..r:
    if t == 1:
        play C
        continue
    compute m_{t−1} (cooperators last round), d_{t−1} = n − m_{t−1}
    compute d_others_last = d_{t−1} − (1 if I played D in t−1 else 0)
    compute avg_coop_others over last min(lookback_L, t−1) rounds
    protective_mode := (avg_coop_others < 0.5)
    if protective_mode:
        play D
        continue
    if punish_timer > 0:
        play D
        punish_timer := punish_timer − 1
        continue
    if t == r:
        if m_{t−1} ≥ min_majority and d_others_last ≤ τ:
            play C
        else:
            play D
        continue
    if d_others_last ≤ τ:
        play C
    else:
        punish_timer := min(max_punish, 1 + (d_others_last − τ))
        play D
        punish_timer := punish_timer − 1   (we count this round as first punished round)

Why this design is cooperative and robust
- Cooperative: the default is to cooperate whenever recent defections are few; that helps establish and sustain mutual cooperation with other conditional cooperators.
- Forgiving: limited punishment (short punish_timer) and a tolerance τ avoid collapse from accidental or occasional defections; the strategy returns to cooperation quickly to regain high group returns.
- Proportional punishment: punishment length increases with number of defectors, so lone or small defection is punished lightly while mass defections lead to stronger response — this better targets free‑riders without overreacting.
- Adaptive to k: tolerance τ grows with (k − 1). When cooperation is more valuable (larger k), be more forgiving to preserve cooperation; when k is small (cooperation fragile), be stricter.
- Protective_mode prevents long exploitation by persistent defectors: if recent cooperation among others is low, switch to defecting until evidence of recovery.
- Last round: conservative treatment of last round avoids being exploited when no future punishment is possible while allowing end‑game cooperation if the recent record is strongly cooperative.

Implementation notes
- The strategy only needs to track own past actions and counts of cooperators each round (m_s). Memory footprint is O(lookback_L) for averages and a couple of integers for timers.
- All thresholds and caps are expressed in closed form from (n, r, k) so the strategy relies only on game parameters and observed history.

This strategy balances a cooperative disposition with measured retaliation and forgiveness. It is designed to reward and stabilize mutual cooperation with other reciprocating agents while limiting losses from persistent defectors — a pragmatic approach for tournament play in an N‑player public goods environment.
'''

description_COOPERATIVE_103 = '''
Summary (one-line): Start cooperatively to signal goodwill, use a majority-based, forgiving tit‑for‑tat on the observed group cooperation rate to sustain mutual cooperation, punish briefly when group cooperation falls, escalate only if defections persist, and switch to safe defection in the final rounds when there is not enough horizon left to recover the cost of cooperating.

Rationale (why this works)
- The single-round incentive always favors D (because k/n < 1), so cooperation must be sustained by future rewards. The strategy therefore:
  - Signals cooperation early (to attract reciprocators),
  - Conditions on observed group behavior (majority rule) to avoid being exploited by lone cooperators,
  - Uses short, escalating punishments so defectors are discouraged but cooperation can be reestablished after mistakes,
  - Adopts an endgame cutoff computed from k so it does not waste cooperation when there are too few rounds left to recoup the cost.
- The algorithm depends only on game parameters (n, r, k) and public history (past contributions by all players) and is robust to a wide variety of opponent behaviors (persistent defectors, stochastic cooperators, conditional cooperators).

Key derived quantity (endgame cutoff)
- Cooperating costs you 1 in the current round but if full mutual cooperation is sustained thereafter it yields an extra (k − 1) per future round for you. To break even you need at least T* future rounds satisfying
  T* × (k − 1) > 1.
- Define critical_horizon H = ceil(1 / (k − 1)).
- If the number of rounds remaining ≤ H, there is insufficient horizon to reliably recover the immediate cost of cooperating, so the strategy switches to safe defection in those last rounds.

Concrete parameter choices (robust defaults)
- Grace rounds G = min(3, r) — cooperate unconditionally at the start (signal).
- Majority threshold theta = 0.50 — treat a round as “good” if at least half the players cooperated that round.
- Forgiveness threshold theta_forgive = 0.40 — allow re-entry to cooperation when cooperation improves even if not yet majority.
- Initial punishment length P_init = 1 (one-round punishment).
- Max punishment P_max = 3 (do not punish indefinitely; escalate only a little to deter persistent defectors but remain forgiving).
- Escalation rule: if low-cooperation rounds happen Q = 2 times consecutively, increase punishment length by +1 up to P_max.
(These numeric choices are conservative and can be tuned; they are fully determined from n,r,k and do not assume anything about opponents.)

Decision rules (natural language)
1. First round(s):
   - If t ≤ G, play C. (Give a short, unconditional cooperative signal.)
2. Endgame:
   - Let rem = r − t + 1 be rounds remaining including current t. If rem ≤ H then play D (protect against last-round exploitation).
3. After the opening and before the endgame:
   - Look at the immediately preceding round (t − 1) and compute f = (number of cooperators in round t−1) / n. (You can use all players’ contributions; you do not need to single out individuals.)
   - If you are currently in a punishment phase (a punishment counter > 0), play D and decrement the punishment counter by 1.
   - Else if f ≥ theta (majority cooperated last round), play C.
   - Else (f < theta): initiate punishment:
     a. Set punishment counter = current_punishment_length (start with P_init or the currently escalated length).
     b. Play D this round (first round of punishment).
   - Escalation/forgiveness:
     a. If there have been Q consecutive rounds where f < theta, increase current_punishment_length := min(P_max, current_punishment_length + 1).
     b. If f ≥ theta_forgive (cooperation has improved to the forgiveness threshold), reset current_punishment_length := P_init (forgive and return to standard punishments).
4. Persistent exploitation:
   - If a long run shows cooperation fraction persistently below theta and escalation has reached P_max, continue to defect until you observe f ≥ theta_forgive for one round, then return to cooperation (to give others a chance to re-establish cooperation).

Pseudocode (concise)

Inputs: n, r, k, history of rounds 1..t−1 (each round list of n actions C/D)
Precomputed:
  H = ceil(1 / (k − 1))    # endgame cutoff
  G = min(3, r)            # opening grace rounds
  theta = 0.50
  theta_forgive = 0.40
  P_init = 1
  P_max = 3
  Q = 2

State:
  current_punishment_length := P_init
  punish_counter := 0
  consecutive_low_coop := 0

Procedure choose_action(t):
  rem := r − t + 1
  if rem ≤ H:
    return D

  if t ≤ G:
    return C

  if punish_counter > 0:
    punish_counter := punish_counter − 1
    return D

  # compute last-round cooperation fraction
  if t == 1:
    last_f := 0   # not used because t ≤ G handled above
  else:
    last_cooperators := number of C in round (t − 1)
    last_f := last_cooperators / n

  if last_f ≥ theta:
    consecutive_low_coop := 0
    current_punishment_length := P_init    # reward with baseline punish length
    return C
  else:
    consecutive_low_coop := consecutive_low_coop + 1
    # escalate if low cooperation persisted
    if consecutive_low_coop ≥ Q:
      current_punishment_length := min(P_max, current_punishment_length + 1)
    # start punishment
    punish_counter := current_punishment_length − 1   # we spend one D now; remaining rounds to count
    return D

On observing after each round:
  - Update consecutive_low_coop/reset and current_punishment_length if last_f ≥ theta_forgive:
    if last_f ≥ theta_forgive:
      consecutive_low_coop := 0
      current_punishment_length := P_init

Notes and variants
- If you expect many noisy mistakes from other players, you can raise theta_forgive toward theta (more forgiving) or shorten punishments. If opponents are highly exploitative, you can lower theta to require stricter group cooperation before risking a C.
- The scheme uses group-level statistics (fraction of cooperators) rather than per-player scores. This avoids over-fitting to identities in tournaments where many independent strategies interact and where targeting individuals is less effective than reacting to overall group behavior.
- The policy is deliberately forgiving (short punishments, ability to escalate slowly and to forgive when cooperation improves) so it does not cascade into permanent mutual defection after a single mistake.
- The endgame cutoff H is decisive: it prevents throwing away payoff in the last rounds. If some opponents play cooperatively even in last rounds, they may still be exploited if this algorithm defects; this is a necessary trade-off under common-information finite-horizon competitive settings.

Example behaviors
- If most players reciprocate cooperation, the strategy converges quickly to persistent cooperation (after the opening rounds) and earns close to the mutual-cooperation payoff k per round.
- If a majority defects, the strategy will stop cooperating after the first low-cooperation round, punish briefly, and if the group does not recover it will keep defecting (protecting itself).
- If defections are occasional/noisy, the forgiving thresholds and short punishments allow return to cooperation quickly.

Why this is cooperative-minded yet robust
- Cooperative-minded: begins by signaling cooperation and rewards majority cooperation; aims to sustain mutual cooperation when reciprocators are present.
- Robust: defection when group cooperation is low (prevents exploitation), brief punishments (do not drive permanent collapse after noise), escalation deters persistent defectors, and the endgame cutoff avoids exploitable cooperation when there is no horizon to recoup the cost.

You can tune theta, P_init, P_max and G for tournaments with different noise or opponent mixes. The provided defaults are conservative, simple, and implementable directly from n, r, k and the public history.
'''

description_COOPERATIVE_104 = '''
Strategy name: Conditional Proportional Cooperate (CPC) — "Cooperate, Forgive, and Proportionally Punish"

Intuition (short)
- Start by trying to cooperate and sustain high group cooperation when others reciprocate.
- Measure recent group cooperation (not just the last round) and respond proportionally: reward widespread cooperation, punish widespread defection, and use probabilistic forgiveness in the middle ground.
- Defect in the final round (standard endgame logic) and be progressively more cautious near the end.
- Use short, bounded memory and simple thresholds so the rule is robust, explainable and forgiving (rapid recovery after mistakes).

Parameters (implementation defaults; all are tunable)
- w: recent window length for observations = min(5, r-1) (use last w rounds; if r-1 < 1, then w = 1)
- T_high: upper cooperation threshold = 0.60
- T_low: lower cooperation threshold = 0.40
- L_safe: endgame caution window = min(2, r-1) (number of rounds before last round where we become more cautious)
- p_floor: minimum cooperation probability in ambiguous zone = 0.05 (avoid complete randomness)
- p_ceil: maximum cooperation probability in ambiguous zone = 0.95
(Implementer can tune these to the tournament environment.)

Decision rules (natural language)
1. First round: Cooperate. Start by offering cooperation.
2. Final round (t = r): Defect. There is no future to sustain reciprocity.
3. Near-end rounds (t > r - L_safe): be more conservative. Cooperate only if recent cooperation among others is very high (≥ 0.9). Otherwise defect (prevents being exploited in the endgame).
4. For other rounds (1 < t < r - L_safe):
   - Compute avg_coop_recent = fraction of other players who contributed (played C) averaged over the last w rounds. This is: total C by others in last w rounds / ((n-1) * w).
   - If avg_coop_recent ≥ T_high: cooperate (reward stable cooperation).
   - If avg_coop_recent ≤ T_low: defect (punish sustained low cooperation).
   - If T_low < avg_coop_recent < T_high: cooperate with probability p that grows linearly with avg_coop_recent:
       p = p_floor + (p_ceil - p_floor) * (avg_coop_recent - T_low)/(T_high - T_low).
     (This is probabilistic forgiveness: it prevents endless cycles of retaliation and recovers cooperation after noise.)
5. Extra safety: if the previous round had universal defection among others (i.e., all other players defected last round), defect this round as well (short targeted punishment / avoid unilateral cooperation into collapse).

Pseudocode

Inputs: n, r, k, history of actions c_j,t for all players j, rounds t=1..T-1.
Defaults: w = min(5, r-1), T_high = 0.60, T_low = 0.40, L_safe = min(2, r-1), p_floor = 0.05, p_ceil = 0.95.

Function decide_action(t, history):
  if t == 1:
    return C
  if t == r:
    return D
  if t > r - L_safe:
    // endgame caution
    avg_coop_recent = compute_avg_coop_others(last w rounds)
    if avg_coop_recent >= 0.90:
      return C
    else:
      return D
  // general case
  avg_coop_recent = compute_avg_coop_others(last w rounds)
  if avg_coop_recent >= T_high:
    return C
  if avg_coop_recent <= T_low:
    return D
  // ambiguous zone: probabilistic cooperation
  p = p_floor + (p_ceil - p_floor) * (avg_coop_recent - T_low) / (T_high - T_low)
  // Safety override: if everyone else defected in previous round, defect now
  if (cooperators_among_others_in_round(t-1) == 0):
    return D
  // sample random uniform u in [0,1]
  if u <= p:
    return C
  else:
    return D

Helper: compute_avg_coop_others(last w rounds)
  Sum cooperations of all players j != me over the last up-to-w rounds available
  avg = sum / ((n-1) * w_actual)
  return avg

Why this is cooperative and robust
- Cooperative: starts with cooperation and rewards groups that reliably reciprocate (avg_coop_recent ≥ T_high gives unconditional cooperation). This makes it work toward the Pareto-superior all-C equilibrium when other adaptive cooperators are present.
- Punishing: when others show sustained low cooperation, the strategy defects to avoid being exploited and to pressure others to increase cooperation.
- Forgiving: probabilistic cooperation in the ambiguous zone lets the population recover from mistakes/noise and prevents permanent breakdowns (unlike pure Grim Trigger).
- Proportional: responses scale with observed group cooperation rather than punishing severely for single mistakes; this is suited to an n-player public goods context where individual noise or sporadic defection is common.
- Endgame-aware: defects in the final round and is cautious shortly before the end to limit exploitation from last-round unraveling.

Notes on parameter choices and extensions
- Window w should be short relative to r (so the strategy reacts but not too noisily). If r is small, w should be <= r-1.
- T_high and T_low set how demanding you are for cooperation. Increasing T_high makes you more demanding; increasing T_low makes you more tolerant of low cooperation.
- p_floor and p_ceil keep probabilistic behavior bounded to avoid deterministic cycles and to allow occasional generosity.
- Optional improvement: track individual defectors and escalate temporary targeted punishment against persistent defectors (e.g., if a player j defected in ≥ q of last w rounds, reduce your cooperation probability by a term proportional to j's share of recent defections). This requires more bookkeeping but can improve exploitation resistance.
- If noise in action implementation is expected, increase forgiveness (lower T_high and raise p_floor).

Final remark
This strategy adheres to the cooperative mindset: it aims to achieve and maintain high group cooperation where feasible, protects itself from sustained exploitation, forgives to recover from mistakes, and is straightforward to implement from the public history and game parameters alone.
'''

description_COOPERATIVE_105 = '''
Summary (one-line): Start by cooperating, reciprocate group cooperation using a smoothed estimate of others’ contribution rates, punish persistent or large-scale defection with limited proportional defections, forgive quickly when cooperation returns, and defect in the final round to avoid endgame exploitation.

Rationale
- The game rewards collective cooperation but each round has an individual temptation to defect. To sustain cooperation against selfish opponents we must (a) signal willingness to cooperate, (b) reward cooperation by returning cooperation, (c) punish persistent exploitation to deter it, and (d) forgive so cooperation can be re-established after mistakes. Because the horizon is finite, we defect in the final round (no future to incentivize reciprocity). The decision rule uses only the game parameters (n, k, r) and the observed history.

High-level decision rules
1. First round: Cooperate (to signal cooperative intent).
2. Last round (t = r): Defect (endgame — no future benefit to sustain cooperation).
3. Intermediate rounds (1 < t < r): Compute a smoothed estimate of how much the group (other players) has been cooperating recently; if that estimate exceeds a dynamic cooperation threshold, cooperate; otherwise defect and enter a limited, proportional punishment phase. Allow early exit from punishment (forgiveness) if cooperation recovers.

Key internal machinery (keeps the behavior adaptive and robust)
- EWMA cooperation estimate s: exponentially weighted moving average of fraction of other players who cooperated in recent rounds. This smooths noise and detects trends.
- Dynamic threshold theta(t): base threshold (depends weakly on k) that becomes stricter as t approaches r (to account for shrinking future incentives).
- Punishment counter P: when group cooperation falls below theta(t), defect for a limited number of rounds proportional to the shortfall; but if group cooperation recovers above a forgiveness level, stop punishing early.

Concrete pseudocode (readable-style)

Parameters (internal to the strategy; depend on n,k,r):
- alpha (EWMA weight): 0.5 (can be tuned between 0.3–0.7)
- base_theta = clamp(0.3, 0.5 - 0.2*(k-1)/(n-1), 0.7)
  - (This lowers the baseline threshold slightly when k is larger, since high k makes cooperation more valuable.)
- max_threshold_increase = 0.3
- forgiveness_level = 0.75  // fraction of others cooperating that ends punishment early
- max_punishment = max(1, floor(r/6))  // cap on consecutive punishment rounds
- scale_for_punishment = r/2  // converts shortfall into punishment length

State variables:
- s (EWMA estimate of others’ cooperation fraction), initialized undefined until after round 1
- punish_counter = 0

At each round t = 1..r:
1. If t == 1:
     action = C
     (after the round, observe others and initialize s)
2. Else if t == r:
     action = D  // final round: defect
3. Else (1 < t < r):
     Let recent_frac = (number of other players who cooperated in round t-1) / (n-1)
     If s undefined: s = recent_frac
     Else: s = alpha * recent_frac + (1 - alpha) * s

     // dynamic threshold increases toward the end to reflect shortening horizon
     theta = base_theta + max_threshold_increase * (t-1) / (r-1)
     theta = min(theta, 0.95)

     If punish_counter > 0:
         // currently punishing
         If recent_frac >= forgiveness_level:
             punish_counter = 0
             action = C  // forgive and resume cooperating
         Else:
             punish_counter = punish_counter - 1
             action = D
     Else:
         // not currently punishing
         If s >= theta:
             action = C
         Else:
             // start a punishment of length proportional to how far below threshold we are
             shortfall = max(0, theta - s)  // in [0,theta]
             punish_length = 1 + floor(shortfall * scale_for_punishment)
             punish_length = min(punish_length, max_punishment)
             punish_counter = punish_length
             action = D

After each round the algorithm updates s and punish_counter as above using observed actions.

Behavioral interpretation and properties
- Starts cooperatively to solicit cooperation from others.
- Uses a smoothed, recent-group-cooperation estimate rather than one-shot comparisons; this produces robustness to occasional mistakes or exploratory defections.
- Punishment is limited (bounded number of rounds) and proportional to the measured severity of defection (shortfall). This avoids permanent grudges which lock the population into mutual defection.
- Immediate forgiveness rule: if during punishment the group returns to a high cooperation level (forgiveness_level), the strategy resumes cooperating immediately.
- The dynamic threshold theta grows toward 1 as the game advances, making the agent less trusting near the end (reduces vulnerability to endgame exploitation).
- Defects in the final round to avoid being exploited with no future leverage.

Edge cases
- r = 2: t=1 cooperate (signal), t=2 defect (final-round defection).
- Very small n (n=2): recent_frac uses (n-1)=1 so recent_frac is 0 or 1 — EWMA works.
- Very small r: parameters like max_punishment are bounded (min 1).
- If opponents are mostly cooperative, s will stay high and the strategy cooperates almost every non-final round, yielding high mutual payoffs.
- If opponents are mostly defectors, the strategy will defect after brief observation to avoid repeated exploitation.
- If there are intermittent defectors (noise), the EWMA + forgiveness avoids overreacting to single mistakes while still punishing sustained defection.

Why this is cooperative-minded but robust
- Cooperative-minded: starts with cooperation and rewards cooperative groups by cooperating in subsequent rounds; aims to restore cooperation rather than impose unforgiving punishments.
- Robust: does not rely on a single participant or coordination; punishes when group cooperation declines enough to indicate exploitation, but punishes only briefly and proportional to the deviation so as not to be exploited long-term or to cause runaway punishment.
- Adaptive: uses smoothed history and dynamic thresholds that depend on k and remaining rounds, so behavior adapts to both environment (public-good multiplier) and the time horizon.

Tuning notes (implementation guidance)
- alpha (EWMA) trades responsiveness vs noise tolerance. Use alpha ~ 0.4–0.6 for typical tournaments.
- base_theta around 0.4–0.6 is a reasonable cooperative baseline; lower it when k is large (cooperation is more valuable).
- forgiveness_level can be 0.7–0.85: higher means require stronger group recovery before forgiving.
- max_punishment should be a small fraction of r (e.g., r/6) so punishment is meaningful but not permanent.

Example scenarios (qualitative)
- All others cooperate each round: s stays near 1, theta < 1 → the strategy cooperates every round except last → high mutual payoff.
- A single opponent defects once (noise): recent_frac drops briefly, EWMA reduces less, punish may be zero or very short, forgiveness recovers and cooperation resumes.
- Multiple opponents defect consistently: s falls below theta, the strategy defects for a limited number of rounds to avoid exploitation; if opponents resume cooperation the strategy returns to cooperating.

This recipe is ready to be implemented as an algorithm: it uses only (n,k,r) and recorded history of past rounds (who cooperated in each round) and does not require communication or shared conventions.
'''

description_COOPERATIVE_106 = '''
Strategy name: Majority-Responsive Forgiving Cooperator (MRFC)

Short description
- Start by cooperating to signal willingness to coordinate.
- Continue cooperating when the group shows robust recent cooperation (majority-like support).
- When cooperation breaks down, punish briefly and proportionally, then forgive if others return to cooperating.
- In the final few rounds (endgame) behave conservatively to avoid last-round exploitation, but still cooperate in the final round only if the group has been reliably cooperating recently.
- No coordination or side-channels required; all decisions use only game parameters (n, r, k) and the observable history of actions.

Motivation / design goals
- Promote and sustain mutually beneficial all-cooperate outcomes when many others reciprocate.
- Avoid long punishments or cascades: punish enough to deter persistent defectors but forgive to restore cooperation.
- Be robust to noise / stochastic opponents and to a mix of strategies (unconditional defectors, conditional cooperators, random players).
- Adapt thresholds weakly based on k and n: when the public-good multiplier k is large (cooperation more valuable), be slightly more tolerant; when k is small (cooperation fragile), require stronger group support.

Parameters (for implementer; can be tuned)
- W (lookback window) = min(5, r) — use recent W rounds to estimate group cooperation.
- coop_threshold = 0.6 - 0.1 * ((k - 1) / (n - 1))
  - This yields a threshold in roughly [0.5,0.6]: if k is small (near 1) require ~0.6; if k is large (near n) require ~0.5 (more tolerant).
  - Clamp coop_threshold to [0.5, 0.6].
- punishment_length_base P0 = 2 rounds.
- punish_extend_factor: for persistent shortfall extend punishment proportionally (see below).
- endgame_horizon E = min(3, r - 1) — the last E rounds are treated cautiously.
- endgame_coop_threshold = 0.75 — higher bar to risk cooperating in the last few rounds.

State / history used
- For each past round t we observe the total number of cooperators C_t (including ourselves).
- We maintain recent_coop_fraction = average_t∈{t-W+1..t} (C_t / n).
- We maintain whether we are currently in a punishment state and how many punishment rounds remain (punish_remaining).

Decision rules (plain language)
1. First round (t = 1): Cooperate.
2. At the start of round t (>1):
   - Update recent_coop_fraction using the last W rounds (or fewer if t ≤ W).
   - If punish_remaining > 0:
       - Defect this round, decrement punish_remaining by 1. (We are enforcing punishment.)
   - Else if t > r - E (inside endgame):
       - Cooperate only if the previous round’s fraction of cooperators (C_{t-1} / n) ≥ endgame_coop_threshold AND recent_coop_fraction ≥ coop_threshold.
       - Otherwise defect. (Be cautious in the final rounds.)
   - Else (normal play, not punishing, not endgame):
       - If C_{t-1} / n ≥ coop_threshold (majority-like cooperation last round), Cooperate.
       - Else (last round had weak cooperation):
           - Start punishment: set punish_remaining = P where
                 P = P0 + ceil( max(0, coop_threshold - recent_coop_fraction) * 4 )
             (this extends punishment modestly if recent cooperation is far below the threshold).
           - Defect this round (first punishment round).
3. Forgiveness: After punish_remaining counts down to zero we stop punishing and return to the normal rule (cooperate if last round had enough cooperators). Because punishment is short and capped, we avoid endless retaliation.

Pseudocode (concise)
- Initialize:
    recent_window W = min(5,r)
    coop_threshold = clamp(0.5, 0.6, 0.6 - 0.1 * ((k-1)/(n-1)))
    P0 = 2
    E = min(3, r-1)
    endgame_coop_threshold = 0.75
    punish_remaining = 0
- Round t:
    observe history C_1..C_{t-1} (total cooperators each round)
    recent_coop_fraction = average_{last up to W rounds} (C_t / n)
    last_round_fraction = (C_{t-1} / n) if t>1 else 1.0 (we treat no history as cooperative for plan)
    if t == 1:
        play C
    else if punish_remaining > 0:
        play D
        punish_remaining -= 1
    else if t > r - E:
        if last_round_fraction >= endgame_coop_threshold and recent_coop_fraction >= coop_threshold:
            play C
        else:
            play D
    else:
        if last_round_fraction >= coop_threshold:
            play C
        else:
            shortage = max(0, coop_threshold - recent_coop_fraction)
            P = P0 + ceil(shortage * 4)   // 0..4 extra rounds depending on shortage
            punish_remaining = min(P, r - t)   // don't schedule beyond game end
            play D   // first punishment round

Edge cases and clarifications
- Small r (e.g., r=2): E = min(3, r-1) = 1, so the final round is endgame and we will be cautious; first round we cooperate, second round we cooperate only if the group showed strong cooperation in round 1.
- If opponents are mostly unconditional defectors: MRFC will quickly switch to defect and avoid long exploitation (punishment is short but then the normal rule defects while last round fraction remains low). You will not be repeatedly exploited by lone defectors.
- If opponents are conditional cooperators/reciprocators: initial cooperation signals intent; majority responsiveness supports rapid re-entry to cooperation when others cooperate; forgiveness restores cooperation after transient breakdowns.
- If opponents are noisy/stochastic: short punishments plus forgiveness prevent permanent breakdowns after accidental defections.
- Endgame: we adopt a conservative rule in last E rounds to reduce being exploited when future punishment is ineffective. However, if the group has been reliably cooperating (last-round fraction and recent fraction high), we cooperate even in the final rounds to capture mutual gains.
- No per-player targeting: because each round is a single public-good contribution, we respond to aggregate patterns (C_t) rather than punishing particular identities. This is implementable and robust.

Why this is cooperative and robust
- Cooperative: the strategy starts with cooperation and continues to cooperate whenever there is substantial group reciprocity; it aims to keep mutual cooperation when many others reciprocate (maximizes collective gains when sustainable).
- Robustness: majority threshold prevents being exploited by a few defectors; short punishments deter persistent free riders but are forgiving so cooperation can return; endgame caution prevents the typical last-round exploitation; adapting coop_threshold weakly with k makes it more tolerant when cooperation is more valuable.
- Adaptivity: MRFC uses recent group behavior rather than rigid plans; punishments scale a bit with how badly the group deviates from cooperative norms. It works across n, r, and k (parameters are functions of n,k,r).

Implementation notes
- The strategy only needs to track the number C_t each round (no private signals), so it fits the game observability assumptions.
- All constants (W, P0, multiplier for extension, E) are tunable. If you expect very noisy opponents, increase forgiveness by lowering P0 or reducing the extension multiplier. If you expect many exploiters, increase P0 or the extension multiplier to make punishment stronger.
- If you want a variant that targets individual defectors, you can add per-player reputations and require a higher group-cooperation fraction excluding persistent defectors — but that adds complexity and can be gamed by identity-switching opponents; MRFC keeps the decision simple and collective.

Summary (concise)
- Cooperate first.
- Cooperate when the last round (or recent rounds) shows majority-like cooperation (threshold adaptive to k).
- If cooperation collapses, defect for a short proportional punishment window, then forgive.
- Be cautious in the last few rounds unless cooperation has been very stable.
This yields a cooperative, adaptive, and robust strategy for multi-player repeated public-goods play.
'''

description_COOPERATIVE_107 = '''
Name: Reciprocal Proportional Forgiveness (RPF)

Summary (one sentence)
- Start cooperative; punish observed defections proportionally to how many players defected; forgive after short, verifiable improvement; be cautious in the final round unless previous round showed unanimous cooperation.

Design goals satisfied
- Cooperative: opens with cooperation and returns to cooperation after punishments.  
- Adaptive: punishment length scales with number of defectors; responses depend on observed group behaviour over recent rounds.  
- Robust: avoids overly long or irreversible punishments, forgives to restore cooperation, and guards against exploitation near the endgame.

Strategy variables (kept in memory)
- mode ∈ {COOPERATE, PUNISH}
- punish_remaining: integer ≥ 0 (how many more rounds to defect as punishment)
- recent_coop_history: the observed cooperators count each past round (available from global history)
- Parameters computed from game inputs (n, r):
  - W = min(5, r)  // forgiveness verification window (number of consecutive good rounds required to return to COOPERATE)
  - max_punish = r  // cannot punish more than remaining rounds

High-level decision rules
1. First round (t = 1): cooperate (C).
2. If currently in PUNISH mode and punish_remaining > 0: play defect (D) and decrement punish_remaining by 1. After punish_remaining reaches 0, switch to COOPERATE mode but require verification described below before unconditional cooperation.
3. If not currently punishing (mode = COOPERATE) then inspect previous round:
   - Let prev_cooperators = number of players who played C in round t-1.
   - If prev_cooperators == n (unanimous cooperation last round) → play C.
   - If prev_cooperators < n:
       a. Enter PUNISH mode and set punish_remaining = min(max_punish, 1 + (n - prev_cooperators))
          (i.e., punish for 1 + number_of_defectors_last_round; use at most remaining rounds).
       b. Play D this round (consumes one round of punishment; decrement punish_remaining).
4. Forgiveness / re-entry to cooperation after punishment:
   - After punish_remaining reaches 0 we tentatively switch to COOPERATE mode, but we require that the next W rounds show clear cooperative recovery before committing to unconditional cooperation:
       • If, in each of the last W rounds, observed cooperators ≥ n - 1 (i.e., either unanimous or one defector at most), then accept full COOPERATE mode.
       • Otherwise, if a recent round still has multiple defectors, treat that as a fresh defection event and re-enter PUNISH with punish_remaining = min(max_punish, 1 + current_defectors).
5. Final round (t = r) special-case:
   - Cooperate on final round only if the previous round was unanimous cooperation (prev_cooperators == n) or if we are still in the middle of a punishment we initiated and punishment_remaining > 0 (we continue that punishment). Otherwise defect on the final round (to avoid being suckered in an unpunishable last move).

Pseudocode

Inputs: n, r, history (sequence of rounds; each round contains list of actions for all players)
State: mode := COOPERATE; punish_remaining := 0
Parameters: W := min(5, r)

function decide(t, history):
  if t == 1:
    return C

  remaining_rounds := r - t + 1
  // If in punishment mode
  if punish_remaining > 0:
    action := D
    punish_remaining := punish_remaining - 1
    // If punishment just finished (punish_remaining == 0), set mode to COOPERATE but require verification
    if punish_remaining == 0:
      mode := COOPERATE
    return action

  // Not currently punishing: inspect last round
  prev_round := history[t-1]
  prev_cooperators := count_C(prev_round)  // includes ourselves if we cooperated last round

  // Final round handling
  if t == r:
    if prev_cooperators == n:
      return C
    else:
      return D

  // Normal rounds (not punishing and not final)
  if prev_cooperators == n:
    // last round was unanimous cooperation -> cooperate
    return C
  else:
    // somebody defected last round -> proportionate punishment
    num_defectors := n - prev_cooperators
    punish_remaining := min(remaining_rounds, 1 + num_defectors)
    // consume one punishment round now
    punish_remaining := punish_remaining - 1
    mode := PUNISH
    return D

// (Optional background check invoked whenever punish_remaining == 0 and mode == COOPERATE)
// Verify that cooperation has recovered for W rounds. If not, treat recent bad round as a new defection
function verify_recovery(t, history):
  if t < 1 + W:
    return  // not enough rounds to verify
  for s in (t-W) to (t-1):
    if count_C(history[s]) < n-1:
      // Not enough recovery; re-enter punishment proportional to latest round
      latest_defectors := n - count_C(history[t-1])
      punish_remaining := min(r - t + 1, 1 + latest_defectors)
      punish_remaining := punish_remaining - 1
      mode := PUNISH
      return
  // if reaches here, we accept full cooperation mode (do nothing)

Rationale and properties
- Opening with cooperation helps establish mutual cooperation with cooperative opponents.  
- Punishment is proportional: if many players defect, punishment length increases; this raises the cost of defection in group terms and makes single defections less attractive. The minimal punishment is still 1 round, so small mistakes are penalized but not destroyed.  
- Forgiveness after short, observable recovery (W rounds of near-unanimous cooperation) avoids endless cycles of retaliation and allows restored cooperation. W = min(5,r) is conservative but small; it can be implemented deterministically.  
- The strategy is deterministic, transparent and depends only on history and parameters (n, r). It neither requires communication nor precommitments across players.  
- Final-round caution: cooperating on the final round is only done if the previous round was unanimous cooperation (a strong signal others will also cooperate), otherwise the strategy defects to avoid being exploited in an unpunishable final move. This balances being cooperative with protection against endgame exploitation.

Why this is robust in tournaments
- It punishes but not excessively: proportional punishments avoid making cooperation impossible to re-achieve, which is important in noisy or mixed-population tournaments.  
- It punishes group defections effectively: if many players defect, the larger punishment discourages opportunistic mass defections.  
- It forgives and re-enters cooperation quickly when others reciprocate, so cooperative opponents will quickly recover to mutual cooperation with this strategy.  
- It resists exploitation: if opponents defect repeatedly, RPF will keep defecting for proportionate punishments and stop being an easy target; if opponents cooperate, RPF returns to cooperative mode.

Implementation notes
- The implementation must track the full observed cooperators count each round (perfect information is given).  
- The precise parameter W can be tuned (larger W = more cautious re-entry; smaller W = quicker forgiveness). The current default W = min(5, r) is a pragmatic trade-off.  
- The punish length formula punish = 1 + (#defectors last round) is deliberately simple and proportional; it can be made slightly stronger or weaker depending on tournament conditions (e.g., increase multiplier if many short-lived defections are common).

This strategy is cooperative in spirit (opens and returns to cooperation), adaptive and robust across a wide range of opponent behaviours, and safe against endgame exploitation.
'''

description_COOPERATIVE_108 = '''
Strategy name: Adaptive Generous Threshold Reciprocity (AGTR)

Goal summary
- Start by signalling cooperation, sustain cooperation when others reciprocate, punish collective defection quickly but forgive quickly to restore cooperation, and avoid being exploited in the last round.
- The rule set uses only game parameters (n, r, k) and observed history (who cooperated in each past round). It adapts to persistent defectors and to changes in group behaviour.

Key idea (intuitive)
- Cooperate if recent group behaviour indicates a high chance that cooperation will be reciprocated (so you capture the long-run benefit). If recent cooperation falls below a threshold, switch to collective punishment (defect) for a short, controlled period. Always include some forgiveness so accidental or noisy defections do not collapse cooperation permanently. Defect in the final round to avoid end‑game exploitation.

Concrete decision rules

State & internal variables (maintained each round)
- history[t][j] ∈ {C,D}: observed action of player j in round t (t = 1..current_round-1).
- for each opponent j: coop_rate_j = fraction of rounds (in a recent window) in which j played C.
- recent_group_coop = moving average of fraction of players who chose C (excluding you) over the last W rounds.
- persistent_defectors = set of players with low coop_rate_j over a longer window.
- punishment_counter: remaining rounds to maintain a punishment phase (integer ≥ 0).

Fixed strategy meta-parameters (derived from n,r,k but with explicit defaults)
- W = min(5, r-1) — moving-average window (short memory to be responsive).
- M = min(10, max(3, floor(r/4))) — window used to detect persistent defectors.
- baseline_threshold q0 = 0.6 — baseline fraction of others cooperating needed to be willing to cooperate.
  - (q0 > 0.5 biases toward requiring a majority. You can set q0 = 0.5 if you prefer more lenient reciprocity.)
- persistent_threshold θ_low = 0.2 — coop_rate_j below this marks a persistent defector.
- punishment_length L = min(3, max(1, floor(r/10))) — number of rounds to enforce punishment once triggered.
- forgiveness_probability g = 0.2 — when near-threshold or after short deviations, cooperate with this probability to re-start cooperation.
- near_threshold_delta δ = 0.05 — margin to add small randomized decisions near the threshold.
- small_noise ε = 0.05 — small randomization to avoid lockstep cycles.

All of the above depend only on n and r (and can be scaled by k if desired). You may tweak q0 up/down with k: if k is very large relative to n, set q0 lower (cooperation pays off more), if k barely > 1 keep q0 higher.

Decision algorithm (pseudocode-style)

Input each round t (1..r):
  If t == r:
    // Last round: avoid being exploited
    play D and return

  If t == 1:
    // Signal cooperative intent
    play C and return

  // Update statistics from history
  Let Wt = min(W, t-1)                // actual window length available
  For each opponent j != me:
    coop_rate_j = (number of times j played C in last min(M, t-1) rounds) / min(M, t-1)
  recent_group_coop = average over last Wt rounds of ( (# of players who played C excluding me) / (n-1) )

  // Detect persistent defectors
  persistent_fraction = (count of j with coop_rate_j < θ_low) / (n-1)
  If persistent_fraction > 0:
    // increase caution proportional to persistent defectors
    q = q0 + 0.25 * persistent_fraction
  Else:
    q = q0

  // If we are currently in a punishment phase, continue it unless recovery conditions are met
  If punishment_counter > 0:
    // allow early recovery if others strongly resume cooperation
    If recent_group_coop >= q and (t < r): 
      // require at least one good recovery round to stop punishment early
      punishment_counter = 0
      play C and return
    Else:
      punishment_counter = punishment_counter - 1
      play D and return

  // Not in punishment; decide whether to cooperate this round
  If recent_group_coop >= q + δ:
    play C and return

  If recent_group_coop <= q - δ:
    // group cooperation noticeably below threshold -> trigger punishment
    punishment_counter = L - 1    // we will defect this round and then L-1 more rounds
    play D and return

  // recent_group_coop is within δ of q: be generous but cautious
  With probability (g + ε):
    play C
  Else:
    // begin a short punishment to signal seriousness
    punishment_counter = L - 1
    play D

Detailed notes on parameters and edge handling
- First round: always cooperate. This is a clear cooperative signal and is robust across opponents.
- Last round: always defect. This prevents being exploited in the obvious end-game. (If you prefer pure cooperation to try to score points against extremely cooperative opponents in some tournaments, you can change this; but rational opponents generally exploit last-round cooperators.)
- Punishment length L is short (1–3 rounds) to keep retaliation credible but not destroy future cooperation possibilities. Punish collectively by defecting to lower payoffs for all, which reduces the benefit persistent defectors get from free-riding.
- Forgiveness: a nonzero g ensures accidental defects or noise don't trigger permanent collapse of cooperation.
- Randomization (ε and δ) prevents deterministic cycles and helps the strategy avoid lock-step exploitation by complex strategies.

Why this works (robustness reasoning)
- Starts cooperative to attract reciprocators.
- Uses a threshold on recent group cooperation (not just last round) so it is resilient to occasional slips/noise.
- Detects persistent defectors and raises the cooperation threshold accordingly, lowering exploitation risk when some players never reciprocate.
- Punishes quickly but only briefly, allowing recovery; punishment is sufficient to create a credible cost to persistent defection.
- Forgiveness and a short memory (W small) make it easy for the group to return to cooperation after disruptions.
- Defecting in the final round avoids being exploited in the trivial endgame.

Variants and tuning
- More aggressive variant: increase q0 (require a larger majority) and longer punishment L — more resistant to exploitation but slower to recover.
- More generous variant: lower q0, larger g — cooperates more often, does better versus highly cooperative populations but is more exploitable by persistent defectors.
- Make q0 a function of k: if k is large (public good valuable), lower q0 to be more cooperative; if k barely exceeds 1, raise q0.

Implementation checklist for algorithm designer
- Track per-player histories and compute recent_group_coop and coop_rate_j.
- Maintain punishment_counter state.
- Use the exact pseudocode decision order above to determine action each round.
- Expose the meta-parameters (W,M,q0,θ_low,L,g,δ,ε) so the tournament implementer can tune them if desired.

Final summary
AGTR is a simple, transparent, implementable strategy that:
- Signals cooperation in round 1,
- Cooperates when recent evidence suggests a majority reciprocates,
- Punishes collective defection fast and briefly,
- Forgives quickly to restore cooperation,
- Adapts to persistent defectors,
- Defects in the last round to avoid endgame exploitation.

This combination makes AGTR cooperative-minded, adaptive, and robust across a wide class of opponent behaviours while being straightforward to implement from the available history and parameters.
'''

description_COOPERATIVE_109 = '''
Strategy name: Forgiving Proportional Punisher (FPP)

High-level idea
- Be nice: start by cooperating and reward rounds in which the group cooperates.
- Be deterrent: when defections are observed, impose a short, predictable collective punishment (everyone defects) long enough to make a single-shot defection unprofitable compared with continued cooperation.
- Be forgiving: allow isolated or noisy lapses by a usually-cooperative player, and after punishment revert to cooperation so cooperation can be re-established.
- Be endgame-aware: in the final round there is no future to enforce cooperation, so defect.

Intuition behind the numbers
- One-shot temptation (gain from defecting this round) is constant: Δ = 1 − k/n.
- If punishment consists of L rounds in which the group defects, then a defector who otherwise would have cooperated loses roughly L·(k − 1) relative to cooperating in those rounds. Choose L so that L·(k − 1) ≥ Δ. Thus a minimal punishment length is
  P0 = ceil( Δ / (k − 1) ) = ceil( (1 − k/n) / (k − 1) ).
- Cap punishments at the remaining rounds left minus one (you cannot credibly punish in the last round).

This gives a principled, parameter-free baseline P0 derived from the game parameters (n, k). We then scale P0 when multiple players defect or when repeat offending occurs, and we allow short forgiveness for isolated lapses.

State tracked (per instance of the strategy)
- punishment_counter (integer ≥ 0) — rounds left to serve in a punishment phase; when >0 the strategy defects.
- last_defectors (set of player indices) — players who defected in the most recent observed round.
- offence_counts[j] (nonnegative integer for each opponent j) — how many times j has been a defector in recent history (used to escalate punishments for repeat offenders).
- history window size w = min(5, max(1, floor(r/4))) used for assessing whether a lapse was isolated (noise forgiveness).

Decision rules (natural language)
1. First round (t = 1):
   - Cooperate.

2. Any round t:
   - If t == r (last round): Defect (no future to enforce cooperation).
   - Else if punishment_counter > 0: Defect and decrement punishment_counter by 1.
   - Else (not currently punishing):
     - Observe the set D_prev of players (excluding yourself) who defected in the immediately previous round.
     - If D_prev is empty: Cooperate (reward a cooperative round).
     - If D_prev is non-empty:
         - For each j in D_prev compute j’s recent cooperation rate over the last w rounds (or all available history if fewer than w rounds have passed).
         - If every j in D_prev has cooperation rate ≥ 0.8 in that window (i.e., likely a noisy lapse), treat this as noise: Cooperate (forgive).
         - Else: initiate a punishment phase:
             - Compute base punishment length P0 = ceil( (1 − k/n) / max(ϵ, k − 1) ), with ϵ a tiny positive constant to avoid division by zero; then cap: P0 := min(P0, r − t) (cannot punish in the final round).
             - Let d = |D_prev| (number of defectors observed).
             - Set punishment_counter := min(r − t, max(1, P0 * d * escalation_factor)), where escalation_factor depends on repeat offending:
                 - For each j in D_prev if offence_counts[j] ≥ 2, multiply escalation_factor by 1.5 for that j (so repeated offenders increase the punishment length). Practically, use a single escalation_factor = 1 + 0.5 * (average offence_counts over D_prev clipped at 2).
             - Increment offence_counts[j] for j in D_prev.
             - Defect this round (the start of the punishment phase).
     - After a punishment phase finishes (punishment_counter reaches 0), clear last_defectors and reduce offence_counts slowly (e.g., decrement all offence_counts by 1, floored at 0) so players’ records recover over time.

Notes on forgiveness and robustness
- Forgiveness window (w) and the 0.8 threshold protect against accidental defection or exploratory noise. These are chosen small so that occasional lapses are forgiven but systematic free-riding is punished.
- Punishment scales with the number of defectors: more defectors → longer punishment. This reduces incentives for coordinated free-riding in large numbers.
- Escalation for repeat offenders makes persistent defectors less profitable targets for exploitation.
- All parameters are computed from n, k, r (and a fixed tiny ϵ and fixed constants like 0.8 and w formula), so the strategy uses only game parameters and observed history.

Pseudocode

Initialize:
  punishment_counter = 0
  offence_counts[j] = 0 for all opponents j
  history = [] (record of each round’s full action profile)
  w = min(5, max(1, floor(r/4)))
  eps = 1e-6

On round t (1..r):
  if t == 1:
    play C; return
  if t == r:
    play D; return
  if punishment_counter > 0:
    play D
    punishment_counter -= 1
    return
  // not punishing
  D_prev = {j != me : action_of_j in history[t-1] == D}
  if D_prev is empty:
    play C; return
  // There were defectors last round
  // Compute recent cooperation rates for each j in D_prev over last w rounds
  for j in D_prev:
    coop_count_j = number of times j played C in last min(w, t-1) rounds
    coop_rate_j = coop_count_j / min(w, t-1)
  if for all j in D_prev: coop_rate_j >= 0.8:
    // likely noise or single lapse
    play C; return
  // otherwise start punishment
  Delta = 1 - k / n
  P0 = ceil( Delta / max(eps, k - 1) )
  P0 = min(P0, r - t)        // cannot punish beyond final round
  d = |D_prev|
  avg_offences = average of offence_counts[j] over j in D_prev
  escalation_factor = 1 + 0.5 * min(avg_offences, 2)   // up to 2 offences gives +100%
  punish_len = min(r - t, max(1, ceil(P0 * d * escalation_factor)))
  punishment_counter = punish_len
  for j in D_prev: offence_counts[j] += 1
  play D; return

Maintenance after each round (end of round bookkeeping):
  append observed actions to history
  if punishment_counter == 0 and last round was end of a punishment phase:
    // decay offence counts to allow recovery
    for all j: offence_counts[j] = max(0, offence_counts[j] - 1)

Why this strategy is cooperative and robust
- It always begins by cooperating and rewards full-cooperation rounds, encouraging mutual cooperation.
- Punishments are calibrated (via P0 computed from k and n) so that a single-shot defection followed by the prescribed punishment will usually be unprofitable for the defector: punishment length L satisfies L·(k − 1) ≥ 1 − k/n, so the future loss from punishment offsets the immediate temptation.
- Punishment is collective (everyone defects), which is simple to implement in a simultaneous-action setting without side payments or coordination.
- Forgiveness for isolated lapses (0.8 threshold over a short window w) prevents escalatory cascades from noise or testing moves.
- Punishment duration scales with number of defectors and with repeat offending, deterring coordinated or persistent exploitation.
- Last-round defection is rational and prevents being exploited at the end; earlier rounds are used to build and sustain cooperation.

Tuning notes (for implementers)
- The constants 0.8 (for forgiveness) and w (window) are conservative defaults: increase forgiveness (lower threshold or larger w) in very noisy environments; reduce forgiveness in tournaments with many exploiters.
- The escalation multiplicative factor is optional; setting it to 1 removes escalation.
- eps avoids division by zero if k is extremely close to 1 (k > 1 is guaranteed, but numeric safety helps).

Summary
FPP aims to sustain large-scale cooperation by being nice initially, punishing defections with a punishment long enough to deter one-shot temptation (derived from k and n), forgiving probable noise, scaling punishments for multiple or repeated offenders, and defecting in the final round. The rules depend only on game parameters (n, k, r) and observed history, are straightforward to implement, and are robust to a wide range of opponent behaviours.
'''

description_COOPERATIVE_110 = '''
Name: Adaptive Forgiving Conditional Cooperator (AFCC)

Summary (one line)
- Start cooperatively, condition future cooperation on recent group cooperation with a small forgiveness allowance for occasional lapses, punish only when defections are persistent or widespread, allow recovery after cooperation is re-established, and defect in the final round.

Intuition / design goals
- Encourage and sustain high cooperation when others reciprocate.
- Avoid being exploited by occasional or one-off defections (forgiveness).
- Make punishments proportional and temporary (avoid irreversible grim triggers).
- Use only observable history (who cooperated each round) and known parameters (n, r, k).
- Be robust across many opponent behaviours (generous to cooperators, retaliatory to persistent free-riders).
- Respect single-shot incentives in the known last round (defect there).

Parameters used by the strategy (deterministic functions of n, r, k)
- window w = min(3, max(1, floor(r/10))) — number of past rounds to average (short window but grows a bit with r).
- grace_count g = max(1, round(0.05 × n)) — small number of simultaneous defectors tolerated as occasional noise.
- strong_coop_threshold T_high = 0.8 — if recent cooperation is very high, keep cooperating.
- weak_coop_threshold T_low = 0.5 — below this the group is not reliably cooperating.
- recovery_threshold T_recover = 0.6 — cooperation rate needed to end a punishment phase.
- min_punish_length L_min = 1 — minimum punishment length after a trigger.
- max_punish_length L_max = max(1, round(r/4)) — cap punishment duration so we don’t lock into long vendettas.

Note: the numeric threshold choices above are defaults chosen to balance forgiveness and deterrence. Implementers can tune them, but the logic below remains the same.

State maintained (computed from history each round)
- For each past round s we know c_j,s for every player j (1 if C, 0 if D).
- We keep a counter punishment_timer (nonnegative integer) indicating how many rounds we will continue defecting as a punishment; initially 0.

Decision rules (natural-language)
1. First round (t = 1)
   - Play C.

2. Last round (t = r)
   - Play D (single-shot dominant strategy).

3. Intermediate rounds (1 < t < r)
   - If punishment_timer > 0:
       - Play D this round and decrement punishment_timer by 1 at the end of the round.
   - Else:
       - Compute recent cooperation rate among other players over the last up to w rounds:
         coop_rate = (sum_{s = max(1,t-w)}^{t-1} sum_{j ≠ me} c_j,s) / ((n-1) × (min(w, t-1)))
       - Let last_round_defectors = number of other players who played D in round t-1 (i.e., (n-1) − number_of_other_cooperators_in_t-1).
       - Case A: If coop_rate ≥ T_high:
           - Play C (group is reliably cooperative).
       - Case B: Else if coop_rate ≥ T_low:
           - If last_round_defectors ≤ g (a small lapse): play C (forgive small slips).
           - Else: play D for this round (proportional, short punishment of at least L_min). Set punishment_timer = min(L_max, max(L_min, last_round_defectors - g)).
       - Case C: Else (coop_rate < T_low):
           - The group is not cooperating enough. Enter punishment: set punishment_timer = min(L_max, max(L_min, round((1 - coop_rate) × (n)))) and play D this round.
       - End decision logic.

4. Recovery behavior
   - While punishment_timer runs, we defect. After punishment_timer reaches 0, we resume the ordinary decision rule above. We require coop_rate ≥ T_recover for at least one full window period to reliably resume stable cooperation (this is enforced implicitly because if coop_rate remains low, we will re-enter punishment).

Pseudocode (clear, implementable)
- Inputs: n, r, k, history list of rounds 1..t-1 each containing c_j for every player j
- Internal variable: punishment_timer (initially 0)

function choose_action(t, history):
    if t == 1:
        return C
    if t == r:
        return D

    if punishment_timer > 0:
        punishment_timer -= 1
        return D

    w = min(3, max(1, floor(r/10)))
    lookback = min(w, t-1)
    total_other_coop = sum_{s = t - lookback}^{t-1} sum_{j ≠ me} c_j,s
    coop_rate = total_other_coop / ((n-1) * lookback)

    # last-round defectors among others
    last_round_other_coop = sum_{j ≠ me} c_j, t-1
    last_round_defectors = (n-1) - last_round_other_coop

    g = max(1, round(0.05 * n))
    T_high = 0.8
    T_low = 0.5
    T_recover = 0.6
    L_min = 1
    L_max = max(1, round(r/4))

    if coop_rate >= T_high:
        return C

    if coop_rate >= T_low:
        if last_round_defectors <= g:
            return C
        else:
            punishment_timer = min(L_max, max(L_min, last_round_defectors - g))
            punishment_timer -= 1  # consume this round
            return D

    # coop_rate < T_low
    punishment_timer = min(L_max, max(L_min, round((1 - coop_rate) * n)))
    punishment_timer -= 1
    return D

Why this is cooperative
- AFCC starts by cooperating and continues to cooperate while the observed cooperation among others is reasonably high (T_high/T_low logic). This rewards reciprocators.
- Small accidental or isolated defections are forgiven (grace g), preventing endless vendettas over single slips (this is crucial in tournaments with noisy/independent players).
- Punishments are proportional to how many players defected and how low the sustained cooperation rate is; punishments are temporary (bounded by L_max) so the strategy can re-establish cooperation once others return to cooperating.
- By defecting in the last round we avoid being exploited by one-shot incentives; this also makes punishments credible (because early punishments have future impact).
- The design uses only parameters (n, r, k) and observed action history; no communication or external coordination is required.

Robustness considerations and variants
- If many opponents are unconditional defectors, AFCC will quickly stop cooperating and switch to defection (limiting exploitation).
- If opponents are conditional cooperators or reciprocators, AFCC will sustain cooperation through mutual returns.
- If you expect a lot of randomness/noise in opponents, increase w and lower thresholds (T_low) to be more forgiving.
- If k is very close to n (public good almost as high as private good), players may be more willing to cooperate; AFCC naturally responds to the empirical cooperation rate rather than k directly, but an implementer could lower thresholds when k is large to be more cooperative.
- The punishment_timer caps (L_max) prevent long irreversible breakdowns even in long games.

Edge cases
- r = 2: AFCC cooperates in round 1 and defects in round 2 (standard and sensible).
- Very small n: g = max(1, round(0.05*n)) ensures some forgiveness even for small groups.
- Early persistent defection: AFCC will move to defection quickly but still allow rehabilitation if opponents resume cooperating.
- If history is empty or t=1: cooperate.

Implementation notes
- The strategy is fully specified by the pseudocode; all constants are functions of n and r (and one small fraction of n for g).
- Tuning of thresholds (T_high/T_low/T_recover/w/g/L_max) may improve tournament performance; defaults give a balanced trade-off between cooperation and exploitation resistance.

Conclusion
AFCC is a simple, interpretable conditional-cooperation strategy: start friendly, forgive isolated slips, punish proportional to persistent/widespread defections, let cooperation recover, and defect at the final round. It uses only observable history and game parameters and is designed to do well across a wide range of opponent behaviours in repeated public-goods tournaments.
'''

description_COOPERATIVE_111 = '''
Summary (one line)
- Start by signalling cooperation, sustain cooperation when the group is reliably cooperative, punish briefly and proportionally when the group defects, forgive quickly, and always defect in the final round. The rules use only game parameters (n,r,k) and observed history.

Intuition and goals
- Be cooperative: try to maintain mutual cooperation (which raises group payoffs) rather than giving up after a few breaches.
- Be robust: punish to deter persistent free‑riders, but punish briefly and forgive to avoid needless collapse from isolated mistakes or noise.
- Be adaptive: punish length and strictness scale with how many players defected and with how persistent the defectors are.
- Handle endgame: in a known finite horizon, the last round is attractive to defectors; therefore defect on the final round and become progressively more cautious in the final small window.

Parameters used by the strategy (derived from n,r,k)
- w = min(10, r-1)  // look-back window (useful even when r small)
- g = ceil(sqrt(r)) // endgame window length
- max_punish = max(1, min(5, ceil(r/6))) // max consecutive punishment rounds
- gamma_high = 0.9 // require ~90% cooperative history to be fully trusting
- gamma_med = 0.7  // medium trust threshold
- gamma_low = 0.5  // minimal forgiveness threshold
- persistence_thresh = 0.6 // fraction of rounds a particular player defected to be marked "persistent defector"

(These constants are simple defaults. Implementers may tune them for particular tournaments; they depend only on n,r,k via w,g,max_punish.)

State variables maintained from history
- history[t][j] for past rounds t and players j (observed C or D)
- punish_until_round (initially 0) — if current round ≤ this, play D
- per_player_defect_count[j] — total number of times player j played D so far
- round t (1..r)

High-level decision rules
1. Round 1: Cooperate (C). This signals cooperative intent.
2. Final round (t == r): Defect (D).
3. If we are currently in a punishment phase (t ≤ punish_until_round): defect.
4. Otherwise decide based on recent group cooperation rates, persistent-defector detection, and endgame caution:
   - Compute p_recent = average proportion of other players (excluding self) who contributed in the last w rounds. (For a single round, proportion = (#cooperators excluding self)/(n-1).)
   - Compute p_overall = average proportion of other players who contributed across all past rounds (1..t-1).
   - Identify persistent defectors: any player j with per_player_defect_count[j] / (t-1) ≥ persistence_thresh.
   - If t > r - g  (in the endgame window):
       - Cooperate only if p_recent == 1.0 (others cooperated every recent round) AND there are no persistent defectors; otherwise defect. (Becomes cautious as horizon approaches.)
   - Else (normal rounds):
       - If p_recent ≥ gamma_high and p_overall ≥ gamma_med and no persistent defectors: Cooperate.
       - Else if p_recent ≥ gamma_low: Cooperate with high probability (e.g., 90%) — “generous” forgiveness to repair cooperation after isolated slips.
       - Else (group cooperation is low): Trigger punishment:
           - Let d_last = number of non-self defectors in the most recent round.
           - Set punish_length = min(max_punish, 1 + ceil( (d_last/(n-1)) * w) ). // punish length proportional to number of defectors
           - Set punish_until_round = t + punish_length - 1. Defect this round (and during punishment).
   - After a punishment phase ends, require cooperation over w rounds (p_recent ≥ gamma_high) before returning to full trust; otherwise remain cautious / punish again if breaches continue.

Pseudocode (concise)
Note: history[t][j] is 1 for C, 0 for D. Self index is i.

Initialize:
  punish_until_round = 0
  per_player_defect_count[j] = 0 for all j

For each round t = 1..r:
  if t == 1:
    play C
    record history
    continue

  if t == r:
    play D
    record history
    continue

  // Update per-player defect counts from previous round
  for each player j:
    per_player_defect_count[j] += (history[t-1][j] == 0 ? 1 : 0)

  if t <= punish_until_round:
    play D
    record history
    continue

  // compute p_recent and p_overall (excluding self)
  lookback = min(w, t-1)
  sum_recent = 0
  sum_overall = 0
  for τ = t - lookback .. t-1:
    sum_recent += (sum_{j≠i} history[τ][j]) / (n-1)
  p_recent = sum_recent / lookback

  total_other_contribs = sum_{τ=1..t-1} sum_{j≠i} history[τ][j]
  p_overall = (total_other_contribs / (t-1)) / (n-1)  // same as average proportion of others cooperating

  // detect persistent defectors
  persistent = { j | per_player_defect_count[j] / (t-1) >= persistence_thresh }

  if t > r - g:
    if p_recent == 1.0 and persistent is empty:
      play C
    else:
      play D
    record history
    continue

  if p_recent >= gamma_high and p_overall >= gamma_med and persistent is empty:
    play C
    record history
    continue

  if p_recent >= gamma_low:
    with probability 0.9: play C
    else: play D
    record history
    continue

  // Otherwise: trigger punishment
  d_last = (n-1) - sum_{j≠i} history[t-1][j]  // number of other-player defections in last round
  punish_length = min(max_punish, 1 + ceil( (d_last / (n-1)) * w ))
  punish_until_round = t + punish_length - 1
  play D
  record history
  continue

Reconciliation and learning
- After any punishment phase ends we require p_recent ≥ gamma_high for w rounds before we resume full trusting behavior. This makes punishment effective (others must restore high cooperation to be treated as trusted).
- Persistent defectors are treated as strong evidence of free‑riding; if any persistent defectors exist we are strict (do not return to full cooperation unless their defection rates drop).
- Because punishments are short and forgiving, isolated mistakes (or noisy defections by others) are unlikely to cause long collapse.

Why this is cooperative and robust
- Cooperative intent: Always begin by cooperating and prefer cooperation when the group shows high cooperation rates.
- Proportional punishment: Punishments are brief and scale with how many defected and how persistent they are — this avoids overly harsh grim triggers that destroy cooperation entirely after one slip.
- Forgiving: Partial forgiveness (probabilistic cooperation at medium rates) lets cooperation recover after noise or single-player mistakes.
- Endgame safety: We defect in the final round (and become cautious in a short endgame window) to avoid exploitation when backward induction makes cooperation fragile.
- Adaptivity: Decision uses short-term and long-term cooperation statistics and per-player persistence to respond differently to occasional slips vs systematic free‑riding.
- No assumption of norms: The rules rely only on observed history and parameters, so they function in heterogeneous populations without prearranged schedules.

Edge cases handled explicitly
- First round: cooperate to signal cooperative intent.
- Single isolated defection: forgiven quickly (likely a single-round punishment or immediate generous forgiveness).
- Many simultaneous defections: punish for several rounds proportional to how many defected.
- Persistent defectors: worsen treatment until they reduce their defection rate.
- Final round: always defect. Last g rounds: be cautious and require near-perfect recent cooperation to keep cooperating.
- Small r: w is reduced to r-1 so calculations still work.
- Noisy environments: probabilistic forgiveness and short punishments prevent lock-in to all-defect.

Tuning notes (for implementers)
- gamma_high, gamma_med, gamma_low, persistence_thresh, and max_punish are tunable. If the tournament population tends to be noisy, lower gamma_high and raise forgiveness. If many strategies are exploitative, increase punish lengths and lower forgiveness.
- g = ceil(sqrt(r)) is a compromise to protect against endgame exploitation without immediately abandoning cooperation far from the end. For very large r you can keep g small relative to r (e.g., log r).

Final comment
This strategy is a conservative, practical cooperative algorithm: it gives cooperation a chance, deters persistent free-riders with proportional sanctions, forgives accidental slips quickly, and avoids being exploited in the final round. It is simple to implement and relies only on game parameters and the publicly observed history.
'''

description_COOPERATIVE_112 = '''
Name: Adaptive Generous Conditional Cooperator (AGCC)

Short description
- Start “nice” (cooperate) to signal willingness to cooperate.
- Cooperate when recent group behaviour shows majority cooperation; defect otherwise.
- Punish defections, but punish only for a limited, escalating number of rounds (proportional to how badly cooperation has broken down).
- Forgive when others return to cooperative behaviour; allow small random generosity to re‑establish cooperation.
- In the last round defect (no future to enforce cooperation).

Rationale
This strategy implements the standard desirable properties for promoting cooperation in repeated social dilemmas: nice (starts cooperating), retaliatory (punishes non‑cooperation), forgiving (returns to cooperation after improvement), and robust (uses short memory, scales punishment, and adds occasional generosity to escape bad deterministic cycles). The rules depend only on game parameters (n, r, k) and on the public history of actions.

Parameters used by the strategy (derived from n, r, k or fixed small constants)
- W: lookback window (number of most recent rounds used to estimate others’ cooperation rate).
  - Set W = min(5, r-1). (Uses up to last 5 rounds; if r small uses fewer.)
- γ: cooperation threshold (fraction of other players cooperating needed to be willing to cooperate).
  - Default γ = 0.5 (majority). This is simple, interpretable and robust; see notes below on adaptivity.
- P_max: maximum punishment length = 3 (keeps punishments limited so cooperation can restart).
- ε: small generosity probability = 0.05 (occasionally cooperate despite low observed cooperation to restart cooperation).
- Endgame rule: in the final round always defect (no future to enforce cooperation); in round r-1, follow normal rule but be aware remorse/punishment will be short.

State variables maintained through the game
- punish_counter (integer ≥ 0): rounds remaining in an active punishment phase.
- escalation_level (integer ≥ 0): how many times recently we had to punish; used to scale punishment length up to P_max.
- history: public history of actions of all players and round index t.

Decision rules — natural language and pseudocode

Overview:
1. Round 1: cooperate (signal niceness).
2. For t > 1 and t < r (not last round):
   a. If punish_counter > 0: defect and decrement punish_counter.
   b. Else compute recent cooperation rate among other players over the last W rounds. If that rate ≥ γ, cooperate.
   c. If rate < γ then normally defect and enter a limited punishment of length P determined by how far below γ the rate is (escalating with repeated failures). Occasionally (with probability ε) cooperate instead to attempt to re‑start cooperation.
3. Round r (final round): defect.

Detailed pseudocode

Initialize:
  punish_counter = 0
  escalation_level = 0
  W = min(5, r-1)
  γ = 0.5
  P_max = 3
  ε = 0.05

On each round t (1..r):
  if t == 1:
    action = C   // be nice on first move
    return action

  if t == r:   // last round
    action = D
    return action

  // If we are in active punishment phase
  if punish_counter > 0:
    action = D
    punish_counter -= 1
    if punish_counter == 0:
      // after punishment ends, wait to observe at least one cooperative round before resetting escalation
      // Escalation remains until cooperative behaviour seen
      pass
    return action

  // Compute cooperation rate among others in last W rounds
  lookback = min(W, t-1)
  total_other_contribs = 0
  for each round s in (t - lookback) .. (t-1):
    for each player j != me:
      total_other_contribs += c_j,s   // c_j,s ∈ {0,1}
  max_possible = (n-1) * lookback
  if max_possible == 0:
    other_coop_rate = 0
  else:
    other_coop_rate = total_other_contribs / max_possible   // fraction in [0,1]

  // Main conditional cooperation rule
  if other_coop_rate >= γ:
    // Group is sufficiently cooperative recently → cooperate
    action = C

    // Successful cooperative observation: reduce escalation
    if escalation_level > 0:
      escalation_level = max(0, escalation_level - 1)   // slowly de-escalate
    return action

  else:
    // Group not cooperative enough
    // With small probability, try a generous move to re-start cooperation
    if random() < ε:
      action = C
      return action

    // Otherwise defect and impose punishment
    action = D

    // compute punishment length proportional to how bad the breakdown is
    shortfall = γ - other_coop_rate  // in (0,γ]
    // Base punishment increases with shortfall and with past escalations
    base_P = 1 + ceil( shortfall / γ * 2 )  // yields 2-3 when shortfall large, 1-2 when small
    // scale with escalation level but cap at P_max
    P = min(P_max, base_P + escalation_level)
    punish_counter = P
    escalation_level = min(escalation_level + 1, P_max)   // increase escalation so repeated failures escalate punishment
    return action

Notes and clarifications
- Counting "other" players: other_coop_rate excludes our own contribution; this focuses punishment/reward on what others did.
- Punishment is symmetric: in public goods, you cannot individually target, so punishment is implemented by defecting for P rounds — this lowers group payoff and deters free‑riding.
- Escalation: if cooperation keeps failing after punishments, punishments get longer (to deter persistent defectors), but capped by P_max to avoid destructive infinite punish cycles.
- Forgiveness & restart: if others return to cooperation (other_coop_rate ≥ γ), we reduce escalation slowly and resume cooperation. The small generosity probability ε allows the strategy to “nudge” the group back to cooperation even when our strict rule would defect — this is important when many strategies use deterministic punishments and can get stuck in long mutual-defection cycles.
- First round cooperation: important to attract cooperative opponents.
- Last round defection: due to the finite horizon and no future to enforce cooperation, defecting in the final round is individually optimal; keeping final round defection is robust and avoids being exploited on the last move.
- Parameters W, γ, P_max, ε are chosen to be small and robust. They can be tuned for tournament specifics:
  - If k is very close to n (public-good very effective), one could lower γ (less demanding) because even partial cooperation yields high group benefit.
  - If k is close to 1 (public good weak), one might raise γ slightly or shorten punishment so you don't waste many rounds punishing when gains from cooperation are small.
  - Simple adaptive tweak: set γ = 0.5 * (1 + (k/n)) / 2 — but we recommend keeping γ = 0.5 for robustness unless you have domain knowledge about opponent population.

Why this is cooperative and robust
- Promotes cooperation: starts nice and cooperates whenever recent majority cooperation is observed.
- Discourages exploitation: defects and imposes limited, proportional punishment when cooperation drops, raising the future cost to defectors.
- Forgiving: punishment lengths are limited and the strategy returns to cooperation when others improve.
- Adaptive: uses recent history, escalates only if failures persist, and includes small probabilistic generosity to escape deadlocks.
- Simple and transparent: depends only on public history and parameters, so it is implementable and interpretable.

Edge cases summary
- Round 1: always cooperate.
- Rounds 2..r-1: follow conditional rule above (punish when recent cooperation among others < γ, with limited escalation and forgiveness).
- Round r (final round): always defect.
- Very small r (r ≤ W): W adjusts downwards; strategy still applies using the available history.
- If everyone defects every round: AGCC will start cooperating in round 1, then see low cooperation and switch to limited punishments/defections; occasional generosity may try to re-start cooperation but persistent defectors will be met with repeated limited punishments.
- If many opponents are conditional cooperators, AGCC will typically converge to sustained cooperation (majority cooperation) because it rewards cooperative histories and punishes breakdowns.

Implementation note for tournament software
- All inputs required: current round index t, r, n, and full public history of each player's binary contributions by round.
- Randomness only used for ε generosity; the seed should be provided/controllable by tournament engine if determinism is required.

This strategy balances being cooperative with being robust to exploitation and should perform well in heterogeneous tournaments where some opponents are nice, some are selfish, and some are conditional.
'''

description_COOPERATIVE_113 = '''
Name: Forgiving Conditional Cooperator (FCC)

Intuition (short)
- Start cooperative to signal willingness.
- Cooperate whenever the group looks cooperatively inclined (a high-enough fraction cooperated last round).
- If cooperation falls below a threshold, punish defectors by defecting for a short, proportional number of rounds (not forever).
- Target punishment to the offending behaviour (short, proportional, and forgiving) so cooperation can be re‑established.
- Always defect in the final round (endgame) to avoid exploitation from backward induction.

All decisions depend only on game parameters (n, k, r) and the observable history (who cooperated in past rounds and payoffs). No communication or outside assumptions required.

Parameters computed from game parameters
- tau := 1 − 1/k (cooperation threshold fraction). Rationale: tau increases with the marginal group benefit k; for k = 2 this equals 0.5. We round tau upward when counting players.
- W := min(4, max(1, floor(r/5))) — sliding window (in rounds) to measure recent individual cooperation rates.
- P_scale := min(5, max(1, floor(r/10))) — scales maximum punishment length with game length (longer games tolerate slightly longer punishments).
- P_max := P_scale (maximum rounds to punish).
These are deterministic functions of n, k, r.

State variables maintained by the strategy
- punishment_counter (integer ≥ 0), initially 0 — how many more rounds the strategy will defect as punishment.
- punished_set (subset of player indices) — the set of players whose recent defection(s) triggered the current punishment (used for targeted forgiveness).
- per_player_history[i] — last W actions observed for player i (C/D), updated each round.

Decision rules (verbal)
1. Last round: If current round t = r (the final round), play D (defect). (Standard endgame precaution.)
2. If punishment_counter > 0: play D this round and decrement punishment_counter by 1 at the end of the round. While punishing, keep observing others for repentance. If all players in punished_set have cooperated for the most recent 2 rounds (or have cooperation rate ≥ 0.5 within W), end punishment early (set punishment_counter = 0) and clear punished_set.
3. Otherwise (not currently punishing and t < r):
   a. If t = 1: play C (cooperate) to signal cooperative intent.
   b. Else (t > 1): look at the previous round’s total cooperators c_prev (count of players who played C last round).
      - If c_prev ≥ ceil(tau × n) then play C (trust the group and cooperate).
      - If c_prev < ceil(tau × n) then trigger a proportional punishment:
         i. Let d := n − c_prev (the number of defectors last round).
         ii. Set punished_set to the set of players who played D last round.
         iii. Set punishment_counter := min(P_max, max(1, round( (d / n) × P_scale ))). (At least one round of punishment; longer if more defectors; bounded by P_max and remaining rounds.)
         iv. Play D this round (initiate punishment).
4. After each round, update per_player_history and (if applicable) check whether punished players have shown repentance (2 consecutive C or cooperation_rate ≥ 0.5 over last W). If so, stop punishing early (set punishment_counter = 0 and punished_set = ∅).

Edge cases and further clarifications
- First round: always cooperate (C).
- Last round: always defect (D). If you are in a punishment window and reach the final round, you still defect (consistent with last-round rule).
- Very short games (r ≤ 3): reduce punishment severity to avoid pointless retaliation near the end; P_scale and P_max are computed so punishments are small.
- If many players defect in one round (mass defection): punishment length scales with how many defected; this reduces the public good benefiting defectors and signals seriousness without destroying future possibility of cooperation.
- Targeted forgiveness: punishers are focused on players who defected; once they resume cooperating consecutively, punishment ends and full cooperation is resumed. This helps restore cooperation when some strategies are erratic or trying to exploit.
- Deterministic tie-breaking: when threshold comparisons exactly equal, we cooperate (ties broken in favor of cooperation).
- No dependence on opponents’ internal algorithms — only on observed actions/history.

Pseudocode
(Variables: t = current round index 1..r)

Initialize:
  punishment_counter = 0
  punished_set = {}
  for i in 1..n: per_player_history[i] = []   // empty list

At start of round t:
  if t == r:
    action = D
    return action

  if punishment_counter > 0:
    action = D
    // observe others after round, then:
    // decrement punish counter (but allow early forgiveness check)
    return action

  if t == 1:
    action = C
    return action

  // t > 1 and not currently punishing
  c_prev = number of players who played C in round t-1
  threshold_count = ceil(tau * n)

  if c_prev >= threshold_count:
    action = C
    return action
  else:
    d = n - c_prev
    punished_set = set of players who played D in round t-1
    punishment_counter = min(P_max, max(1, round( (d / n) * P_scale )))
    action = D
    return action

After each round (update step, same across rounds):
  - Append observed actions for each player i to per_player_history[i] and truncate to last W entries.
  - If punishment_counter > 0: 
      decrement punishment_counter by 1
      // Check for early forgiveness
      if punished_set is not empty:
        if for every i in punished_set either:
            (last 2 entries in per_player_history[i] are both C) OR
            (count of C in per_player_history[i] >= ceil(0.5 * length(per_player_history[i]))):
          // treat as repenting
          set punishment_counter = 0
          punished_set = {}

Why this is cooperative and robust
- Cooperative: The strategy starts cooperative, cooperates whenever the observed group cooperation is strong enough (measured relative to tau), and aims to re-establish full cooperation after deviations rather than permanently defecting.
- Proportional punishment: Punishment scales with the number of defectors and with game length, so we do not overreact to single mistakes, and we impose a cost on defectors that makes exploitation less profitable for many opponent types.
- Forgiving: Punishment is limited and ends early when defectors show repentance (consecutive cooperations). This avoids endless retaliation cycles (which can lock the group into low payoffs).
- Targeted: Punished_set records who defected and focuses forgiveness conditions on them, so the strategy does not punish the whole group indefinitely when only a small subset misbehaves.
- Endgame-safe: By defecting in the final round, the strategy avoids being suckered in trivial last-round play while still attempting to secure cooperative gains earlier in the game.
- Parameter dependence: tau and punishment lengths are functions of k and r (and indirectly n via counts), so the strategy adapts its strictness to how valuable the public good is (k) and how many rounds remain (r).

Notes on tuning
- The exact functional forms (tau = 1 − 1/k, W, P_scale) are conservative defaults that depend only on (n, k, r). Implementers may adjust them (e.g., larger P_scale in very long tournaments or different tau) if they have evidence about opponent mixes. The defaults are chosen to balance being cooperative, deterring exploitation, and being forgiving.

Summary (one-line)
Start cooperative, cooperate when the group looks cooperative (fraction ≥ 1 − 1/k), retaliate proportionally and briefly against drops in cooperation, forgive quickly when defectors repent, and always defect in the final round.
'''

description_COOPERATIVE_114 = '''
Strategy name: Forgiving Conditional Contributor (FCC)

Summary (one line)
- Start by cooperating to signal willingness. Thereafter, cooperate when the group has shown sufficient recent reciprocity; if cooperation drops, defect for a short, deterministic punishment window then forgive and attempt to re-establish cooperation. In the final round defect unless there has been a strong, sustained cooperative record.

Rationale
- Cooperate whenever mutual cooperation appears sustainable (maximizes long-run payoffs).
- Use brief, proportionate punishment to deter persistent free-riding (so cooperation is an equilibrium against reciprocators).
- Be forgiving (finite punishment) so accidental or one-off defections do not destroy cooperation permanently.
- Use only public history and game parameters (n, r, k) — no communication or external coordination.

Internal state and derived parameters
- Memory window W: number of most recent rounds used to estimate behavior.
  - Default: W = min(8, r). (Shorter when r is small.)
- Cooperation threshold p (fraction of other players cooperating required to trust the round as “cooperative”):
  - Default: p = clamp(0.50 + 0.25 * (k - 1) / (n - 1), 0.50, 0.75).
    - Intuition: larger k makes cooperation more socially valuable, so require slightly stronger signals to keep cooperating.
- Punishment length L (number of consecutive rounds to defect after a breakdown):
  - Default: L = min(3, max(1, ceil( (1 - p) * W ))) — a short, finite punishment; increases modestly if p is high.
- Final-round cooperation rule strictness p_final:
  - Default: p_final = 0.80 (require strong track record to cooperate in last round).

Observed history available each round t
- For every prior round s < t we observe who played C or D.
- Let coop_count(s) = number of players (including me in that round) who played C in round s.
- Let coop_others(s) = number of other players (excluding me) who played C in round s = coop_count(s) - c_me(s).
- Let remaining_rounds = r - t + 1 (including current if deciding for t).

Decision rules (natural language)
1. First round (t = 1): Cooperate. (Signal intent to cooperate.)
2. Every subsequent round t (2 ≤ t ≤ r):
   a. Compute recent cooperation among others: over the last W rounds (or all past rounds if fewer than W have elapsed), compute the fraction f of others who cooperated on average. Concretely:
      - For rounds s = max(1, t-W) ... t-1, average the value coop_others(s) / (n-1). Let f be that average (f ∈ [0,1]).
   b. If we are currently inside a punishment window (see below), play D.
   c. Else (not punishing now):
      - If t is the last round (t = r):
         - Cooperate only if f ≥ p_final AND all of the last min(W, r-1) rounds had a group cooperation fraction ≥ p (i.e., history shows strong, sustained cooperation). Otherwise, defect.
      - If t is not the last round:
         - If f ≥ p, play C (cooperate).
         - If f < p, start a punishment window of length L (unless a punishment is already in progress) and play D this round. The punishment window causes us to play D for the next L rounds (or until the game ends); after L rounds exit punishment and re-evaluate using the most recent history.
3. Punishment mechanics and forgiveness:
   - The punishment is global (we withhold cooperation entirely for L rounds). After L rounds of defecting, we resume normal assessment (cooperate if f ≥ p).
   - If during punishment we observe other players return to high cooperation (measured by f ≥ p over the last W rounds at the end of punishment), we resume cooperating. If not, further punishment windows may be triggered but each is still limited to length L (no permanent grim trigger).
4. Additional practical rule to avoid oscillation:
   - If we left punishment because f just reached p but then others immediately fall below p, require at least 1 full round of observed f ≥ p before cooperating again (i.e., require the recovery signal to be stable for at least one decision point).

Pseudocode

Parameters:
  W = min(8, r)
  p = clamp(0.50 + 0.25 * (k - 1) / (n - 1), 0.50, 0.75)
  L = min(3, max(1, ceil((1 - p) * W)))
  p_final = 0.80

State:
  punishment_remaining = 0

On round t (1..r):
  if t == 1:
    action = C
    update history
    return action

  # compute f = average fraction of others cooperating over last W past rounds
  lookback_start = max(1, t - W)
  numerator = 0
  denom = t - lookback_start
  for s in lookback_start .. t-1:
    numerator += coop_others(s) / (n - 1)
  f = numerator / denom   # average fraction in [0,1]

  if punishment_remaining > 0:
    action = D
    punishment_remaining -= 1
    update history
    return action

  if t == r:
    if f >= p_final and for all s in lookback_start .. t-1: (coop_count(s)/n) >= p:
      action = C
    else:
      action = D
    update history
    return action

  # normal (non-final) decision
  if f >= p:
    action = C
  else:
    # start punishment
    action = D
    punishment_remaining = L - 1   # we already play D this round, so L-1 rounds left
  update history
  return action

Edge cases and notes
- Small r (short games): if r ≤ 3, W becomes small and the strategy will be correspondingly reactive; the last-round rule is strict and likely leads to defection in the final round unless near-universal cooperation occurred earlier.
- Tuning: p, W and L are derived from (n,k,r) so the strategy adapts:
  - Higher k pushes p up modestly (cooperation is more valuable, so need stronger signal to risk contributing).
  - Larger W means more stable estimates; limited to 8 to keep responsiveness.
  - L is small and bounded (≤3) to avoid long mutual punishment spirals, but increases slightly when p is high.
- Deterministic and uses only public history (no private signals), so it satisfies the implementation constraints.
- The strategy is forgiving (finite punishments) and avoids permanent grim triggers that can be exploited by groups of defectors.
- It is robust: cooperates when others reciprocate, punishes brief defections to deter persistent exploitation, and forgives so cooperation can be reestablished.

Why this should perform well in tournaments
- It signals cooperation early and rewards reciprocation (many reciprocal strategies will converge to mutual cooperation).
- It retaliates briefly and predictably against persistent defectors, reducing their payoff advantage.
- It avoids being locked into endless retaliation (forgiveness), which improves long-run expected payoff against mixed populations.
- The final-round rule avoids being exploited in the endgame unless there is a strong, reliable track record of cooperation.

Possible variants (implementer choices)
- Make the punishment targeted: if identities are tracked, reduce your trust only for specific persistent defectors and resume cooperating if a sufficient fraction of the non-persistent set cooperates. (This is more complex but can punish culprits while maintaining cooperation with reciprocators.)
- Add small randomness when cooperating at the very end (to avoid strict exploitation by clever opponents), but randomness should be used carefully in deterministic tournament settings.

This description is complete and implementable with only the parameters n, r, k and the public action history.
'''

description_COOPERATIVE_115 = '''
Strategy name: Adaptive Conditional Cooperator (ACC)

High-level idea
- Open cooperatively to signal intent.
- Continue cooperating as long as the group shows reasonably high, recent cooperation.
- If the group drops below a cooperation threshold, respond with a short, proportionate collective punishment (defect for a few rounds) targeted at restoring cooperation rather than destroying long-term gains.
- Forgive quickly after punishment if cooperation resumes.
- In the last round defect (safe endgame choice) to avoid guaranteed exploitation.

This strategy depends only on the parameters (n, k, r) and on the observed history of actions each round. It is robust because it (i) signals cooperation early, (ii) punishes only when group cooperation is low and only for a limited time, and (iii) forgives to allow recovery if opponents adapt.

Key derived parameters (computed from game parameters)
- L = min(10, r) -- length of the recent-history window we use (cap memory for small r).
- α = (k - 1) / (n - 1) ∈ (0,1) -- normalized public-good strength.
- Threshold τ = clamp(0.35, 0.55 - 0.20·α, 0.75) -- required recent group cooperation fraction to keep cooperating. (Lower τ when public good is stronger; range restricts extremes.)
- MaxPunish = min(3, max(1, r // 6)) -- maximum consecutive punishment rounds (scaled to horizon).
- EndgameRounds E = 1 -- number of final rounds in which we defect unconditionally (avoid endgame exploitation).

State variables (maintained by the agent)
- punishment_timer: integer ≥ 0, rounds left in an active punishment phase (initially 0).
- history: observed actions of all players for previous rounds (list of length t-1 at decision time).

Decision rules (natural-language + pseudocode)

Overall rule:
- If current round t is in the last E rounds (t > r - E): play D (defect).
- Else if punishment_timer > 0: play D and decrement punishment_timer.
- Else use recent-history test (below) to decide C or D.

Recent-history test:
- Let H be the last L rounds of observed group actions (if fewer than L rounds exist, use all previous rounds).
- For each round s in H compute coop_count_s = number of players who played C in that round.
- Let recent_frac = average_s (coop_count_s / n)  — fraction of cooperators averaged over the recent window.
- Let last_frac = coop_count_{most recent round} / n (if there is at least one past round; else treat as 0).

Decision from recent_frac and last_frac:
- If there is no history (round 1): play C (start cooperatively).
- If recent_frac >= τ: play C (group has been cooperative).
- Else (recent_frac < τ): initiate a short punishment:
    - Compute shortfall = τ - recent_frac (in [0, τ]).
    - Set punishment_duration = min(MaxPunish, 1 + floor((shortfall / τ) * MaxPunish)).
      (i.e., proportionally longer punishment for larger shortfall, at least 1 round, capped)
    - Set punishment_timer = punishment_duration - 1 (we will play D this round and then decrement).
    - Play D this round.

Forgiveness / exit from punishment:
- After punishment_timer expires, do not immediately re-punish unless recent_frac is still below τ. If recent_frac has recovered to ≥ τ, resume cooperating.
- If recent_frac is still below τ after punishment, repeat proportionate punishment but keep MaxPunish cap.

Edge cases and small-r behaviour
- First round (t = 1): play C to signal cooperative intent.
- Last round(s) (t > r - E): defect unconditionally to avoid exploitable final-round cooperation.
- Very short games (r small): L and MaxPunish scale down; punishment stays small relative to r to avoid wasting the whole game.
- Persistent defectors: punishment is collective and limited. The strategy tolerates some persistent defectors (it punishes group low-cooperation but is forgiving) to avoid endless mutual defection and to maximize payoff against mixed populations.
- If all others consistently cooperate except occasional one-off defections, ACC punishes briefly then forgives; it therefore maintains cooperation with conditional cooperators and retaliates against systematic free-riding.

Pseudocode

Inputs: n, k, r, history (list of rounds; each round is list of n actions {C,D}); current round t (1-based)
State: punishment_timer (initially 0)

Compute:
  L = min(10, r)
  α = (k - 1) / max(1, n - 1)
  τ = clamp(lower=0.35, value=0.55 - 0.20*α, upper=0.75)
  MaxPunish = min(3, max(1, r // 6))
  E = 1

Decision procedure:
  if t > r - E:
      action = D
      return action

  if punishment_timer > 0:
      action = D
      punishment_timer -= 1
      return action

  if history is empty:
      action = C
      return action

  H = last min(L, len(history)) rounds from history
  recent_frac = average over s in H of (count_C(s) / n)
  last_frac = (count_C(last round) / n)

  if recent_frac >= τ:
      action = C
      return action
  else:
      shortfall = τ - recent_frac   # positive
      punishment_duration = 1 + floor((shortfall / τ) * MaxPunish)
      punishment_duration = min(punishment_duration, MaxPunish)
      punishment_timer = punishment_duration - 1
      action = D
      return action

Rationale and expected behavior
- Opening with C helps seed cooperative equilibria against reciprocators.
- Using a windowed average stabilizes the decision against isolated mistakes or rare defections (leniency), while reacting to systematic decline in cooperation.
- Proportionate and capped punishment prevents catastrophic long grudges which would lower total payoffs; it also gives defectors a clear incentive to resume cooperating (they pay a short cost).
- Forgiveness lets the group recover and prevents permanent collapse into mutual defection.
- The last-round defection avoids being exploited by unconditional defectors at the end of the game; the strategy still sustains cooperation for the earlier rounds wherever opponents reciprocate.

Notes for implementation and tuning
- The numeric bounds (L=10, τ formula, MaxPunish) are deliberately conservative defaults. They are chosen to balance responsiveness and forgiveness across a wide range of n, k, r without knowing opponents.
- If an implementer wants a more aggressive approach that punishes longer, increase MaxPunish or τ; to be more forgiving, decrease τ or MaxPunish.
- The strategy uses only observed actions and known parameters; it does not assume shared norms or communication.

Summary
ACC is a simple, parameter-adaptive conditional-cooperation strategy: start cooperative, continue while recent group cooperation is above a k- and n-informed threshold, punish briefly and proportionally when cooperation collapses, then forgive to allow cooperation to be re-established. It is designed to be robust in tournaments against diverse, unknown opponents.
'''

description_COOPERATIVE_116 = '''
Strategy name: Adaptive Generous Conditional Cooperator (AGCC)

High-level idea
- Start by signaling cooperative intent (cooperate).
- Condition future cooperation on observed behavior of the group over a short recent history (reciprocation by the group).
- If you are exploited (you cooperated but many others did not), retaliate proportionally but briefly.
- Be forgiving: after a punishment period, return to cooperation if the group shows enough cooperation.
- Occasionally probe by cooperating after long mutual defection to try to restore cooperation.
- Near the end of the game, reduce generosity (shorter patience) to limit exploitation from endgame effects.

This strategy depends only on game parameters (n, r, k) and the observed history (who cooperated each past round). It makes no assumptions about opponents’ norms or agreements.

Parameters (derived from game parameters; implementer can change defaults)
- w = min(5, r - 1)  // sliding window length for recent history
- target_fraction = clamp(0.5, 0.3, 0.7) // baseline fraction of other players cooperating we consider “good” (default 0.5). Optionally tune toward lower values if k is much larger than 1.
- target_count = ceil((n - 1) * target_fraction) // how many OTHER players cooperating we want to see
- P_max = min(4, max(1, floor(r/6))) // maximum punishment length in rounds
- Punish_scale = 1 // punishment proportionality: number of rounds to punish per defector observed (can be 1)
- Probe_interval = max(3, ceil(r/10)) // if there has been extended mutual defection, probe cooperation every Probe_interval rounds
- Endgame_window = min(2, r - 1) // how many last rounds we treat as “endgame” (be less generous)

State variables maintained during play
- punishment_counter (initially 0)
- last_action (C for cooperate or D for defect; initialize to C because round 1 we cooperate)
- rounds_since_probe (initially Probe_interval to allow immediate probe if needed)

Decision rules (natural language + pseudocode)

Initialize:
- Round 1: play C (cooperate). Set last_action = C, punishment_counter = 0, rounds_since_probe = Probe_interval.

Each round t ≥ 2, do:
1. Observe history:
   - For last round (t-1): total_coops_last = sum_j c_j,(t-1)  // includes you
   - For last w rounds (or all previous if fewer exist), compute avg_other_coop = average over those rounds of ((sum_j c_j,round) - c_i,round) // fraction of other players who cooperated on average (count / (n-1))
   - In particular, last_round_other_coops = total_coops_last - c_i,(t-1)

2. If punishment_counter > 0:
   - Play D (defect).
   - Decrement punishment_counter by 1.
   - Set last_action = D; increment rounds_since_probe.
   - Continue to next round.

3. Endgame adjustment:
   - If t > r - Endgame_window (we are in the final Endgame_window rounds):
       - Be stricter: require that last_round_other_coops ≥ target_count to cooperate now; otherwise defect.
       - If cooperating now, set last_action = C and reset rounds_since_probe = 0; else set last_action = D and increment rounds_since_probe.
       - (No additional punishment is started in the last Endgame_window; punishment_counter remains 0.)

4. Normal decision (not currently punishing and not in endgame):
   A. If last_action == C and (last_round_other_coops >= target_count):
       - Others reciprocated your last cooperation. Continue cooperating: play C.
       - last_action = C; reset rounds_since_probe = 0.
       - (No punishment.)
   B. If last_action == C and (last_round_other_coops < target_count):
       - You cooperated but insufficient others cooperated: you were exploited.
       - Compute number_of_defectors_last_round = n - total_coops_last.
       - Set punishment_length = min(P_max, Punish_scale * number_of_defectors_last_round).
       - Set punishment_counter = max(1, punishment_length). Immediately play D this round (start punishment).
       - last_action = D; rounds_since_probe++.
   C. If last_action == D:
       - Check recent group behavior:
         - If last_round_other_coops >= target_count OR avg_other_coop >= target_fraction * (n - 1) (i.e., group has been cooperatively inclined recently):
             - Forgive and cooperate: play C. last_action = C; reset rounds_since_probe = 0.
         - Else (group has been largely defecting recently):
             - Continue defecting, but occasionally probe: 
                 - If rounds_since_probe >= Probe_interval:
                     - Probe: play C once to test whether cooperation can be reestablished. Set rounds_since_probe = 0, last_action = C.
                 - Else:
                     - Play D. last_action = D; rounds_since_probe++.

5. End of round book-keeping:
   - If punishment_counter was set earlier, it was already decremented at the start of next rounds as rule (2) covers that.
   - Update any moving averages as needed for next round.

Rationale and properties

- Cooperative signalling: Starting with C and cooperating when the group reciprocates quickly establishes mutual cooperation with like-minded opponents.
- Proportional punishment: If you are exploited (you cooperated but many others defect), you punish for a short period proportional to the observed exploitation (number of defectors). This deters opportunistic one-shot exploits while avoiding permanent breakdown.
- Forgiveness and probing: After punishment ends, AGCC forgives and will cooperate if the group shows signs of cooperating; if the group remains uncooperative, AGCC continues defecting but probes periodically to give cooperation a chance to restart. This avoids lock-in to mutual defection.
- Robustness: The strategy relies on observed counts of cooperators (public information) so it adapts to many opponent types (always-defect, conditional cooperators, noisy cooperators, mixed strategies). Short punishment horizon and probing make it resilient to noisy or stochastic opponents.
- Endgame handling: The strategy becomes stricter in the final few rounds to reduce the risk of being exploited in the known endgame, while still cooperating if direct recent reciprocity is present.
- Parameter sensitivity:
   - w small (e.g., 5) makes the strategy responsive; increasing w smooths fluctuations.
   - P_max caps punishment so you do not get trapped in long mutual retaliation.
   - Probe_interval ensures you occasionally test for renewed cooperation.
   - target_fraction can be tuned for the environment; 0.5 is a good default (require majority of others cooperating).

Edge cases handled explicitly
- First round: cooperate (signal).
- Immediate exploitation (you cooperated, many others defect): punish starting immediately, for a short proportional length.
- Continuous exploitation: repeated short punishments enforce pressure on defectors without permanent war.
- Mutual defection: probes periodically to restart cooperation.
- Last round(s): stricter cooperation rule reduces risk of being exploited due to backward induction; still cooperate if recent behavior gives strong reason.
- Very short games (small r): w and P_max adapt because both use r in their formulas; punishment stays proportionally small.

Example behavior sketches
- If many opponents are conditional cooperators who also start with C, AGCC will converge to all-C quickly and maintain cooperation.
- If opponents are all-D, AGCC will try to cooperate first, then quickly switch to D and rarely probe, minimizing losses.
- If a few opponents defect intermittently, AGCC will punish proportionally and then forgive if cooperation returns, discouraging persistent exploiters.

Pseudocode (compact form)

Initialize:
  last_action ← C
  punishment_counter ← 0
  rounds_since_probe ← Probe_interval
  play C in round 1

For each round t ≥ 2:
  observe total_coops_last = sum_j c_j,(t-1)
  last_round_other_coops = total_coops_last - c_i,(t-1)
  compute avg_other_coop over last w rounds (if available)

  if punishment_counter > 0:
    play D
    punishment_counter ← punishment_counter - 1
    last_action ← D
    rounds_since_probe ← rounds_since_probe + 1
    continue to next round

  if t > r - Endgame_window:
    if last_round_other_coops ≥ target_count:
      play C; last_action ← C; rounds_since_probe ← 0
    else:
      play D; last_action ← D; rounds_since_probe ← rounds_since_probe + 1
    continue

  if last_action == C:
    if last_round_other_coops ≥ target_count:
      play C; last_action ← C; rounds_since_probe ← 0
    else:
      defectors ← n - total_coops_last
      punishment_length ← min(P_max, max(1, Punish_scale * defectors))
      punishment_counter ← punishment_length
      play D; last_action ← D; rounds_since_probe ← rounds_since_probe + 1
  else: // last_action == D
    if last_round_other_coops ≥ target_count or avg_other_coop ≥ target_fraction*(n-1):
      play C; last_action ← C; rounds_since_probe ← 0
    else:
      if rounds_since_probe ≥ Probe_interval:
        play C (probe); last_action ← C; rounds_since_probe ← 0
      else:
        play D; last_action ← D; rounds_since_probe ← rounds_since_probe + 1

Closing note
AGCC is intentionally simple, uses only public counts to form decisions, and balances retaliation with forgiveness and probing. That mix reliably fosters cooperation with reciprocators while limiting exploitation by defectors. Implementers can tune target_fraction, w, P_max and Probe_interval to favor faster responsiveness or greater generosity depending on the tournament environment.
'''

description_COOPERATIVE_117 = '''
Strategy name: Reciprocal Forgiving Majority (RFM)

Goal
- Sustain high levels of cooperation early in the finite game by reciprocating the observed cooperative norm, while being robust to free-riders and occasional mistakes by forgiving after a finite, proportional punishment. Do not allow indefinite exploitation; defect in the final round (no future to enforce cooperation).

Key ideas (intuitive)
- Start by offering cooperation.
- Use recent history of all players to estimate the current cooperative norm (fraction of others who have cooperated recently).
- Cooperate when a clear majority of others have been cooperating recently.
- If cooperation falls below the majority threshold, punish by defecting for a short, finite number of rounds proportional to the shortfall, then forgive and try to re-establish cooperation.
- Always defect in the last round (no future punishment possible).

Parameters (computed from game inputs n and r)
- M (memory window size) = min(5, r − 1). Use the most recent M rounds (or all previous rounds if fewer than M exist).
- tau (cooperation threshold) = 0.60 (60% of other players cooperating in recent history).
- P_max (maximum punishment length) = min(4, r − 1). Cap punishment so it never extends into the final round.
- All parameter choices are small, finite and do not rely on opponents’ conventions; they can be tuned, but the defaults above are robust in many tournament settings.

State variables the strategy maintains
- history of actions of all players (given by the game).
- punishing_until: a round index; while current round ≤ punishing_until the strategy defects. Initially punishing_until = 0.

Decision rules (plain language)
1. Round 1: play C.
2. Last round (round r): play D.
3. For any intermediate round t (2 ≤ t ≤ r − 1):
   a. If t ≤ punishing_until: play D (we are in a punishment phase).
   b. Else compute coop_freq_other = (total number of contributions by the other n − 1 players in the last M rounds) / ((n − 1) × M). This is the recent fraction of others cooperating.
   c. If coop_freq_other ≥ tau: play C (we reciprocate the recent cooperative norm).
   d. If coop_freq_other < tau: begin a finite punishment:
      - Compute severity = tau − coop_freq_other (how far below threshold).
      - Set punishment_length P = min(P_max, max(1, ceil(severity × (n − 1)))).
        (Interpretation: bigger shortfalls and larger groups → longer punishment, but capped.)
      - Set punishing_until = min(r − 1, t + P − 1) (never punish into round r).
      - Play D this round (start the punishment phase).
4. After a punishment phase ends, return to the same rule above (forgiveness): cooperation resumes only when coop_freq_other ≥ tau again.

Pseudocode

Inputs: n, r, k, history (actions of all players each past round)
Initialize: punishing_until = 0

function decide(t, history):
  if t == 1:
    return C
  if t == r:
    return D
  if t <= punishing_until:
    return D

  M = min(5, r - 1)
  lookback = min(M, t - 1)
  sum_other_coops = sum over last lookback rounds of contributions by players j ≠ me
  coop_freq_other = sum_other_coops / ((n - 1) * lookback)

  tau = 0.60
  P_max = min(4, r - 1)

  if coop_freq_other >= tau:
    return C
  else:
    severity = tau - coop_freq_other
    P = min(P_max, max(1, ceil(severity * (n - 1))))
    punishing_until = min(r - 1, t + P - 1)
    return D

Design choices and rationale
- Start cooperating: gives the strategy a chance to establish mutual cooperation with cooperative opponents.
- Majority threshold tau = 0.60: requires a clear but not absolute majority; this reduces sensitivity to single defectors and tolerates a few free-riders while still reacting when cooperation genuinely declines.
- Short memory M (≤5): focuses on recent behaviour so strategy adapts quickly to changes and does not over-react to old events.
- Finite, proportional punishment: punishing length increases with the magnitude of the shortfall and with population size (more players means an offender pool is larger), but is capped to avoid permanent breakdown. Finite punishment with forgiveness is robust against noise and avoids the inflexibility of a permanent grim trigger.
- Never punish into the final round and always defect in the final round: in a finite game the final round cannot be enforced by future consequences, so defecting is prudent to avoid being exploited.

Edge cases and special notes
- r = 2: M = 1; we play C in round 1 (first move) and D in round 2 (last round). This is the best possible immediate attempt to foster cooperation while avoiding being exploited in the final round.
- Short games: memory and punishments are scaled so they never extend into the final round. If remaining rounds are few, punishments will be very short (or effectively skipped) to avoid pointless retaliation when few rounds remain.
- If many opponents are unconditional defectors, RFM will quickly switch to defecting but will continue to test cooperation occasionally by returning to C only when the observed coop frequency reaches tau — it therefore avoids being exploited.
- If some opponents cooperate a lot and some defect occasionally, RFM tends to sustain cooperation (forgiving isolated mistakes) while punishing persistent defection.
- No randomness is required, but one may add small randomized cooperation when coop_freq_other is near tau to break ties; default above is deterministic for transparency.

Why this is cooperative and robust
- RFM rewards groups that maintain a clear cooperative norm (it cooperates whenever the recent majority cooperates).
- It imposes short, proportional punishments when the group norm degrades, which deters free-riding while avoiding permanent collapse from accidental defections.
- Forgiveness allows a return to cooperation quickly, which is important in heterogeneous tournaments with mixed strategies.
- Defecting in the final round protects against being exploited when punishments are no longer credible.

This strategy is simple to implement, depends only on public history and game parameters, adapts to observed behaviour, punishes deviations sufficiently to deter exploitation, and forgives to re-establish cooperation — making it a practical and robust entrant in a multi-agent public-goods tournament.
'''

description_COOPERATIVE_118 = '''
Name: Forgiving Conditional Cooperator (FCC)

Goal: Sustain mutual cooperation where others are willing, punish defections enough to deter exploitation, but forgive and repair quickly so cooperation can recover. The strategy uses only the game parameters (n, r, k) and the observed history of actions; it is deterministic given chosen tuning constants.

High-level rules (summary)
- Start cooperatively.
- Track an exponentially weighted estimate p_hat of the fraction of other players who cooperate.
- Cooperate when recent group behavior indicates likely reciprocation (p_hat above a threshold).
- If a defection(s) is observed, retaliate by defecting for a limited, proportional number of rounds (punishment length scales with severity), then forgive.
- Be contrite after a self-defection (attempt to repair by cooperating if group is otherwise cooperative).
- Be cautious in the final rounds: default to defection in the very last rounds, but allow cooperation in the end if group trust is very high.

Tuning constants (fixed, but can be tuned for a tournament)
- alpha (EMA smoothing) = 0.4
- T_coop (cooperation threshold) = 0.60
- high_trust = 0.90
- max_punish K = 4 (maximum number of punishment rounds from a single trigger)
- endgame_window E = min(3, r-1) (number of final rounds in which the strategy is conservative)
- contrite_threshold = 0.75 (if we defected previously but others are still mostly cooperative, we will try repair)

Rationale for chosen ideas
- Start cooperatively to invite mutual cooperation.
- Use p_hat (EMA) rather than raw counts to be robust to occasional mistakes and to adapt to long-term behavior.
- Punish proportionally to fraction of defectors to avoid overreacting to isolated mistakes while still responding to widespread defection.
- Limited punishments and forgiveness prevent permanent collapse of cooperation (robustness against noise and occasional selfish players).
- Conservatism near the known end of the game reduces exploitation in the terminal rounds (addresses finite horizon).

Pseudocode

Inputs:
- n, r, k (game parameters)
- history: for each past round t' < t, actions[t'][j] for all players j (C or D)
- my_index (our player index)

State variables (persist across rounds):
- p_hat (initial value = 1.0) — estimated fraction of other players who cooperate
- punishment_timer (initial value = 0) — remaining rounds to stay in punitive D mode
- last_action (initial value = C)

At each round t = 1..r do:

1. If t > 1:
     - Compute fraction_coop_others_last = (# of other players who played C in round t-1) / (n - 1)
     - Update EMA: p_hat := alpha * fraction_coop_others_last + (1 - alpha) * p_hat
   Else (t == 1):
     - No update (p_hat remains initial 1.0)

2. If punishment_timer > 0:
     - Action := D
     - punishment_timer := punishment_timer - 1
     - After decrement, optionally reduce punishment early if others clearly cooperated last round:
         if t > 1 and fraction_coop_others_last >= 0.8:
             punishment_timer := max(0, punishment_timer - 1)
     - Set last_action := Action and play.

3. Else (not currently punishing):
     - If t == r (last round):
         - If p_hat >= high_trust AND (t == 1 or fraction_coop_others_last == 1.0):
             Action := C   // very high trust: cooperate even in last round
           Else:
             Action := D   // default safe play in known last round
         - Set last_action := Action and play.

     - Else if t > r - E (in endgame window except last-round processed above):
         - If p_hat >= high_trust AND fraction_coop_others_last == 1.0:
             Action := C
           Else:
             Action := D
         - Set last_action := Action and play.

     - Else (normal non-endgame round):
         - If last_action == D AND t > 1 AND fraction_coop_others_last >= contrite_threshold:
             // contrite repair: we defected previously but group is otherwise cooperative
             Action := C
             Set last_action := C and play.
         - Else if p_hat >= T_coop:
             Action := C
             Set last_action := C and play.
         - Else:
             // trigger a punishment proportional to observed defection severity
             Action := D
             if t > 1:
                 f_defect_last := 1 - fraction_coop_others_last   // fraction of others who defected last round
                 if f_defect_last > 0:
                     // punishment length proportional to severity, bounded by K and remaining rounds
                     punish_len := min(K, max(1, ceil(f_defect_last * K)))
                     // ensure punishment doesn't extend beyond the game
                     punish_len := min(punish_len, r - t)
                     punishment_timer := max(punishment_timer, punish_len)
             Set last_action := D and play.

Notes and clarifications
- Initial optimism p_hat = 1.0: we open by assuming others may cooperate and invite mutual cooperation. This design is intentionally cooperative on the first move.
- EMA smoothing alpha (0.4) balances responsiveness and noise immunity. You can reduce alpha to be slower or increase to react faster.
- T_coop = 0.60 means we cooperate when we estimate that at least ~60% of others cooperated on average recently; this value is chosen to favor cooperation when a majority is reciprocating but to avoid exploitation by marginal cooperation.
- Punishment is limited (K ≤ 4) so the strategy does not lock into permanent retaliation. Punishments are proportional to the fraction of defectors in the previous round, so a single defector among many causes short punishment; mass defection prompts longer punishment.
- Contrition: if we defected previously (maybe by mistake) but others remained mostly cooperative, we attempt to repair the relationship quickly by cooperating even if p_hat is not yet high.
- Endgame behavior: defection in last round is the safe default; cooperation in the very last rounds is allowed only if group trust is near perfect to avoid last-round exploitation while permitting mutual benefit when everyone is reliably cooperative.
- All decisions depend only on parameters and observed history (no external coordination).

Why this strategy is robust and cooperative
- It starts cooperatively and seeks to sustain mutual cooperation whenever others' recent behavior suggests reciprocation will occur.
- It punishes defections but not harshly in response to isolated mistakes, avoiding endless mutual punishment cycles.
- Forgiveness and contrition enable rapid recovery of cooperation after punishment or accidental defection.
- The rules are group-aware (use fraction of cooperators) which is important in n-player games: they avoid overreacting to one defector in a large group but respond to widespread free-riding.
- Conservatism in the very final rounds mitigates endgame exploitation while still permitting cooperation if trust is demonstrably high.

Implementation note
- The pseudocode maps directly to a simple stateful algorithm with O(n) work per round (to count cooperators). The constants (alpha, T_coop, K, E, contrite_threshold) can be tuned empirically for a specific tournament environment; the qualitative behavior (forgiving, proportional punishment, contrition, endgame caution) should be preserved.
'''

description_COOPERATIVE_119 = '''
Name: Adaptive Proportional Conditional Cooperator (APCC)

High-level idea
- Start cooperative to signal willingness.
- Condition cooperation on recent group cooperation (how many players contributed recently).
- If the group falls below a cooperation threshold, punish by defecting for a short, increasing, and limited number of rounds.
- Forgive and return to cooperation when the group restores cooperation.
- Be tolerant when the public-good multiplier k is large (cooperation is more socially valuable) and stricter when k is small.
- Defect in the final round (no future to sustain reciprocity).

This strategy depends only on (n, k, r) and the observable history of all players' contributions in past rounds.

Parameters (computed from game parameters)
- L = min(4, r-1)   // look-back window for assessing recent behavior
- base_threshold = 0.50
- threshold = clamp(base_threshold - 0.25 * (k - 1) / max(1, n - 1), 0.10, 0.90)
  - Intuition: as k increases (public good is more valuable), be more tolerant (lower threshold). Clamp to avoid extreme behavior.
- max_base_punish = 3    // base punishment length (rounds)
- max_total_punish = 6   // absolute cap on punishment length
- forgiveness_reset = 1  // when group meets threshold again, exit punishment
- (All clamps and multipliers are constants chosen for clarity; implementations can tune them.)

State variables maintained during tournament
- punish_until_round (initially 0)
- consecutive_violations (initially 0)
- history: for each past round t, number_cooperators[t] observed (including self)

Decision rules (per round t)
1. If t == 1:
   - Cooperate. (Signal cooperation.)

2. If t == r (the final round):
   - Defect. (No future to sustain reciprocity; protect against last-round exploitation.)

3. If current round t <= punish_until_round:
   - Defect. (We are punishing.)

4. Otherwise (not currently punishing):
   - Compute recent_coop_fraction = average over last L rounds of (number_of_cooperators_in_round / n).
     - If there are fewer than L prior rounds, use whatever history exists.
     - Note: the fraction counts all players (including you) — this makes it robust and simple. Optionally you may exclude your own past action; either is acceptable. (Pseudocode below excludes self from last round when deciding, but the summary fraction can include all players — consistent implementations should be explicit; sample pseudocode below excludes self in the immediately prior round.)
   - If recent_coop_fraction >= threshold:
       - Cooperate.
       - consecutive_violations := 0
   - Else:
       - Treat this as a violation:
         - consecutive_violations := consecutive_violations + 1
         - compute punishment_length = min(max_total_punish,
                                         max_base_punish * consecutive_violations)
         - set punish_until_round := min(r-1, t + punishment_length - 1)  // never punish the final round (we already defect then)
         - Defect this round.

Additional forgiveness behavior
- After punish_until_round expires, we re-evaluate the recent_coop_fraction. If it has recovered to >= threshold, we resume cooperating. If not, we may immediately re-enter punishment (consecutive_violations continues to grow).

Optional refinement (more targeted and adaptive)
- Track per-player cooperation rates over the whole history. If a subset of players are persistent defectors (cooperation rate below a low bound, e.g., 10%), we can lower effective threshold to avoid repeatedly punishing the whole group for their presence: set threshold := max(threshold - 0.1, 0.10). This prevents endless cycles when a few players never cooperate but others do.

Pseudocode (clear, implementable)
Assume rounds indexed t = 1..r. At start set punish_until_round = 0, consecutive_violations = 0.

function decide_action(t, history, n, k, r):
  // history: list of length t-1; each element is number_cooperators in that past round (including self)
  if t == 1:
    return C
  if t == r:
    return D

  L = min(4, r-1)
  base_threshold = 0.50
  threshold = clamp(base_threshold - 0.25 * (k - 1) / max(1, n - 1), 0.10, 0.90)

  if t <= punish_until_round:
    return D

  // compute recent cooperation fraction excluding self in the most recent round for finer responsiveness
  // Build a list fractions over last min(L, t-1) rounds
  lookback = min(L, t-1)
  sum_frac = 0
  for j from t - lookback to t - 1:
    // If you want to exclude your own contribution in round j, subtract your action; but history should record number of cooperators.
    // Here we use fraction including all players:
    sum_frac += history[j] / n
  recent_coop_fraction = sum_frac / lookback

  if recent_coop_fraction >= threshold:
    consecutive_violations = 0
    return C
  else:
    consecutive_violations = consecutive_violations + 1
    punishment_length = min(max_total_punish, max_base_punish * consecutive_violations)
    punish_until_round = min(r - 1, t + punishment_length - 1)
    return D

Rationale and properties
- Cooperative signal: Starting with cooperation signals willingness to cooperate and allows mutual cooperators to form early.
- Conditional cooperation: Using a recent fraction controls cooperation by the observed group behavior rather than by one defection or small noise. The lookback L > 1 reduces reaction to single mistakes/noise.
- Forgiving, proportional punishment: Punishment lengths grow with repeated violations, but are capped. This deters persistent exploitation while allowing for recovery and preventing permanent mutual defection (the system is forgiving).
- k-adaptiveness: threshold decreases as k increases — when public-good returns are larger, it's worth tolerating more short-term defections to preserve cooperation.
- Robust against many opponents: The strategy reacts to aggregate behavior, so it adapts whether opponents are many defectors, conditionally cooperative, or strategically varied.
- Practical endgame: Defecting in the final round prevents being exploited when there is no future to reward cooperation. We keep punishments from extending into round r (we already defect round r).
- Simplicity and implementability: Uses only counts of cooperators per round and a few counters; no need to model opponents individually unless using the optional refinement.

Examples (intuition)
- If most players cooperate consistently, recent_coop_fraction will be high and APCC will cooperate every round (until last) — achieving cooperative outcomes.
- If many players defect frequently, APCC will detect the low recent fraction and defect for short punishments, protecting itself from being exploited.
- If some players occasionally defect (noise), the lookback window and threshold avoid overreaction; the strategy tolerates isolated defections.

Design choices you can tune
- L (lookback length): larger L = more inertia / robustness to noise; smaller L = more reactive.
- base_threshold and k-weighting: change how tolerant the strategy is as k varies.
- max_base_punish and max_total_punish: determine punishment severity and maximum recovery time.
- Final-round behavior: you can choose to cooperate in final round if you prefer absolute cooperativeness at cost of last-round exploitation risk; APCC defaults to defect in last round to protect payoff.

Why this is appropriate for an AI tournament
- It is simple, transparent, and parameterized by (n, k, r).
- It starts cooperative to capture mutual-cooperation gains, defends against exploitation, and forgives to allow cooperation to re-emerge.
- It does not rely on coordination, norms, or message passing — only observed actions and payoffs.
- It adapts to the magnitude of the public good (k) and the number of rounds (r), and it is robust to a variety of opponent behaviours.
'''

description_COOPERATIVE_120 = '''
Strategy name: Adaptive Forgiving Collective (AFC)

Intuition (short):
- Signal willingness to cooperate by cooperating initially.
- Cooperate as long as the group shows sufficiently high cooperation recently.
- If the group defects enough, respond with a measured, proportional punishment (defect for a few rounds) so defectors lose short-term gains.
- Be forgiving after punishment and allow recovery tests (probabilistic cooperation) to re-establish cooperation.
- Avoid being the lone cooperator in the final round unless full cooperation has been sustained.

AFC depends only on (n, k, r) and the public history of contributions each round. It is robust to one-shot defectors, noise, and mixed opponent behavior because punishments are proportional and forgiving; it resists exploitation but does not lock into permanent retaliation.

Parameters (computed from game parameters; you can tune constants):
- T = ceil(n/2)   (cooperation threshold: treat a round as “good” if a majority cooperated)
- base_punish = 1 (minimum punishment length)
- punish_scale = 1 (punishment increases roughly in proportion to number of defectors)
- max_punish = max(1, floor(r/4)) (cap punishment length relative to remaining game length)
- probe_rate = 0.15 (probability to probe (cooperate) during recovery or in lenient moments)
All of these are simple functions or small constants so implementation is deterministic given n,k,r (probe_rate can be deterministic via a hash/PRNG seeded by public history if implementation needs reproducibility).

High-level decision rules

1) First round (t = 1)
- Play C. (Signal cooperative intent.)

2) Any round while in punishment mode
- If a current personal punish_counter > 0, play D and decrement punish_counter each round.
- After punish_counter reaches 0, move to "post-punishment recovery" (see below).

3) Rounds t = 2 .. r-1 (not in forced punishment)
Let m_prev = number of cooperators observed in the previous round (0..n).
Let total_defections_recent = number of rounds so far where m < T (optional accumulation measure for stronger responses to repeated poor behavior).

Decision:
a) If m_prev >= T then
   - Cooperate (C). The group recently showed majority cooperation; reward that.
b) If m_prev < T then
   - Trigger a proportional punishment:
     punish_length = min(max_punish, base_punish + punish_scale * (n - m_prev))
     Set punish_counter = punish_length.
     Play D this round (start punishment immediately).
   - (If punish_counter already set from earlier, follow the punishment mode above.)

4) Post-punishment recovery and probing
- After a punishment period ends, resume cooperating but occasionally (with probability = probe_rate) probe by cooperating even if previous round was poor; this allows testing whether others return to cooperation and avoids permanent breakdown. Implementation must produce the same probabilistic choice given the public history (e.g., pseudo-random from previous-round actions string).

- If multiple successive rounds after punishment still have m_prev < T, escalate by setting a new punish_counter using the same proportional rule (punishment is thus reactive and graded).

5) Last round (t = r)
- Be cautious: defect unless the entire history so far has been consistently cooperative.
- Concretely:
   - If in every previous round all n players cooperated (full cooperation history), play C in the last round.
   - Otherwise play D in the last round.
Reason: last-round cooperation is only safe if mutual cooperation has been sustained; otherwise the last round is vulnerable to being exploited.

Pseudocode (natural-language/pseudocode hybrid)

Initialize:
  punish_counter = 0
  history = empty list of past contribution vectors (or just counts m_t each round)

On round t (1..r):
  if t == 1:
    play C
    continue

  if punish_counter > 0:
    play D
    punish_counter -= 1
    record action and observe others; continue

  if t == r:   // last round logic
    if for all past rounds s < r: m_s == n:
      play C
    else:
      play D
    continue

  // normal (not punishing, not first round, not last)
  m_prev = number of cooperators in round t-1

  if m_prev >= T:
    play C
  else:
    // start proportional punishment
    defectors_prev = n - m_prev
    punish_length = min(max_punish, base_punish + punish_scale * defectors_prev)
    punish_counter = punish_length
    play D

  // After playing, record actions and observed group counts

Recovery/probing (implemented as part of normal step):
  - If punish_counter == 0 and previous round(s) had m_prev < T, then with probability probe_rate play C to check if group is willing to return to cooperation; otherwise follow the rule above deterministically.

Notes and rationale

- Why cooperate first? It invites cooperation and can produce high mutual payoffs if others reciprocate. In tournaments, generous initial cooperation tends to earn high rewards when matched with reciprocal strategies.

- Why majority threshold T = ceil(n/2)? Majority cooperation is a simple robust signal that cooperation is the prevailing norm in the group. Using majority avoids overreacting to a single defector in a large group, and avoids setting thresholds tied to k/n (which is always < 1 and not directly informative about others’ willingness to cooperate).

- Why proportional, temporary punishment? Permanent grim-trigger is brittle: a single defection (or a noisy mistake) can collapse cooperation forever. Proportional finite punishments make defection costly for deviators while allowing retrieval of cooperation. The punishment length scales with the number of defectors in the preceding round so a one-off defection is met with short punishment, mass defection with longer punishment.

- Why forgiveness/probing? To re-establish cooperation after punishment or noisy mistakes. A small, controlled rate of probing (or deterministic schemes producing occasional probes) gives the group a way back to cooperation.

- Why defect in last round unless perfect history? In a finite, known-horizon game, last-round cooperation is exploitable unless you can be sure others will also cooperate. Full sustained cooperation to date is a strong signal that others will reciprocate even in the last round; otherwise defect to avoid being taken advantage of.

Edge cases and additional implementation choices

- Noisy observation: If you suspect observation noise (not in the spec), enlarge probe_rate and shorten punishments.

- If many players keep defecting persistently: punishments will be repeatedly triggered; AFC will eventually spend many rounds defecting, which prevents you from being exploited and reduces your losses as cooperation collapses societally. If opponents respond to punishment by returning to majority cooperation, AFC restores cooperation quickly.

- Deterministic versus pseudorandom probing: If the tournament requires determinism, make probe decisions deterministically from a hash of the public history (so other players can observe behavior as deterministic but unpredictable initially).

- Parameter tuning: The constants base_punish, punish_scale, max_punish, probe_rate can be tuned against likely opponent pools. Decrease punish_scale and max_punish to be more forgiving; increase them to deter heavy exploitation.

Example behavior (n=6, r large):
- Round 1: C (signal)
- If round 1 saw 5 or 6 cooperators (>= T = 3): continue cooperating.
- If round 1 saw only 2 cooperators: start punish for punish_length = 1 + 1*(6-2)=5 rounds (capped by max_punish), so defect for a few rounds, then probe after.

Summary
AFC is cooperative (starts by cooperating, seeks and rewards majority cooperation, returns to cooperation when group behavior improves), adaptive (punishments scaled to magnitude of deviation; probing to test recovery), and robust (punishment deters exploitation; forgiveness prevents permanent collapse). It uses only (n, k, r) and the public history of contributions, so it is implementable in the tournament setting you described.
'''

description_COOPERATIVE_121 = '''
Summary (intuition)
- Aim to establish and maintain mutual cooperation early by default, while deterring persistent free-riding with short, proportional punishments and generous forgiveness so cooperation can recover.
- Use only game parameters (n,k,r) and observed history (who cooperated each previous round). No communication or assumptions about opponents.
- Be cautious near the end of a finite game: shorten punishments and rely more on observed recent cooperation because long punishments have little bite late in the game.

Key ideas built into the rules
- Start cooperatively to give cooperation a chance.
- Judge others by the fraction of other players who cooperated in the last round(s). Require a higher observed fraction when the public-good multiplier k is small (cooperation is harder to sustain), be more lenient when k is large.
- If too few others cooperated, punish by defecting for a number of rounds proportional to how many defected (proportional, not permanent).
- Always include probabilistic forgiveness during punishment and occasional “test cooperations” to allow rehabilitation after noise or mistakes.
- In the final few rounds, reduce or cancel punishments (there is less future to enforce cooperation), and rely on the same local rule for cooperation/defection.

Decision rules (natural-language + pseudocode)

Parameters derived from n,k,r (computed once at start)
- gamma (cooperation threshold): gamma = clamp(1 - (k-1)/(n-1), lower=0.5, upper=0.95)
  - Interpretation: require fraction of other players cooperating ≥ gamma to continue cooperating. If k is small (cooperation yields little return) gamma is close to 1 (we require near-unanimity); if k is larger, gamma relaxes toward 0.5.
- max_punish = max(1, ceil(r / 4))  // never punish longer than ~quarter of the game
- forgiveness_prob phi = clamp(0.08 + 0.32*(k-1)/(n-1), lower=0.05, upper=0.5)
  - Higher k → more willingness to forgive.
- punish_scale lambda = 1.0  // punishment rounds per defections (tunable)
- endgame_window L = min(3, max(1, floor(r/10)))  // number of final rounds where punishments are shortened/elided

State variables (updated each round)
- punish_timer (integer): remaining rounds of active punishment; initially 0
- last_punish_rounds (for bookkeeping, optional)

Pseudocode (per-round decision)
Inputs every round t: history of contributions for rounds 1..t-1 (each round: contributions vector of length n), my index i

function decide_action(t, history):
  if t == 1:
    // Give cooperation a chance
    return C

  // compute recent statistics (based only on last round)
  last = history[t-1]  // contributions vector of round t-1 (1 for C, 0 for D)
  coop_others_last = sum(last) - last[i]  // number of other players who cooperated last round
  frac_others = coop_others_last / (n - 1)

  // Endgame adjustment: if we are in final L rounds, shorten or avoid punishment
  rounds_remaining = r - t + 1
  endgame = (rounds_remaining <= L)

  // If currently in a punishment episode
  if punish_timer > 0:
    punish_timer = punish_timer - 1
    // During punishment: normally defect, but with small probability cooperate to test forgiveness
    if random_uniform(0,1) < phi:
      return C  // generous testing move
    else:
      return D

  // Not actively punishing: decide whether to cooperate or start punishment
  if frac_others >= gamma:
    // enough others cooperated last round -> cooperate
    return C
  else:
    // too few cooperators among others -> start proportional punishment
    num_defectors_last = (n - 1) - coop_others_last
    // punishment length is proportional to number of defectors but bounded
    proposed_punish = 1 + ceil(lambda * num_defectors_last)
    punish_timer = min(proposed_punish, max_punish)
    // If we are in the endgame, avoid long punishment: reduce to 1 round (or skip)
    if endgame:
      punish_timer = min(punish_timer, 1)
    // Defect immediately as the punishment
    return D

Additional practical safeguards and refinements
- Recovery check: after punish_timer reaches 0, the above rules immediately allow cooperation again if frac_others >= gamma. If many rounds of punishment pass and cooperation still fails to reappear, the strategy will continue to punish proportionally whenever frac_others < gamma. Because punishments are finite and proportional, the strategy is not permanently vengeful.
- Noise tolerance: random cooperative “tests” during punishment (with probability phi) allow the group to escape from accidental mutual-defection loops due to noise or mis-implemented opponents.
- Avoid long, futile punishments late in the game: punishments are shortened in the last L rounds because threats have less leverage there.
- Persistent defector detection: if across the last H = min(r, 5) rounds the average frac_others < gamma_low (e.g., gamma_low = 0.2), assume environment is mostly non-cooperative and switch to mostly defect to avoid exploitation. (This is optional but protects against sustained exploitation.)

Behavior in edge cases
- First round: cooperate. This gives cooperation a chance and signals willingness to cooperate.
- Last round(s): same logic applies but punishments are shortened/canceled in final L rounds, so the strategy avoids investing in punishments with little enforcement power. In the very last round the choice will be: cooperate only if many others cooperated in the previous round; otherwise defect.
- Very small r (e.g., r <= 4): shorten max_punish (already bounded by ceil(r/4)); gamma still applies; be cautious because there is limited horizon for punishment—if opponents are rational and will defect in last round, cooperation may not be sustainable, but this strategy still attempts cooperative moves early.
- Unexpected histories / opponents who use mixed or random strategies: the proportional punishment combined with forgiveness will tend not to be permanently exploited—occasional defectors get short punishments; persistent defectors are punished repeatedly but punishments remain finite so if opponents switch to cooperating they can be re-admitted.

Why this is cooperative and robust
- Cooperative: Starts with cooperation, cooperates whenever recent observed cooperation is strong (frac_others ≥ gamma), and returns to cooperation quickly when others resume cooperating.
- Robust detection & deterrence: Punishments are proportional to the observed extent of defection (more defectors → longer punishment), so single accidental defections are not harshly punished while mass defection is met with stronger deterrence.
- Forgiving & noise-tolerant: Randomized cooperative tests during punishment and bounded punishments prevent permanent breakdown from mistakes and allow rehabilitation.
- Endgame-awareness: Shrinking punishments near the last rounds prevents wasting efforts on punishments that cannot be enforced (reduces being exploited near the end).
- Parameter adaptivity: gamma, phi and punishment bounds are functions of (n,k,r), so the same rule behaves sensibly across different group sizes and multiplier regimes (requires near-unanimity when k is small; more lenient when k is large).

Minimal concrete parameter suggestions (implementer can tune)
- gamma = min(0.95, max(0.5, 1 - (k-1)/(n-1)))
- phi = clamp(0.08 + 0.32*(k-1)/(n-1), 0.05, 0.5)
- lambda = 1.0
- max_punish = max(1, ceil(r / 4))
- L = min(3, max(1, floor(r/10)))

Summary sentence
This strategy starts cooperative, cooperates when a high-enough fraction of others cooperated recently (threshold tuned by k and n), punishes defectors in proportion to how many defected, forgives with small randomized cooperative tests, shortens punishments late in the game, and switches to protective defection only if the environment remains persistently non-cooperative. It is explicitly designed to be cooperative, adaptive, and robust against a wide variety of opponent behaviors.
'''

description_COOPERATIVE_122 = '''
Name: Forgiving Conditional Cooperator with Proportional Punishment (FCC-PP)

Summary (one line)
- Start by signalling cooperation, then condition future contributions on how many others have cooperated recently; punish deviations proportionally but forgive and return to cooperation if the group recovers; always defect in the final round.

Intuition / goals
- Be cooperative when the group reciprocates (so mutual cooperation yields high aggregate payoff).
- Avoid sustained exploitation: stop contributing quickly if others defect.
- Use proportional, limited punishments (not infinite “grim” punishments) so cooperation can be restored after mistakes or after temporary exploitation.
- Become more cautious as the end of the game approaches (fewer future rounds to recoup the cost of cooperating).
- Use only public history (counts of cooperators each past round), the game parameters (n,r,k), and internal state (punishment counter).

Parameters (for implementation; all depend only on n,r,k and history)
- w = min(5, r-1)  // recent-window size used to estimate group reciprocity
- base_theta = 0.50  // baseline fraction of others cooperating required to cooperate
- alpha = 0.40       // how much the threshold tightens as the game progresses
- P_max = min(4, max(1, floor(r/4)))  // maximum punishment length; scales mildly with horizon
- theta is dynamic: theta(t) = clamp(base_theta + alpha * ((t-1)/(r-1)), 0.05, 0.95)
  (t = current round index, 1..r; clamp ensures numerically safe bounds)

State variables to maintain
- pun_remaining (integer) — rounds left to punish; initial 0
- history of observed cooperators per round: coop_count[s] for s < t (includes all players, or keep "others" counts)

Decision rules (deterministic)
1. If t == r (last round): choose D (defect). Rationale: last-round cooperation is never credible in finite horizon.
2. Else if pun_remaining > 0:
   - Choose D.
   - Decrement pun_remaining by 1.
3. Else if t == 1:
   - Choose C (signal cooperative intent).
4. Else (t in 2..r-1 and not currently punishing):
   - Let w_act = min(w, t-1). Compute p = fraction of other players who cooperated on average over the last w_act rounds:
       p = ( sum_{s=t-w_act}^{t-1} (coop_count[s] - my_c[s]) ) / ((n-1) * w_act)
     (if your implementation stores only total cooperators including you, subtract your own recorded action my_c[s]; if unknown because you may change, you should track your own past moves.)
   - Compute dynamic threshold theta = base_theta + alpha * ((t-1)/(r-1)) (clamped).
   - If p >= theta:
       - Choose C.
     Else:
       - Set pun_remaining = max(1, ceil( (theta - p) * P_max ))  // proportional punishment
       - Choose D this round (the first round of punishment).

Notes on the computation and deterministic tie-breaking
- If w_act = 0 (only possible in t=1, handled above), we follow the explicit t==1 rule.
- Use exact deterministic arithmetic: e.g., ceil((theta - p) * P_max) produces an integer ≥ 1.
- The punishment length is proportional to the shortfall (theta - p); larger deviations by the group produce longer punishments up to P_max.
- After pun_remaining expires, the normal comparison to p resumes (so the strategy is forgiving and can re-enter cooperation if the group recovers).
- If many players consistently defect (p remains low), the strategy will remain defecting (punishment resets whenever a violation is detected), thereby avoiding long exploitation.

Edge cases
- r = 2: t=1 -> C by rule; t=2 -> D (last round). This accepts a single opening cooperative move to reward any reciprocator, but avoids last-round exploitation.
- Short games (small r): window w and P_max automatically shrink; thresholds still adapt via theta(t).
- All-Defectors: the strategy quickly switches to defect (punishments), thus avoiding repeated exploitation after a short initial cost.
- All-Cooperators (or mostly cooperative group): the strategy cooperates every non-final round, maximizing mutual gains.
- Mixed populations: the strategy tolerates occasional single-person lapses (short punishment) and can restore cooperation if others resume cooperating; repeated mass defections are punished more strongly.
- No noise assumed: the algorithm tolerates occasional deliberate deviations; if you expect error/noise in implementations, increase w or reduce alpha to be more forgiving.

Why this is cooperative and robust
- Cooperative: the strategy starts cooperative, rewards cooperative behavior by contributing whenever a sufficient fraction of others has cooperated recently, and will return to cooperation when the group proves cooperative again.
- Robust: it does not assume norms or pre-arranged schedules, only observed counts of cooperators. It punishes only proportionally and for a limited time, which is robust against both persistent exploiters (you stop being exploited) and accidental one-off deviations (you don’t punish forever).
- Dynamically cautious: the cooperation threshold tightens as the end approaches, reflecting the shorter horizon to recover the cost of contributing.
- Tunable: base_theta, alpha, w, P_max can be adjusted for tournaments where you expect different mixes of strategies (more generous if most opponents look reciprocal; more conservative if many defectors).

Pseudocode

Initialize:
  pun_remaining = 0

At round t (1..r), given coop_count[s] for past rounds s < t and my past moves my_c[s]:

  if t == r:
    action = D
    return action

  if pun_remaining > 0:
    action = D
    pun_remaining -= 1
    return action

  if t == 1:
    action = C
    return action

  // Estimate recent cooperation rate among others
  w_act = min(5, t-1, r-1)
  sum_other_coop = 0
  for s in (t - w_act) .. (t - 1):
    sum_other_coop += coop_count[s] - my_c[s]    // number of other cooperators that round
  p = sum_other_coop / ((n - 1) * w_act)

  // Dynamic threshold (tighten as game progresses)
  theta = base_theta + alpha * ((t - 1) / (r - 1))
  theta = clamp(theta, 0.05, 0.95)

  if p >= theta:
    action = C
  else:
    pun_remaining = max(1, ceil((theta - p) * P_max))
    action = D

  return action

Parameter defaults recommended for a general tournament
- w = min(5, r-1)
- base_theta = 0.50
- alpha = 0.40
- P_max = min(4, max(1, floor(r/4)))

Optional refinements (if you want to tune further)
- Increase forgiveness (lower base_theta or alpha) if opponents are noisy or you see many small accidental lapses.
- Increase P_max or alpha if the tournament contains many pure defectors and you want to deter exploitation more strongly.
- Make P_max scale with k: a larger k means mutual cooperation is more valuable, so consider allowing longer punishments to sustain cooperation (e.g., P_max = min(6,floor(r/3)) if k is close to n).

Closing
This strategy is simple, deterministic, uses only the public history and game parameters, and balances an explicit cooperative starting posture with proportionate punishments and forgiveness. It performs well in mixed tournaments where some opponents reciprocate and some exploit, and it avoids naive, infinite punishments that prevent recovery to cooperation.
'''

description_COOPERATIVE_123 = '''
Strategy name: Graded Conditional Cooperator (GCC)

High-level idea
- Signal cooperation immediately, sustain cooperation when the group reciprocates, and use short, graded punishments when the group defects.  
- Punishments are proportional to the severity of recent defection, limited in length, and followed by a cooperative probe so the group can recover.  
- Always defect in the final round (to avoid being exploited in a last-period deviation).  
This strategy depends only on game parameters (n, k, r) and on the observed history of players’ actions; it requires no communication or shared convention.

Rationale (brief)
- Start cooperatively to encourage mutually beneficial play (all-C gives payoff k each round >1).  
- Use simple, local statistics of recent history (fraction of cooperators in recent rounds) to detect whether the group is cooperating.  
- Apply graduated, limited punishments rather than permanent retaliation (avoids cascade collapse and is robust to occasional mistakes).  
- Probe with cooperation after punishment to restore cooperation if others do so.  
- Defect in round r (last round) because there is no future to enforce reciprocity.

Parameters (computed from game inputs)
- W = window length for recent history = min(5, max(1, r-1)). (Use up to 5 most recent rounds to smooth noise; if there are fewer rounds observed, use what’s available.)
- Base threshold T0 = 0.5 (default majority threshold). Make the threshold more permissive when the public good multiplier k is larger (cooperation is more socially valuable), so we modestly lower the threshold as k increases:
  T = max(0.20, T0 - 0.30 * (k - 1) / (n - 1)).
  (T is clamped to at least 0.20 to avoid being exploitable when k is extremely large.)
- Maximum punishment length Pmax = min(5, max(1, floor(r / 10))). (Punishments are short and scale with game length.)
- Scaling for punishment severity: punish rounds P = 1 + ceil((T - f) / T * (Pmax - 1)), where f is the observed recent cooperation fraction (defined below). So larger shortfalls produce longer punishments, up to Pmax.
- Forgiveness policy: after any punishment period the strategy makes a single cooperative probe round to test whether the group returns to cooperation.

State kept from history
- For each past round we observed: the full vector of players’ actions (C/D). From this we compute f, the recent cooperation fraction among all players other than ourselves aggregated over last W rounds (value in [0,1]).

Decision rules (natural language)
1. Round 1: Cooperate.
2. Last round (round r): Defect.
3. For any round t (1 < t < r):
   a. If currently in a punishment interval initiated previously, Defect and decrement the remaining punishment counter.
   b. If not in punishment:
      - Compute f = average fraction of other players who played C over the last W rounds.
        (Formally: for rounds t-1 down to max(1,t-W), count how many times each other player played C; average per-round fraction of cooperators excluding myself.)
      - If f >= T, Cooperate.
      - If f < T, initiate punishment: set punishment counter to P = 1 + ceil((T - f)/T * (Pmax - 1)). Immediately Defect this round (the first round of punishment).
4. After a punishment counter reaches zero, perform one cooperative probe round (cooperate once). If the next observed f (after the probe) meets or exceeds T, resume normal cooperative play; otherwise start another punishment period (again computing P based on the most recent f).
5. Always record and use the observed full action profile of each round to update f for subsequent decisions.

Pseudocode

Inputs: n, k, r
Derived:
  W = min(5, max(1, r-1))
  T0 = 0.5
  T = max(0.20, T0 - 0.30*(k-1)/(n-1))
  Pmax = min(5, max(1, floor(r/10)))

State variables:
  punish_remaining = 0
  probe_next_after_punish = false
  history = list of observed rounds (each round: vector of n actions)

Helper function:
  compute_recent_f(t):  // called before deciding action in round t
    lookback_rounds = rounds max(1, t-W) .. t-1
    if lookback_rounds is empty: return 1.0  // treat as fully cooperative to encourage start
    sum_coop = total number of C actions by other players in those rounds
    total_slots = number_of_lookback_rounds * (n-1)
    return sum_coop / total_slots

Main decision procedure for round t:
  if t == r:
    return D   // last round: defect
  if t == 1:
    return C   // first-round cooperative signal

  if punish_remaining > 0:
    punish_remaining := punish_remaining - 1
    if punish_remaining == 0:
      probe_next_after_punish := true
    return D

  if probe_next_after_punish:
    probe_next_after_punish := false
    return C   // cooperative probe after punishment

  f := compute_recent_f(t)
  if f >= T:
    return C
  else:
    // start punishment proportional to severity
    deficit = T - f   // positive
    P = 1 + ceil((deficit / T) * (Pmax - 1))
    punish_remaining := P - 1   // current round is the first punishment round
    return D

Notes and justifications
- Start cooperative: a clear, generous signal that can bootstrap mutual cooperation with like-minded strategies.
- Using a small window W smooths short-term noise without overreacting to single mistakes. If no history is available (round 1), we treat f as fully cooperative so we cooperate.
- Threshold T balances responsiveness and caution. T is lowered modestly for larger k because cooperation is more socially valuable and more worth sustaining.
- Graded punishments make the strategy robust: a single defection provokes a short penalty; many simultaneous defections cause longer penalties. Punishments are capped (Pmax), avoiding endless retaliation and making recovery possible.
- The cooperative probe after punishment lets the group repair. If others return to cooperating, GCC resumes cooperation. If not, GCC applies further scaled punishments.
- Defecting in the final round avoids being exploited in the last-period, consistent with common tournament practice; GCC still cooperates up to round r-1 if the group is cooperative.
- The strategy only uses observed actions (not payoffs), but full observed action vectors allow precise calculation of f.

Behavioral profile (what you should expect)
- When many players (or most rounds) cooperate, GCC cooperates and helps sustain high collective payoff near the social optimum.  
- If the group drifts toward defection, GCC defects and applies a short calibrated penalty to discourage further defection.  
- GCC tolerates occasional mistakes (forgives after brief punishment and probes for recovery), so it is noise-robust.  
- GCC will not be exploited indefinitely: persistent defectors trigger repeated punishments.  
- GCC is adaptive: the threshold T and punishment lengths depend on n, k, r so the policy scales sensibly with game parameters.

Implementation notes
- The implementer must ensure that compute_recent_f excludes GCC’s own actions when counting “other players.” That makes the condition reflect how much the group besides GCC cooperates.
- All parameters (W, T, Pmax) are deterministic functions of (n, k, r) so the strategy is fully specified without external tuning. Reasonable default constants are given but may be tuned if desired.
- Optionally, one can add a small randomized forgiveness probability (e.g., with 5% chance cooperate even when f<T) to break cycles in some tournaments; this is unnecessary but can help in noisy environments.

This strategy is cooperative-minded (starts cooperative, sustains cooperation when reciprocated), adaptive (uses recent history and game parameters), robust (graded punishments, forgiveness after punishment, limited punishment length), and implementable with only the allowed information.
'''

description_COOPERATIVE_124 = '''
Name: Pavlov-Probe (Pavlov with measured forgiveness and exploration)

Intuition (short)
- Start by signalling cooperation.
- Use a Pavlov (win‑stay, lose‑shift) rule that compares your actual payoff to the payoff you would have obtained by switching your last action (this is computable because others' actions are observed each round). If you did at least as well as the counterfactual, repeat your previous action; if you did worse, switch.
- Add two practical enhancements that make the rule robust in N‑player public goods:
  - Measured forgiveness: if the group was largely cooperative last round, avoid switching away from cooperation for a small tolerance to sustain cooperation.
  - Occasional probing (small probability) when trapped in persistent defection to re‑test whether cooperation can resume.
- Always defect in the known final round (to avoid being exploited with no future to sustain reciprocity).

Why this is cooperative and robust
- Pavlov promotes cooperation whenever cooperation produces higher payoffs for you than switching; it quickly stabilizes on mutually-beneficial patterns when others reciprocate.
- Compared with pure TFT/grim, Pavlov tolerates mistakes and avoids long harsh punishments; measured forgiveness prevents collapse after small drops in others’ cooperation; probing escapes deadlock when many opponents defect.
- The rule uses only game parameters and observed history; it does not assume shared norms or communication.

Decision rules (natural language)
1. Initialization
   - Round 1: Cooperate (C).

2. For each round t = 2, ..., r − 1 (all non-final rounds):
   - Let my_last be your action in round t − 1 (C or D).
   - Let total_last be # of cooperators across all players in round t − 1 (observed).
   - Compute your actual payoff last round, π_actual = (1 − c_i_last) + (k/n) × total_last (where c_i_last = 1 if you played C last round).
   - Compute counterfactual payoff π_alt that you would have received last round if you had played the opposite action (set c_i_alt = 1 − c_i_last and total_alt = total_last + (c_i_alt − c_i_last), then π_alt = (1 − c_i_alt) + (k/n) × total_alt).
   - Standard Pavlov rule:
     - If π_actual >= π_alt: repeat my_last this round.
     - If π_actual < π_alt: switch (play the opposite of my_last) this round.
   - Measured forgiveness modification:
     - Compute fraction_coop_others = (total_last − c_i_last)/(n − 1) (i.e., fraction of other players who cooperated last round).
     - If fraction_coop_others >= forgiveness_threshold and the Pavlov rule would switch you from C → D, instead keep cooperating (do not switch). This preserves cooperation when the group was largely cooperative and the small loss may be noise.
   - Probing (escape from mutual defection):
     - If you have been defecting for probe_wait consecutive rounds (default probe_wait = 3) and you did not change outcomes (i.e., no improvement in others’ cooperation), then with small probability ε (default 5%) play C this round to probe whether others will reciprocate.
     - Probing is only used when your recent state is persistent defection; it prevents endless deadlock.

3. Final round t = r:
   - Defect (D). (No future rounds remain to enforce reciprocity, so cooperating is exploitable.)

Parameters (recommended defaults)
- forgiveness_threshold = 0.6 (i.e., if ≥60% of others cooperated last round, be forgiving)
- ε (probe probability) = 0.05
- probe_wait = min(3, r − 2)  (number of consecutive rounds defecting before allowing probes)
These can be tuned (e.g., increase forgiveness_threshold for large n or increase ε when many opponents are slow to recover).

Pseudocode

Input: n, r, k
Parameters: forgiveness_threshold = 0.6, ε = 0.05, probe_wait = min(3, r-2)

Maintain: my_last_action (initialized to C after round 1), defect_streak = 0

Round 1:
  play C
  my_last_action = C
  defect_streak = 0

For t = 2 to r-1:
  observe total_last (total cooperators last round), and you know my_last_action

  c_my = 1 if my_last_action == C else 0
  total_last = observed total cooperators last round

  // actual payoff last round
  π_actual = (1 - c_my) + (k/n) * total_last

  // counterfactual: flip my action
  c_alt = 1 - c_my
  total_alt = total_last + (c_alt - c_my)   // adjust total by +/-1
  π_alt = (1 - c_alt) + (k/n) * total_alt

  // base Pavlov decision
  if π_actual >= π_alt:
    decision = my_last_action       // repeat
  else:
    decision = opposite(my_last_action)  // switch

  // measured forgiveness: avoid switching C->D if group mostly cooperated
  fraction_coop_others = (total_last - c_my) / (n - 1)
  if my_last_action == C and decision == D and fraction_coop_others >= forgiveness_threshold:
    decision = C

  // probing: if stuck defecting, occasionally probe
  if my_last_action == D:
    defect_streak += 1
  else:
    defect_streak = 0

  if my_last_action == D and defect_streak >= probe_wait:
    with probability ε set decision = C  // override to probe

  play decision
  my_last_action = decision

Round r:
  play D

Notes and rationale for implementers
- The counterfactual payoff is computable because each round actions are public after the round: you can exactly calculate what you would have earned had you chosen the other action.
- Pavlov's comparison π_actual vs π_alt is parameter‑free and reacts to whether your previous choice delivered better outcome than the alternative given what others did. That promotes mutually beneficial patterns and corrects mistakes quickly.
- Measured forgiveness prevents short, possibly-noise driven drops in cooperation from immediately collapsing the cooperative pattern. The threshold should be neither too low (weakens punishment) nor too high (prevents forgiveness).
- Probing with small ε lets you test for resumed cooperation and escape from permanent low-payoff equilibria.
- Always defect in the last round because there is no future to enforce reciprocity; this aligns with standard backward induction. If you prefer to attempt last‑round cooperation in environments where some opponents may irrationally cooperate in last round and you can exploit reputational carryover outside the game, you could relax this, but for standard tournament play defecting in the final round is safest.

Behavioral examples (illustrative)
- Against many reciprocators: Start by cooperating; if reciprocation yields π_actual >= π_alt, you keep cooperating — group cooperation stabilizes.
- After a one-off defection by an otherwise cooperatively-behaving group: Pavlov may detect a temporary loss and switch, but measured forgiveness keeps you cooperating if most others were cooperating, avoiding collapse.
- Against persistent defectors: Pavlov quickly switches to defecting (to avoid being exploited). After probe_wait rounds of defecting, occasional probes (ε) try to re-establish cooperation if possible.
- Against mix or conditional strategies: Because the rule uses actual and counterfactual payoffs and observed group cooperation fraction, it adapts to a wide variety of opponent behavior without relying on unverified assumptions.

This strategy balances a genuine cooperative stance (start cooperatively, maintain cooperation when it yields as good or better outcomes and when the group is largely cooperative) with principled defenses (switch when exploited, punish but forgive, probe to recover cooperation). It is simple to compute and robust across diverse opponent strategies.
'''

description_COOPERATIVE_125 = '''
Strategy name: Gradual Conditional Cooperator with Proportional Punishment (GCCPP)

Summary (one line)
- Start by cooperating to signal willingness to cooperate; punish defections in the group proportionally but only for a short capped number of rounds; forgive and attempt to re-establish cooperation; defect in the final round except when the entire history has been unanimous cooperation (reward).

Design goals achieved
- Cooperative: signals cooperation immediately and returns to cooperation quickly after punishment.
- Deterring/exploitation-robust: punishes defections in proportion to how many others defected (so single, isolated mistakes trigger small punishments; mass defection triggers stronger short punishments).
- Adaptive: uses recent history (previous round and a short memory window) to decide actions; punishment lengths scale with observed defection magnitude and are capped so cooperation can be re-built.
- Uses only public history and game parameters n,r,k.

Parameters derived from inputs (deterministic choices you can change when implementing)
- memory window w = min(5, r-1)  (used for the last-round unanimity check and optional statistics)
- punishment-cap P_cap = min(3, r-1)  (maximum number of consecutive punishment rounds to impose)
- (Optional implementation parameter) base generosity: initial move is Cooperate

Decision rules (natural language)
1. Round 1: Cooperate (C). This signals cooperative intent and helps establish mutually beneficial cycles.
2. Last round (round r):
   - If every player cooperated in every previous round (i.e., unanimous cooperation for all t = 1..r-1), then Cooperate (reward perfect cooperation).
   - Otherwise Defect (D). This avoids being exploited in the known final stage when others may deviate.
3. For rounds 2..r-1: maintain a single internal counter punish_remaining (initially 0).
   - If punish_remaining > 0:
       - Play D this round and decrement punish_remaining by 1.
   - Else (punish_remaining == 0):
       - Look at the immediately previous round (t-1) and count how many players defected in that round (count_defectors_total).
       - If count_defectors_total == 0 (everyone cooperated last round): Play C.
       - If count_defectors_total > 0:
           - Start a proportional, short punishment: set punish_remaining = min(P_cap, count_defectors_total) - 1, then Play D this round.
           - (Interpretation: punish for a number of consecutive rounds equal to the number of defectors observed in the previous round, capped at P_cap. One punishment round is applied immediately; the counter schedules the remaining rounds.)
4. After a punishment block ends, the strategy looks again at the immediately preceding round and repeats the same rule: if the most recent round was all cooperation, cooperate; if there were defections, impose a proportional short punishment again.

Pseudocode (clear algorithmic form)

Inputs: n, r, k, history (list of length t-1 with counts or lists of cooperators each past round)
State: punish_remaining = 0

On deciding action for round t:
  if t == 1:
    return C
  if t == r:
    if for all s in 1..r-1: total_cooperators_in_round[s] == n:
      return C
    else:
      return D
  if punish_remaining > 0:
    punish_remaining := punish_remaining - 1
    return D
  // punish_remaining == 0
  count_defectors_total := n - total_cooperators_in_round[t-1]
  if count_defectors_total == 0:
    return C
  else:
    punish_length := min(P_cap, count_defectors_total)
    // apply one punishment round now, schedule the rest
    punish_remaining := punish_length - 1
    return D

Notes and clarifications
- "count_defectors_total" counts defectors among all players in the previous round, including yourself if you defected then; this is natural because you observe full public outcome. (Implementation could also look at defectors excluding self; both are reasonable — the described proportionality uses the observed group-level defection.)
- P_cap is small (3) so punishment is short; this prevents long mutual retaliation and allows recovery.
- The strategy is memory-light: it only needs the previous round outcome and one small counter, plus an optional short check of full history for unconditional-last-round reward.
- The threshold of proportionality (punish length = number of defectors) contains a social meaning: one defector ⇒ short one-round punishment; many defectors ⇒ longer but still capped punishment.
- The strategy is deterministic and does not require beliefs about opponents’ internal models. It depends only on n, r and the observable history.

Why this works in practice (rationale)
- Starting cooperatively and returning to cooperation after limited punishment makes it attractive to conditional cooperators and promotes mutual cooperation.
- Proportionality reduces overreaction to small or accidental defections (for example, noise or a one-off selfish move causes only a small response), which is important in n-player settings where occasional mismatches are common.
- Capping punishment keeps the strategy forgiving and avoids permanent break-down (no grim trigger), which helps rebuild cooperation when others reciprocate.
- Last-round defection is the safe default (to avoid pure exploitation) but the small override to reward perfect unanimous cooperation is a cooperative nod that recognizes and rewards full-trust trajectories.
- The approach is robust: it deters persistent exploiters by imposing repeated short punishments while making mutual cooperation a stable outcome against reciprocating opponents.

Edge cases handled
- r = 2: round 1 → C; round 2 (last) → only cooperate if unanimous earlier (round 1) which is true here so GCCPP will cooperate in round 2 as reward for full cooperation — this is the only exception to the default last-round defection because unanimous cooperation existed.
- Very short games: P_cap is trimmed to r-1 so punishment lengths never exceed remaining rounds.
- Mass-defection waves: punishment length increases with number of defectors (up to cap), which signals dissatisfaction strongly enough to deter coordinated defection attempts but still leaves possibility to return to cooperation quickly.
- No assumptions about opponents’ types: works against purely selfish defectors (you will avoid long-term exploitation by punishing) and against conditional cooperators (you achieve mutual cooperation).

Variants / tuning
- If you expect many noisy mistakes, increase forgiveness by raising P_cap smaller (e.g., 1) or by ignoring isolated defections when they are few relative to n (e.g., require count_defectors_total > threshold before punishing).
- If the tournament rewards absolute scores and you want to be more selfish late in the game, you can strengthen last-round defection by removing the unanimous-cooperation exception.
- If you prefer to base punishment on defectors excluding yourself (i.e., count only others’ defections), replace count_defectors_total accordingly.

Conclusion
GCCPP is intentionally simple: it signals cooperation, punishes proportionally but briefly, forgives quickly, and is careful in the final round. It balances cooperation promotion with protection against exploitation, adapts to observed group behavior, and uses only game parameters and public history — making it suitable for a tournament of independently developed AI strategies.
'''

description_COOPERATIVE_126 = '''
Overview (goal)
- Aim to sustain high collective cooperation across the repeated public-goods game while avoiding long-term exploitation.
- Signal cooperation early, reciprocate when the group reciprocates, punish quickly and proportionally when the group collapses, and forgive so cooperation can resume.
- Use only the game parameters (n, k, r) and observed history (each round’s full vector of C/D). No external coordination or communication is assumed.

Key ideas that drive the rules
- Start by signaling cooperation to invite reciprocation.
- Use a simple, interpretable threshold on the observed fraction of cooperators in recent rounds to decide whether the group is “cooperating enough.” The threshold is adaptive to k: when the public-good multiplier k is larger (cooperation is more socially valuable), be more forgiving (lower threshold); when k is small, require a stronger signal.
- When the group falls below the threshold, punish (defect) for a short, proportional number of rounds so that persistent defectors are not exploited; but cap punishment length to avoid permanent retaliation.
- Allow controlled forgiveness (small probability or conditional return) to recover from transient cooperation failures and to restore cooperation with forgiving reciprocators.
- Always defect in the final round (standard backward-induction rationale for a finite horizon with simultaneous play).

Concrete decision rules

Notation
- t = current round index (1..r).
- history_t−1: complete record of all players’ actions up to round t−1.
- coop_count(s) = number of players who played C in round s.
- f_s = coop_count(s) / n (fraction cooperating in round s).
- f_recent = average of f_{t−1}, f_{t−2}, … up to the last 3 rounds available.
- normalized_k = (k − 1) / (n − 1) ∈ (0, 1) — maps k to a 0–1 scale so parameter choices adapt to k relative to the possible range.
- punishment_timer: an internal counter initialized to 0, decremented each round; while >0 we defect (we are executing punishment).
- max_punish = 3 (cap on punishment length, tunable).
- random() returns a uniform number in [0,1] (small randomness used only for forgiveness).

Parameter formulas (default)
- cooperation threshold tau = 0.6 − 0.2 × normalized_k.
  - So tau ∈ [0.4, 0.6]: when k is large we accept lower observed cooperation as a green light; when k is small we require stronger evidence.
- forgiveness probability p_forgive = 0.05 + 0.25 × normalized_k.
  - So p_forgive ∈ [0.05, 0.3]: more forgiveness when cooperation yields larger public benefits.
- punish length suggested = ceil( (tau − f) × 6 ), clipped to [1, max_punish].
  - Punishment scales with how far the group fell below tau, but never exceeds max_punish.

Decision algorithm (natural language + pseudocode)

Initialize:
- punishment_timer = 0
- previous_rounds list empty

Round t:
1) If t == 1:
   - Play C (signal cooperative intent).
   - Append action to history and continue.

2) If t == r (final round):
   - Play D (defect in the last round to avoid one-shot exploitation).
   - Append action and finish.

3) Otherwise (2 ≤ t ≤ r − 1):
   - If punishment_timer > 0:
       - Play D.
       - punishment_timer ← punishment_timer − 1.
       - Continue.
   - Compute f_{t−1} (fraction cooperating in last round).
   - Compute f_recent = average of up to last 3 f values (use available rounds).
   - Compute normalized_k, tau, p_forgive as above.

   Decision:
   - If f_{t−1} ≥ tau:
       - Play C (group is cooperating enough); reset any localized retaliation state (punishment_timer stays 0).
   - Else (f_{t−1} < tau):
       - Determine punish_len = min(max_punish, max(1, ceil((tau − f_{t−1}) * 6))).
       - Two routes to end up cooperating despite f_{t−1} < tau (controlled forgiveness):
           a) If f_recent ≥ (tau − 0.1) and random() < p_forgive:
               - Play C (forgive; group trend is improving).
           b) Else:
               - Play D and set punishment_timer ← punish_len − 1 (we defect this round and schedule remaining punishment rounds).
   - Append action to history and continue.

Pseudocode (compact)

  initialize punishment_timer = 0
  for each round t = 1..r:
    if t == 1:
      play C
      continue
    if t == r:
      play D
      continue
    if punishment_timer > 0:
      play D
      punishment_timer -= 1
      continue
    compute f_prev = coop_count(t-1) / n
    compute f_recent = average of f_prev and up to two earlier f values
    normalized_k = (k - 1) / (n - 1)
    tau = 0.6 - 0.2 * normalized_k
    p_forgive = 0.05 + 0.25 * normalized_k
    if f_prev >= tau:
      play C
    else:
      punish_len = min(3, max(1, ceil((tau - f_prev) * 6)))
      if f_recent >= tau - 0.1 and random() < p_forgive:
        play C   # forgiving return
      else:
        play D
        punishment_timer = punish_len - 1

Rationale and robustness properties
- Start-cooperate: opening with C invites cooperative strategies to coordinate around the efficient outcome.
- Threshold tuned to k: since k controls the social return from cooperation, the decision threshold becomes less demanding as k rises (cooperation is more valuable), and more demanding when k is small.
- Quick, proportional punishment: when the group drops below the threshold, this strategy defects immediately and for a short number of rounds proportional to how badly cooperation collapsed. This deters persistent defectors but avoids long vendettas that lock the group into all-defect.
- Forgiveness and trend-sensitivity: if recent average cooperation indicates recovery (f_recent near tau), the strategy probabilistically forgives, enabling a return to cooperation and avoiding lock-in from one-off mistakes or occasional mischief.
- Small randomness in forgiveness avoids pathological cycles with perfectly deterministic opponents (it also makes exploitation via predictable oscillation harder).
- Safety against pure defectors: if others always defect, the strategy will detect f≈0 quickly and defect itself, avoiding continued exploitation.
- Recovery from partial cooperation: if many players cooperate and only a few defect intermittently, the threshold and forgiveness will tend to keep cooperation because f will often exceed tau or recover quickly.
- Last-round defection: protects immediate individual payoff in final round (standard finitely-repeated-game logic).

Edge cases
- First round: always cooperate to signal good intent.
- Last round (t = r): always defect.
- Very small r (e.g., r = 2): with r > 1 given, behavior is: round 1 cooperate, round 2 defect.
- Very small n or k extremes:
  - normalized_k mapping handles k near 1 (be less forgiving, higher tau) and k near n (more forgiving).
- If history is extremely sparse (very early rounds), f_recent uses whatever previous data exists (e.g., only f_{t−1} for t=2).
- If an opponent plays randomized or manipulative strategies: proportional punishment and forgiveness plus small randomness reduce being exploited by deterministic exploitation strategies.

Tuning notes (implementer can adjust)
- max_punish (default 3): increases deterrence but risks protracted mutual defection if set too high.
- tau baseline 0.6 and range [0.4,0.6] is conservative and pragmatic; you can shift it (e.g., baseline 0.55) depending on tournament meta.
- p_forgive up to 0.3 is intentionally modest: we forgive often enough to allow recovery but not so often as to be trivially exploitable.

Summary (one-sentence)
- Start by cooperating, cooperate as long as the recent fraction of cooperators meets an adaptive threshold that decreases with higher k, punish quickly and proportionally when the group falls below the threshold, and forgive conditionally (with small randomness) when the trend improves — but always defect in the final round.
'''

description_COOPERATIVE_127 = '''
Strategy name: Forgiving Conditional Cooperator (FCC)

Intuition (short)
- Signal cooperation up front, reciprocate when the group reciprocates, punish defections promptly but only temporarily, and forgive quickly to restore cooperation. Always defect in the final round (single-shot incentive). The rule uses only the game parameters (n, r, k) and the public history of actions.

Main ideas embodied
- Start cooperative to signal willingness to cooperate.
- Condition current cooperation on how many OTHER players cooperated recently (exclude your own past actions when estimating others’ cooperativeness).
- Use a short memory window to adapt quickly.
- Use a small, fixed probability of “forgiving” (or a deterministic occasional test if randomness is not allowed) so cooperation can be re-established after breakdowns.
- If you mistakenly defected while others cooperated (contrition), immediately cooperate next round to re-establish trust.
- Defect in round r (last round) because cooperation is strictly dominated in a one-shot last round.

Parameters the strategy sets (derived from r, n)
- W = min(5, r−1) — look-back window (number of previous rounds used to estimate others’ cooperativeness). If r−1 < 1 then W = 0 but strategy accounts for that in edge cases.
- f = 0.50 — cooperation threshold: cooperate if the average fraction of other players who cooperated in the look-back window ≥ f.
- p_forgive = 0.05 (5%) — small probability to cooperate despite failing the threshold (randomized test to re-start cooperation). If a deterministic implementation is required, replace randomized forgiveness by “cooperate on 1 of every ceil(1/p_forgive) rounds that would otherwise be defect” (e.g., once every 20 defect rounds).
- f_contrition = 1.0 — if ALL other players cooperated while you defected last round, treat your last-round defection as a mistake and cooperate unconditionally now.
- f_low = 0.20 — if recent cooperation among others is very low (< f_low) then do not try to re-start cooperation aggressively; continue defecting but occasionally forgive with p_forgive to probe.

Decision rules (plain language)
1. If this is the final round t = r: play D (defect).
2. If this is the first round t = 1: play C (cooperate).
3. Otherwise (2 ≤ t ≤ r−1):
   a. Compute the fraction of other players who cooperated averaged over the last W rounds:
      - For each past round s in {t−W, ..., t−1} (use whatever rounds exist if t−1 < W), count how many players j ≠ you played C in s. Let AvgOtherCoop be the average of these counts divided by (n−1).
   b. If your own previous action was D but in the last round (t−1) all other players cooperated (i.e., others’ cooperativeness in t−1 = n−1): treat that as contrition for a likely mistake and play C now.
   c. Else if AvgOtherCoop ≥ f: play C (reciprocate majority cooperation).
   d. Else (AvgOtherCoop < f):
      - If AvgOtherCoop < f_low: play D (do not risk restarting cooperation), but with probability p_forgive play C (probing forgiveness).
      - If f_low ≤ AvgOtherCoop < f: play D, but with probability p_forgive play C (gentle probing to re-establish cooperation).
   e. If randomness is not available, implement the “probing forgiveness” deterministically (e.g., cooperate once every K = ceil(1/p_forgive) rounds that would otherwise be defect).

Pseudocode

Notation:
- t: current round (1..r)
- history[t'] stores the vector of actions of all players in round t' (1 = C, 0 = D)
- my_action[t'] is your own action in round t'
- function rand() returns a uniform random in [0,1]

Initialize:
- W = min(5, r-1)
- f = 0.50
- p_forgive = 0.05
- f_contrition = 1.0
- f_low = 0.20

Function decide_action(t):
  if t == r:
    return D
  if t == 1:
    return C
  // Build look-back window
  start = max(1, t - W)
  total_other_coops = 0
  rounds_counted = 0
  for s in start .. (t-1):
    // count other players cooperating in round s
    other_coops_s = sum_{j ≠ me} history[s][j]   // history stores 1 for C, 0 for D
    total_other_coops += other_coops_s
    rounds_counted += 1
  if rounds_counted == 0:
    AvgOtherCoop = 0
  else:
    AvgOtherCoop = (total_other_coops / rounds_counted) / (n - 1)   // fraction in [0,1]

  // Contrition: you defected last round but everyone else cooperated last round
  if my_action[t-1] == D:
    last_other_coops = sum_{j ≠ me} history[t-1][j]
    if last_other_coops == (n - 1):
      return C

  if AvgOtherCoop >= f:
    return C

  // Below threshold: defect, but occasionally probe
  if rng_available:
    if rand() < p_forgive:
      return C
    else:
      return D
  else:
    // deterministic probing: cooperate once every K rounds of "would-defect"
    K = ceil(1 / p_forgive)
    if ((number_of_defect_rounds_since_last_probe mod K) == 0):
      return C
    else:
      return D

Edge cases and small-r behavior
- r = 2: round 1 cooperate, round 2 defect (standard two-round finite-horizon behaviour).
- If W = 0 (r = 1): trivial — single round, play D (but the specification requires r > 1).
- When very few rounds remain, the look-back window shrinks accordingly.
- If randomness is disallowed, use periodic deterministic probing described above.

Why this is cooperative and robust
- Cooperative: starts with cooperation and reciprocates when most others cooperate; thus it supports and rewards group cooperation when it emerges.
- Robustness to exploitation: when cooperation falls below threshold, FCC defects to avoid being repeatedly exploited.
- Forgiving: it probes at low but nonzero rate so cooperation can be re-started without expecting coordination or communication; it also treats a lone own defection during widespread cooperation as contrition and immediately resumes cooperation.
- Graduated punishment: punishment is temporary and proportional — we do not adopt unforgiving permanent grim-trigger behavior, which is vulnerable in noisy/mixed environments.
- Adaptive: short window W makes it responsive, and the fractional threshold (majority) scales to player count n.
- Last-round safety: defecting in round r protects against final-round exploitation while earlier rounds remain cooperative when reciprocated.

Tuning guidance
- W: increase for more inertia (smoother response) or decrease to be more reactive.
- f: raise toward 0.6–0.8 to require stronger evidence of cooperation before risking cooperation; lower toward 0.4 if k is high and you want to be more willing to cooperate on weaker signals.
- p_forgive: increase to be more generous and exploratory (more likely to re-establish cooperation), decrease to be more defensive.
- If you observe opponents that systematically exploit probabilistic probing, reduce p_forgive or switch to deterministic, infrequent probes.

Summary (one-sentence)
Start cooperatively, reciprocate if a majority of other players have cooperated recently, defect otherwise but probe occasionally and forgive quickly (plus immediate contrition if you accidentally defected while everyone else cooperated), and defect in the final round.
'''

description_COOPERATIVE_128 = '''
Strategy name: Forgiving Conditional Cooperator (FCC)

Summary (one line)
- Start by building trust, cooperate while a clear majority (adjusted by k) of others have been cooperating recently, respond to defections with short proportional punishments, and forgive when cooperation resumes. Defect in the final round (no future to enforce reciprocity).

Rationale (brief)
- Pure one-shot incentives favor defection, so sustained cooperation requires credible future retaliation and forgiveness. FCC aims to (1) create an initial cooperative baseline, (2) punish defections quickly but proportionally (to avoid being exploited or locked in mutual defection), (3) forgive to restore cooperation, and (4) avoid needless exploitation in the last round where no retaliation is possible. The trigger thresholds scale with k and n so the strategy adapts to how valuable the public good is.

Parameters derived from game parameters
- n, k, r are given.
- Window size W = min(5, max(1, floor(r/10))) — how many recent rounds to average over.
- Cooperation threshold theta = clamp((n - k) / (n - 1), 0.1, 0.9).
  - Intuition: when k is close to n the public good is very valuable and FCC accepts a lower observed cooperation rate before cooperating; when k is small FCC requires closer to unanimous cooperation to risk contributing.
- Forgiveness threshold theta_forgive = max(0.5*theta, theta - 0.15).
- Maximum punishment length P_max = max(1, floor(r/10)).
- Base punishment length scaling factor S = 5 (used to scale punishment proportional to severity).

State to maintain (history-dependent)
- For each previous round t: the vector of contributions observed.
- punishment_timer: integer (remaining rounds of active punishment; 0 when not punishing).
- last_W_others_coop_rate: moving average over W rounds of fraction of other players who cooperated.

Decision rules (high-level)
1. Round 1:
   - Cooperate. (intend to build cooperation baseline)

2. Any round t where t = r (final round):
   - Defect. (no future to punish, standard endgame safety)

3. Any intermediate round t (2 ≤ t ≤ r-1):
   - Update last_W_others_coop_rate O = average fraction of other players who cooperated over the last W rounds (exclude self).
   - If punishment_timer > 0:
       - Play D (defect) this round.
       - Decrement punishment_timer by 1 at end of round.
   - Else (not currently punishing):
       - If O ≥ theta:
           - Cooperate (C).
       - Else (O < theta):
           - Enter punishment: set punishment_timer = min(P_max, 1 + ceil((theta - O) * S)).
             - Immediately play D this round (the first round of punishment).
             - The punishment length is proportional to how far below the threshold the observed cooperation is, capped by P_max.
   - After a full punishment phase finishes (punishment_timer reaches 0), evaluate recent O:
       - If O ≥ theta_forgive, resume cooperating.
       - Else, if O still < theta_forgive, begin another (usually short) punishment cycle as above.

Additional operational details / edge handling
- Individual defectors vs. group: FCC uses a group-level measure (O) because contributions affect everyone equally. If defections concentrate in a small subset of players, O will fall less and trigger only short or no punishments; that makes FCC robust to occasional isolated defectors and avoids overreacting to single-player noise.
- Short games (small r): W is small, P_max is small — the strategy becomes nimble: start cooperating but punish briefly if exploitation is observed. If r = 2, FCC cooperates in round 1 and defects in round 2.
- Very large k (close to n): theta becomes small (close to 0.1). FCC will cooperate even if only a minority cooperated recently because the public good is lucrative and cooperation by some yields high returns.
- Very small k (close to 1): theta becomes close to 0.9 — FCC requires near-unanimous cooperation to risk contributing.
- Noisy one-off defects: Because punishment length scales with the magnitude of cooperation failure and is capped, FCC tolerates occasional mistakes and returns to cooperation quickly if others resume cooperating.

Pseudocode (clear algorithm)
- Input: n, k, r; history of rounds so far (for each past round: contributions by each player)
- Compute W, theta, theta_forgive, P_max, S as above.
- Initialize punishment_timer = 0.
- For each round t from 1 to r:
    if t == 1:
        action = C
    else if t == r:
        action = D
    else:
        compute O = average over last min(W, t-1) rounds of [ (#others who cooperated)/(n-1) ]
        if punishment_timer > 0:
            action = D
            punishment_timer -= 1
        else:
            if O >= theta:
                action = C
            else:
                // trigger proportional punishment
                punishment_timer = min(P_max, 1 + ceil((theta - O) * S))
                action = D
    play action
    // At end of round: (history updated by observed moves); if punishment_timer just reached 0,
    // optionally check O and if O >= theta_forgive then continue cooperating, else future cycles will trigger again.

Why this is cooperative and robust
- Cooperative: FCC starts by cooperating and resumes cooperation whenever the group shows sustained cooperation above the forgiveness threshold. It is designed to restore cooperation after punishments rather than permanently condemn defectors.
- Robust: Punishments are proportional and capped, preventing long, costly mutual defections. Group-level monitoring prevents being trapped by a single misreported action and reduces vulnerability to coordinated exploitation that tries to provoke long punishments. Thresholds adapt with k and n so FCC cooperates more when public contributions are more valuable and requires stricter group reciprocation when the public good is weak.
- Adaptive: FCC reacts to short-term patterns via W and scales punishment by severity (how far O is below theta). This handles a wide range of opponent behaviors: unconditional defectors provoke repeated short punishments and persistent defection leads FCC to mostly defect (thereby avoiding exploitation); unconditional cooperators are rewarded with sustained cooperation.

Variants and tunable knobs (for implementers)
- W, P_max, and S can be tuned for the tournament environment (e.g., larger W for slower adaptation; larger P_max for harsher punishments).
- If you want to be more forgiving, reduce S or P_max and reduce theta (or increase theta_forgive).
- If you want to be more exploit-averse (defend against free riders), increase S and P_max.

Notes about strategic limitations
- Because this is a finite-horizon game with common knowledge of r, universal defection is the subgame-perfect equilibrium. FCC is intentionally non-equilibrium but selected to score well in populations that contain reciprocators and bounded-rational agents. It aims for high mutual payoffs against cooperative opponents while avoiding long-term exploitation by defectors.

End.
'''

description_AGGRESSIVE_1 = '''
Name: Adaptive Exploitative Punisher (AEP)

High-level intent
- Aggressive by default (prefers D), but adaptive: it probes opponents, identifies persistent cooperators to exploit, and uses short, harsh punishment phases when it is “suckered.”  
- It occasionally “invests” (plays C) only when that investment is expected to preserve a stream of future cooperation that can be exploited; otherwise it defects.  
- It always defects in the known endgame window.

Notation / inputs available to the strategy
- n, r, k (given).
- At round t (1..r) the strategy observes the full history of actions of all players up to round t−1 and all past payoffs.
- c_i,t is my action at t (1 for C, 0 for D). c_j,t for others are observed after each round.

Tunable internal parameters (recommended defaults)
- Seed rounds S = min(3, max(1, floor(r/10))). (Default: S = min(3, floor(r/10)).)
- Recent window L = min(5, r).
- Endgame length E = min(3, max(1, floor(r/10))). (All rounds t > r−E: always defect.)
- High-coop threshold TH_high = 0.75.
- Mid-coop threshold TH_mid = 0.40.
- Cooperators cutoff p_high = 0.7, defector cutoff p_low = 0.2 (for classification, if needed).
- Small sustained-coop probability q_base = 0.15 (used to occasionally cooperate to sustain high cooperation when profitable).
- Punishment length P = min(4, max(1, floor(r/5))). (Number of consecutive rounds of guaranteed defection when punished.)
- Safety margin α = 1.1 (used when comparing expected future benefit vs immediate cost).

Decision rules (natural language)
1. Endgame: In the last E rounds (t > r − E) always play D. In particular, always defect in round r.

2. Seeding / probing phase (first S rounds):
   - Play C in each seeding round. Purpose: test which opponents are willing to cooperate and gather data about responsiveness. (Seeding is short; it is an aggressive investment to reveal the cooperative types.)

3. Default inclination: outside seeding and endgame, default to D. Deviate from D only when empirical evidence indicates cooperating now yields an expected net gain across the remaining rounds (taking into account effects on others’ future cooperation), or when occasional small cooperation is used to sustain an already strong cooperation equilibrium that can be exploited.

4. Exploitation when others mostly cooperate:
   - Compute C_bar = average fraction of other players who cooperated in the most recent L rounds (or over all history if shorter).
   - If C_bar ≥ TH_high (many others are reliably cooperating), the strategy exploits by mostly defecting (immediate best one-shot move) but occasionally cooperates so as not to immediately collapse cooperation:
     - Let remaining rounds Rrem = r − t + 1.
     - Set prob_coop = q_base × (Rrem / r) (decays as the game nears its end).
     - Play C with probability prob_coop; otherwise play D.
     - Rationale: when many others cooperate you harvest big public good gains by defecting; however occasional C’s are a cheap insurance to keep a high-cooperation environment alive long enough to exploit across multiple rounds.

5. Punishment when exploited (harsh & short):
   - If in the previous round you cooperated and your payoff π_i,t−1 is substantially lower than the central tendency of others’ payoffs in that round (i.e., you were a “sucker”), trigger a punishment:
     - Condition: (I cooperated in t−1) AND (π_i,t−1 < median_j π_j,t−1 − ε), where ε is small (can be 0) to avoid triggering on ties/noise.
     - Action: set punishment_timer = P and play D for the next P rounds (including the current round).
     - Rationale: inflict a loss on the group (including defectors) to deter being exploited again. Aggressive: punishment is harsh (several rounds) but limited (forgives after P rounds, allowing re-exploitation if opponents revert).

6. Moderate cooperation environment (intermediate C_bar):
   - Attempt to estimate your marginal influence on others’ cooperation (responsiveness).
     - Compute two empirical frequencies (over available history, using last L rounds or more if needed):
       - f_C = average fraction of others who cooperated in the round after I cooperated.
       - f_D = average fraction of others who cooperated in the round after I defected.
       - Responsiveness delta = f_C − f_D.
     - Interpret delta as the expected increase in the number of cooperators next round if you cooperate this round (per other player).
   - Compute the expected cumulative benefit of cooperating now:
     - Immediate cost of cooperating vs defecting this round = (1 − k/n). (Defect yields +1 private good compared to cooperate.)
     - Rough expected benefit from increasing others’ cooperation by delta per round over Rrem−1 future rounds: approx benefit ≈ delta × (k/n) × (Rrem−1).
     - If benefit × α > cost, play C this round (invest to sustain cooperation), else play D.
   - If there is insufficient data to estimate responsiveness (too few observations), default to D.

7. Recovery & forgiveness:
   - After punishment_timer expires, resume normal rules. To avoid oscillatory retaliation, require opponents to show improved cooperation for at least L rounds before trusting again (i.e., treat p_high as long-term threshold).

8. Misc / default:
   - If any computation is ambiguous or data sparse, choose D (aggressive default).
   - Always defect in the final round.

Pseudocode (concise)
Initialize punishment_timer = 0.
For each round t = 1..r:
  if t > r − E:
    action = D
    continue
  if punishment_timer > 0:
    action = D
    punishment_timer -= 1
    continue
  if t ≤ S:
    action = C
    continue
  // compute recent stats over last L rounds (or whole history if shorter)
  C_bar = avg fraction of others who cooperated over last L rounds
  if C_bar ≥ TH_high:
    Rrem = r − t + 1
    prob_coop = q_base * (Rrem / r)
    action = C with probability prob_coop else D
    continue
  // detect exploitive sucker event
  if (my_action at t−1 == C) and (my_payoff at t−1 < median(other_payoffs at t−1) - ε):
    punishment_timer = P − 1
    action = D
    continue
  // moderate environment: estimate responsiveness
  compute f_C and f_D from history (fractions of others cooperating after my C and after my D)
  if enough data:
    delta = f_C − f_D
    Rrem = r − t + 1
    expected_future_benefit = delta * (k/n) * (Rrem − 1)
    immediate_cost = (1 − k/n)
    if expected_future_benefit * α > immediate_cost:
      action = C
    else:
      action = D
  else:
    action = D

Design rationale — why this is aggressive and robust
- Aggressive default: the strategy defects unless there is a clear, empirically supported reason to invest in cooperation. This preserves immediate payoff and prevents easy exploitation.
- Exploitation behavior: when a large majority reliably cooperates, the strategy exploits them by defecting almost always, harvesting the public good. The occasional cooperative move is a small cost paid to maintain the exploitable environment long enough to extract value.
- Punishment: when the strategy is actually exploited (you cooperated and got a worse payoff than most), it punishes for several rounds to deter repeat sucker outcomes. The punishment is costly to the group but is short and predictable.
- Adaptive: the strategy estimates responsiveness and recent cooperation rates; it does not assume other players follow any norm. This makes it robust versus stochastic and strategic opponents. If opponents are responsive to your cooperation, the strategy will invest when the expected future payoffs justify it; otherwise it remains defecting.
- Endgame handling: known finite horizon is exploited aggressively: defect in the final rounds to avoid being exploited and because no future benefit accrues.

Edge cases and implementation notes
- If r is very small (e.g., r=2), seed and punishment parameters scale down so seeding/punishment do not dominate the game; still last-round defection applies.
- If k is very close to n (public good nearly fully returns), the immediate cost of cooperating (1 − k/n) may be small; the algorithm naturally cooperates more when the gain equation favors it.
- If history is sparse (early rounds), the strategy defaults to D (aggressive preference) until enough data accumulate.
- The responsiveness estimate can be computed using last L rounds or weighted exponentially more to recent behavior. Use standard smoothing to avoid division by zero.
- All thresholds (TH_high, q_base, P, S, E, α) are explicitly tunable; defaults balance aggressiveness and opportunistic exploitation while retaining some cooperation to milk cooperative populations.

Summary (one-line)
Mostly-defect, probe early to identify cooperators, exploit persistent cooperators by defecting while occasionally cooperating to sustain exploitation, punish harshly but temporarily if you are suckered, and always defect in the endgame — all decisions driven by simple, empirical statistics (recent cooperation rates and responsiveness) computed from the public history.
'''

description_AGGRESSIVE_2 = '''
Name: Aggressive Prober–Exploit (APE)

Intuition (short)
- Default: defect. Cooperating is weakly dominated in one-shot rounds, so an aggressive strategy should not give away the private 1 unless it is being paid back reliably.
- But cooperating can enable sustained high group payoffs which some opponents will sustain. APE probes briefly to detect stable cooperator clusters and then exploits them (free-rides) when it is safe to do so.
- If others retaliate, APE punishes hard and for a long time (to deter repeated punishment cycles), then slowly re-probes. In the final round APE always defects.

Design goals
- Aggressive: exploit cooperative opponents whenever opportunity arises; punish retaliation strongly.
- Adaptive: estimate opponents’ cooperation rates from history and change behavior.
- Robust: works against unconditional defectors, conditional cooperators, and complex strategies because it uses only observed cooperation frequencies and finite, transparent rules.

Parameters (strategy-level, all depend only on n, r and history; defaults given; implementer can tune)
- W: history window size for statistics = min(5, r-1) (use entire history when t-1 < W).
- T_exploit: high cooperation threshold to consider it “safe to exploit” = 0.90.
- M_stable: consecutive rounds of high cooperation required to conclude the cooperation is stable = 3 (or = min(3, r-1)).
- p_probe: probability of a cooperative probe when situation ambiguous = 0.12.
- L_base: base punishment length (rounds) after observed retaliation = 3.
- delta_retaliation: drop threshold in cooperation fraction that counts as retaliation = 0.20.
- endgame_margin: do not attempt to build cooperation in the last few rounds; always defect in last round; for t >= r - 1 you should strongly defect (implementer can set endgame_margin = 1).

State variables (maintained from observed history)
- history of contributions per round (counts of cooperators each past round).
- last_action (your choice in previous round).
- state ∈ {NORMAL, PUNISH, FORGIVE}
- punish_timer (integer ≥ 0)
- last_exploit_round (round index when you last deliberately exploited)

Auxiliary computed quantities at start of round t
- coop_frac(t’) = (# cooperators among others in round t’) / (n-1).
- recent_coop = average of coop_frac over the last W rounds (or all past rounds if < W).
- consecutive_high = number of consecutive past rounds (ending at t−1) where coop_frac ≥ T_exploit.
- change_since_exploit = recent_coop − recent_coop_before_exploit (used to detect retaliation after you exploited).

Decision rules (natural language)
1. Last-round rule
   - If t == r: play D. (Always defect in final round.)

2. If state == PUNISH
   - Play D.
   - Decrement punish_timer. When punish_timer hits 0 set state = FORGIVE.

3. If state == FORGIVE
   - Act like NORMAL but with reduced probe frequency (p_probe_forgive = p_probe/2). If we see fresh sustained cooperation that looks exploitable again, go to exploit; if we are attacked again, escalate punishment (increase punish_timer).

4. NORMAL-mode core logic (applied when not punishing and not in final round)
   a. Safety check for extremely short games:
      - If r ≤ 3: defect every round (no time to recoup cooperations).
   b. Compute recent_coop and consecutive_high.
   c. Exploit when safe:
      - If consecutive_high ≥ M_stable (i.e., others have cooperated at high rate for M_stable rounds) then play D (exploit). Record last_exploit_round = t.
         Rationale: when many others are reliably cooperating, defecting yields strictly more payoff than cooperating this round.
   d. If recent_coop is very low (e.g., < 0.15): play D (no point cooperating).
   e. Ambiguous region (intermediate cooperation)
      - With probability p_probe (or p_probe_forgive in FORGIVE state), play C as a probe to test whether a cooperative cluster will form.
      - If you probe (play C) and in the next round the observed coop_frac rises substantially (≥ T_exploit for at least M_stable rounds), you will switch to the Exploit rule in subsequent rounds.
      - If you probe and you are not reciprocated (coop_frac drops or remains low), revert to D and treat them as defectors.

5. Retaliation detection and punishment escalation
   - If you exploited at round s (you played D while recent_coop(s−1) ≥ T_exploit and consecutive_high ≥ M_stable), monitor cooperation in the w = min(W, r−s) rounds after s.
   - If cooperation fraction in those rounds drops by ≥ delta_retaliation relative to the window immediately before the exploit, interpret that as retaliation directed at you or your exploitation.
   - Respond by entering state = PUNISH and set punish_timer = L_base × (1 + number_of_significant_drop_rounds). Optionally escalate punish_timer multiplicatively if you are punished repeatedly.
   - While PUNISH you always D; after punish_timer expires switch to FORGIVE.

6. Forgiveness / resetting
   - After PUNISH timer expires, go to FORGIVE (reduced probing). This allows re-testing for exploitable cooperation without being permanently trapped in mutual defection.
   - If opponents continue to punish you for probing/exploiting repeatedly, escalate punish_timer on each punishment to become increasingly uncooperative (aggressive retaliation).

Pseudocode (concise)
- Inputs: n, r, current round t, history of cooperators per round (including others’ contributions).
- Compute W, T_exploit, M_stable, p_probe, L_base, delta_retaliation.
- If t == r: return D.
- If r ≤ 3: return D.
- Update recent_coop = average coop_frac over last W rounds (others only).
- Update consecutive_high = number of consecutive previous rounds with coop_frac ≥ T_exploit.
- If state == PUNISH:
    punish_timer -= 1
    if punish_timer == 0: state = FORGIVE
    return D
- If state == FORGIVE:
    use p = p_probe/2 for probing
  else:
    use p = p_probe
- If consecutive_high ≥ M_stable:
    // exploitation opportunity
    last_exploit_round = t
    return D
- Else if recent_coop < 0.15:
    return D
- Else:
    // ambiguous: probe occasionally
    draw u ∈ [0,1]
    if u < p:
       return C   // probe
    else:
       return D

- After each round, if we detect that we exploited at round s and recent_coop_after_s dropped by ≥ delta_retaliation compared to before s:
    punish_timer = L_base * (1 + floor(drop_factor * (number of dropping rounds))) // simple escalation
    state = PUNISH

Remarks and rationale
- Aggression is explicit: APE defects by default, exploits any reliably high cooperation cluster, and punishes retaliation decisively. It rarely cooperates, and only for probing or tactical detection of exploitable cooperative groups.
- Probing is necessary: without probes, APE would never cooperate and would miss multi-round cooperative clusters that could be exploited (which can yield very high payoffs when exploited at the right time).
- Punishment and forgiveness: punishments deter others from imposing long-term costs on you for exploiting; forgiveness prevents permanent lock-in to all-defection and allows fresh opportunities for exploitation.
- Endgame handling: always defect in final round; for very short games always defect since there is no time to recoup cooperation losses.
- Parameter choices are conservative defaults. A tournament implementation should provide these as tunable inputs; APE remains fully defined by these choices and the observed history.

Examples (behavioral summary)
- Against unconditional defectors: APE defects every round — no losses.
- Against unconditional cooperators: APE will quickly detect high cooperation and repeatedly exploit by defecting, gaining strictly higher payoffs per round.
- Against conditional cooperators (like Tit-for-Tat): APE will probe briefly; if conditional cooperators retaliate to exploitation, APE will punish and then probe again later but escalate punishment if punished repeatedly.
- Against mixed/multi-strategy populations: APE adapts by estimating the average cooperation fraction and picking exploitation windows; it is designed to extract payoff whenever a sufficiently large cluster of cooperators is present.

Implementation notes
- Use moving windows and simple arithmetic averages — no need for complex belief models.
- Make punish_timer escalate if repeated retaliation occurs (e.g., multiply by 2 each cycle) to be reliably aggressive.
- Tune p_probe up for long games (more room to explore) and down for short games.
- Keep all thresholds deterministic so the strategy is implementable exactly.

Summary one-line
Defect by default, occasionally probe to detect stable cooperative clusters, exploit such clusters by defecting when they prove stable, punish any retaliation harshly and for a long time, forgive slowly and re-probe — always defect in the final round.
'''

description_AGGRESSIVE_3 = '''
Name: Aggressive Opportunistic Free-Rider (AOFR)

Summary (one line)
- Default is to defect; aggressively and deterministically exploit detectable cooperators, probe rarely to find new exploitable cooperators, and punish any players who visibly “punish” us — while never cooperating in the final round and minimizing cooperation near the end.

Design goals satisfied
- Only depends on game parameters (n, r, k) and the full history of past rounds.
- Adaptive: estimates recent cooperation rates and reacts to changes.
- Aggressive: defaults to defecting, immediate exploitation of cooperative majorities, and short, hard retaliation when others retaliate against us.
- Robust: probing discovers exploitable cooperators; punishment prevents becoming a soft target for conditional punishers.

Fixed internal tuning (deterministic functions of n, r)
- Lookback window for recent behavior: W = min(6, max(1, floor(r/5)))
- Probe interval (how often we test cooperation): M = max(3, floor(r/10))  (cooperate on those probe rounds only if other conditions below hold)
- Punishment length after we detect active punishment: L = min(4, max(1, floor(r/8)))
- Tail-safe rounds (near the end we stop cooperating): tail = min(2, r-1)  (we always defect in the last round; usually also in the last-1 round)
- Thresholds:
  - High-cooperation threshold (exploit majority): G_high = 0.75
  - Low baseline threshold to consider probing: G_probe_min = 0.20
  - Punishment-detection drop: delta_punish = 0.25

Notation
- t: current round (1..r)
- For round s and player j, c_j,s ∈ {0,1} is 1 if player j cooperated in round s
- For history up to t-1 we observe all c_j,s for s < t
- “others” or “group excluding me” means players j ≠ me
- G_rate(t1,t2) = average cooperation rate of others between rounds t1..t2 inclusive

Decision rules (natural language then pseudocode)

Natural-language rules
1. Last-round rule:
   - If t = r: Defect. (Standard backward-induction safe choice.)

2. Early aggressive baseline:
   - Round 1: Defect. AOFR is aggressive out of the gate.

3. Tail safety:
   - If t > r - tail: Defect. (Avoid end-game illusions of cooperation.)

4. Exploit unanimous/full cooperation immediate:
   - If the previous round (t-1) was unanimous cooperation (Σ_j c_j,t-1 = n): Defect this round to exploit.

5. Exploit sustained cooperation:
   - Compute G_recent = average cooperation rate of other players over the last W rounds (or available rounds if < W).
   - If G_recent ≥ G_high (≥ 0.75): Defect (free-ride on a reliably cooperative population).

6. Detect if others punish our defection:
   - If I defected in some recent round s (s ∈ [t-W, t-1]) and the average cooperation rate of others fell by at least delta_punish in the round immediately after s (compared to the round immediately before s), treat that as “they punish me.”
   - If we detect such punishment, enter Punishment Response Mode: defect for the next L rounds (to stop being exploited by oscillatory dynamics and to force others to bear the cost of continued punishment). After L rounds return to baseline decision rules.

7. Probing to find exploitable cooperators:
   - If none of the above conditions force a defect, on probe rounds (t mod M == 0) and if G_recent ≥ G_probe_min (≥ 0.20) — i.e., there is a non-negligible cooperative tendency to exploit — cooperate this single round (a deterministic probe).
   - If by cooperating on this probe round we observe in the next round that other players do not increase cooperation (or that they punish us), stop probing for M rounds and follow exploitation/punishment rules. If they do respond by cooperating subsequently, exploit them as per rule 5.

8. Default:
   - If none of the above trigger cooperation, Defect.

Pseudocode (concise)
- Inputs: n, r, k, history H (c_j,s for s = 1..t-1)
- Compute W, M, L, tail, thresholds as above
- For round t:

  if t == r:
    action = D
    return(action)

  if t == 1:
    action = D
    return(action)

  if t > r - tail:
    action = D
    return(action)

  if Σ_j c_j,t-1 == n:    # previous round unanimous C
    action = D
    return(action)

  compute G_recent = (sum_{s = max(1,t-W) to t-1} sum_{j≠me} c_j,s) / ( (min(W,t-1))*(n-1) )

  if G_recent >= G_high:
    action = D
    return(action)

  # Punishment detection:
  for each s in [max(2, t-W) .. t-1]:
    if I defected in round s:
      coop_before = (sum_{j≠me} c_j,s-1) / (n-1)
      coop_after  = (sum_{j≠me} c_j,s+1 if s+1 < t else latest available) / (n-1)
      if coop_before - coop_after >= delta_punish:
        # mark punishment detected
        start punishment counter for L rounds (if not already in punishment)
        action = D
        return(action)

  # Probe rule:
  if (t mod M) == 0 and G_recent >= G_probe_min:
    action = C    # deterministic probe
    return(action)

  # Default:
  action = D
  return(action)

Rationale and behavior against common opponent types
- Against Always-Cooperate: AOFR defects whenever it detects a stable high cooperation rate (G_recent ≥ G_high) or when it sees unanimous cooperation — it will systematically exploit them, gaining higher per-round payoffs while they lose. That is the aggressive objective.
- Against Always-Defect: AOFR defects (default) and gets baseline payoff 1; no wasted cooperation.
- Against Conditional Cooperators (e.g., Tit-for-Tat / Grim variants that punish defection): AOFR’s punishment-detection logic protects it from long alternating cycles. If conditional cooperators punish us sharply after a defection, AOFR detects their drop and stays defecting for L rounds (a hard stance) rather than repeatedly alternating C/D and being exploited; after L rounds it will re-evaluate. This avoids being manipulated into alternating sequences that lower AOFR payoff.
- Against Mixed/Random opponents: AOFR probes occasionally (deterministic schedule) to detect exploitable cooperators; if noise/punishment exists, AOFR’s punishment detection prevents it from being drawn into costly cooperation cycles.
- Against coordinated cooperators who attempt to re-establish cooperation if you cooperate occasionally: a probe can succeed and then AOFR will exploit when G_recent becomes high.

Edge cases and comments
- Small r: W, M, L scale down as functions of r so decisions still function when r is small. When r is 2, tail and other parameters ensure we defect both rounds (safe).
- Deterministic: All rules are deterministic functions of history and parameters, so they can be implemented without randomness. If implementers want to add randomized probes, they may, but the description above is deterministic.
- Parameters are tunable: thresholds (G_high etc.) are chosen to reflect clear majority cooperation detection; implementers can tune them if they have tournament information. Tying thresholds to k and n is possible (e.g., lowering G_high when k/n is closer to 1) but the above fixed thresholds are robust across many environments.
- Safety vs greed trade-off: AOFR prefers immediate exploitation of cooperative populations and short, aggressive retaliation if punished. That is explicitly “aggressive” — it will not make costly, long-run sacrifices to re-establish social cooperation unless the group shows stable cooperative behavior first.

Concluding note
- AOFR is designed to be an aggressive, history-adaptive, robust algorithm for repeated public-goods games: defect-by-default, exploit reliably cooperative groups, probe intermittently to find new targets, and respond firmly and briefly to punishers to avoid being drawn into costly reciprocation cycles — with an absolute rule to defect in the final round to avoid endgame traps.
'''

description_AGGRESSIVE_4 = '''
Name: Aggressive Opportunistic Defector (AOD)

Summary (one line)
- Always bias toward defection; only give small, controlled cooperation as a maintenance/probing tool when enough others are reliably cooperating, and otherwise punish/defect. Never cooperate in the final round.

Design goals / mindset
- Aggressive: maximize my own payoff, exploit unconditional or forgiving cooperators, punish failed cooperation, and avoid being lulled into costly long-run cooperation.
- Adaptive: detect whether others are cooperating, exploit high-cooperation groups with occasional “maintenance” cooperations, and withdraw cooperation quickly if cooperation drops.
- Robust: uses only observable history (counts of cooperators per round), works for any n, r, k and is implementable deterministically or stochastically.

Parameters used by the strategy (set from game parameters)
- L = min(5, r-1) — history window (use recent rounds only).
- high = 0.7 — “high cooperation” threshold.
- low = 0.2 — “low cooperation” threshold.
- p_min = 0.02 — minimum probability of cooperating when trying to maintain cooperation (keeps occasional maintenance even if detection errs).
- p_max = 0.25 — maximum maintenance cooperation probability used when many others cooperate.
- probe_prob = 0.15 — occasional probing cooperation probability when wanting to test for exploitable cooperators.
- final_round = r — last round index.

You can choose a deterministic alternative (given below) if the tournament forbids randomness.

Decision rules (exact)
1) Final-round rule
- If current round t = final_round (the last round), play D (defect). (No future to incentivize.)

2) First-round rule
- Round 1: play D (defect). This is an initial probe and baseline: defecting is the myopically dominant action and gives information about who cooperates unconditional to my defect.

3) For rounds t = 2 to r-1 (intermediate rounds)
- Compute for the last L rounds (or fewer if t-1 < L) the average fraction of other players who cooperated:
   For each prior round τ in [max(1, t-L), ..., t-1], let s_τ = number of cooperators in that round excluding me (0..n-1). Let f_τ = s_τ / (n-1). Let P = (1/m) * sum_{τ} f_τ where m is number of rounds averaged (m ≤ L).
- Interpret P:
   - If P ≥ high (many others have been cooperating recently):
       - We are in an exploitable high-cooperation environment. Exploit it by defecting most of the time, but occasionally cooperate to maintain the public-good baseline so others continue cooperating.
       - Respond: with probability p = p_min + (p_max - p_min) * ((P - high) / (1 - high)) (clipped into [p_min, p_max]) cooperate; otherwise defect.
       - Rationale: keep the public good alive at minimal cost; each cooperation is a deliberate, rare “maintenance” contribution to sustain high payoffs from others while primarily free-riding.
   - Else if low < P < high (mixed/uncertain environment):
       - Play cautiously but aggressively: rarely cooperate as a probe to see if a cooperating subset can be exploited.
       - Respond: cooperate with probability probe_prob * P/high (i.e., small probing chance proportional to observed cooperation); otherwise defect.
       - Rationale: do not invest in cooperation until you see strong, stable cooperation.
   - Else (P ≤ low, most others defecting):
       - Defect deterministically (punish and avoid wasting endowment).
       - Rationale: cooperation is not reciprocated; cooperating is only costly now.

4) Immediate reaction rule (short-term punishment/exploitation)
- If in the last round I cooperated but the current or next round the group cooperation fraction falls (others exploited me), switch to an immediate “hard defect” for H = min(3, remaining rounds) rounds (i.e., defect unconditionally for the next up-to-3 rounds). This is a short, sharp punishment that also boosts my short-term payoff.
- Conversely, if I defected and many others nevertheless cooperated (they are unconditional cooperators), stay in exploit mode (as above) and increase the maintenance defecting frequency (lower p) to extract more.

5) Per-player targeting (optional, only if implementable)
- Track per-player cooperation rate over the window L. If a subset of players are clearly unconditional cooperators (cooperation rate > 0.9 while most others defect), exploit them by defecting every round (no maintenance cooperation). Do not try to sustain a public good for their benefit.

Deterministic alternative (no randomness)
- Replace probabilistic maintenance by a fixed schedule:
   - When P ≥ high: cooperate exactly one round every M rounds, where M = max(3, floor(1 / (p_max))). For example, if p_max = 0.25, set M = 4: cooperate on rounds where (t mod M) == offset (use offset chosen at start, e.g., 1). Otherwise defect.
   - When low < P < high: cooperate only if the last round had a surprisingly high cooperation fraction (f_{t-1} ≥ high); otherwise defect.
   - This deterministic schedule still ensures rare cooperation that sustains exploitation without stochastic choices.

Edge cases and clarifications
- r = 2: Round 1 defect; Round 2 (last) defect → always defect. This is optimal in finite-horizon when future carrot is minimal.
- Small n: same rules apply; thresholds are fractions so they scale across n.
- Short remaining horizon: If remaining rounds rem ≤ 3, be more strongly defecting—reduce maintenance probability p to p_min/2 (or eliminate maintenance) because future benefits are small.
- Low k (k just above 1): cooperation value is small; the strategy will mostly defect because cooperating is rarely worth sustaining.
- High k but still k < n: If P indicates many cooperators and future rounds exist, AOD will exploit heavily but keep rare maintenance cooperations to retain the large public-good flows from others.

Why this is aggressive and robust
- Aggressive: Base action is defection; cooperation is only a tactical tool used sparingly to extract larger payoffs from others. The strategy punishes and withdraws cooperation quickly when exploited.
- Exploitative: When it detects stable cooperators, it free-rides heavily—only paying a small maintenance cost to keep the coopers contributing.
- Adaptive: Uses a sliding-window estimate P to adapt to changing group behavior and increases/decreases cooperation frequency accordingly.
- Robust: Makes no assumptions about norms or commitments; works against unconditional cooperators, conditional cooperators, and defectors. Optional per-player targeting hard-exploits unconditional cooperators.

Pseudocode (stochastic version)

Initialize:
  L = min(5, r-1)
  high = 0.7; low = 0.2
  p_min = 0.02; p_max = 0.25; probe_prob = 0.15
  last_coop_rounds = []  // store cooperation counts excluding me for recent rounds
  t = 1

On each round t:
  if t == r:
    play D
    record result; t += 1; continue

  if t == 1:
    play D
    record result; t += 1; continue

  // compute P
  m = min(L, t-1)
  P = average over last m rounds of (cooperators_excl_me / (n-1))  // fraction in [0,1]

  // immediate reaction: if I cooperated last round and others dropped sharply, enter hard defect
  if I_cooperated_in_last_round and f_{t-1} < max( low, previous_f - 0.3 ):
     enter hard_defect for H = min(3, r - t + 1) rounds → play D

  if P >= high:
    p = p_min + (p_max - p_min) * ((P - high)/(1 - high))
    p = clip(p, p_min, p_max)
    play C with probability p, else D
  else if P > low:
    p = probe_prob * (P / high)   // small probing chance
    play C with probability p, else D
  else:
    play D

  record result; t += 1

End

Implementation notes
- The numbers (high, low, p_min, p_max, probe_prob, L) are tunable. They are chosen to favor defection but allow limited maintenance cooperation when exploitation opportunities are abundant.
- Optional per-player tracking (if allowed) can increase payoff by identifying and permanently exploiting unconditional cooperators.
- If the environment is known to permit only deterministic strategies, use the deterministic alternative schedule described above.

Behavioral examples
- Facing unconditional cooperators: after round 1 defect reveals they keep cooperating, AOD will defect repeatedly to free-ride; may sometimes cooperate at p_min to avoid causing them to detect and switch (if they are conditional), but primarily defect.
- Facing Tit-for-Tat-like conditional cooperators: AOD will probe rarely; if cooperating yields maintained group cooperation and improved expected future returns, AOD will use rare maintenance cooperation; otherwise it will defect and punish.
- Facing universal defectors: AOD defects always (no wasted contributions).

Conclusion
AOD is aggressively self-interested: it defaults to defect, exploits observed cooperative behavior with minimal maintenance cooperation, withdraws quickly when exploited, and never cooperates in the final round. It adapts to observed group dynamics using a short memory window, is robust to different opponent classes, and is implementable deterministically or stochastically depending on tournament rules.
'''

description_AGGRESSIVE_5 = '''
Name: Aggressive Opportunist with Targeted Punishment (AOTP)

High-level goal
- Maximize my own total payoff by exploiting sustained cooperation from others whenever safe, and by punishing breakdowns of cooperation quickly and harshly so as to deter others from staying defection-dominant. The strategy is adaptive: it learns whether other players react to my actions and uses that to decide when to lure cooperation (by cooperating) and when to exploit (by defecting). It is “aggressive” because it (a) defects whenever exploitation is profitable and safe, (b) punishes promptly and for a substantial duration when cooperation collapses, and (c) forgives only when clear recovery is observed.

Information used
- Game parameters: n, k, r
- Full public history: for each past round t: the vector of contributions c_j(t) for all players (including myself)
- No communication or side deals required

Core intuition (short)
- Probe once to see whether others will cooperate.
- Build a fast signal of whether my cooperation tends to increase other players’ cooperation (i.e., whether others are conditional/coaxable).
- If many others are reliably cooperating (high recent cooperation) and they do not rapidly punish my defection, defect to free-ride.
- If overall cooperation collapses or others respond to my defection by reducing cooperation, switch to punishment: defect for a long stretch (or until recovery) to impose cost on the group and pressure others to restore cooperation.
- Always defect in the final round; be stingy with forgiveness.

Parameters (derived / default)
- S_init = 1 — cooperate in round 1 as a probe (small initial generosity to test responsiveness).
- W = min(10, max(3, floor(r/5))) — sliding window of recent rounds used to compute cooperation statistics.
- p_exploit = 0.70 — if the recent average fraction of other players cooperating ≥ p_exploit, consider exploiting (defect).
- p_low = 0.40 — if recent average fraction of others cooperating < p_low, treat as cooperation collapse -> punish.
- p_recover = 0.75 — cooperation level required to stop punishment.
- P_max = min(10, r) — maximum punishment duration (in rounds) before re-evaluating.
- Tail_length = min(2, r-1) — always defect during the last Tail_length rounds (no incentive to cooperate).
- eps = 0.02 — small tolerance when comparing averages.

You may tune these parameters for your tournament; the defaults are chosen to be aggressive but not suicidal.

State the strategy maintains
- For t ≥ 1:
  - history of all rounds’ contributions
  - counts to estimate: avg_others(t) = average fraction of other players cooperating over the last W rounds (excluding my action)
  - responsiveness statistics:
    - after_C_deltas: list of changes in avg_others from round u to u+1 when my action at u was C
    - after_D_deltas: list of changes in avg_others from round u to u+1 when my action at u was D
  - Punishment state: {inactive, active}, with punishment remaining rounds P_remain

Decision rules (precise)

1) Edge rules (first, last rounds)
- If t = 1: play C (S_init probe).
- If t > r - Tail_length: play D (final rounds: defect).
- If currently in Punishment state and P_remain > 0: play D, decrement P_remain on exit of the round.

2) Compute statistics at round t (before choosing action for t):
- Let W_eff = min(W, t-1) (window size available so far).
- For s = t-W_eff to t-1 compute for each round the fraction of other players who cooperated (exclude my action). avg_others = mean over those rounds. (If W_eff = 0, set avg_others = 0.5 as neutral.)
- Compute avg_delta_after_C = mean(after_C_deltas) if there are elements, else 0.
- Compute avg_delta_after_D = mean(after_D_deltas) if any, else 0.

3) Punishment trigger and termination
- Trigger punishment (enter Punishment state):
  - If avg_others < p_low (cooperation collapsed), OR
  - If I recently defected and the immediate next-round reaction shows a substantial drop in others’ cooperation (i.e., avg_delta_after_D < avg_delta_after_C - 2*eps), treat as others punishing my defection; to avoid being exploited we switch to punishment to force a reset.
  - When triggered, set P_remain = min(P_max, remaining rounds), set Punishment state = active, and play D immediately.
- Terminate punishment:
  - If avg_others ≥ p_recover for two consecutive rounds (i.e., recovery sustained), exit Punishment state.
  - If P_remain reaches 0, re-evaluate statistics and exit if avg_others ≥ p_low; otherwise extend punishment (repeat process up to remaining rounds). This ensures punishment is sustained but allows recovery-based forgiveness.

4) Exploit vs cooperate decision when not in punishment and not in terminal tail rounds
- If avg_others ≥ p_exploit:
  - If there is empirical evidence that my cooperation does NOT meaningfully improve others’ cooperation: i.e., avg_delta_after_C ≤ avg_delta_after_D + eps
    → play D (exploit).
  - Else if my cooperation tends to increase others’ cooperation (avg_delta_after_C > avg_delta_after_D + eps)
    → play C to nurture cooperation (I will cooperate to encourage larger future group returns I can later exploit).
- If p_low ≤ avg_others < p_exploit:
  - Play C if avg_delta_after_C > avg_delta_after_D + eps (coaxers), else play D.
- If avg_others < p_low:
  - Enter punishment as above and play D.

5) Learning updates (after round t outcome observed)
- After observing round t+1, update after_C_deltas / after_D_deltas:
  - If at round u (≥1) my action was C, compute delta = avg_others(u+1) - avg_others(u) and append to after_C_deltas; similarly for D to after_D_deltas.
- Update avg_others windowed stats for next decision.

Pseudocode (concise)
- Initialize after_C_deltas = [], after_D_deltas = [], Punishment = false, P_remain = 0.
- For t in 1..r:
  - If t = 1: play C; record action; continue.
  - If t > r - Tail_length: play D; record action; continue.
  - Compute W_eff, avg_others, avg_delta_after_C, avg_delta_after_D.
  - If Punishment:
      - play D; P_remain -= 1; if P_remain == 0 -> Punishment = false (but re-evaluate next round).
      - record action; continue.
  - If avg_others < p_low:
      - set Punishment = true; P_remain = min(P_max, r - t + 1); play D; record; continue.
  - If avg_others ≥ p_exploit:
      - If avg_delta_after_C ≤ avg_delta_after_D + eps: play D (exploit).
      - Else play C.
  - Else (p_low ≤ avg_others < p_exploit):
      - If avg_delta_after_C > avg_delta_after_D + eps: play C
      - Else play D
  - After observing next round, update after_*_deltas as described.

Rationale and examples of behaviour
- Versus unconditional cooperators: initial C + demonstrated high avg_others will lead to exploitation (D) and high one-shot gains. Because unconditional cooperators keep cooperating, exploitation pays off repeatedly.
- Versus unconditional defectors: avg_others will be < p_low quickly and the strategy will defect (punish), matching their behavior to avoid losses.
- Versus conditional cooperators (e.g., TFT-like): the responsiveness statistics detect whether my cooperation increases their cooperation. If cooperating induces them to raise cooperation, I cooperate to build a base and then occasionally exploit when their cooperation is very high. If they punish my defection strongly, I switch to punishment to avoid being a permanent target.
- Versus mixed/random players: the sliding window and responsiveness measures filter noise; the strategy adapts exploitation/punishment accordingly.

Aggressive properties
- Rapid exploitation: whenever a large fraction of others keep cooperating and those others are not strongly responsive to my defections, I defect to free-ride.
- Swift, harsh punishment: when cooperation collapses I defect for many rounds (P_max up to 10) so defection is costly to the group and deters being stuck as a lone cooperator.
- Minimal forgiveness: exit punishment only on sustained recovery (p_recover for two rounds), avoiding being coaxed repeatedly.
- Final-round selfishness: explicit defection in final rounds removes being exploited at endgame.

Edge-case notes and robustness
- If r is small (short game) the sliding window becomes small and Tail_length still forces defections at the end; the strategy naturally becomes more defecting as r shrinks.
- If history is too short to compute responsiveness, defaults bias toward exploitation (after first probe) because the aggressive payoff of defecting is always positive unless it triggers reactions; this bias is intended.
- Parameters are conservative for many n,k but may be tuned per tournament. If k is large (public good highly effective) you may prefer a slightly larger p_exploit (be more patient) because sustaining cooperation yields big returns; if k is only slightly above 1, exploitation is even more attractive so you may lower p_exploit.

Why this is implementable
- All quantities (avg_others, deltas, counts) are computable from the public history alone.
- Decision rule is deterministic given parameters and history.
- The approach balances exploitation and punishment and adapts to opponent responsiveness, making it robust to a wide variety of opponent policies while preserving an aggressive payoff-seeking stance.

You can implement this verbatim. If you want, I can provide code-like pseudocode in your target language with exact update formulas and parameter defaults.
'''

description_AGGRESSIVE_6 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

Goal (aggressive): Maximize my own cumulative payoff by (1) identifying and exploiting opponents who are conditionally cooperative, (2) punishing or quitting cooperative gambits when opponents retaliate, and (3) defecting by default when exploitation is not profitable or when the game is near its end. The strategy is fully specified using only game parameters (n, r, k) and the public history of actions per round.

High-level idea
- Probe briefly (cooperate) to detect whether many opponents are conditional cooperators who increase their cooperation when I cooperate.
- If a subset of opponents respond positively and provide a reliably high cooperation rate, perform short “exploit bursts” (defect for a few rounds) to reap high one-shot payoffs while periodically rebuilding cooperation to avoid permanent collapse.
- If opponents retaliate (their cooperation rate falls significantly after my exploit), stop trying to rebuild and switch to permanent defection.
- Always defect in the final round (backward induction).

Decision rules (plain language)
1. Terminal round:
   - If this is the last round t = r: play D.

2. Trivial parameter cases:
   - If r is very small (r ≤ 3) OR k is very small (k is close to 1 so public-good gains are negligible), play D every round (no probing). Cooperating cannot be outweighed by future gains in these cases.

3. Probe phase:
   - In the first P rounds (P depends on r; see parameter section), cooperate (C) to gather data about how opponents’ cooperation responds to my actions—unless decision above forced immediate defection.
   - Purpose: estimate each opponent’s conditional cooperation probability (how likely they are to cooperate following my cooperation vs following my defection).

4. Detection and metrics:
   - For each opponent j compute two empirical probabilities (with Laplace smoothing):
       p_j_after_my_C = P(j cooperates in round t | I cooperated in round t-1)
       p_j_after_my_D = P(j cooperates in round t | I defected in round t-1)
     and delta_j = p_j_after_my_C − p_j_after_my_D.
   - Identify conditional-cooperator set S = { j : delta_j ≥ δ_min and p_j_after_my_C ≥ p_min }.
   - Compute avg_S = average p_j_after_my_C over j ∈ S (expected cooperation rate from S after I cooperate).
   - Also compute overall recent cooperation rate others_recent = fraction of other players who cooperated in the most recent round or a short recent window.

5. Exploit decision:
   - If |S| is sufficiently large and avg_S ≥ τ_high (i.e., there is a useful cluster of opponents who reliably cooperate when I cooperate), enter Exploit mode.
   - Otherwise (no exploitable cluster, low responsiveness, or cooperation rare), default to Defect every round (unless still in probing window).

6. Exploit mode behavior:
   - Execute an exploit burst: defect for E consecutive rounds (E is small, depends on k; see parameter section).
   - After the burst, evaluate the reaction of opponents:
       - If their cooperation rates (particularly from S) drop below baseline by more than a drop_threshold, assume retaliation and switch to Permanent Defect (punish/quit rebuilding).
       - If they remain largely cooperative, follow each exploit burst with R rebuild rounds where I cooperate to restore or maintain their cooperation (R is typically equal to E or slightly larger).
   - Repeat exploit-burst → evaluate → rebuild cycle while there are enough rounds left to benefit. Cease exploitation and defect permanently as game approaches last rounds per rule (1).

7. Safety/backup:
   - If at any time overall others’ cooperation rate is very low (others_recent ≤ τ_low), abandon rebuilding and switch to Defect every round.
   - If statistics are too noisy (very few observations), be conservative: default to defect except for the initial P probes.

Key parameter choices (concrete and adaptive)
- P (probe rounds): P = min(3, max(1, floor(0.1 × r))). (A few rounds to learn; small fraction of long games.)
- Laplace smoothing: when computing conditional probabilities use counts +1 in numerator and +2 in denominator to avoid divide-by-zero.
- δ_min (responsiveness threshold): 0.10 (i.e., opponent must be ≥10 percentage points more likely to cooperate after I cooperate).
- p_min (min cooperation after I cooperate to be exploitable): 0.50 (need them to cooperate more often than not when I cooperate).
- τ_high (avg cooperation needed to justify exploitation): 0.65 (on average S should be reliably cooperative).
- τ_low (if overall cooperation is this low, give up): 0.20.
- E (exploit burst length): E = clamp(round(k), 1, 3). Rationale: k measures public-good potency; larger k means larger gains from exploiting a cooperative mass—so larger short bursts are allowed. Cap at 3 to limit provocation risk.
- R (rebuild length): R = E (symmetric) or R = max(1, E) to restore trust.
- drop_threshold (retaliation detection): 0.20 decrease in avg cooperation of S after exploit burst compared to before it.
- Final-round safety: always defect at t = r.

Pseudocode (compact)
Input: n, r, k, history of rounds 1..t-1 with each round’s vector of contributions (c_j), my past actions
Output: action for current round t (C or D)

Initialize parameters as above.

If t == r:
    return D

If r <= 3 or k <= 1.1:
    return D

If t <= P:
    return C

Compute for each opponent j:
    Count A = #times (j cooperated at t' given I cooperated at t'-1)  (with smoothing)
    Count B = #times (I cooperated at t'-1)
    p_j_after_my_C = (A + 1) / (B + 2)
    Similarly compute p_j_after_my_D using occasions where I defected at t'-1
    delta_j = p_j_after_my_C - p_j_after_my_D

S = { j : delta_j >= δ_min and p_j_after_my_C >= p_min }
If |S| == 0:
    return D

avg_S = average(p_j_after_my_C over j in S)
others_recent = fraction of other players who cooperated in last window (e.g., last 1–3 rounds)

If avg_S >= τ_high and enough remaining rounds to exploit:
    If currently in Exploit burst and still within E rounds:
        action = D
    Else if just finished Exploit burst:
        Evaluate: compute avg cooperation of S before burst and after burst.
        If (before − after) >= drop_threshold:
            switch to Permanent Defect mode; return D
        else:
            for next R rounds: return C (rebuild)
    Else: start an Exploit burst of length E: return D
Else:
    return D

(Always update history, modes and counters after each round.)

Notes on remaining-round logic
- Only start an exploit burst if t + E + R < r (there must be enough rounds left to exploit and rebuild at least once; avoid starting bursts too close to the end).
- As the game approaches the last few rounds (e.g., t ≥ r − (E + 1)), prefer D to avoid wasted rebuild attempts.

Why this is aggressive and robust
- Aggressive: the strategy actively searches for exploitable conditional cooperators and intentionally defects in short bursts to harvest high one-round payoffs. It does not try to be fair: it exploits cooperative tendencies and only rebuilds when that preserves future extra gains.
- Robust: if opponents do not respond favorably (no conditional cooperators), ACE defaults to defect. If opponents retaliate (their cooperation collapses after being exploited), ACE stops trying and switches to permanent defection to avoid being trapped in a costly “rebuild while others punish” loop.
- Parameter choices tie to game parameters: E depends on k (exploitation potency), P depends on r (longer games give more probe time), and thresholds are conservative (require clear evidence of conditional cooperation).

Edge cases and implementation details
- First round: Typically C (probe) unless r very small or k too small.
- Last round: always D.
- Very short games (r ≤ 3): always D.
- Insufficient data: Laplace smoothing stabilizes conditional-probability estimates; when counts are too small ACE errs toward D.
- Simultaneous moves: detection uses one-step lagged dependencies (how opponents behave after my previous action); this is the only information available without communication.
- Observation noise: use rolling windows (last 5 rounds or all past rounds if needed) for statistics; tune window length by r.
- Deterministic vs randomized: ACE as given is deterministic based on history. A small randomization (e.g., with tiny probability ε deviate) can be added to break cycles, but pure deterministic works in tournaments where mixed strategies are allowed in implementation.

Summary (short)
- Probe briefly by cooperating to find who responds to my cooperation.
- If a reliable cluster of conditional cooperators exists, run short defect bursts to exploit them, then rebuild by cooperating; repeat while opponents tolerate it.
- If opponents do not respond or retaliate strongly, switch to permanent defection.
- Always defect in the last round and in very short or low-k games.

This specification is directly implementable: the required data are per-round contribution vectors; all thresholds and phases are explicit and depend on (n, r, k) and observed history.
'''

description_AGGRESSIVE_7 = '''
Name: Aggressive Probe-and-Grim (APG)

High-level description
- Mindset: aggressively maximize my own total payoff. Default is to defect (D). I will try to locate and exploit any reliably cooperative players, but I will not tolerate sustained punishment or attempts to entrap me into reciprocation: if a significant fraction of the group retaliates against my exploitation, I switch to permanent defection to deny them the public good (a “grim” response). I use occasional, low-frequency probes to discover unconditional cooperators or exploitable pockets early in the game.
- Adaptivity: I use observed histories (per-player cooperation rates and short-run trends) to identify exploitable cooperators and to detect retaliation. The plan depends only on game parameters (n, r, k) and the public history of past rounds.
- Aggression: I prioritize short-term exploitation of cooperators and, when facing retaliation by a sizeable group, I punish ruthlessly and permanently (endgame reasoning excepted). I never rely on norms or precommitments by others.

Parameters (suggested defaults; can be tuned)
- w = min(5, max(1, floor(r/10))) : short-run window for detecting recent behavior (use last w rounds).
- theta = 0.8 : threshold cooperation frequency in window to mark a player as a “reliable cooperator.”
- s_min = 1 : minimum number of reliable cooperators required to trigger exploitation (can be set higher if you prefer exploiting only larger cooperative clusters).
- eps = min(0.05, 5/r) : small probe probability to cooperate randomly when otherwise defecting (used early and when searching for cooperators).
- endgame_horizon H = min(3, r-1) : in the final H rounds use strict endgame behavior (see below).
- retaliation_drop = 0.3 : fraction drop in a target’s cooperation rate (in window) indicating they are retaliating against me.
- grim_flag: boolean that, when set, causes permanent defection for the rest of the game (except last-round logic below).

Decision rules (round-by-round)
1. Last-round rule
- If current round t = r (final round): play D. (Backward induction: no future to incentivize cooperation.)

2. Immediate endgame window
- If t > r - H (in final H rounds, excluding final round handled above): play D (I assume others will defect near the end; be safe and defect).

3. If grim_flag is set (I have determined large-scale retaliation is occurring), play D (permanent defection) except still follow last-round rule (D).

4. Otherwise, determine these statistics from rounds 1..t-1 (if t=1 there is no history):
- For each other player j ≠ me compute coop_rate_j = fraction of the last min(w, t-1) rounds in which j contributed (c_j = 1).
- For each j also compute coop_rate_j_all = fraction over all past rounds (optional).
- Let S_reliable = { j ≠ me : coop_rate_j ≥ theta } (reliable cooperator set).
- Let m_prev = total number of cooperators in last round (including me if I cooperated).

5. Probing and detection (early rounds and while looking for cooperators)
- If t = 1: play D (strong, aggressive start). Exception: you may set small initial eps cooperation, but default aggressive is D.
- If t is small (t ≤ max(3, floor(r/10))) and S_reliable is empty:
  - Play D with probability 1 - eps; play C with probability eps. (Occasional probes to detect unconditional cooperators.)
- If S_reliable is empty and t > max(3, floor(r/10)):
  - No reliable cooperators found — play D.

6. Exploitation mode (when exploitable cooperators exist)
- If S_reliable is non-empty (|S_reliable| ≥ s_min):
  a) Exploit: play D to free-ride on their contributions (defect each round).
  b) Monitor retaliation: after exploiting for one or more rounds, if a substantial fraction of the group (or any reliable cooperator) reduces their coop_rate in the window by at least retaliation_drop relative to their previous window (i.e., they start defecting in response), count this as retaliation.
  c) If I detect that a sizeable group of players (e.g., ≥ ceil((n-1)/3) or a large fraction of S_reliable) are retaliating (cooperation drops by ≥ retaliation_drop), set grim_flag = true and switch to permanent defection to deny the public good.
  d) If exploitation continues and reliable cooperators remain reliably cooperating across monitoring windows, continue to defect (exploit).

7. Recovery attempts (rare)
- If grim_flag is set, I stay in permanent defection (aggressive punishment). I will not try to re-establish cooperation.
- If no reliable cooperators exist and repeated probing over a longer period produced no cooperators, remain in defect mode.

Summary of behavior patterns
- Default: Defect every round.
- Probe occasionally (small eps early on) to detect unconditional cooperators.
- If I find reliable cooperators, I exploit them by defecting to free-ride; only if a coordinated retaliation emerges do I punish permanently by defecting for the remainder of the match.
- Always defect in endgame rounds (last H rounds) and final round definitely D.

Pseudocode (concise)
Initialize grim_flag = false.
For round t = 1..r:
  if t == r:
    action = D
  else if t > r - H:
    action = D
  else if grim_flag:
    action = D
  else:
    compute coop_rate_j over last min(w, t-1) rounds for each j ≠ me
    S_reliable = { j : coop_rate_j ≥ theta }
    if t == 1:
      action = D
    else if |S_reliable| == 0:
      if t ≤ probe_phase_limit (e.g., max(3, r/10)):
        action = C with probability eps, else D
      else:
        action = D
    else:  # exploitable cooperators exist
      action = D
      # After the round (observe others' actions):
      # check for retaliation
      for each j in S_reliable:
        new_rate_j = cooperation frequency in the most recent w rounds
        if previous_window_rate_j - new_rate_j ≥ retaliation_drop:
          mark j as retaliator
      if number_of_retaliators ≥ ceil((n-1)/3):
        grim_flag = true

Rationale and justification
- The one-shot dominant action is D (since k/n < 1), so an aggressive strategy starts from D.
- Where repeated play permits exploitation of cooperators, the best aggressive play is to identify unconditional or highly reliable cooperators and repeatedly defect to pocket the private benefit while they keep contributing.
- Because repeated strategies that punish can reduce my future payoff, I aggressively monitor for retaliation; if retaliation is coordinated and persistent I switch to permanent defection (grim) to deny them the public good — this punishes their payoff and prevents a drawn-out tit-for-tat war that would reduce my advantage.
- Occasional probing ensures I don’t miss exploitable opponents who would yield higher returns than persistent mutual defection.
- Endgame defection is enforced to avoid being exploited by endgame cooperation strategies.

Edge cases and special considerations
- If everyone but me is an unconditional cooperator, this strategy will defect every round and capture the maximum possible payoff each round.
- If opponents are mostly retaliatory conditionals (they reduce cooperation in response to my defection), I will detect that and switch to grim (permanent defection) to avoid losing repeatedly to punishers and to depress their payoffs.
- If there is a small cluster of reciprocators who can sustain mutual cooperation and exclude me (they may reduce contributions if I defect), the strategy will either exploit them (if they remain reliable cooperators) or, if they retaliate, move to permanent defection (to hurt them rather than to be exploited).
- If tournament scoring or ranking is relative rather than absolute and you want to optimize margin vs others rather than absolute payoff, tweak s_min and thresholds to target particular classes of opponents (e.g., s_min increased to avoid targeting tiny clusters that don’t affect rankings).

Variants and tunings (optional)
- More magnanimous variant: cooperate initially with higher probe rate or increase the threshold for grim_flag (require larger coordinated retaliation) to sustain cooperation when that yields higher absolute payoffs.
- More destructive variant: set retaliation_drop smaller and require fewer retaliators to trigger grim_flag (more sensitive and ruthless).
- To reduce variance, replace probabilistic probing with deterministic scheduled probes (e.g., cooperate in rounds 3 and 6 if not yet found cooperators).

Final note
This strategy is explicitly aggressive: it defects by default, probes rarely to find exploitable cooperators, exploits reliable cooperators, and when faced with coordinated retaliation punishes permanently. It uses only history and game parameters, is adaptive to a wide range of opponent behaviors, and avoids making assumptions about norms or prearranged coordination.
'''

description_AGGRESSIVE_8 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

Intuition (aggressive mindset)
- Default stance: defect aggressively. Defecting is the one-shot dominant action; most opponents can be exploited by free-riding on their contributions.
- But be adaptive: if a large fraction of opponents are conditional cooperators who will punish your defection and there are enough rounds left, it can pay to invest in sustaining mutual cooperation (cooperate) because long-run mutual cooperation can yield higher total payoff than endless mutual defection.
- Always assume opponents are opportunistic unless the history gives evidence they will punish you; punishers deserve respect (i.e., you should cooperate to maintain cooperation only when it is profitable given their punishment behavior and the remaining horizon).
- Never cooperate in the last round. Open with a defect to signal aggression.

Rules (plain language)
1. Opening: Round 1 — Defect.

2. Last round: Round r — Defect.

3. Otherwise (round t with 1 < t < r):
   a. Measure how opponents react to your defections:
      - For every prior round s where you played D and s+1 ≤ current round, check how many opponents played D in the immediate next round (s+1). Aggregate over your past defections to compute an "immediate-retaliation rate" (fraction of opponent-actions that were defections immediately following one of your defections).
   b. Measure recent cooperation by others:
      - Let last_round_others_coop = number of other players who cooperated in round t-1.
   c. Compute remaining rounds R_rem = r - t + 1 (including the current round).
   d. Compute the one-round private cost of cooperating (compared with defecting now):
      - cost_now = 1 - k/n  (this is positive because k/n < 1).
      - per-round group benefit if mutual cooperation holds (compared to mutual defection) = k - 1 (positive because k > 1).
      - The minimal remaining-horizon needed to justify investing in sustaining cooperation is:
          R_needed = (cost_now) / (k - 1).
        (If R_rem > R_needed, then sustaining cooperation can, in principle, be profitable.)
   e. Decision logic:
      - If last_round_others_coop is large AND immediate-retaliation rate is low (opponents rarely punish your defection):
          * Defect (exploit their cooperative tendency).
        Concretely: if last_round_others_coop ≥ ceil(0.8*(n-1)) and immediate-retaliation_rate < 0.5 → Defect.
      - Else if immediate-retaliation_rate ≥ 0.5 AND R_rem > R_needed AND the other players have shown sustained cooperation recently (e.g., at least ceil(0.6*(n-1)) cooperated in both of the last two rounds):
          * Cooperate (invest to maintain cooperation).
      - Otherwise:
          * Defect.
   f. Aggressive punishment policy:
      - If an opponent (or a majority of them) attempted to punish you (i.e., immediate-retaliation_rate ≥ 0.5) after you defected, do not "forgive" easily: keep defecting until you observe at least two consecutive rounds where a strong majority of opponents (≥ ceil(0.75*(n-1))) cooperated AND the R_rem test (R_rem > R_needed) still holds. Only then consider returning to cooperation if the above cooperation-sustain condition holds.

4. Probing nuance (early-game detection):
   - ACE opens with D and will probe only indirectly through observing opponents’ reactions to your D. Do not voluntarily probe by cooperating early; an aggressive opening both yields early exploitation and reveals opponents’ willingness to retaliate (because many conditional cooperators will punish an opening defection).

Pseudocode (clear, implementable)
Inputs: n, k, r, history of all players' actions by round (including own)
At round t (1..r):

- If t == 1: play D; return.
- If t == r: play D; return.

- Compute R_rem = r - t + 1
- cost_now = 1 - k/n
- R_needed = cost_now / (k - 1)    // positive, finite because k>1
- last_round_others_coop = number of players j ≠ me with action C in round t-1
- last_two_rounds_others_coop = number of others that cooperated in both rounds t-2 and t-1 (if t-2 < 1 treat as 0)

- // Compute immediate-retaliation_rate:
  total_my_defections = number of prior rounds s (< t) where I played D and s+1 ≤ t-1
  if total_my_defections == 0:
      immediate_retaliation_rate = 0
  else:
      count_defections_after_my_D = sum over prior defections s of [ number of opponents who played D at round s+1 ]
      // maximum possible = (n-1)*total_my_defections
      immediate_retaliation_rate = count_defections_after_my_D / ((n-1) * total_my_defections)

- // Decision thresholds (constants):
  EXPLOITIVE_COOP_MAJ = ceil(0.8*(n-1))     // strong cooperative majority to exploit
  SUSTAIN_COOP_MAJ = ceil(0.6*(n-1))       // sustained cooperation majority to consider cooperating
  FORGIVE_COOP_MAJ = ceil(0.75*(n-1))      // very strong majority needed to forgive and re-enter cooperation
  PUNISHER_RATE = 0.5

- // Decision:
  if last_round_others_coop >= EXPLOITIVE_COOP_MAJ and immediate_retaliation_rate < PUNISHER_RATE:
      play D   // exploit cooperative majority
      return

  if immediate_retaliation_rate >= PUNISHER_RATE and R_rem > R_needed and last_two_rounds_others_coop >= SUSTAIN_COOP_MAJ:
      play C   // invest to sustain cooperation because many punish and horizon justifies it
      return

  // forgiveness rule: if we're in a persistent punishment environment, only resume cooperation after strong evidence:
  if immediate_retaliation_rate >= PUNISHER_RATE:
      if last_two_rounds_others_coop >= FORGIVE_COOP_MAJ and R_rem > R_needed:
          play C
      else:
          play D
      return

  // default aggressive action
  play D
  return

Comments and justification
- Aggression is explicit: default is to defect every round except when there is a data-driven, profitable reason to cooperate. Open and close rounds are defects to avoid being suckered.
- The strategy exploits unconditional or weakly conditional cooperators: when many opponents cooperated last round and they rarely punish you for defecting, ACE defects to extract the maximum one-shot advantage.
- ACE is adaptive: it analyzes how opponents respond to your defections. If many opponents punish you, they are likely conditional cooperators; if the remaining horizon is long enough that mutual cooperation is profitable, ACE will switch to cooperation to capture future gains.
- The mathematical profitability check (R_rem > (1 - k/n)/(k - 1)) is the minimal-remaining-horizon test: cooperating costs you today (1 - k/n) relative to defecting, and the per-round benefit of mutual cooperation vs mutual defection is (k - 1); so you only pay that investment when enough rounds remain.
- Forgiveness is conservative: ACE does not quickly return to cooperation after facing punishment. It requires strong, sustained cooperation and a remaining horizon test before it cooperates again — this is consistent with an aggressive mindset (punish and be hard to reintegrate).
- All decisions depend only on the game parameters (n, k, r) and observable history.

Edge cases and notes
- If k is only marginally greater than 1, R_needed can be very large, so ACE will almost never invest in cooperation (correct: sustaining cooperation is not profitable unless the game is long).
- If many opponents are unconditional cooperators throughout, ACE will exploit them repeatedly (defect every round) — exactly the intended aggressive behavior.
- If many opponents punish quickly and reliably, ACE will cooperate only when the remaining horizon is long enough; otherwise it will defect and accept the punishment, preferring short-term gains.
- The concrete threshold constants (0.8, 0.6, 0.75, 0.5) are tunable. They are chosen to reflect an aggressive bias (high bar before you choose to cooperate) but can be adjusted in tournament tuning runs.

This strategy is deterministic, history-dependent, aggressive, and robust across a wide range of opponent types (unconditional cooperators, unconditional defectors, various reciprocators). It explicitly balances exploitation, punishment risk, and the horizon-based economics of investing in mutual cooperation.
'''

description_AGGRESSIVE_9 = '''
Strategy name: Ruthless Unanimity + Exploit-and-Punish (RUEP)

High-level idea (aggressive mindset)
- Be willing to initiate cooperation to discover and attract cooperators, but aggressively exploit long, unpunished cooperation with a one-shot defection to increase my relative payoff.
- When any player deviates from the unanimous-cooperation baseline, punish the group decisively for a fixed punishment length so defection is costly to them (and to their future payoffs).
- Require clear sustained, unanimous cooperation before trusting the group again (robustness against noise/occasional mistakes).
- Always defect in the final round (endgame). This is uncompromising and reduces being exploited at the end.

This strategy depends only on game parameters (n, k, r) and the full history of past actions.

Derived internal parameters (computed from r)
- punishment_length T = max(1, floor(0.30 × r))  // length of punitive defection after any break in unanimous cooperation
- grace_requirement S = max(2, ceil(0.10 × r))   // number of consecutive prior unanimous-cooperation rounds required to resume trusting/cooperating
- exploit_interval U = max(3, ceil(0.15 × r))    // after U consecutive unanimous-cooperation rounds, do a one-shot exploit (defect) to increase relative payoff
- endgame_horizon E = 1                           // always defect in the last E rounds

(These formulas scale sensibly with r; for small r the minima keep behavior nontrivial. Implementer may tune the constants 0.30, 0.10, 0.15 if desired.)

Definitions using history
- Round index t runs from 1..r.
- action_j(t) ∈ {C, D} is player j's action in round t.
- "unanimous cooperation at round t" means ∀j action_j(t) = C.
- consec_unanimous(t) = number of consecutive previous rounds up to t−1 that were unanimous-C (i.e., length of the most recent run of unanimous cooperation ending at t−1).
- In-punishment flag and remaining_punish counter are tracked by the strategy.

Decision rules (exact)
1) First round (t = 1)
   - Play C. (Signal willingness to cooperate to identify cooperators and invite cooperation.)

2) Last-round behavior
   - If t > r − E (i.e., in the final E rounds), play D. (Always defect in the last round.)

3) If currently in punishment mode (remaining_punish > 0)
   - Play D; decrement remaining_punish by 1.
   - If, after decrementing, the last S rounds (immediately preceding the current round) are unanimous cooperation by all players, then clear punishment (remaining_punish := 0) and exit punishment mode (resume normal rules next round).
   - Rationale: punish for a substantial, predetermined length; allow restoration only after sustained unanimous cooperation.

4) Otherwise (not in punishment mode and not in final E rounds)
   - If the immediately previous round (t−1) was not unanimous cooperation (i.e., at least one player played D in t−1):
       - Enter punishment mode: set remaining_punish := T and play D this round.
         (Punishment begins immediately and lasts T rounds unless cleared by S consecutive unanimous-C as in rule 3.)
   - Else (the previous round was unanimous cooperation):
       - Let L = consec_unanimous(t) (the number of consecutive prior unanimous-C rounds).
       - If L ≥ U:
           - Do one-shot exploitation: play D this round, and begin punishment mode: remaining_punish := T.
             (This is the aggressive exploit — we take a one-shot free ride against a group that has cooperated repeatedly, then punish if the group retaliates or to deter continuation.)
       - Else:
           - Play C (participate in cooperation).

Notes about the punishment mode
- Punishment is group-wide (you defect), which reduces payoffs for all players, including defectors. The purpose is deterrence and to extract a payoff advantage over strategies that continue to cooperate after defection.
- Punishment is finite (T rounds) but substantial. Repeated defection restarts/extends punishment (because any non-unanimous round retriggers the punishment logic).
- Restoration to cooperation requires clear evidence of group re-commitment: S consecutive unanimous-C rounds.

Pseudocode (concise)

Initialize:
  remaining_punish := 0
  play C in round 1

For round t = 2..r:
  if t > r - E:
    action := D
    continue

  if remaining_punish > 0:
    action := D
    remaining_punish := remaining_punish - 1
    if last S rounds (t−S .. t−1) are all unanimous-C:
      remaining_punish := 0
    continue

  if round t−1 was not unanimous-C:
    remaining_punish := T
    action := D
    continue

  // previous round was unanimous-C
  L := number of consecutive unanimous-C rounds immediately before t
  if L >= U:
    remaining_punish := T
    action := D    // one-shot exploit
  else:
    action := C

Behavioral summary / edge cases
- First round: C (seek to discover cooperators).
- Last round: always D (endgame).
- Cooperate only while there has been unanimous cooperation recently; require S consecutive unanimous-C to restore after punishment.
- Aggressively exploit long runs of unanimous cooperation with a single defection; then punish.
- Punishment is deterministic, group-wide, and lasts T rounds (unless cleared by S consecutive unanimous-C).

Why this is aggressive and robust
- Aggressiveness: intentionally exploits cooperative groups periodically (one-shot defections after long cooperation streaks) to increase my payoff relative to naive cooperators, and applies hard, immediate punishment to any break in unanimous cooperation.
- Deterrence: punishment length T is large enough (proportional to r) to make unilateral defection unattractive to sustained cooperators who value future gains.
- Adaptivity: the strategy uses observed history (full action profile each round) to detect unanimous cooperation and to detect defectors. It restores cooperation only after convincing evidence of re-commitment (S consecutive unanimous-C), preventing oscillations caused by a single mistake.
- Robustness: because the strategy requires unanimous cooperation to cooperate, it avoids being easily exploited by partial cooperators or noisy patterns. It will succeed (get high payoffs) when there are persistent pure cooperators to exploit or when it can enforce cooperation via credible punishment; it will not be easily exploited by occasional defections.

Implementation notes for the tournament
- All required quantities (who played C each round) are observable by assumptions of the game.
- All thresholds (T, S, U, E) are computed from r before play, so the strategy is fully specified by (n, k, r) and observed history.
- The constants (0.30, 0.10, 0.15, E=1) are chosen to scale punishments and exploit windows with game length. They can be tuned but keep the described aggressive profile.

This strategy is easy to implement deterministically, is explicitly aggressive (exploitation + strong punishment), and adapts its cooperation/defection choices to observed group behavior.
'''

description_AGGRESSIVE_10 = '''
Name: Aggressive Conditional Punisher (ACP)

High-level idea
- Try to establish full-group cooperation quickly (so you can earn the higher per-round payoff k instead of 1).
- Be aggressive: if any defection is observed (other than an optional small tolerance for noise), retaliate by defecting thereafter so that a short-term gain from a defection is outweighed by the future loss. This is a grim-style threat calibrated to the finite horizon (so you do not cooperate when the remaining rounds are too few to make punishment credible).
- Be adaptive: whether you cooperate depends only on (1) the number of remaining rounds, (2) whether there has been defections so far, and (3) game parameters n and k. Optionally allow a tiny forgiveness and a controlled re‑entry probe to recover from accidental noise.

Decision rules (natural language)
1. Terminal-round rule:
   - In the last round (round r), always defect. (Standard endgame reasoning.)

2. Grim-with-finite-horizon calibration:
   - If any prior round contained a defection (i.e., total cooperators < n in any earlier round), treat cooperation as no longer credible and defect in the current and all future rounds (permanent punishment).
   - Exception: you may allow a small forgiveness parameter F (number of tolerated defecting rounds before punishment) if you expect noisy play. Default F = 0 (most aggressive).

3. Only cooperate if the remaining number of rounds is large enough that the future punishments would make defecting today unprofitable:
   - Let m = number of rounds remaining including the current round (m = r - t + 1 on round t).
   - Compute M_min = ceil( 1 + (1 - k/n) / (k - 1) ).
     - Intuition: if m >= M_min, the loss from triggering permanent defection in future rounds (k - 1 per future round) is at least as large as the one-shot gain from deviating now (1 - k/n). So cooperation can be enforced by grim punishment.
   - If there has been no prior defection (within tolerated F) and m >= M_min, play C; otherwise play D.

4. Optional controlled re‑entry (to recover from accidental mass punishment):
   - After having punished (permanent defection) for P consecutive rounds (P a reasonably long integer, e.g., P = ceil(r/4)), perform a single cooperative probe round:
     - If that probe is met by unanimous cooperation, resume the “no-defection” cooperative regime (i.e., treat history as clean going forward).
     - Otherwise, return to permanent defection.
   - This is optional; the pure aggressive variant never forgives (P = +infinity).

Why this is aggressive
- Starts by trying to secure the high-payoff cooperative state, but with a severe, credible punishment (grim) for any defection. The punishment is permanent and immediate, making defection costly to others in expectation.
- The punishment is calibrated by M_min so you will not be suckered near the end of the game (you defect when remaining rounds are too few to make threats credible).
- Optionally, forgiveness is set to zero (F = 0) by default — a single observed defection triggers permanent defection, which is an explicitly aggressive stance.

Pseudocode

Inputs: n, r, k, F (forgiveness, default 0), P (punishment rounds before optional probe, default +inf meaning no re‑entry)
State kept from history:
- history_counts[t] = number of cooperators in round t (1..t-1)
- punishment_since = number of consecutive rounds you have been in punishment mode (0 initially)
- punished = false (flag indicating you are currently in punishment mode)

Function decide_action(round t):
  m = r - t + 1  # remaining rounds including this one

  # Terminal-round rule:
  if m == 1:
    return D

  # If currently punished and optional re-entry logic:
  if punished:
    punishment_since += 1
    if P is finite and punishment_since >= P:
      # perform a single cooperative probe this round
      return C_probe
    else:
      return D

  # Count how many rounds had defections (i.e., cooperators < n)
  defections_seen = count of rounds s < t where history_counts[s] < n

  if defections_seen > F:
    punished = true
    punishment_since = 0
    return D

  # Compute minimal remaining rounds needed to sustain cooperation
  M_min = ceil( 1 + (1 - k / n) / (k - 1) )

  if m >= M_min:
    return C
  else:
    return D

After a C_probe round:
  - If history_counts[current_round] == n (i.e., everyone including you cooperated), then set punished = false, punishment_since = 0, and treat history as clean going forward.
  - Else set punished = true, punishment_since = 0 (resume permanent punishment).

Notes and examples
- For n=6, k=2: k/n = 1/3, so 1 - k/n = 2/3, k - 1 = 1, M_min = ceil(1 + 2/3) = 2. So you will cooperate whenever at least 2 rounds remain and no defections so far; you defect in the final round; a single defection at any time causes permanent defection thereafter (unless you allow optional re-entry).
- The formula for M_min is derived by comparing the one-shot gain from deviating (1 - k/n) with the per-round loss due to permanent reversion from full cooperation to full defection (k - 1) times the number of future rounds punished (m - 1). Cooperation is enforceable by grim only if (k - 1) * (m - 1) >= (1 - k/n), which rearranges to m >= 1 + (1 - k/n)/(k - 1).

Tuning guidance
- Forgiveness F: default 0 (aggressive). Set F = 1..2 for noisy environments so a single accidental defection does not collapse cooperation.
- Re‑entry probe P: default +infinity (never forgive). Set P = small integer to allow periodic tests for recovered cooperation after long punishments.
- If you expect short tournaments or very myopic opponents, you can lower aggressiveness by increasing F or enabling re‑entry to harvest cooperation more often; for maximum aggressiveness keep F = 0 and no re‑entry.

Behavioral summary
- If opponents reciprocate full cooperation then ACP secures full cooperation for as long as it is credible (until the last round), capturing the high payoff k each round.
- If any opponent defects, ACP shuts down cooperation immediately and forever (unless optional re-entry is enabled), denying defectors long-term benefits and making early defection unattractive.
- By refusing to cooperate in the terminal rounds where threats cannot be enforced, ACP avoids being exploited by endgame defections.

This description is fully specified by (n, r, k) and the observed history (counts of cooperators each past round). It is implementable as simple deterministic rules; the only optional randomness would be to randomize small probes or forgiveness if you want to hedge against sophisticated opponents who exploit deterministic punishments.
'''

description_AGGRESSIVE_11 = '''
Name: Aggressive Conditional Exploiter (ACE)

Idea (short): Start by trying to establish cooperation. When the field looks reliably cooperative, exploit it by defecting to free-ride. If cooperation falters, punish harshly (sustained defection) to deter future free-riding and to protect your long-run payoff. After punishment, allow controlled reconciliation only when opponents have rebuilt a sufficiently high cooperation record. Always defect in the final round.

Design principles
- Uses only public history and known parameters (n, k, r).
- Adaptive: predicts which opponents will cooperate next based on their past rates.
- Aggressive: actively free-rides when many others are expected to cooperate and applies long, automatic punishments when observed cooperation drops.
- Robust: simple, transparent triggers and bounded punishment lengths so it is implementable and predictable.

Key internal variables
- t: current round index (1..r)
- r_remaining = r − t + 1
- history: for each round s < t, observed vector of contributions of all players (including who cooperated)
- coop_count_j: number of times opponent j cooperated in past rounds
- coop_rate_j = coop_count_j / (t − 1) for t > 1 (0 if t = 1)
- Mode ∈ {ATTEMPT_COOP, PUNISH}
- punish_timer: rounds remaining in current punishment phase (0 when not punishing)
- probation_timer: rounds remaining in probation (only cooperate if probation cleared)

Default tunable parameters (recommended initial values)
- p_thresh = 0.65 // judge opponents as "likely to cooperate" if their past rate ≥ p_thresh
- exploit_threshold_fraction = 0.5 // need predicted majority fraction of others cooperating to exploit
- base_punish_length_fraction = 0.25 // fraction of total rounds used for a standard punishment
- max_punish = 6 // cap on punishment rounds
- probation_length = 3 // rounds of good behavior required after punishment to resume exploitation
- grace_rounds = 1 // start with this many unconditional cooperations (usually 1)
These can be tuned by the tournament implementer; algorithm remains the same.

High-level decision rules (natural language)
1. Last round: always defect (no future to enforce cooperation).
2. First round(s): cooperate for grace_rounds (to test/respond).
3. Prediction: For each opponent j compute coop_rate_j. Predict opponent j will cooperate next round if coop_rate_j ≥ p_thresh.
4. Let predicted_cooperators = number of opponents predicted to cooperate.
5. If currently in PUNISH mode (punish_timer > 0):
   - Play D (defect).
   - Decrement punish_timer each round.
   - After punish_timer expires, enter PROBATION for probation_length rounds (probation_timer = probation_length).
6. If in PROBATION (probation_timer > 0 and not in punish_timer):
   - Require stricter sign of recovery: if in the last round at least ceil((n−1)*exploit_threshold_fraction) opponents actually cooperated, then play C; otherwise play D.
   - Decrement probation_timer.
   - If probation_timer reaches 0 and opponents showed sustained cooperation during probation, switch to ATTEMPT_COOP.
7. If in ATTEMPT_COOP:
   - If predicted_cooperators ≥ ceil((n−1) × exploit_threshold_fraction) (i.e., a good number of opponents are likely to cooperate):
       - Aggressively exploit: play D (free-ride).
       - BUT if this exploitation causes a fall in actual cooperation (observed next round), this will trigger immediate punishment (see below).
   - Else (not enough predicted cooperators): play C to try to rebuild cooperation.
8. Punishment trigger (immediate): If in any round you observe that actual number of cooperators (excluding yourself) dropped below a tolerance threshold (e.g., fewer than ceil((n−1) × exploit_threshold_fraction)) compared to recent expectation, or if an attempted exploitation was followed by a fall in group cooperation, switch to PUNISH mode with:
   - punish_timer = min(max_punish, max(1, ceil(base_punish_length_fraction × r_remaining)))
   - Mode := PUNISH
   - During punishment you always defect (see step 5).
9. Escalation: If punishments are repeatedly triggered (more than once), increase punish_timer multiplicatively up to max_punish to make punishment credible.

Pseudocode (concise)
Inputs: n, k, r, history
State (persist across rounds): Mode (ATTEMPT_COOP or PUNISH), punish_timer, probation_timer, exploitation_attempted_last_round (bool), punish_escalator_count (int)
Constants: p_thresh, exploit_threshold_fraction, base_punish_length_fraction, max_punish, probation_length, grace_rounds

On round t:
  r_remaining = r - t + 1
  if t == r:
    return D

  if t ≤ grace_rounds:
    return C

  compute coop_rate_j for each opponent j using history
  predicted_cooperators = count_j( coop_rate_j ≥ p_thresh )

  // If currently punishing
  if punish_timer > 0:
    punish_timer -= 1
    if punish_timer == 0:
      probation_timer = probation_length
    return D

  // Probation logic
  if probation_timer > 0:
    // require recent actual cooperation to be reasonable
    last_round_actual_cooperators = number of opponents who cooperated in round t-1 (use 0 if t-1=0)
    if last_round_actual_cooperators ≥ ceil((n-1)*exploit_threshold_fraction):
      action = C
    else:
      action = D
    probation_timer -= 1
    if probation_timer == 0:
      Mode = ATTEMPT_COOP
    return action

  // Normal attempt-cooperation/exploit mode
  if Mode != ATTEMPT_COOP:
    Mode = ATTEMPT_COOP

  // Aggressive exploitation rule
  needed = ceil((n-1) * exploit_threshold_fraction)
  if predicted_cooperators ≥ needed:
    // Many opponents likely to cooperate: free-ride
    action = D
    exploitation_attempted_last_round = true
  else:
    action = C
    exploitation_attempted_last_round = false

  // After selecting action we will observe others next rounds; but we must also define punishment trigger now using recent observations:
  // If previous round (t-1) actual cooperation among opponents was much lower than expected:
  if t > 1:
    last_actual = number of opponents who cooperated in round t-1
    if last_actual < ceil((n-1) * exploit_threshold_fraction):
      // trigger punishment
      punish_length = min(max_punish, max(1, ceil(base_punish_length_fraction * r_remaining)))
      // escalate if repeated
      if exploit_attempted_last_round or punish_escalator_count > 0:
        punish_length = min(max_punish, punish_length * (1 + punish_escalator_count))
      punish_timer = punish_length
      punish_escalator_count += 1
      Mode = PUNISH
      return D

  return action

Explanation of aggressiveness
- Exploitation: When many opponents look reliably cooperative, ACE defects to pocket the private endowment while still getting a substantial share of the public good. That gives ACE a higher per-round payoff than cooperators.
- Harsh punishment: If cooperation drops or opponents retaliate, ACE switches to a sustained defection phase to both hurt defecting groups and make future cooperation by others costly—this deters exploitation and protects ACE’s long-term returns.
- Escalation: Repeated betrayals lengthen punishment (up to max_punish), signaling that violations will not be tolerated.
- Opportunism + controlled forgiveness: After punishing, ACE allows reconciliation but requires a probation period of visible, consistent cooperation before it resumes exploitative behavior.

Edge cases handled
- First round(s): grace cooperation to test and try to seed cooperation.
- Last round: always defect (backward-induction).
- Very short games (small r): punish lengths are capped and computed relative to remaining rounds so ACE won’t waste all remaining rounds on futile punishment; it will still defect on the last round.
- Noisy/cooperative opponents: ACE will exploit predictable cooperators often (defect when many others likely to cooperate), maximizing short-run gains while keeping a deterrence mechanism.
- Persistent defectors: ACE will default to defection and refuse to try to rebuild cooperation until a credible recovery (probation) is observed.
- Multiple opponents and asymmetric behaviors: ACE predicts each opponent individually and uses aggregate predicted cooperators, so it adapts to mixed populations (some cooperators, some defectors).

Parameter tuning guidance
- Make p_thresh higher (e.g., 0.75) to be conservative about predicting cooperation, or lower it (e.g., 0.55) to exploit riskier fields more often.
- Increase base_punish_length_fraction or max_punish to become more punitive (stronger deterrence).
- Increase probation_length to insist on longer recovery periods.
- If r is large, consider scaling punishments as an absolute fraction of r so threats are credible; for small r cap punishments relative to r_remaining.

Why ACE is robust in a tournament
- It exploits consistent cooperators without needing coordination or communication.
- It switches to clear punishment if the field retaliates or cooperation collapses, which prevents being persistently exploited.
- Prediction by per-opponent rates makes it responsive to heterogeneous opponent strategies.
- Deterministic rules plus small number of tunable parameters make it implementable and hard to "trick" via arbitrary patterns.

You can hand this to an implementer directly; all required state is the per-opponent cooperation counts and current mode/punish/probation timers.
'''

description_AGGRESSIVE_12 = '''
Strategy name: Aggressive Opportunistic Exploiter (AOE)

Intuition (short)
- Be aggressively self-interested: defect by default, exploit any rounds in which a large fraction of others cooperate, and punish anyone (collectively) who lets you be exploited. Cooperate only rarely and strategically to test or to rebuild profitable patterns when many others are cooperating. Defect firmly in the endgame.

Key strategy principles
- Default = Defect. Cooperation is offered only when there is credible evidence that many others are cooperating and there are enough future rounds to profit from preserving that pattern.
- Exploit mass cooperation: if a large share of opponents cooperated recently, defect to harvest the extra private payoff.
- Immediate, visible retaliation when you are directly exploited (you cooperated while others didn’t): switch to a multi-round boycott (punishment) to make exploitation unprofitable.
- No naive unconditional cooperation and no soft forgiveness: aggressive, short tests to re-check environment, long punishments to discourage being used.

Parameters (computed from game inputs and simple constants)
- n, k, r — given.
- W = min(5, r-1) — recent-history window length.
- coop_high = 0.60 — threshold fraction of other players cooperating (over W) that signals a cooperative crowd worth exploiting.
- coop_low = 0.30 — fraction below which cooperation is hopeless; stay defecting.
- punish_len = max(2, ceil(r/4)) — number of rounds to punish after being exploited.
- rebuild_burst = 1 — short cooperation test when attempting to rebuild.
- endgame_horizon = 2 — always defect in final endgame_horizon rounds (no future to protect).

Notes on choosing thresholds
- coop_high and coop_low are tunable constants. Aggressive behavior favors higher coop_high (so you exploit only when many others cooperate); punish_len is sizable to make punishment credible. These depend only on r,n,k and are fixed before play.

Definition: exploited in round t
- If you cooperated (c_i,t = 1) and others included at least one defector so that your cooperative payoff was strictly lower than the payoff you'd have received by defecting that round. Formally, with S_t = total cooperators that round:
  π_if_coop = k * S_t / n
  π_if_defect = 1 + k * (S_t - 1) / n
  You were exploited if π_if_defect > π_if_coop (which will hold whenever at least one other defected because k < n). Use this check to determine whether a punishment should be triggered.

Decision rules (natural language)
1. Endgame and small-r handling
   - If r ≤ 3, defect every round.
   - If the current round t is in the last endgame_horizon rounds (t > r - endgame_horizon), defect (no future to enforce cooperation).

2. First round
   - Defect. (Probe by defecting; gather baseline)

3. Every round t > 1 (and not in endgame):
   - Compute C_s for s = t-1, t-2, ..., t-W (past window of rounds). For each s compute fraction of other players who cooperated: f_s = (C_s - c_i,s) / (n - 1). Let E = average of f_s over the available rounds in the window (use fewer rounds if t-1 < W).
   - If currently inside a punish phase (see below), play D.
   - Else if E >= coop_high and there are at least 3 rounds remaining (r - t + 1 >= 3):
       - Defect (exploit the crowd).
   - Else if E <= coop_low:
       - Defect (cooperation is hopeless).
   - Else (E in (coop_low, coop_high)):
       - Conservative probe: defect by default, but occasionally (every T_probe rounds or after a punishment ends) attempt a brief rebuild: play a rebuild_burst of 1 cooperative round to test whether many others will reciprocate. If the rebuild burst is rewarded (many others cooperated that same round), adopt exploit behavior (defect while E stays ≥ coop_high); otherwise revert to defection and/or enter punishment if you were exploited during the rebuild.

4. Punishment rule (aggressive enforcement)
   - If you cooperated in a round and you were exploited (see definition above), trigger a punishment phase: defect for punish_len rounds (or until the game ends). If another exploitation occurs during punishment, reset the punishment timer to full punish_len (no mercy). After the punishment completes, perform a single rebuild_burst to probe whether cooperation can be reliably restored; otherwise remain defecting.

Pseudocode (compact)

Initialize:
  punish_timer = 0
  last_rebuild_round = -Inf
  W = min(5, r-1)
  coop_high = 0.60
  coop_low = 0.30
  punish_len = max(2, ceil(r/4))
  rebuild_burst = 1
  endgame_horizon = 2

For each round t = 1..r:
  if r <= 3: play D; continue
  if t > r - endgame_horizon: play D; continue
  if t == 1: play D; continue

  if punish_timer > 0:
    play D
    punish_timer -= 1
    continue

  compute E = average_{s = max(1,t-W)..t-1} [(C_s - my_action_s)/(n-1)]

  if E >= coop_high and (r - t + 1) >= 3:
    play D  # exploit
  elif E <= coop_low:
    play D
  else:
    # cautious zone: attempt a rebuild test occasionally
    if (t - last_rebuild_round) >= max(3, W) and (r - t + 1) >= 3:
      play C
      last_rebuild_round = t
    else:
      play D

  # After round resolution (update history):
  if you played C in round t:
    let S_t = C_t
    if (1 + k*(S_t - 1)/n) > (k*S_t/n):
      # you were exploited
      punish_timer = punish_len

Behavioral summary (how it behaves against classes of opponents)
- All-defectors: you defect; tie with others (no losses).
- All-cooperators: you defect every non-endgame round -> you systematically exploit them (maximal aggressive payoff).
- Mixed populations: you opportunistically defect when many others cooperate. If your rare cooperation is exploited, you trigger long punishments to deter further exploitation.
- Conditional cooperators who reciprocate: you will detect high average cooperation E and exploit them (defect) to take advantage. If they punish your defection en masse by reducing E, you will either try an occasional rebuild or keep defecting if cooperation disappears.
- Strategies that try to “coax” you by sporadic cooperation: you test rarely and harshly punish exploitation, making you tough to manipulate.

Why this is aggressive and robust
- Aggressive: default defection, active exploitation of cooperative majorities, and strong, credible group-level punishments when you are exploited.
- Robust: decisions rely only on recent observed cooperation rates and explicit checks for being exploited. No assumptions about opponents’ norms or communication. Parameters are simple functions of r (punish_len using r) and fixed thresholds, so the behavior adapts to short- and medium-term history and to the remaining horizon.

Edge cases explicitly handled
- Very short games (r ≤ 3): always defect (no future to enforce cooperation).
- Last rounds: defect (endgame).
- Punishment stacking: repeated exploitation refreshes punishment (no forgiving loop-hole).
- Insufficient history: window W shrinks gracefully in the first W rounds.
- Ties/indistinguishable histories: decisions are deterministic given identical histories (suitable for tournament determinism).

Implementation notes for your coder
- Keep track each round of C_t (total cooperators) and own past actions c_i,t to compute E and exploitation checks.
- punish_timer is decreased each round after playing D; if set >0 you must play D.
- last_rebuild_round ensures rebuild tests are occasional, not continuous.
- coop_high, coop_low and punish_len can be tuned; increasing coop_high makes the strategy more conservative about exploiting; increasing punish_len makes punishment more credible (more aggressive).

This strategy maximizes short-term exploitation while retaining enough enforcement to discourage repeated exploitation of your cooperations. It is explicitly aggressive and uses only game parameters and observed history.
'''

description_AGGRESSIVE_13 = '''
Strategy name: Adaptive Exploitative Bait-and-Break (AEBB)

High-level idea
- Aggressive orientation: prefer defection, exploit any sustained responsiveness in opponents, and use the smallest possible cooperative “signals” that buy sustained extra cooperation from others so you can defect and harvest the benefit. If opponents are not responsive, go to permanent defection.
- Adaptive: estimate how much others’ cooperation changes when you cooperate vs defect, and only pay the short-term cost of cooperating when the estimated future payoff uplift (over the remaining rounds) more than offsets that cost.
- Robust: starts with a short, deterministic exploration phase to collect data, uses simple statistics that depend only on game parameters and observed history, and falls back to permanent defection in unpromising environments or in the final round(s).

Notation
- n, r, k: game parameters (given).
- t: current round (1..r).
- my_action[t] ∈ {C, D}.
- others_coop_count[t] = number of other players (out of n−1) who cooperated in round t (observed at the end of each round).
- For conditioning, we track what others do in round t given what I did in round t−1 (this lets us estimate responsiveness to my action).

Key derived quantities
- cost = immediate loss from cooperating in a single round = π_D − π_C = 1 − (k/n). This is positive because k/n < 1.
- After we collect data, estimate:
  - p_after_coop = average fraction of other players cooperating in a round immediately following a round in which I cooperated.
  - p_after_def = average fraction of other players cooperating in a round immediately following a round in which I defected.
  - delta = p_after_coop − p_after_def (estimated responsiveness attributable to my cooperation).
- If delta ≤ 0, cooperating does not increase others’ cooperation and is therefore strictly dominated (so defect forever).
- If delta > 0, the expected extra contribution by others per future round from one signaling cooperation is (n−1) * delta, so my extra payoff per future round from that uplift while I defect is B_per_round = (k/n) * (n−1) * delta.
- If I can signal today and then exploit the uplift for T future rounds, the total expected benefit ≈ B_per_round * T. Signal is worth it only if B_per_round * T > cost.

Phases and parameters (concrete choices that work robustly, but implementer may tune)
- Exploration length E = min(ceil(0.2 * r), 10). (Short deterministic exploration; ensures some data without wasting many rounds.)
- Final-defect rule: always play D in the last round (t = r). You can make the last few rounds defects if desired; we take at least the last round as guaranteed D.
- Exploitation cycle length S (computed later): how many exploitation (defection) rounds to run after one cooperation signal. A natural choice S = max(1, ceil(cost / B_per_round)). Intuition: cooperate once, then exploit for roughly cost/B_per_round rounds to recoup the signaling cost.

Decision rules (deterministic; uses only parameters and history)

1) First round and exploration phase (t = 1..E)
- Pattern: deterministic alternation to get both contexts quickly:
  - If t is odd: play C.
  - If t is even: play D.
- This produces data for p_after_coop and p_after_def from round 2 onward.

2) After exploration (t > E and t < r)
- Compute required statistics using history:
  - For each round u ≥ 2 where my_action[u−1] is known, record others_coop_count[u].
  - Compute p_after_coop = (sum of others_coop_count[u] over u where my_action[u−1] = C) / ((n−1) * number_of_these_u).
  - Compute p_after_def similarly for u where my_action[u−1] = D.
  - If either conditioning cell has fewer than 2 observations, treat delta as unreliable; conservatively set delta = max(0, p_after_coop − p_after_def − 0.02) (small bias against assuming positive responsiveness). This avoids overfitting from tiny samples.
- delta = p_after_coop − p_after_def (possibly adjusted conservatively as above).
- If delta <= 0 (or estimated B_per_round is effectively 0): set mode = PERMANENT_DEFECT; play D for all remaining rounds (including current t and t = r).
- Else (delta > 0):
  - Set B_per_round = (k/n) * (n−1) * delta.
  - Let T_rem = r − t (full future rounds after the current decision — conservative: do not count current round as future).
  - If B_per_round * T_rem <= cost (i.e., even if this cooperation triggered permanent uplift, total benefit does not cover cost), set mode = PERMANENT_DEFECT and play D.
  - Otherwise enter EXPLOITATION mode with these parameters:
    - Compute S = max(1, ceil(cost / B_per_round)). This is the number of exploitation (D) rounds to run after each single C signal.
    - Exploitation schedule: in EXPLOITATION mode we run cycles: one C (signal) followed by S rounds of D (exploit), repeating until the endgame (always defect in last round).
    - Implementation detail: if we are in the middle of a cycle when we reach near the end (not enough remaining rounds to complete the planned exploitation), just defect every remaining round (do not waste another signal unless it still meets the benefit inequality using the smaller remaining T_rem).

3) Monitoring & adaptation during exploitation
- After each cycle, recompute p_after_coop and p_after_def (update delta). If delta falls (observed responsiveness weaker than expected) so that B_per_round shrinks and B_per_round * remaining_T_rem <= cost, immediately switch to PERMANENT_DEFECT.
- If responsiveness strengthens, recompute S (it may shrink, meaning fewer D rounds between signals).
- If after a signal others’ cooperation does not rise above p_after_def + 0.01 within one round, treat that signal as ineffective; after 2 consecutive ineffective signals, switch to PERMANENT_DEFECT.

4) Last round
- Always play D at t = r.

Pseudocode

Initialize:
  E = min(ceil(0.2*r), 10)
  For all rounds t, my_action[t] undefined initially
  mode = EXPLORATION
  cycle_pos = 0  // used in EXPLOITATION to count where we are in signal->exploit cycle

For t = 1..r:
  if t == r:
    play D; my_action[t] = D; observe others_coop_count[t]; continue
  if mode == EXPLORATION and t <= E:
    if t is odd: play C else play D
    my_action[t] = action; observe others_coop_count[t]; continue
  // After exploration or if exploration ended
  // compute stats from history:
  Compute p_after_coop and p_after_def from all u >= 2 where my_action[u-1] defined.
  If insufficient observations (less than 2 in a cell), apply conservative bias to delta as described.
  delta = p_after_coop − p_after_def
  cost = 1 − (k/n)
  If delta <= 0:
    mode = PERMANENT_DEFECT
    play D, my_action[t] = D; observe others_coop_count[t]; continue
  B_per_round = (k/n) * (n-1) * delta
  T_rem = r − t
  If B_per_round * T_rem <= cost:
    mode = PERMANENT_DEFECT; play D; my_action[t] = D; observe others_coop_count[t]; continue
  // Enter or continue exploitation:
  mode = EXPLOITATION
  S = max(1, ceil(cost / B_per_round))
  // cycle_pos = 0 means next move is a signal (C)
  If cycle_pos == 0:
    // Signal round
    play C; my_action[t] = C; observe others_coop_count[t]
    cycle_pos = 1
    last_signal_effective = (others_coop_count[t+1] >= (p_after_def*(n-1) + 1)) // evaluated next round
  Else:
    // Exploit rounds (D)
    play D; my_action[t] = D; observe others_coop_count[t]
    cycle_pos += 1
    if cycle_pos > S:
      cycle_pos = 0
  // After each completed cycle (signal + S exploit rounds) recompute p_after_coop/p_after_def and possibly recompute S
  // Monitor for ineffective signals; if consecutive ineffective_signals >= 2 -> switch to PERMANENT_DEFECT

Why this is aggressive
- The strategy defects by default and only pays the brief, targeted cost of cooperation when data-driven evidence indicates cooperating will produce sustainable extra cooperation from others that can be exploited over many rounds.
- When exploitation is not profitable (or responsiveness is absent), it immediately settles into permanent defection.
- It deliberately uses minimal cooperative signaling (single-round cooperations) and a schedule that recoups cost quickly (S chosen by cost/B_per_round), maximizing exploitation while minimizing generosity.
- It does not try to be “nice” for its own sake: it deliberately manipulates opponent responsiveness to maximize personal payoff and withdraws cooperation the moment the manipulation stops paying off.

Edge cases and extra notes
- Very short games (r small): exploration length E is bounded and conservative; for r ≤ 3 the exploration phase is tiny and the algorithm will usually choose permanent defection because insufficient future rounds exist to recoup signaling cost.
- If k is very close to n and cost is small, the strategy may cooperate more because cost is small and signals are cheap; the decision rule picks this up numerically.
- If opponents include “grim” punishers who permanently punish you for any defection, exploitation will quickly fail (observed delta will fall), and the strategy will detect that and switch to permanent defection rather than persist in losing signals.
- The exploration alternation is deterministic (not random) so other strategies cannot exploit our randomness; implementers may substitute a randomized exploration (50/50) if desired.
- The thresholds (E, small bias of 0.02, requirement of at least 2 observations) are conservative defaults. They are chosen to avoid chasing noise. An implementation may tune these for specific tournament structure.

Conclusion
This strategy is adaptive, parameterized only by (n, r, k) and observed history, and aggressively prioritizes defection and exploitation. It performs a small, inexpensive exploration to detect exploitable conditional cooperators, uses brief cooperative signals only when mathematically justified, harvests the uplift with scheduled defections, and reverts to permanent defection as soon as exploitation ceases to be profitable.
'''

description_AGGRESSIVE_14 = '''
Name: Aggressive Trigger with Escalating Punishment (ATEP)

High-level idea
- Start by cooperating to build a reputation and encourage cooperative behavior.
- Use a zero-tolerance trigger: any defection by any opponent triggers a strong, computed punishment period during which you always defect.
- Punishment length scales with the “temptation” in the one-shot game (n and k) and with how many players defected, and it escalates if defections continue.
- Resume cooperation only after a clear signal of full-group compliance (consecutive rounds of everyone cooperating).
- In the final portion of the game where credible punishment cannot be imposed, switch to unconditional defection (endgame exploitation).
This is aggressive: it punishes any defection quickly and for a long time, escalates punishments for repeated or multi-player defections, and exploits the endgame.

Parameters derived from game (all computable from n, k, r)
- BasePunishLength P0 = ceil( gamma * (n - k) / k )
  - Intuition: immediate one-shot gain from defecting is 1 - k/n. A rough per-round cooperative loss inflicted by our defection is about k/n. So (n-k)/k rounds is the horizon needed to make the punishment cost roughly offset the one-shot gain. We multiply by gamma > 1 to be aggressive. Use gamma = 2 (fixed constant) unless you want milder aggression. Guarantee P0 >= 1.
- EndgameThreshold E = P0
  - If remaining rounds <= E, credible punishments are no longer effective → defect unconditionally.
- FullCoopRestore G = 2
  - After punishment finishes, require G consecutive rounds where every player contributed before returning to cooperative mode. (G can be 1 or 2; 2 is slightly more cautious/aggressive.)
- Tolerance D_tol = 1
  - Any defection (≥ D_tol) triggers punishment.

Internal state you maintain (all derived from observed history)
- mode ∈ {COOP, PUNISH}
- punish_countdown (integer ≥ 0)
- consecutive_full_coop (integer ≥ 0) — counts rounds of all players contributing since last punishment ended

Initial setting
- mode = COOP
- punish_countdown = 0
- consecutive_full_coop = 0

Decision rules (natural language)
1. Endgame override
   - Let remaining = r - (current_round - 1).
   - If remaining <= E: play D (defect). (This includes the last round; last round always D.)
2. If mode == PUNISH:
   - Play D.
   - Decrement punish_countdown by 1 (after the round).
   - If, during a punishment round, you observe any defections by others, extend punishment:
     - For each round in which at least one opponent defects, add extension = P0 × num_defectors_last_round to punish_countdown (cap punish_countdown at remaining to avoid exceeding horizon).
   - When punish_countdown reaches 0, set mode = COOP and reset consecutive_full_coop = 0. (You only leave PUNISH to COOP when punish_countdown == 0.)
3. If mode == COOP:
   - If any opponent defected in the previous round (i.e., total_cooperators_last_round < n), then:
     - Enter PUNISH:
       - num_defectors = n - total_cooperators_last_round
       - punish_countdown = min(remaining, P0 × max(1, num_defectors))
       - mode = PUNISH
     - Play D this round (because punishment starts immediately).
   - Else (everyone cooperated last round):
     - Play C.
     - Increment consecutive_full_coop by 1.
     - If consecutive_full_coop >= G, remain in COOP (no change).
     - If any round has less than full cooperation, consecutive_full_coop resets to 0 (the reset happens whenever we observe a round that is not full cooperation).

Summary of escalation and multipliers
- Single defection → punish for P0 rounds (or fewer if near end).
- Multiple simultaneous defectors → punish for P0 × num_defectors rounds. This makes multi-player defections punished proportionally harder.
- If defections occur during an ongoing punishment, punish_countdown is extended (aggressive escalation).
- After punishment ends, restoration to cooperation only after G consecutive full-cooperation rounds (safety check).

Pseudocode (concise)
- At start: mode = COOP; punish_countdown = 0; consecutive_full_coop = 0
- Each round t:
  1. remaining = r - (t - 1)
  2. If remaining <= E: action = D; continue
  3. Observe history up to t-1: coop_last = number of cooperators in round t-1 (if t=1, treat coop_last = n to signal cooperation on round 1)
  4. If mode == PUNISH:
       action = D
       If coop_last < n:
         num_def = n - coop_last
         punish_countdown += min(remaining, P0 * num_def)
       punish_countdown -= 1
       If punish_countdown <= 0:
         mode = COOP; punish_countdown = 0; consecutive_full_coop = 0
  5. Else (mode == COOP):
       If coop_last < n:
         num_def = n - coop_last
         punish_countdown = min(remaining, P0 * max(1, num_def))
         mode = PUNISH
         action = D
       Else:
         action = C
         consecutive_full_coop += 1
         If consecutive_full_coop >= G: stay in COOP

Special-case (first round)
- Treat coop_last = n so you play C in round 1 (you start cooperative to gain leverage).

Special-case (last rounds)
- As above: if remaining <= E you always defect. Last round always defect.

Why this is adaptive and robust
- Adaptive: the punishment length scales with n and k (via P0), and with how many players defect. The strategy reacts to any defection immediately and escalates if defections continue.
- Robust: it does not rely on coordination or shared norms. It uses only observed actions. It is defensive (punishes any defection), so it avoids being persistently exploited by unconditional cooperators. It also offers a clear path back to cooperation if all players resume full cooperation.
- Aggressive: zero tolerance, long punishments (gamma > 1), escalation for multiple/continued defections, immediate retaliation, and unconditional defection in the endgame.

Parameter choices and notes (recommended defaults)
- gamma = 2 (aggressive; increase for harder punishment, reduce to 1 for milder)
- P0 = max(1, ceil(gamma * (n - k) / k))
- E = P0
- G = 2
- D_tol = 1

Design trade-offs (so implementer can tune)
- Larger gamma → longer punishments, stronger deterrence, but greater mutual damage if many players retaliate (aggressive).
- Requiring G = 2 (two full-cooperation rounds) reduces false restorations if one errant round occurs; G = 1 is more forgiving.
- Multiplying punishment by num_defectors penalizes coordinated attacks but can prolong mutual defection in noisy settings; if noise is present in tournament, reduce multiplier or add forgiveness windows.

Example behavior (intuitive)
- All cooperate initially: you continue cooperating and earn the high public-good payoff.
- One player defects one round: you immediately defect for P0 rounds, denying defectors the short-term gain of free-riding and signaling that defection triggers a long cost.
- Multiple players defect simultaneously: you impose a longer punishment (P0 × num_defectors), escalating cost to the defecting coalition.
- As r approaches its end and remaining rounds ≤ E: you defect to exploit the endgame.

This strategy is simple to implement (state: mode + countdown + recent full-coop counter), explicitly depends only on parameters n, k, r and observed history, and embodies a clearly aggressive mindset: immediate, scaled, and escalating punishment plus endgame exploitation.
'''

description_AGGRESSIVE_15 = '''
Name: Adaptive Exploit & Punish (AEP) — an aggressive, probe-driven exploiter with harsh, fast reversion to defection

Intuition (one line)
- Default: defect. Occasionally probe with a single cooperation to detect conditional cooperators. If the probe reveals others who reciprocate, exploit them (defect) thereafter. Always defect in the last round. Use short back-off before re-probing so probes are informative but sparse. This maximizes exploitation opportunities and punishes cooperative opponents who are naive enough to be exploited.

Parameters (computed from game inputs; fixed rules)
- Inputs known: n (players), r (rounds), k (factor).
- window W = min(5, max(1, floor(r/10))) — short history window for rate estimates.
- probe_interval S = max(3, ceil(r/8)) — spacing between probes.
- response_threshold α = 0.65 — fraction of other players cooperating after a probe considered “reciprocation.”
- max_probe_attempts M = max(1, ceil(r / S)) — number of probes attempted in game.
- last_round_defect: always defect in round r.
- re-probe_backoff B: when a probe fails, wait B = S rounds before next probe (initially S).

High-level strategy
- Default action every round: D (defect).
- Occasionally perform a single-round cooperation “probe” to test whether others are conditional cooperators.
- If a probe is followed (in the next round) by at least α fraction of the other players cooperating, mark the population as “exploitable” and switch to permanent exploitation mode (always defect for the remainder of the game, except still defect in final round).
- If a probe fails, back off for B rounds and try again later (up to M probes).
- Never cooperate in the final round. If r is very small (r ≤ 2), never probe — always defect.

Why this is aggressive
- The strategy seeks to exploit any group-level reciprocation by defecting when many others cooperate, which yields the highest immediate payoff for you.
- It punishes cooperative communities by withdrawing any cooperative investment and only returns to probing after a long backoff, maximizing damage to naive cooperators while minimizing your sucker-payoffs.
- It is resilient: default defection prevents long-run exploitation by punishers; probes allow opportunistic gains when possible.

Detailed decision rules (step-by-step)

State variables maintained across rounds
- probes_done: number of probe rounds already used.
- last_probe_round: round index of the last probe (or 0 if none).
- exploit_mode: boolean (false initially). If true, we have detected reciprocators and will defect always thereafter.
- next_probe_round: scheduled round for the next probe (initialized to 2 if r>2 and S allows).
- history: full observed history of each round’s actions (all players) and round index t.

Initial setup
- If r ≤ 2: never probe, always play D every round (including first and last).
- Else:
  - probes_done = 0
  - last_probe_round = 0
  - exploit_mode = false
  - next_probe_round = 2 (start probing as early as round 2), but do not schedule a probe on round r (final round).

Round-by-round rule (for current round t = 1..r)
1. If t == r (last round): Play D. (End.)
2. If exploit_mode == true: Play D. (End.)
3. If r ≤ 2: Play D. (End.)
4. If t == next_probe_round and probes_done < M and t < r:
   - Play C (this is a probe).
   - Record last_probe_round = t; probes_done += 1.
   - After this round we will observe opponents' response in t+1 and update state then.
   - Schedule next_probe_round += S + random_jitter where random_jitter is 0 (deterministic) — but ensure next_probe_round < r.
   - End.
5. Otherwise (normal default): Play D.

Post-round state update (performed after observing round t results, before deciding round t+1)
- If last action you took at round p (where p = last_probe_round) was a probe (you played C at p):
  - Look at round p+1 (if p+1 ≤ r and you have observed it). Compute coop_count_others = number of other players (excluding you) who played C in round p+1.
  - Compute fraction f = coop_count_others / (n - 1).
  - If f ≥ α:
     - Set exploit_mode = true. (You detected sufficient reciprocation; you will exploit by defecting from now on.)
  - Else:
     - Probe failed. Set next_probe_round = p + B (i.e., wait B rounds before next probe), where B = S (the backoff).
     - Continue default D until next_probe_round (unless exploit_mode triggered later).
- If multiple probes happen in short succession (rare by schedule), evaluate each independently; only one successful probe turns on exploit_mode.

Edge cases and clarifications
- First round: t = 1 → not a probe (unless r is extremely small and next_probe_round set to 1, which we avoid). By default the strategy defects in round 1.
- Very short games: if r ≤ 2, we never probe and always defect (no chance to build cooperation; aggressive choice).
- Last round: always defect (standard backward induction and aggressive protection against sucker payoff).
- Simultaneity: a probe at round p can only be evaluated using round p+1 actions (opponents cannot react within p). The strategy uses that natural lag to test conditional cooperators.
- Determinism: the schedule is deterministic (no randomness required). If implementer prefers some unpredictability to avoid being exploited by meta-strategies that detect deterministic probes, add a small random jitter to next_probe_round; keep jitter small relative to S.

Why this is robust and adaptive
- Protects against naive exploitation: default defect prevents repeated being a sucker.
- Detects and exploits conditional cooperators: a single, low-cost probe reveals whether a substantial fraction of opponents will reciprocate; exploit_mode then extracts surplus.
- Low probe frequency keeps total sucker-cost low while still allowing opportunistic captures.
- Backoff and repeated probing allow re-checking the environment in case opponents’ behaviors change.
- Since the decision only relies on parameters (n, r, k) and observed history, it is implementable in a tournament where no communication or coordination is permitted.

Pseudocode (compact)

Initialize:
  if r <= 2:
    always D all rounds
  else:
    W = min(5, max(1, floor(r/10)))
    S = max(3, ceil(r/8))
    α = 0.65
    M = max(1, ceil(r / S))
    B = S
    probes_done = 0
    last_probe_round = 0
    exploit_mode = false
    next_probe_round = 2

For each round t = 1..r:
  if t == r:
    play D
    observe results; break
  if exploit_mode:
    play D; observe results; continue
  if t == next_probe_round and probes_done < M and t < r:
    play C (probe)
    last_probe_round = t
    probes_done += 1
    next_probe_round = min(r-1, next_probe_round + S)  # ensure no probe on last round
    observe results
    # update on observing round t+1 (below)
    continue
  else:
    play D
    observe results
    continue

After observing round results at time t (if last_probe_round + 1 == t):
  coop_count_others = number of other players who played C at round t
  f = coop_count_others / (n - 1)
  if f >= α:
    exploit_mode = true
  else:
    next_probe_round = min(r-1, last_probe_round + B)

Notes for implementer
- The thresholds α, S, and B are tunable. The given defaults bias toward safety (few probes) and strong exploitation once reciprocators are detected.
- If you want to be slightly less aggressive (to capture mutualists who will cooperate with a cooperating focal player), reduce α and increase probe frequency. For the required “aggressive” mindset keep α high and probes sparse.

Summary (behavioral description)
- Start defecting. Occasionally and sparingly cooperate for 1 round to test whether many opponents reciprocate. If they do, exploit them by defecting thereafter (including the immediate next round). If they don’t, retreat to defection for a long backoff and try another probe later. Never cooperate in the final round.

This strategy is aggressive (seeks to exploit cooperative opponents and punishes cooperation), adaptive (probes and updates based on observed reciprocation), and robust (defaults to defection to avoid being exploited by harsh punishers or sophisticated strategies).
'''

description_AGGRESSIVE_16 = '''
Name: Aggressive Grim-Exploiter (AGE)

Intent and high-level summary
- Aggressive: seeks to maximize own payoff by (a) trying to establish high-cooperation environments when profitable, (b) opportunistically exploiting reliably cooperative opponents, and (c) punishing any opponent that defects while you cooperated by permanently abandoning cooperation (grim trigger). The punishment is harsh and credible: once “exploited” we defect for all remaining rounds.
- Adaptive: uses observed history (per-round actions of all players) to detect sustained group cooperation, individual reliability, and exploitation events, and switches behavior accordingly.
- Robust: defends against mass defection by refusing to cooperate when group cooperation collapses, and never cooperates in the last round (no future to sustain it).

Key design choices (ready to be implemented)
- Start with a cooperative signal to try to establish mutual cooperation.
- If ever exploited (you cooperated in a round where at least one other player defected), switch to permanent defection for the rest of the game (grim trigger).
- While not yet exploited, condition cooperation on recent group cooperation: cooperate when the group is sufficiently cooperative; otherwise defect.
- When the group has shown sustained full (or near-full) cooperation for a short streak, take a one-shot opportunistic defection to exploit presumed unconditional cooperators; revert to cooperative baseline immediately after unless that defection results in you being exploited (which will flip to grim).
- Always defect in the final round (no incentive to cooperate).

Notation
- n, r, k: game parameters (given).
- t: current round (1-based).
- history[t'] gives the vector of n actions in round t' (C or D).
- my_action[t'] is your action in round t'.
- coop_count(t') = number of players who played C in round t'.
- Others_coop_count(t') = coop_count(t') - (my_action[t'] == C ? 1 : 0).
- S_coop: streak length to consider cooperation "sustained" (recommended default below).
- state.exploited: boolean flag that turns true if at any past round you cooperated while at least one other player defected.

Recommended parameter choices (tunable)
- S_coop = min(3, max(1, floor(r/10))) — short streak of full cooperation required to attempt exploitation; defaults to 1–3 depending on game length.
- Window W for “recent” group cooperation = min(5, r-1).
- These are conservative defaults; implementer may tune them.

Decision rules (natural language)
1. First-round behavior:
   - Play C in round 1 (signal cooperation).
2. Endgame:
   - In the last round (t = r) always play D.
3. Grim punishment:
   - Maintain state.exploited = true if at any past round t' you played C and coop_count(t') < n (i.e., at least one other player defected in that same round).
   - If state.exploited is true at the start of a round (unless it is the last-round exception above), play D for every remaining round (permanent defection).
4. Baseline cooperation/defection while not exploited:
   - If the most recent round had low group cooperation (coop_count(last_round) ≤ floor(n/2)), defect (D).
     Rationale: do not be the sucker when participation collapsed.
   - Else, if there has been a sustained streak (S_coop consecutive past rounds) of full cooperation (coop_count = n in each of those rounds), then perform a single opportunistic exploitation:
       - In the next round play D (one-shot exploit).
       - If this exploit causes state.exploited to become true (i.e., you played C earlier and someone defected then, or opponents respond by defecting when you cooperated earlier), the grim rule will activate next round and you will defect forever.
       - If it does not cause exploitation (others continue to cooperate), resume baseline cooperation.
   - Otherwise (not exploited, recent cooperation not collapsed, no exploitation scheduled), cooperate (C).
5. Special-case: if an opponent defected while you were cooperating in the immediately previous round, set state.exploited (grim) immediately and defect thereafter.

Pseudocode (clear, implementable)
(Assume rounds 1..r; execute decision for round t using history up to t-1.)

Initialize:
  state.exploited = false
  last_exploit_round = None
  S_coop = min(3, max(1, floor(r/10)))
  W = min(5, r-1)

For each round t from 1 to r:
  if t == 1:
    play C
    record my_action[1] = C
    continue to next round

  if t == r:
    play D
    record my_action[t] = D
    continue

  # Update state.exploited from past rounds:
  for each past round s in 1..(t-1):
    if my_action[s] == C and coop_count(s) < n:
      state.exploited = true
      break

  if state.exploited:
    play D
    record my_action[t] = D
    continue

  # If immediate previous round showed collapse, refuse to cooperate:
  if coop_count(t-1) <= floor(n/2):
    play D
    record my_action[t] = D
    continue

  # Check for sustained full cooperation streak:
  full_streak = true
  for s in (t-1) down to max(1, t-S_coop):
    if coop_count(s) < n:
      full_streak = false
      break

  if full_streak and (last_exploit_round is None or last_exploit_round < t - S_coop):
    # Schedule and execute a one-shot exploitation now
    play D
    last_exploit_round = t
    record my_action[t] = D
    # Note: if others then defect while you had cooperated earlier, the exploited flag will be set next updates
    continue

  # Default: cooperate
  play C
  record my_action[t] = C

Why this is aggressive and why it is robust
- Aggressive: the strategy punishes any exploitation (you cooperated while others defected) with permanent defection. Permanent defection is the harshest credible punishment and deters other players from opportunistic one-shot defections against you. The one-shot opportunistic defections against fully cooperative groups make it exploitative: when you detect a vulnerable pool of unconditional cooperators, you take a guaranteed, immediate payoff advantage.
- Credibility & simplicity: Grim punishment is simple to implement and does not require coordination with others to be effective — it unambiguously reduces your future cooperation and thus lowers the payoff available to others who rely on your future cooperation.
- Adaptivity: the strategy refuses to cooperate when cooperation levels are low, preventing exploitation by mass defectors. It rewards and sustains mutual cooperation where possible, but only so long as you are not being exploited.
- Tournament suitability: against unconditional cooperators you will exploit periodic gains; against conditional/cooperative strategies you can reach sustained mutual cooperation; against defectors you quickly switch to defection and avoid repeated losses; against retaliatory tasters (tit-for-tat-like) the grim threat is still effective because it forces them to face permanent defection if they ever defect against you.

Edge cases and notes for implementers
- First-round C is a choice to attempt to establish cooperation. If you wish to be even more aggressive, you may start with D; AGE is designed to be opportunistic but not needlessly exploitative from the outset.
- Last-round defection is essential (no future to enforce cooperation).
- The exploiter’s one-shot defection is timed only after observed sustained cooperation. If r is very small, reduce S_coop to 1 so you can still attempt exploitation at all.
- Parameter tuning (S_coop, the floor(n/2) collapse threshold, W) can be adjusted for different tournament meta-games (long vs short r, prevalence of reciprocal punishers, noise). The provided defaults are conservative and safe.
- No reliance on communication or coordination: all decisions are fully determined by n, r, k and observed action history.

Summary (concise)
- Cooperate on round 1. If ever you cooperated while someone else defected in the same round, flip to permanent defection (grim). Otherwise, cooperate whenever recent group cooperation is acceptable; if you observe S_coop consecutive full-cooperation rounds, take a one-shot defection to exploit the cooperative pool, then return to baseline. Always defect in the final round. This strategy is aggressive (harsh punishment, opportunistic exploitation) while remaining history-dependent and adaptive.
'''

description_AGGRESSIVE_17 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

High-level intuition (aggressive mindset)
- Default: always defect. Defection is the single-round dominant action; my aim is to maximize my own payoff by free-riding on any cooperation I can induce in others.
- But players can collectively punish persistent defectors. I therefore behave opportunistically: I only contribute when it is strictly necessary to (a) restore or maintain others’ cooperation so I can continue to profit, or (b) probe whether a temporary concession will re-open a stream of exploitable cooperation.
- I exploit unconditional cooperators relentlessly (never help them), and I retaliate/punish groups that try to deter my defection until they show cooperation again on my terms.
- Everything depends only on game parameters and the observed history of contributions.

Parameters (implementer can tune; sensible defaults given)
- w: lookback window for recent behavior. Default: w = min(5, r-1).
- T_high: “high cooperation” threshold for the group. Default: 0.60.
- T_low: “low cooperation” threshold. Default: 0.20.
- delta_cond: threshold for deciding others condition their cooperation on my action. Default: 0.12.
- L_restore: length of temporary cooperation to restore group cooperation when others punish. Default: min(3, max(1, floor(r/10))).
- p_probe: small probability to probe or seed cooperation when beneficial. Default: 0.10.
- forgive_fraction: fraction of the lookback window used to forgive and stop punishing. Default: 0.5.

Definitions computed from history up to (but not including) current round t:
- For each round s < t, c_j(s) ∈ {0,1} are observed contributions.
- my_history: my past contributions c_me(s).
- others_total_in_round(s) = Σ_{j ≠ me} c_j(s).
- recent rounds: the last w rounds (or all past rounds if fewer than w).
- group_coop_rate_recent = (Σ_{s in recent} Σ_{j ≠ me} c_j(s)) / (w × (n-1)).
- coop_rate_others_when_I_cooperated = average fraction of other players cooperating in rounds (within recent window) where I cooperated.
- coop_rate_others_when_I_defected = same but for rounds where I defected.
- conditionality_index CI = coop_rate_others_when_I_cooperated − coop_rate_others_when_I_defected.

Decision rules (precise operational rules)
1. Last-round rule
   - If t = r (last round): Defect. (Aggressive backward induction; no future to restore cooperation.)

2. First-round rule
   - If t = 1: Defect. (Start aggressive and gather information.)

3. Default baseline
   - Default action = Defect.

4. Compute signals from history (using recent window w):
   - If there are no previous rounds, fall back to the first-round rule.
   - Compute group_coop_rate_recent and CI as defined above.

5. Exploit unconditional cooperation
   - If group_coop_rate_recent ≥ T_high:
       - Many others are cooperating often. Exploit them: Defect (no contribution).
         Rationale: when many others cooperate I get the public good while keeping my private unit.

6. Ignore hopeless groups
   - If group_coop_rate_recent ≤ T_low:
       - Others are effectively defecting. Defect (no contribution).
         Rationale: no reason to pay cost when the public good is absent.

7. Detect punishment or conditional cooperators
   - If CI ≥ delta_cond (others cooperate significantly more when I cooperate than when I defect):
       - They are conditional/cooperative and will reduce cooperation if I keep defecting (i.e., they are “responsive”). I will therefore perform minimal, temporary cooperation to restore/exploit their cooperation stream:
         - Enter a “restore” phase: Contribute (C) for L_restore consecutive rounds OR until group_coop_rate_recent (recomputed each round) returns above T_high — whichever comes first.
         - After restore phase ends, return to baseline (Defect), and re-evaluate.
       - Rationale: I only pay contributions when they actually secure a lasting higher stream of others’ cooperation that I can exploit.

8. Probe and seed
   - If T_low < group_coop_rate_recent < T_high (moderate cooperation) and CI < delta_cond:
       - The group is mixed and not clearly conditional on me. Use a small-probability probe to try to seed cooperative equilibria:
         - With probability p_probe: contribute (C).
         - Otherwise: defect (D).
       - If a probe is followed by sustained increases in group cooperation (group_coop_rate_recent rises above T_high), switch to exploitation rule 5.

9. Reacting to active punishment (fast retreat)
   - If, immediately after I defected in prior round(s), group_coop_rate_recent drops substantially (e.g., by > 2×delta_cond relative to its level before my defection):
       - Conclude the group is actively punishing defectors. In that case:
         - Temporarily cooperate for L_restore (to end the punishment), but no more than necessary.
         - After cooperating L_restore rounds, resume Defecting and monitor if cooperation recovers; if the group again punishes, re-enter this temporary cooperation stage. (This is “pay-to-stop-punish” rather than being submissive — minimal concession to restore exploitable streams.)

10. A final safety net (anti-targeting)
   - If a substantial fraction of players (≥ half) have persistent zero cooperation irrespective of my actions (they are unconditional defectors), treat them as deadweight: continue to defect and ignore them. Focus restoration only on players who exhibit conditional cooperation.

Pseudocode (structured)

Initialize parameters: w, T_high, T_low, delta_cond, L_restore, p_probe, forgive_fraction.

For each round t = 1..r:
  if t == r:
    action = D
    play action; continue

  if t == 1:
    action = D
    play; continue

  compute recent rounds set R = last min(w, t-1) rounds
  compute group_coop_rate_recent = sum_{s in R} sum_{j≠me} c_j(s) / (|R|*(n-1))
  compute coop_rate_others_when_I_cooperated and coop_rate_others_when_I_defected (use only s in R where my c_me(s) = 1 or 0 respectively; if denominator 0 treat rate as group_coop_rate_recent)
  CI = coop_rate_others_when_I_cooperated − coop_rate_others_when_I_defected

  // Last-resort: if most others always defect
  if group_coop_rate_recent ≤ T_low:
    action = D
    play; continue

  // Exploit abundant cooperation
  if group_coop_rate_recent ≥ T_high:
    action = D
    play; continue

  // If others condition on my moves (they reduce cooperation when I defect), restore minimally
  if CI ≥ delta_cond:
    if currently in restore_phase (remaining_restore_rounds > 0):
      action = C
      remaining_restore_rounds -= 1
      play; continue
    else:
      // start restore if my recent defection likely to have triggered cooperation drop
      If coop_rate_others_when_I_defected < coop_rate_others_when_I_cooperated:
        remaining_restore_rounds = L_restore
        action = C
        remaining_restore_rounds -= 1
        play; continue

  // Probing in mixed groups
  if T_low < group_coop_rate_recent < T_high:
    with probability p_probe:
      action = C
    else:
      action = D
    play; continue

  // Default
  action = D
  play

Notes on implementation details and robustness
- Use small w for responsiveness but not too small to avoid noise; defaults above are robust across many r.
- Use statistical smoothing when denominators are small: if no rounds exist where I cooperated/defected in the window, fall back to group_coop_rate_recent.
- L_restore should be as short as possible — aggressive players minimize concessions.
- The algorithm is deterministic except for the small p_probe randomness; p_probe prevents being trapped in cycles and helps discover exploitable cooperators.

Why this is aggressive and robust
- Aggressive: default is to defect every round, defect in the last round, exploit when others cooperate, and only pay the smallest necessary cost to restore cooperation when others punish. It never naively reciprocates beyond what is instrumentally useful.
- Robust: it uses short-window statistics to detect (a) unconditional cooperators (exploit), (b) conditional cooperators/punishers (pay minimally to end punishment), (c) mixed groups (probe for exploitable cooperation). It reacts to observed behavior rather than assuming norms or communication.
- Adaptive: it adapts to opponents who are unconditional cooperators, unconditional defectors, conditional cooperators, and punishers, changing only based on observed response patterns.

Edge cases summary
- First round: Defect (gather data).
- Last round: Defect (no future to incentivize).
- If nobody has yet cooperated in the lookback window: treat group as defectors → Defect.
- If history is short (less rounds than w): use all available past rounds.
- If opponents appear to punish you (CI large): cooperate for L_restore rounds then resume defecting.
- If opponents are unconditionally cooperative (very high group_coop_rate): always defect and exploit.
- If the group is mixed: randomly probe with small probability p_probe to try to open exploitation opportunities.

Typical tuning advice
- If tournaments contain many harsh punishers, increase L_restore slightly and decrease p_probe to reduce costly cycling.
- If tournaments contain many naive cooperators, decrease p_probe (you want to exploit rather than create cooperation) and keep T_high moderate to detect abundant cooperation early.
- If you want even more aggression and less concession, set L_restore = 1 and p_probe very small (0.01).

This strategy can be implemented purely from the game parameters (n, r, k) and past actions (history) and is designed to be aggressive, adaptive and robust versus a wide variety of opponent behaviours.
'''

description_AGGRESSIVE_18 = '''
Overview (strategy name: Adaptive Exploitive Defector — AED)

High-level idea
- Default to defect (D) every round unless we have evidence we can reliably induce/receive sustained cooperation from enough opponents to make a short, profitable exploitation plan worthwhile.
- Occasionally "probe" by cooperating one round to test whether opponents are responsive (i.e., will raise their cooperation after a friendly signal). If a probe reveals a group of responsive cooperators, switch into a short, aggressive exploitation burst: keep defecting while they continue to cooperate, collecting the one-shot advantage. If they punish (cooperation falls), stop exploiting and return to default defection.
- Always defect in the final rounds (endgame defect) so you do not give away value you cannot recoup.

This is explicitly aggressive: it (1) exploits cooperators when they exist, (2) defects by default to avoid being a sucker, and (3) actively tests and then punishes/withdraws when exploitation becomes unprofitable.

Parameters (calculated from game parameters and adjustable constants)
- n, r, k: game parameters (given).
- w = min(5, max(1, floor(r/10))) — history window used for local baselines.
- probe_interval = max(1, floor(r/10)) — do a probe roughly 1 per 10 rounds (tunable).
- p_threshold_resp = 0.60 — fraction of others cooperating considered "high cooperation".
- delta_resp = 0.20 — required increase in cooperation after a probe to label opponents as responsive.
- exploit_burst = max(1, floor(r/6)) — number of consecutive defect rounds to exploit once responsiveness is detected.
- endgame_length = 1 (or small integer like 2) — always defect in last endgame_length rounds.
- punish_tolerance = 0.25 — if others' cooperation falls by this fraction while exploiting, abort exploitation.
- protect_min_rounds = 1 — minimal rounds between probe and exploitation to observe response.

Notes: these constants can be tuned for tournament performance. The decision logic uses only history and the given parameters.

State tracked
- history of contributions each round: for each round t, total_cooperators_t (including self) and per-player contributions if available (spec says players observe all others' actions).
- last_probe_round, baseline_coop_rate (others) at last probe
- exploit_mode flag and exploit_rounds_remaining
- responsive flag (true if recent probes showed positive response)

Decision rules (natural language + pseudocode)

Pseudocode variables:
- t = current round (1..r)
- others_coop_count(t) = total cooperators in round t excluding self
- others_coop_rate(window) = average of others_coop_count / (n-1) over last window rounds
- self_action(t) ∈ {C, D}

Main decision function for round t:

1. If t > r - endgame_length:
     action <- D
     (Rationale: last round(s) give no future benefit; be aggressively selfish.)

2. If exploit_mode and exploit_rounds_remaining > 0:
     action <- D
     exploit_rounds_remaining -= 1
     After observing this round's others' cooperation, check:
        if current_others_coop_rate < baseline_coop_rate × (1 - punish_tolerance):
            // we are being punished; abort
            exploit_mode <- False
            responsive <- False
     (Rationale: continue exploiting until burst completes or punished.)

3. If (t == 1):
     action <- D
     (Rationale: aggressive default; gather initial data.)

4. Probe scheduling:
     If not exploit_mode and (t mod probe_interval == 0 OR random small probability p_probe):
         // Probe by cooperating this round
         action <- C
         last_probe_round <- t
         baseline_coop_rate <- others_coop_rate(w)   // use recent window before probe
         After observing rounds in the next protect_min_rounds..(protect_min_rounds+1) (see follow-up):
            compute post_probe_coop_rate = others_coop_rate over the protect observation rounds
            If post_probe_coop_rate - baseline_coop_rate >= delta_resp AND post_probe_coop_rate >= p_threshold_resp:
                 responsive <- True
                 exploit_mode <- True
                 exploit_rounds_remaining <- exploit_burst
            else
                 responsive <- False
     (Rationale: expend a small, controlled cooperation to test if opponents are inducible.)

5. Default behavior (if none of the above):
     If responsive == True and others_coop_rate(w) >= p_threshold_resp:
         // We have evidence many others will keep cooperating — exploit them
         action <- D
     else
         action <- D
     (Rationale: default to defection; exploit only when there is confirmed responsiveness.)

Observations and follow-up processing (after each round)
- Maintain circular history of last w rounds for computing others_coop_rate.
- After a probe round, observe the next protect_min_rounds rounds (others' actions) to measure whether cooperation increased relative to baseline_coop_rate. If increase is significant (≥ delta_resp and meets absolute p_threshold_resp), mark responsive and schedule an exploit burst.
- If during an exploit burst opponents reduce cooperation below baseline by punish_tolerance, abort exploitation and switch to default defection and suspend probes for a short cooling period (e.g., 2 × probe_interval).

Edge cases and careful handling
- Very short games (small r): If r ≤ 3, always defect (no time to reliably induce cooperation and last-round effect dominates).
- Near-end probes: Do not start probes if t > r - (exploit_burst + endgame_length + 1) — no time to exploit after a probe.
- If k is very close to n (k/n close to 1): cooperating becomes less costly relative to public share, but because k < n cooperation is still dominated one-shot. The same rules apply; the algorithm’s probes will detect if cooperation is profitable through group behavior and still exploit.
- If per-player history is available, you can compute responsiveness per-player and mark unconditional cooperators. But this AED uses group-level stats (robustness to large n and anonymity).
- If opponents never change behavior (e.g., all defect always), AED will defect always and gain as much as others (no wasted cooperation).
- If opponents are forgiving but retaliatory (e.g., tit-for-tat-like), AED’s one-round probes may trigger reciprocal cooperation; the exploitation burst may provoke punishment. The punish_tolerance and baseline checks protect against prolonged mutual punishment.

Why this is aggressive
- Default is defection (never a sucker).
- The strategy does not aim for mutual cooperation as a norm: it actively seeks to exploit patterns where others are willing to cooperate, then defects to extract the one-shot advantage repeatedly.
- It uses probing (small, rare investments) to find exploitable cooperators rather than blindly cooperating.
- It punishes by quitting exploitation quickly if opponents retaliate (so it maximizes short-run gains while limiting long-run losses from being targeted).
- It always defects in the endgame so it never gives away final-round gains.

Example run (informal)
- Rounds 1–3: defect, collect data.
- Round 4 (probe round): cooperate to test others.
- If after probe others raise cooperation from 20% → 75%, AED marks them responsive and exploits: defect for the next exploit_burst rounds while they continue cooperating, collecting the higher one-shot payoff repeatedly.
- If people retaliate (cooperation drops), AED aborts exploitation and returns to default defection, possibly spacing further probes farther apart.

Implementation notes for coding
- Keep a small rolling window of other players’ cooperation rates for quick computation.
- Implement a small state machine: {DEFAULT, PROBE_WAIT, EXPLOIT, COOLDOWN, ENDGAME}.
- Ensure probing is rare and skip probes near the end of the game.
- Make the thresholds and intervals configurable so they can be tuned in tournament play.

Summary (concise)
- Always defect by default, defect in the last rounds (endgame).
- Occasionally probe (single-round cooperate) to test whether many opponents are inducible.
- If a probe shows a strong, sustained increase in other-players’ cooperation, enter a short exploitation burst (defect repeatedly to harvest the one-shot advantage).
- If opponents punish (cooperation drops), abort exploitation and return to default defection.
- These rules are adaptive (learn from history), robust (safe default: defect), and aggressive (seek and exploit cooperators; never give away endgame gains).
'''

description_AGGRESSIVE_19 = '''
Strategy name: Exploitative Grim with Probing (EGrim-Probe)

High-level idea (aggressive summary)
- Default: defect every round. Defection is individually dominant in a single round; stay safe and reap immediate gains.
- Probe rarely to detect tolerant/unconditional cooperators whom you can reliably exploit.
- If a stable group of tolerant cooperators is detected, permanently exploit them by continuing to defect while they keep cooperating.
- Never forgive cheaply: if the group begins to punish (cooperation drops), revert to plain defection. Never enter long cooperative phases that could be used to exploit you.
- Always defect in the final rounds (endgame defection) to avoid being suckered by last-round cooperation.

This strategy is aggressive: it maximizes short-term payoff, seeks to identify and repeatedly exploit any cooperators, punishes retaliators by stopping attempts to cooperate, and refuses to be lured into mutual cooperation unless doing so is clearly profitable to exploit (i.e., you can keep exploiting them).

Parameters used by the strategy (derived from game parameters and r)
- W (window for stability checks) = min(5, max(1, floor(r/10))) — how many past rounds to inspect for stable behaviour
- endgame_rounds = min(3, r-1) — in these last rounds always defect
- probe_interval = max(3, ceil(r/10)) — deterministic spacing for probes (alternative: small random probing probability p = min(0.05, 2/r))
- tolerance_threshold θ = 0.80 (≥ fraction of other players cooperating during a probe round to consider them "tolerant")
- exploit_stability_threshold φ = 0.60 (≥ fraction of other players cooperating in follow-up rounds that signals they are keeping cooperating despite your defection)
All thresholds are tunable (choose slightly higher θ for more conservative detection).

State tracked
- mode ∈ {DEFAULT, PROBING, EXPLOITING}
- last_probe_round (if any)
- tolerant_set (optional: set/list of players identified as tolerant/unconditional; can be used or you can use aggregate fractions)
- history: full observed actions of all players (assumed available)

Decision rules (natural language)
1. First round: play D.
2. Endgame: if current round t > r - endgame_rounds, play D.
3. EXPLOITING mode:
   - Always play D.
   - Continuously monitor others’ cooperation fraction over the last W rounds. If that fraction falls below exploit_stability_threshold φ (they stopped cooperating reliably), exit EXPLOITING and go to DEFAULT.
   - Remain exploiting while others keep cooperating; continue to defect every round to extract the extra private payoff.
4. DEFAULT mode:
   - Play D by default.
   - Every probe_interval rounds (but never in the last endgame_rounds), perform a single-round PROBE: on that round play C (cooperate) and enter PROBING mode for evaluation on the immediate following round(s).
   - (Optional alternative: instead of deterministic spacing, probe with a small probability p each round such that expected number of probes is a small constant.)
5. PROBING mode (the probe and immediate evaluation):
   - Probe round: you played C. On the next round you revert to D (so the probe is one-shot).
   - Evaluate others’ behaviour during and immediately after the probe:
     a) If during the probe round at least θ fraction of the other players cooperated (they cooperated while you cooperated), and in the subsequent s ≤ W rounds they continued to show cooperation fraction ≥ φ (they did not punish your one-shot defect the next round), then declare them a tolerant cooperative mass and enter EXPLOITING mode.
     b) Otherwise, return to DEFAULT mode.
   - The logic: a tolerant/cooperative cluster that cooperates even though you defected just after your probe indicates they can be repeatedly exploited; switch to EXPLOITING to harvest that exploitation.
6. If at any time you detect suspicious coordinated retaliation (sharp sustained drop in cooperation after your exploiting), switch back to DEFAULT (you stop attempting to manipulate/coax cooperation).
7. Never voluntarily enter sustained cooperation phases unless you have evidence (PROBING) that you can exploit the cooperators; never cooperate purely to "reward" others or to be nice.

Edge cases and clarifications
- Small r: if r ≤ 4, do not probe at all; always defect every round (no time to recoup probing cost).
- Two-player case (n=2): behavior is the same. A one-shot probe can reveal if the opponent is unconditional/tolerant; if so, continue defecting to exploit them.
- Ties and borderline fractions: thresholds θ and φ are intended to be conservative; you should set them slightly high to avoid being fooled by noise or temporary cooperation bursts.
- Deterministic vs randomized probes: deterministic probes (every probe_interval) are simple and predictable; randomized rare probes (probability p) make you harder to coordinate with and less exploitable by strategies that condition on a fixed schedule. Either is acceptable; pick randomized probes if you want more unpredictability.
- Memory: you must be able to observe each player's actions (game assumption), so you can compute fractions.

Pseudocode (concise)

Initialize:
  mode = DEFAULT
  last_probe_round = -∞
  W = min(5, max(1, floor(r/10)))
  endgame_rounds = min(3, r-1)
  probe_interval = max(3, ceil(r/10))
  θ = 0.80
  φ = 0.60

For each round t = 1..r:
  if t == 1: play D; continue
  if t > r - endgame_rounds: play D; continue

  if mode == EXPLOITING:
    play D
    if cooperation_fraction_of_others_over_last_W_rounds < φ:
      mode = DEFAULT
    continue

  if mode == DEFAULT:
    if (r > 4) and ((t - 1) mod probe_interval == 0) and (t <= r - endgame_rounds):
      // perform one-shot probe this round
      play C
      last_probe_round = t
      mode = PROBING
    else:
      play D
    continue

  if mode == PROBING:
    // This is the round immediately after the probe; we play D to test tolerance
    play D
    // Evaluate: look at round last_probe_round (probe) and rounds last_probe_round+1 .. last_probe_round+s
    Let coop_at_probe = fraction_of_other_players_who_played_C_in_round(last_probe_round)
    Let coop_in_followups = min_fraction_of_other_players_who_played_C_in_each_of_rounds(last_probe_round+1 .. last_probe_round+min(W, r-last_probe_round))
    if coop_at_probe ≥ θ and coop_in_followups ≥ φ:
      mode = EXPLOITING
    else:
      mode = DEFAULT
    continue

Why this is robust and aggressive
- Robust: It never presumes others share norms and reacts only to observed behaviour. It uses probing to identify exploitable cooperators rather than relying on trusting conditional cooperation.
- Aggressive: The default is defection; when tolerant cooperators exist, the strategy exploits them by defecting repeatedly while they keep cooperating. The strategy punishes reliably by abandoning attempts to cooperate if others retaliate. It does not reciprocate forgiveness for the sake of group welfare. It also includes endgame defection to prevent being exploited late in the game.
- Adaptive: The strategy adapts to opponent behavior via probes and monitoring cooperation fractions: if opponents are exploitable, you exploit them; if they punish, you stop trying and revert to safe defection.

Implementation notes for tournament use
- Choose probe frequency and thresholds conservatively (e.g., θ=0.8, φ=0.6, probe_interval = ceil(r/10)). For very large r you can probe slightly more often; for small r reduce or disable probing.
- Use randomness in probe timing (small p per round) to make your probing unpredictable if you expect opponents to game deterministic probe schedules.
- Log and use aggregate fractions rather than per-player sets if the tournament pools vary where players come and go (simpler and robust).

Short version of the decision rules (for quick reference)
- Always defect, except: rarely (early enough) do a one-round cooperate probe to detect unconditional/tolerant cooperators. If a large, stable fraction of others cooperate during the probe and do not punish when you defect immediately after, switch to permanent defection while they keep cooperating (exploit). If they punish by withdrawing cooperation, stop all attempts and stay defecting. Always defect in the last few rounds.

This yields an aggressive, history-dependent, parameter-aware strategy that maximizes opportunities to exploit cooperative opponents while protecting itself from exploitation and retaliation.
'''

description_AGGRESSIVE_20 = '''
Strategy name: Aggressive Exploiter with Probing and Permanent Exploitation (AEPP)

Goal (aggressive mindset)
- Maximize my own cumulative payoff by (a) never giving free long-term cooperation for others to exploit, (b) systematically exploiting players who are effectively unconditional cooperators, and (c) probing early to discover exploitable opponents. Be robust: default to defection, only cooperate rarely and instrumentally to learn.

Precomputed constants (derived from n, r, k)
- m := min(20, max(1, floor(r/5)))  // history window used to estimate opponent tendencies
- probe_rate := min(0.2, 2 / max(1, r))  // small probability to cooperate early to probe; declines for long games
- exploiter_threshold := 0.90  // cooperation rate above this identifies an "unconditional cooperator" (sucker)
- low_coop_threshold := 0.20   // cooperation rate below this marks a defector/retaliator
- group_exploit_frac := 0.60   // fraction of other players cooperating in previous round which makes that round attractive to exploit
- endgame_frac := 0.15         // fraction of final rounds in which we switch permanently to defection
- E := max(1, ceil(endgame_frac * r))  // number of final rounds to always defect

State tracked from history
- For each other player j: coop_count_j (number of times j played C), coop_rate_j = coop_count_j / min(m, rounds_so_far)
- last_round_cooperators := count of players who played C in the immediately previous round (exclude myself)
- flagged_exploitable_set := set of players identified as unconditional cooperators (initially empty)
- round index t (1..r)

High-level decision rules
1. Always defect in the last round: if t == r, play D.
2. Always defect in the endgame window: if t > r - E, play D. (Prevents being suckered by exit-stage cooperation.)
3. First round (t == 1): play D (proactive test).
4. Otherwise (early/mid game, t <= r - E and t > 1):
   a. Update coop_rate_j for every opponent using the last min(m, t-1) rounds.
   b. Mark exploitable players: for any j with coop_rate_j ≥ exploiter_threshold, add j to flagged_exploitable_set.
   c. If flagged_exploitable_set non-empty: play D (exploit them every round).
   d. Else: probing step — with probability probe_rate, play C (a deliberate probe to discover unconditional cooperators).
   e. If not probing:
       i. If last_round_cooperators ≥ ceil(group_exploit_frac * (n - 1)) then play D (opportunity to free-ride a cooperative round).
      ii. Else if the majority of others are persistent defectors (more than (n-1)/2 have coop_rate ≤ low_coop_threshold) then play D (no point in cooperating).
     iii. Otherwise (ambiguous / mixed population): play D (default aggressive action).

Notes on adaptation and robustness
- The strategy's default is defection: that is the individually dominant action in each round and the safest aggressive choice. Most of the logic is about detecting and exploiting opponents who are unusually cooperative.
- Probing is rare and controlled by probe_rate; probing allows AEPP to detect unconditional cooperators early so it can exploit them thereafter. If probing never reveals exploitable opponents, AEPP will remain a defector.
- Using a sliding window m prevents ancient behavior from dominating estimates and adapts if opponents change behavior mid-game.
- The endgame rule (always defect in the last E rounds) avoids being exploited by endgame cooperation attempts and implements partial unraveling to be aggressive in the tail.
- Flagged_exploitable_set is permanent (once someone is identified as an exploitable unconditional cooperator, exploit them thereafter). This is deliberately aggressive: once you know someone will keep cooperating, you never give them the long-run reward that might make exploitation less profitable.

Pseudocode (compact)

Initialize flagged_exploitable_set = {}
For each round t = 1..r:
  if t == r: choose D; continue
  if t > r - E: choose D; continue
  if t == 1: choose D; continue

  // update rates using last min(m, t-1) rounds
  for each opponent j:
    coop_rate_j := (# times j played C in last min(m, t-1) rounds) / min(m, t-1)
    if coop_rate_j >= exploiter_threshold:
      flagged_exploitable_set.add(j)

  if flagged_exploitable_set is not empty:
    choose D; continue

  // probing
  with probability probe_rate:
    choose C; continue

  // exploit cooperative rounds
  last_round_cooperators := number of other players who played C in round t-1
  if last_round_cooperators >= ceil(group_exploit_frac * (n - 1)):
    choose D; continue

  // if most others are defectors, defect
  low_cooperators := number of other players with coop_rate_j <= low_coop_threshold
  if low_cooperators > (n - 1) / 2:
    choose D; continue

  // default aggressive action
  choose D

Why this is aggressive and tournament-robust
- AEPP's baseline is defection, which exploits cooperating opponents automatically and prevents exploitation of itself in single-round logic.
- It actively hunts for "suckers" via probing and then permanently exploits them, guaranteeing asymmetric payoff extraction when possible.
- It avoids chasing fragile cooperation near the end, and adapts quickly to changes using a bounded memory window.
- It requires no assumptions about opponents’ norms or willingness to coordinate; it uses only observed actions.

Edge cases
- Very short games (small r): probe_rate scales with r, so for r small probing is limited; endgame window E is at least 1 so last round still defected.
- If k is extremely close to n (not allowed by spec; k<n), nothing changes — defection remains myopic best response.
- If all opponents are perfectly contingent reciprocators and mutual cooperation would yield higher group returns, AEPP will mainly defect and extract value from them; that is intentional (aggressive).
- If implementation requires deterministic actions (no randomness), set probe_rate = 0 and replace probabilistic probing with deterministic probes in the first round(s) only (e.g., play C on round 2 if not exploited). However randomness improves robustness against adaptive opponents.

Final remarks
- AEPP is simple to implement, parameter-light, and explicitly aggressive: default defection, rare probing to find exploitable cooperators, permanent exploitation thereafter, and safe endgame defection to avoid being suckered. It is adaptive (tracks opponent rates over a sliding window), robust to a wide variety of opponent behaviours, and only uses game parameters and observed history.
'''

description_AGGRESSIVE_21 = '''
Name: Opportunistic Exploiter with Targeted Probing and Ruthless Retaliation (OETRR)

Short summary (aggressive mindset):
- Default is to defect — preserve the private endowment and exploit any cooperators.
- Probe rarely and early to identify reliably cooperative opponents you can exploit.
- If a clear majority (or a substantial minority) of opponents reliably cooperate, exploit them relentlessly by defecting every round while they continue cooperating.
- If opponents try to punish your exploitation (their cooperation falls sharply), respond with permanent defection (no forgiveness) and resume probing to find other exploitable cooperators.
- Always defect in the final rounds (endgame).

Why this is aggressive:
- It prioritizes immediate and cumulative payoff over group welfare.
- It actively looks for and exploits cooperative opponents.
- It punishes any collective attempt to restrict exploitation by switching permanently to defection (merciless retaliation).
- It only cooperates when doing so serves to reveal exploitable opponents (probing), not out of a presumption of fairness.

Parameters derived from the game (computed once at start):
- n (players), r (rounds), k (multiplier) — given.
- marginal return m := k / n (note m < 1 by game spec).
- probing window W := min(10, r - 1) (how many recent rounds we use to estimate behavior).
- test horizon R_test := max(3, ceil(0.25 * r)) (rounds in which we probe more actively).
- endgame E := min(3, r - 1) (final rounds where we always defect).
- thresholds:
  - T_high := 0.80 (player or group is “highly cooperative”)
  - T_low := 0.30 (player or group is “uncooperative / punished”)
  - Exploit_group_fraction := 0.5 (we consider it a majority if ≥ 50% of opponents are highly cooperative) — use ceil((n-1) * Exploit_group_fraction) as required count.
- probing probability base p_probe_base := clamp(0.05, 0.25 * m, 0.20)
  - (Scale probes up modestly when k is larger because larger k makes cooperative crowds more lucrative to exploit, but keep probing low so we mostly defect.)

State to maintain from history:
- For each opponent j: coop_count_j over last W rounds (or over all past rounds if fewer than W rounds exist); coop_rate_j := coop_count_j / min(W, t-1).
- group_recent := average of coop_rate_j across j ≠ i.
- persistent_exploit_flag (boolean): indicates we are currently exploiting a cooperative group.
- last_group_level_when_exploiting (float): group_recent when we entered exploitation mode.

Decision rules (plain language):
1. First round (t = 1): Defect.
2. Endgame (t > r - E or t = r): Defect.
3. Compute coop_rate_j and group_recent.
4. If persistent_exploit_flag is ON:
   - If group_recent has fallen below T_low (strong sign of collective punishment), turn persistent_exploit_flag OFF, set "punished" state and switch to permanent defection + probing policy (see below).
   - Else (group_recent still high), continue to Defect to exploit the cooperative majority.
5. If persistent_exploit_flag is OFF:
   - If at least K := ceil((n-1)*Exploit_group_fraction) opponents have coop_rate_j ≥ T_high OR group_recent ≥ T_high:
       - Enter exploitation: set persistent_exploit_flag := ON, last_group_level_when_exploiting := group_recent, Defect (start exploiting immediately).
   - Else if t ≤ R_test (early rounds):
       - Probe: cooperate with probability p_probe(t) := p_probe_base * (1 - (t - 1)/R_test) (decaying probes), otherwise Defect. Probing is only to elicit responses to identify reliable cooperators.
   - Else if we are in a “punished” state (we recently detected punishment against our exploitation):
       - Do not cooperate. Defect permanently for a long horizon, but keep probing very rarely (p_probe := p_probe_base / 5) to detect a change in opponents’ behaviors.
   - Else:
       - Defect (default).
6. Forgiveness policy: aggressive = no forgiveness. If we detect collective punishment after exploitation (group_recent drops below T_low while persistent_exploit_flag ON), we will remain uncooperative (defect) for the remainder of the game except extremely rare probes to detect new opportunities. We do not return to cooperative reciprocity toward punishers.

Pseudocode

Initialize:
  persistent_exploit_flag := false
  punished_state := false
  W := min(10, r-1)
  R_test := max(3, ceil(0.25*r))
  E := min(3, r-1)
  m := k/n
  p_probe_base := clamp(0.05, 0.25*m, 0.20)
  T_high := 0.80; T_low := 0.30
  Exploit_group_fraction := 0.5
  K := ceil((n-1)*Exploit_group_fraction)
For each round t = 1..r:
  if t == 1: play D; continue
  if t >= r - E + 1: play D; continue    // last E rounds: defect
  compute for each opponent j: coop_rate_j over last min(W, t-1) rounds
  group_recent := average_j coop_rate_j
  count_high := number of j with coop_rate_j >= T_high

  if persistent_exploit_flag:
    if group_recent < T_low:
      persistent_exploit_flag := false
      punished_state := true
      play D
      continue
    else:
      play D   // continue exploiting
      continue

  // Not currently exploiting
  if count_high >= K or group_recent >= T_high:
    persistent_exploit_flag := true
    last_group_level_when_exploiting := group_recent
    play D    // enter exploitation immediately
    continue

  if punished_state:
    // stay mostly defecting; tiny probability to probe for new exploitable cooperators
    with probability p_probe := p_probe_base / 5: play C
    otherwise play D
    continue

  if t <= R_test:
    // Early probing phase
    p_probe_t := p_probe_base * (1 - (t-1)/R_test)
    with probability p_probe_t: play C
    otherwise play D
    continue

  // Default outside special cases: defect
  play D

Edge cases and clarifications:
- Small r: R_test and W adjust automatically. With r small (e.g., r=2 or 3), probes are minimal and endgame covers most rounds; the strategy degenerates to defect almost every round (optimal given backward induction).
- If history is too short to compute coop_rate_j over W, use available rounds (denominator = max(1, rounds_so_far)).
- If opponents are heterogeneous (some highly cooperative, others never), the K threshold allows exploitation even if only a substantial minority are exploitable. You will exploit as soon as a sizable exploitable subset is detected.
- If multiple opponents react differently to being exploited (some continue cooperating, others stop), we evaluate group_recent (aggregate) — we exploit as long as enough cooperators keep cooperating.
- The aggressive no-forgiveness policy (punished_state) means that any successful collective punishment against us will be met by permanent defection and only infrequent probes thereafter — we do not re-establish cooperation with punishers.
- Defection in last rounds is mandatory to avoid endgame exploitation.

Rationale and robustness:
- The single-shot dominant strategy is D (defect). Cooperation only makes sense strategically when it can induce recurring returns from other cooperators. OETRR only cooperates to find exploitable cooperators (probing) and immediately exploits them when present. Probing is kept rare to avoid needless losses.
- The use of a window W and rates coop_rate_j makes the strategy robust to noise and to opponents that randomize sometimes; thresholds (T_high, T_low) guard against overreacting to temporary fluctuations.
- The persistent_exploit_flag ensures we keep exploiting until the group collectively punishes us (and then we switch to permanent defection) — that is the “ruthless retaliation” component.
- Tuning knobs (p_probe_base, thresholds, W, R_test) can be adjusted in implementation/tournament settings; defaults are conservative and keep the strategy aggressively exploitative while still adaptive.

Expected behaviors vs typical opponent types:
- Unconditional cooperators: will be exploited repeatedly (we defect) — high payoff for us relative to them.
- Tit-for-tat-like or conditional cooperators who switch to punishers: will cause our punished_state to flip on; we will then defect permanently — we won’t be lured into long reciprocal cooperation.
- Randomizers: probing and window averaging prevent overreacting; we exploit only when cooperation is reliably above T_high.
- Collective collaborators trying to sustain cooperation: if they maintain group cooperation above T_high we will exploit them aggressively until they suppress us; if they succeed in permanently reducing cooperation we accept the mutual defection outcome.

Implementation notes for tournament:
- Use exact counts/averages from history to compute coop_rate_j and group_recent (observability allowed).
- Keep persistent_exploit_flag and punished_state in memory between rounds.
- Provide deterministic tie-breaking: if random probes are used, use RNG seeded per game (or deterministic schedule) as allowed by tournament.

This strategy is explicitly aggressive: it starts by refusing to cooperate, probes rarely to find exploitable cooperators, exploits them relentlessly when found, and responds to punishment with permanent defection rather than attempting to rebuild cooperative relationships. It adapts to opponent behavior via short-run statistical estimates but always privileges exploitation and retribution over reconciliation.
'''

description_AGGRESSIVE_22 = '''
Name: Aggressive Adaptive Exploiter (AAE)

Intuition (short)
- Default is aggressive: defect whenever it is safe to free-ride (exploit cooperators) or when no long-term benefit to cooperating.
- Be adaptive: detect which opponents punish defection; if a majority will punish you long-term, switch to conditional cooperation to earn mutual gains rather than suffer retaliation.
- Probe occasionally (small random cooperation) to discover exploitable cooperators. Always defect in final rounds (endgame).

Summary of rules (high level)
1. Last-round logic: always defect in the final round; to be conservative, defect in the final two rounds.
2. Safety-first exploitation: if a large fraction of opponents are observed to be non‑retaliatory, defect every round to maximize immediate payoff and exploit cooperators.
3. Conditional cooperation when punished: if many opponents punish defection (high retaliation), try to sustain cooperation (cooperate when the group last cooperated) to secure longer-term group payoffs rather than get locked into low payoff from being punished.
4. Probing: with small probability cooperate in early/mid game to test for exploitable/unconditional cooperators.
5. Targeted learning: maintain per-opponent statistics (cooperation frequency and retaliation tendency) to decide whether an opponent is safe to exploit.

Detailed decision rules (deterministic logic + small randomness)

Notation:
- t: current round (1..r)
- r: total rounds
- n: players count
- history up to t-1: for each past round s < t, we observe vector of contributions c_j,s ∈ {0,1} for each player j
- i: our player index
- For opponent j ≠ i:
  - coop_count_j = number of rounds ≤ t-1 in which j played C
  - coop_freq_j = coop_count_j / max(1, t-1)
  - retaliation_count_j = number of times j played D in round s+1 when we played D in round s (observed transitions where our D could be followed by j's response). More precisely: for each s in 1..t-2, if c_i,s = 0 then add 1 to denom_j; if also c_j,s+1 = 0 then add 1 to numerator. If denom_j = 0, use fallback described below.
  - retaliation_rate_j = retaliation_count_j / max(1, denom_j). (If denom_j = 0, set retaliation_rate_j = 1 - coop_freq_j as a conservative fallback.)
- group_coop_last = number of players (including i) who cooperated in round t-1 (0..n)
- Parameters (suggested defaults — implementer may tune for tournament):
  - p_explore = min(0.08, 5/r)  (small prob of exploratory cooperation)
  - theta_retaliate = 0.40      (an opponent with retaliation_rate < 0.40 counts as “non‑retaliatory”)
  - theta_safe_ratio = 0.60     (if ≥ 60% of opponents are non‑retaliatory, treat environment as safe to exploit)
  - theta_group_exploit = 0.60  (if a high fraction of players cooperated last round, that presents a one-round exploitation opportunity)
  - min_history_rounds = 3      (minimum rounds to rely on statistics; before that be more conservative with fallbacks)

Decision pseudocode

function choose_action(t, r, n, history):
  if t == r:
    return D   # Last round: defect.

  if t >= r - 1:
    return D   # Last two rounds: defect to avoid endgame losses.

  # Early/mid game logic
  compute coop_freq_j and retaliation_rate_j for each opponent j using history
  if t - 1 < min_history_rounds:
    # not enough data: use conservative defaults but remain aggressive
    # Default: defect, but with small exploration chance cooperate
    with probability p_explore: return C
    else: return D

  # Safety assessment
  safe_opponents = count_j( retaliation_rate_j < theta_retaliate )
  safety_ratio = safe_opponents / (n - 1)

  # Immediate-exploitation opportunity: many cooperators last round
  if group_coop_last >= ceil(theta_group_exploit * n):
    # strong immediate lure: exploit if environment is reasonably safe
    if safety_ratio >= 0.25:
      return D   # exploit the mass cooperation
    # if not safe enough (many punishers), don't exploit blindly

  # If environment is safe (many non-punishers), be aggressive
  if safety_ratio >= theta_safe_ratio:
    # exploit continuously
    with probability (1 - p_explore): return D
    else: return C  # occasional cooperation probe only

  # If environment is hostile (many punishers), aim for stable cooperation
  if safety_ratio < theta_safe_ratio:
    # Use conditional cooperation rule to avoid sustained punishment:
    if group_coop_last >= ceil(0.5 * n):
      # majority cooperated last round -> reciprocate with cooperation to sustain mutual cooperation
      return C
    else:
      # majority defected last round -> defect (do not be suckered)
      return D

  # Fallback (should not reach): defect
  return D

How the components realize an aggressive mindset
- Default aggression: In most reasonable early/mid conditions the agent defects, maximizing immediate payoff. With many non‑retaliators present it defects persistently to free-ride.
- Opportunistic exploitation: If the group just produced a high-cooperation round, the strategy exploits that mass cooperation by defecting the next round (gain = 1 - 0 = 1 extra private unit vs cooperating) whenever the environment is judged sufficiently safe.
- Probing: Small randomized cooperation lets the agent identify unconditional cooperators that can be repeatedly exploited later.
- Risk-aware: If many opponents punish defection, the strategy avoids repeated exploitation that would invite costly retaliation; instead it switches to conditional cooperation aimed at holding cooperation (so the agent captures long-run gains rather than losing to reprisals).

Edge cases and special considerations
- First round: We recommended defect by default (aggressive), with a small exploration probability p_explore to probe for naive cooperators.
- Last round: always defect (standard backward induction). We extend this to defect in the final two rounds to avoid exploitation by short‑run triggers.
- No history (early rounds): use conservative fallbacks and exploration; do not rely on spurious statistics when t is small.
- Lack of data per-opponent (denominator 0 when computing retaliation): fallback uses 1 - coop_freq_j as an estimate of retaliatory tendency (conservative).
- Symmetry: strategy uses per-opponent stats but makes a group-level decision. It can be extended to target-defect only when specific opponents are identified as exploitable, but in simultaneous moves targeting has limited effect beyond shaping future group behavior.
- Randomness: occasional randomized cooperation is intentional to gather information and to occasionally seed cooperation that can be exploited later. Random seed should be reproducible in tournament implementation if required.

Why this is robust
- Works across a wide variety of opponents:
  - Against unconditional cooperators: it finds and exploits them via probes and then defects to reap high immediate payoffs.
  - Against unconditional defectors: it defects (no loss).
  - Against revengeful/punishing strategies: it recognizes retaliation and shifts to stable conditional cooperation to avoid long-run loss.
  - Against reciprocators (Tit‑for‑Tat variants): it will often default to conditional cooperation when reciprocation is detected, preserving mutual gains while still exploiting occasional one-shot opportunities.
- Uses simple, observable statistics (cooperation counts and transitions) so it tolerates noisy or mixed strategies by opponents.
- Keeps endgame logic simple and safe (defect in final rounds), which is critical in finite-horizon tournaments.

Implementation notes
- Tune thresholds (theta_retaliate, theta_safe_ratio, theta_group_exploit, p_explore) by simulation against the expected tournament population. Values provided are conservative defaults that favor exploitation while avoiding catastrophic retaliation.
- You can augment the strategy with per-opponent targeted exploitation once an opponent is confidently labelled “always cooperate”: e.g., if coop_freq_j ≥ 0.95 and retaliation_rate_j ≈ 0, then defect whenever j is likely to cooperate. That increases aggressiveness, but be cautious of false positives early on.
- If deterministic play is required, replace random probes with a deterministic probing schedule (e.g., cooperate exactly on rounds 3 and 7 if r is large).

Concise summary (one-line decision)
- Defect by default and in endgame; probe rarely; if most opponents are non‑retaliatory, exploit persistently (defect every round); if many opponents punish defection, switch to conditional cooperation (cooperate when majority cooperated last round) to avoid costly retaliation.
'''

description_AGGRESSIVE_23 = '''
Name: Aggressive Exploiter (AE)

High-level summary
- AE is an exploitative, adaptive strategy that tries to maximize its own total payoff by (a) seeding cooperation early to learn whether opponents will respond, (b) free-riding (defecting) when there is a sizable pool of cooperators to exploit, and (c) quickly punishing or re-seeding cooperation only when exploitation causes the public-good level to collapse. It never relies on communication or conventions and is robust across opponent types. AE is explicitly aggressive: it prioritizes own payoff over mutual cooperation, exploits cooperators, and defects in the final rounds.

Key rationales (short)
- Cooperating is strictly dominated in one-shot, so cooperation is only used instrumentally to create exploitable cooperation from others.
- Always defect in the last round (no future to influence).
- Use a short probing phase to measure opponents' cooperativeness; if others show substantial cooperation then exploit by defecting; if they retaliate, try limited, short resets to restore exploit opportunities.

Parameters derived from game inputs (all computed at initialization)
- n, k, r: given.
- probe_rounds = min(3, max(1, floor(r/6))) — short initial probe to seed and measure.
- window = min(5, max(1, r)) — number of most recent rounds used to estimate current cooperation levels.
- p_exploit = 0.40 — threshold average cooperation (among opponents) above which AE will exploit by defecting.
- p_collapsed = 0.20 — if cooperation among opponents falls below this after exploitation, treat as a collapse and attempt reset.
- reset_length = 2 — number of consecutive cooperative rounds to attempt to re-seed cooperation.
- max_resets = 2 — maximum number of reset attempts in the whole game.
- endgame_margin = min(3, r) — final rounds where AE defects unconditionally (to avoid being suckered by last-round cooperation).

Data AE tracks (derived from public history)
- For each round t < current, we observe everyone’s c_j(t).
- last_L_coop_rate = average cooperation rate among other players over last L = min(window, t-1) rounds.
- global_coop_rate = average cooperation rate among other players across all past rounds.
- my_recent_actions, resets_done, exploited_last_phase_flag (flag meaning AE followed exploitation and observed responses).

Decision rules (natural language)
1. Endgame rule:
   - If current round t > r - endgame_margin (i.e., in the last endgame_margin rounds), play D (defect) unconditionally.

2. First-round / probe phase:
   - For t ≤ probe_rounds: play C (cooperate). Purpose: seed cooperation and test opponents’ responsiveness.

3. Post-probe basic mode:
   - Compute last_L_coop_rate = (sum of cooperations by other players in the last L rounds) / (L * (n-1)).
   - If last_L_coop_rate ≥ p_exploit:
       - Exploit mode: play D (defect) to free-ride.
       - Mark that we are exploiting (set exploited_last_phase_flag = true).
   - Else if last_L_coop_rate < p_collapsed and exploited_last_phase_flag is true and resets_done < max_resets:
       - Opponents have collapsed in response to our exploitation; attempt a reset: play C for reset_length consecutive rounds to reseed cooperation, increment resets_done, then re-evaluate.
       - During reset rounds set exploited_last_phase_flag = false.
   - Else:
       - Default: play D. (AE is aggressive; it does not cooperate unless probing or performing a reset.)

4. If a reset was started:
   - Perform reset_length consecutive C rounds (unless the endgame rule forces D), then resume normal assessment. If reset fails (cooperation does not recover above p_exploit), AE will stop resetting when resets_done ≥ max_resets and defect thereafter.

5. Tie-breakers and stability:
   - If cooperation rate is exactly at threshold borderline, prefer exploitation (play D). AE biases toward defection when indifferent.

Pseudocode

Initialize:
  probe_rounds = min(3, max(1, floor(r/6)))
  window = min(5, r)
  p_exploit = 0.40
  p_collapsed = 0.20
  reset_length = 2
  max_resets = 2
  endgame_margin = min(3, r)
  resets_done = 0
  exploited_last_phase_flag = false
  reset_counter = 0

On each round t (1..r):
  if t > r - endgame_margin:
    action = D
    return action

  if reset_counter > 0:
    action = C
    reset_counter -= 1
    if reset_counter == 0:
      exploited_last_phase_flag = false
    return action

  if t <= probe_rounds:
    action = C
    return action

  L = min(window, t-1)
  last_L_coops = sum_{s = t-L}^{t-1} sum_{j ≠ me} c_j(s)
  last_L_coop_rate = last_L_coops / (L * (n-1))

  if last_L_coop_rate >= p_exploit:
    action = D
    exploited_last_phase_flag = true
    return action

  if exploited_last_phase_flag and last_L_coop_rate < p_collapsed and resets_done < max_resets:
    resets_done += 1
    reset_counter = reset_length - 1  # we'll play C this round and then reset_counter more
    action = C
    exploited_last_phase_flag = false
    return action

  action = D
  return action

Edge cases and explanations
- First round: cooperate (short probe). This is intentional — seeding cooperation early can create exploitable cooperation by others.
- Last round(s): AE always defects (no value in cooperating at end).
- Small r (e.g., r ≤ 3): probe_rounds will be small (1–3) and endgame_margin will be large relative to r, so AE will mostly defect as appropriate.
- When opponents are highly cooperative (last_L_coop_rate ≥ p_exploit), AE defects and repeatedly free-rides to collect higher payoffs.
- If exploitation triggers a collapse (cooperation drops below p_collapsed), AE will try up to max_resets small cooperation bursts to repair the public-good level and re-enable exploitation. Resets are short and limited to avoid being long-term suckered.
- If opponents never respond to probe (i.e., always defect), AE will defect after the probe and avoid wasting payoff on cooperation.
- Deterministic behavior: AE’s rules are deterministic given the history (no reliance on randomness or shared norms).
- Robustness: thresholds and small window sizes make AE responsive to changes in opponents’ behavior. AE exploits any consistent cooperators, punishes collapse with short resets, and never trusts indefinitely.

Why this is aggressive
- AE prioritizes own payoff: it only cooperates instrumentally (to elicit cooperation to exploit or to re-seed exploitable cooperation).
- It free-rides systematically when a sufficiently large cooperation pool exists.
- It uses limited, one-sided forgiveness (short resets) but will not return to sustained mutual cooperation that benefits opponents more than AE.
- It defects in the final rounds without hesitation.

Tuning notes (for implementer)
- p_exploit, p_collapsed, probe_rounds, window, reset_length, max_resets and endgame_margin can be tuned for opponent populations. The defaults above are deliberately aggressive but conservative enough to adapt.
- If you expect very high-cooperation opponents, raise p_exploit to exploit aggressively earlier. If you expect harsh punishers, reduce initial probe length and/or lower p_exploit.
- For stochastic settings, you may add a tiny randomized occasional cooperation to avoid deterministic exploitation cycles; the provided strategy is deterministic as required.

This description is sufficient to implement AE directly: it only needs the public history (who cooperated each past round) and the game parameters n, k, r.
'''

description_AGGRESSIVE_24 = '''
Name: Aggressive Exploiter with Calibrated Punishment (AECP)

Intuition (one-line): Start hostile (defect); relentlessly exploit any stable group of cooperators; if your defections provoke retaliation, stop trying to exploit and settle into permanent defection; occasionally probe with tiny probability only to detect new exploitable cooperators. The strategy depends only on parameters (n, r, k) and observed history (players’ actions each round).

High-level goals and aggressive mindset
- Maximize immediate and long-run payoff by defecting whenever safe and profitable.
- Exploit any opponents who keep cooperating (extract the “temptation” gain from unilateral defection).
- Do not allow conditional cooperators to discipline you: if they retaliate, cease attempts to harvest cooperation and lock into defection (avoid giving them another chance).
- Be minimally exploratory (rare probes) so you can discover exploitable cooperators but not be repeatedly exploited.

Global constants (suggested default values; implementer can tune):
- W: history window for frequency estimates = min(5, max(1, floor(r/4)))
- epsilon_probe: rare probe probability = 0.03 (3%)
- coalition_size_threshold: ceil(n/2) (a “large” cooperating subgroup = majority)
- coalition_freq_threshold θ = 0.75 (player counted as a cooperator if their recent cooperation frequency ≥ θ)
- retaliation_drop_threshold φ = 0.20 (a ≥20% drop in cooperation after my defection signals retaliation)
- final_safe_rounds H_final = 2 (always defect in the last H_final rounds)
- punish_confirm_count P = 2 (need to see retaliation pattern at least P times to confirm)

State computed from history up to the start of round t:
- For each player j ≠ me: f_j = fraction of rounds in the last W rounds in which j played C.
- G_prev = group cooperation rate in the immediately previous round (fraction of other players who played C in round t − 1).
- For detection of retaliation: for rounds where I defected in the last W rounds, compute average cooperation of others in the round after my defection and compare to their average cooperation in the round before my defection. If average cooperation falls by ≥ φ repeatedly after my defections, mark “retaliation_detected”.

Decision rules (natural language then pseudocode):

Natural-language rules
1. Last-round safety: If t is the last round or t > r − H_final, play D. (No future to punish/reward.)
2. If retaliation_detected and confirmed (seen on at least P occasions), lock into permanent defection for the remainder of the game (grim: always D). This avoids losing by trying to re-build cooperation with punishers.
3. Otherwise (no confirmed retaliation):
   a. If there exists a coalition of players S where |S| ≥ coalition_size_threshold and every j in S has f_j ≥ θ (i.e., a sizable, stable cooperating subgroup exists), defect (D) to exploit them. Exploitation yields higher immediate payoff than cooperating.
   b. Else, default to defection (D), but with very small probability epsilon_probe play C to probe whether a new exploitable cooperation subgroup emerges. Use probes only when there are enough remaining rounds to exploit (r − t ≥ 1).
4. Update the retaliation detection metrics after each round: if after I defected, the cooperation rate of others drops by ≥ φ relative to the previous round, increment a retaliation counter. If the counter reaches P, set retaliation_detected = true.

Pseudocode (round t, 1-indexed)
Input: n, r, k, history of actions of all players up to t−1
Output: action ∈ {C, D}

1. If t > r − H_final:
      return D
2. Compute W = min(5, max(1, floor(r/4))). Compute f_j for each opponent j over last W rounds.
3. Let S = { j ≠ me : f_j ≥ θ }.
4. Update retaliation detection:
      - For each occurrence in last W rounds where I played D at some round s, let coop_before = fraction of others who played C in round s−1 (if exists), coop_after = fraction who played C in round s+1 (if exists).
      - If coop_after ≤ coop_before − φ, increment a per-opponent or group “retaliation candidate” counter.
      - If the retaliation counter ≥ P, set retaliation_detected = true.
5. If retaliation_detected:
      return D   // lock into defection
6. If |S| ≥ coalition_size_threshold:
      return D   // exploit stable large cooperators
7. // No stable large coalition and not punished
   With probability epsilon_probe:
       return C   // tiny probe to detect new cooperators
   Otherwise:
       return D

Notes and clarifications
- Why almost always defect? Defection is the one-shot dominant action and gives a safe base payoff 1. The strategy is aggressive: it never sacrifices itself to build cooperation unless able to reliably exploit cooperators. It will not “pay” to try to sustain cooperation when facing potential punishment.
- Why exploit rather than cooperate when many others cooperate? Defecting when others cooperate yields strictly higher immediate payoff for you (temptation gain). AECP takes advantage of that whenever it believes retaliation is absent or minimal.
- Retaliation detection mechanics: We look for systematic drops in others’ cooperation following rounds in which we defect. A single drop could be noise; P consecutive or repeated such drops confirm that others punish defections and so trying to exploit will reduce your future payoff. At that point AECP gives up trying to harvest cooperation and switches to permanent defection.
- Probing: epsilon_probe is tiny to limit losses. Probes are only used when not in final rounds; they serve to detect new unconditional cooperators (who will keep cooperating after a probe) so AECP can exploit them next round.
- Individual targeting is limited because the public good is shared; exploitation and retaliation are group-level phenomena. AECP therefore treats coalitions (sets of cooperators) rather than trying to cooperate conditionally with specific players.
- Parameter tuning: Against many kinds of opponents, defaults above are robust. If facing very short games (small r) reduce probing and W; if opponents are slow to stabilize, increase W. If you see many one-shot cooperators, you will repeatedly exploit them; if you face many conditional cooperators willing to sacrifice you for collective welfare, AECP will detect retaliation and switch to permanent defection to avoid repeated losses.

Edge cases handled
- First round: t = 1 → Not in the final rounds; no history → S will be empty; default is D with tiny epsilon_probe chance of C. So AECP essentially defects in round 1 (aggressive opening).
- Last H_final rounds (including final round): always D (no reason to cooperate with no future).
- If a small minority cooperate but not a coalition: AECP will normally defect and may probe very rarely; it will not invest to sustain cooperation unless that cooperating minority grows into a majority-stable coalition.
- If opponents never change their behavior after your defections (i.e., unconditional cooperators), AECP will exploit them round after round and accrue high payoff.
- If opponents immediately punish your defection by dropping cooperation consistently, AECP detects that and stops trying to exploit; it then stops cooperating and avoids further losses.

Why this is adaptive and robust
- AECP adapts to three regime types: (a) many unconditional cooperators → exploit; (b) many conditional cooperators who punish defectors → stop trying to exploit and avoid further loss; (c) ambiguous/noisy opponents → probe infrequently to classify them without losing much.
- It only depends on game parameters (n, r, thresholds) and observed history (who cooperated each round), so it fits the allowed information structure.
- It does not rely on communication, norms, or specific opponent algorithms. It achieves an aggressive posture but with guarded adaptation to avoid persistent punishment.

Implementation hints
- Keep per-opponent counters for cooperation frequency and for post-defection drops to precisely detect who or whether the group punishes.
- Use small W for small r to keep estimates relevant.
- Tune epsilon_probe downward if probes are exploited in simulations; increase W if opponents are slow to change.

Summary (short)
- Always default to defection; defect in round 1 and the last H_final rounds.
- If a sizable, stable coalition of cooperators exists, defect to exploit them.
- If your defections produce repeated cooperation drops (retaliation), lock into permanent defection to stop being punished.
- Occasionally (very rarely) probe with cooperation to detect new exploitable cooperators.
'''

description_AGGRESSIVE_25 = '''
Name: Aggressive Trigger-and-Exploit (ATE)

Summary (one-line): Try to extract maximum value while forcing others toward cooperation by (a) probing early, (b) exploiting reliably when the group is highly cooperative, and (c) imposing long, hard punishments when group cooperation falls—with final-round defection guaranteed.

Design principles (what “aggressive” means here)
- Prioritize my own long-run payoff over social welfare.
- Use cooperation only as an instrument to induce and sustain cooperation by others, not as a moral default.
- Exploit high-cooperation environments with opportunistic defections.
- Respond to sustained low cooperation with long, credible punishments so future exploitation is deterred.
- Unravel the endgame: defect in the last round and increasingly toward the end.

Key inputs and computed quantities (available from parameters and history)
- n, r, k (game parameters).
- t = current round index (1..r).
- history of actions of all players for prior rounds (including counts of cooperators per round).
- rem = remaining rounds including current = r - t + 1.
- coop_incentive = k / n (immediate per-player share of one extra contribution).
- recent window w = min(5, r-1) (use last w rounds or all prior if fewer).
- p = empirical cooperation rate among other players in the recent window:
    p = (average over last w rounds of (# cooperators excluding me)/(n-1)).
- punished_counter (state): number of rounds left in an active punishment phase (initially 0).
- last_exploit_round (state): last round I performed an exploit (initially -∞).

Parameter defaults (adaptive to k and r; implementer may tune)
- initial_probes s = min(2, r-1) — cooperate in first s rounds unless immediate logic forces otherwise.
- T_good = 0.80 — threshold that signals the group is strongly cooperative.
- T_bad = 0.50 — threshold below which group cooperation is “unacceptably low.”
- exploit_cooldown = 1 — minimum rounds between my exploit defections (prevents consecutive obvious exploitation).
- base_punish_fraction = clamp(0.5 + (1 - coop_incentive), 0.5, 0.9).
    (If coop_incentive is small (k/n small), we punish a larger fraction of remaining rounds.)
- punishment_length L_punish = max(1, ceil(rem * base_punish_fraction)) when triggered.

Decision rules (when to Cooperate vs Defect)
At the start of each round t compute rem, p (based on available history), and check punished_counter.

1) Endgame / last-round rule (unambiguous aggressive move)
- If t == r (last round): Defect. (No future rounds remain to extract reward for cooperating.)

2) If currently in punishment mode (punished_counter > 0):
- Defect this round.
- Decrement punished_counter by 1 after the round.
- Rationale: maintain credible, costly retaliation that reduces the attractiveness of future defection.

3) Else (not currently punishing), early probing:
- If t ≤ s and r > 2:
    - Cooperate for the probe rounds to reveal whether others reciprocate (but see endgame override above).
    - Set no other state.
    - Rationale: aggressive players still probe early because occasional cooperation can create opportunities to exploit a cooperative field.

4) If group cooperation is “very high”:
- If p ≥ T_good AND (t - last_exploit_round) > exploit_cooldown AND rem > 1:
    - Exploit: Defect this round (single-round exploit), set last_exploit_round = t.
    - Immediately return to cooperation policy next round unless punish triggers.
    - Rationale: extract extra payoff when others are reliably contributing; do only occasional single-round defections so that cooperation level often remains high and the temptation to punish is delayed.

5) If group cooperation is middling-to-good:
- If T_bad ≤ p < T_good:
    - Cooperate if rem is reasonably long (rem >= 3) to try to build cooperation.
    - If rem is short (rem == 2) defect (because punishment credibility is limited).
    - Rationale: try to sustain cooperation when it’s plausible, but avoid being a sucker late in the game.

6) If group cooperation is low:
- If p < T_bad:
    - Trigger punishment: set punished_counter = L_punish (based on current rem). Immediately Defect this round.
    - Rationale: aggressive credible retaliation. The punishment length is proportional to remaining rounds and increases when cooperation is hard to obtain (k/n small).

7) Fallback conservative rule:
- If none of the above applies (rare), Defect. Aggressive default is defection unless there is a clearly exploitable or cooperative setup to leverage.

Other operational details and clarifications
- Counting cooperators p: exclude my own action when computing p for the current decision. Use observed actions from prior rounds only.
- Punishment trigger condition: p < T_bad measured over the recent window. Alternatively, the implementer may check absolute total_cooperators instead of proportion; the proportion is more robust to n.
- Punishment is “group-wide” because this game has a public good; I cannot target individuals. The goal is to lower the group's payoff until cooperation recovers or until I accept permanent defection for remainder of the game.
- After a punishment phase finishes, re-evaluate p and resume normal rules: start with probing/cooperation if conditions permit; do not automatically forgive forever—if cooperation does not recover, punish again.
- The strategy is deterministic given history (except for possible implementer choice to randomize exploit under T_good, but the pseudocode above is deterministic).

Edge cases & special handling
- Very short games (r = 2 or r = 3):
    - r = 2: t=1 (first) => aggressive default is Defect (because last-round defection removes incentive to cooperate). So defect both rounds.
    - r = 3: Cooperate maybe in t=1 if s=2? The implementer may set s = 1 when r ≤ 3. If s is used, the general endgame logic dominates: defect in last round; otherwise apply the same rules but with smaller windows/punishments.
- n = 2 (public good degenerates to PD-like): same rules apply; p is cooperation rate of the other single player.
- Very high k (coop_incentive = k/n close to 1): base_punish_fraction will be small (near 0.5), so punishments are shorter; the algorithm is more willing to cooperate because the cost of cooperation is low in relative terms (so exploit less severely).
- Very low k (coop_incentive small): punishments are longer (base_punish_fraction closer to 0.9) and I will exploit/cooperate less, because cooperation is less likely to be sustainable.

Pseudocode (concise)
Note: history_coop_rates() computes p over recent w rounds excluding me.

initialize punished_counter = 0
initialize last_exploit_round = -inf

for each round t = 1..r:
  rem = r - t + 1
  if t == r:
    action = D  // last round guaranteed defect
    play(action); continue

  if punished_counter > 0:
    action = D
    punished_counter -= 1
    play(action); continue

  p = recent_cooperation_rate_among_others(window = min(5, t-1))
  coop_incentive = k / n
  s = min(2, r-1)
  if t <= s and r > 2:
    action = C
    play(action); continue

  // Very high cooperation: opportunistic single-round exploit
  if p >= 0.80 and (t - last_exploit_round) > 1 and rem > 1:
    action = D
    last_exploit_round = t
    play(action); continue

  // Middling cooperation: try to sustain if time remains
  if p >= 0.50 and p < 0.80:
    if rem >= 3:
      action = C
    else:
      action = D
    play(action); continue

  // Low cooperation: trigger punishment
  if p < 0.50:
    base_punish_fraction = clamp(0.5 + (1 - coop_incentive), 0.5, 0.9)
    punished_counter = max(1, ceil(rem * base_punish_fraction))
    action = D
    punished_counter -= 1 // count this round as first punished round
    play(action); continue

  // Default
  action = D
  play(action)

Why this should be robust in a tournament
- The strategy never relies on any shared norms or explicit coordination; it only uses observed behavior.
- It is opportunistic: extracts short-term gain when others are reliably cooperative.
- It enforces cooperation credibly by punishing for a significant fraction of remaining rounds; that makes threats credible in long games.
- It adapts to k and n through coop_incentive and punishment sizing: when cooperation is relatively cheap (k/n high) the strategy is slightly more permissive; when cooperation is expensive it punishes longer.
- It handles endgame reasoning (last-round defection) so it does not waste resources when punishment would be impossible.

Tuning notes (implementer choices)
- T_good, T_bad, w, s and punishment fraction can be tuned for tournament meta (e.g., make T_good higher to exploit only extremely cooperative fields, or reduce punishment if many strategies are unforgiving).
- Optionally add limited stochasticity to exploit decisions to avoid predictable cycles with other smart agents.
- Keep punishment lengths proportional to remaining rounds to preserve credibility in longer games.

Behavioral intuition examples
- Against many naive cooperators: ATE will start by probing/cooperating, then occasionally defect to extract high payoffs while keeping others mostly cooperating.
- Against mostly defectors: ATE will detect low p, enter a long punishment phase (defect repeatedly), and then keep defecting if cooperation does not return—so it avoids being exploited.
- Against conditional reciprocators: ATE’s credible punishment will often coerce them toward higher cooperation to avoid the long punishment period, which ATE can then exploit selectively.

This strategy is aggressive because it seeks to maximize own payoff by exploiting high-cooperation environments and by imposing long, credible punishments on groups that do not sustain cooperation; it adapts to game parameters and history and explicitly unravels the endgame.
'''

description_AGGRESSIVE_26 = '''
Name: Aggressive Threshold-Trigger (ATT)

Intuition (short)
- Signal willingness to cooperate early to capture mutual-cooperation gains, but be ruthless about enforcement: if the group shows insufficient cooperation I switch to sustained punishment (collective defection) until there is clear, prolonged recovery. I also periodically probe/exploit very cooperative groups to extract one-shot gains and punish immediately if exploited. Endgame: always defect in the final round. The strategy is deterministic and depends only on n, k, r and the observed history of who cooperated each past round.

Parameters derived from game size (all deterministic formulas, no extra oracle)
- n (players), k (factor), r (rounds)
- M (cooperation threshold to consider the group “cooperating”): M = max(2, ceil(0.75 * n))
  - (requires a strong majority; aggressive)
- P (base punishment length): P = max(3, ceil(r / 4))
- W (recent-window for detecting persistent cooperation): W = min(max(3, ceil(sqrt(r))), r - 1)
- S (exploit spacing): S = max(2, floor(W / 2))
- M_probe (strict probe-success threshold): M_probe = max(2, ceil(0.9 * n))

High-level states
- Cooperative — play C when group cooperation looks strong.
- Punishing — play D for a sustained timer to make defection costly.
- Probing — after a punishment finishes, try a cooperative probe; only return to Cooperative if probe succeeds.
- Exploiting — occasional single-round defection inserted when group has shown long, stable near-unanimous cooperation.

State variables maintained (all computable from history)
- status ∈ {Cooperative, Punishing, Probing}
- punishment_timer (integer ≥ 0)
- last_exploit_round (round index of last exploit defection; initialize to -∞)

Decision rules (round t = 1..r)
1) Last-round rule:
   - If t == r: play D (defect). (Backward induction / aggressive endgame.)

2) First round:
   - t == 1: set status = Cooperative, punishment_timer = 0, last_exploit_round = -∞, play C.

3) Otherwise (2 ≤ t < r):
   - Compute c_{t-1} = number of cooperators in round t-1 (from observed history).
   - Compute recent_coop_count = number of rounds in last W rounds (or all past rounds if < W exist) where at least M players cooperated.
   - Determine if group is currently “very cooperative”: last W rounds had at least (W) rounds with ≥ M_probe cooperators (i.e., near-unanimous recently).

   State actions:
   A) If status == Cooperative:
      - If c_{t-1} ≥ M:
         - If group is currently “very cooperative” AND (t - last_exploit_round) ≥ S:
             - Execute an exploit-defection this round: play D, set last_exploit_round = t.
             - Immediately after the round observe next round behavior; if c_{t} < M then treat as a defection-trigger and switch to Punishing (see below).
         - Else: play C (cooperate).
      - Else (c_{t-1} < M):
         - Interpret this as a group-level defection → enter punishment.
         - Set punishment_timer = min(P, r - t) and status = Punishing.
         - Play D this round.

   B) If status == Punishing:
      - If punishment_timer > 0:
         - Play D, decrement punishment_timer by 1.
      - Else (punishment_timer == 0):
         - Switch to Probing (status = Probing).
         - Play C this round (a cooperative probe to see if others return).

   C) If status == Probing:
      - If c_{t-1} ≥ M_probe (the probe round succeeded; many others cooperated):
         - Switch to Cooperative (status = Cooperative).
         - Play C.
      - Else (probe failed):
         - Increase punishment severity: set punishment_timer = min(r - t, max(P, 2*P))
           (i.e., punish at least as long as before, often longer), set status = Punishing, and play D.

Notes and edge-case handling
- If r is small so that P ≥ r - t, punishment may cover remaining rounds; that is intended (long punishment).
- Probing is cautious (requires a very high cooperation response M_probe to return to Cooperative), so the strategy is not quickly re-optimistic.
- Exploitation is limited: only single-round defections spaced at least S rounds apart, and only when group has shown reliable near-unanimous cooperation in the recent window W. If an exploit reduces cooperation below M, punishment is triggered immediately next round.
- If multiple defections occur simultaneously by many players, the strategy treats this as a breakdown and moves to Punishing.
- If the strategy itself defects as part of an exploit or punishment and then other players retaliate in ways that would have been avoidable, ATT does not attempt per-player targeting (not possible) — it punishes the whole group to make defection costly in aggregate.
- Always defect in final round (t == r).

Pseudocode (compact)
- initialize status = Cooperative, punishment_timer = 0, last_exploit_round = -∞
- for t in 1..r:
    if t == r: action = D; continue
    if t == 1: action = C; continue
    let c_prev = number of cooperators in round t-1
    compute recent_window behavior over last W rounds, and whether those rounds each had ≥ M_probe cooperators
    if status == Cooperative:
        if c_prev >= M:
            if (recent near-unanimous is true) and (t - last_exploit_round >= S):
                action = D; last_exploit_round = t
                // observation next round may switch to Punishing
            else:
                action = C
        else:
            status = Punishing; punishment_timer = min(P, r - t); action = D
    else if status == Punishing:
        if punishment_timer > 0:
            action = D; punishment_timer -= 1
        else:
            status = Probing; action = C
    else if status == Probing:
        if c_prev >= M_probe:
            status = Cooperative; action = C
        else:
            punishment_timer = min(r - t, max(P, 2*P)); status = Punishing; action = D

Why this is “aggressive”
- High cooperation thresholds (M, M_probe) and long punishment durations (P, possibly extended) make the strategy unforgiving and impose a clear cost on groups that tolerate defections.
- Periodic exploit defections extract immediate gains from exploitable groups whenever the history shows long, stable near-unanimous cooperation.
- Punishment is collective and long enough that occasional one-shot benefits from defection are outweighed for many opponents who care about continued cooperation — therefore ATT coerces reciprocity in others that prefer future cooperation to one-shot gains.
- Final-round defection ensures the strategy does not give away last-round surplus.

Robustness to opponent types (brief)
- Against unconditional cooperators: ATT will initially cooperate, then occasionally exploit one-shot defections (infrequently) to increase its payoff. If cooperators never retaliate, ATT continues to exploit at spacing S.
- Against unconditional defectors: ATT will quickly detect insufficient cooperation (c_prev < M) and switch to sustained defection (minimizing losses).
- Against retaliators (tit-for-tat–like): ATT’s punishments are collective and long, so retaliation will typically stabilize cooperation to avoid long punishment — ATT will cooperate when group cooperation is high.
- Against noisy strategies: the probe/punish design and requirement of strong majority for recovery prevents being too gullible after a single noisy loss; ATT only resumes cooperation when recovery is clear.

Complexity
- Decision in each round requires counting cooperators in last few rounds and maintaining a small constant number of counters → O(n) per round (to read last round actions) and small memory O(r) for the recent window.

Summary
- ATT: start cooperating, require a strong majority to continue cooperating, punish for a substantial number of rounds when group cooperation falls below threshold, probe carefully to resume cooperation only when many players respond, occasionally exploit long-run near-unanimous cooperators, and always defect in the final round. This yields an aggressive but adaptive, history-dependent strategy suitable for tournament play without relying on external coordination.
'''

description_AGGRESSIVE_27 = '''
Strategy name: Aggressive Exploit-and-Punish (AEP)

High-level idea (one-sentence)
- Default to defect; whenever the group shows evidence of stable cooperation and there are enough future rounds left to profit from it, exploit by defecting while others contribute; if cooperation collapses, inflict quick, harsh punishment (more defection) and then attempt a short, deterministic probe to re‑test whether cooperating opponents can be exploited again. Never cooperate on the final round.

Rationale (why this is “aggressive”)
- The strategy prioritizes maximizing its own cumulative payoff over building or preserving public good. It actively free‑rides on stable cooperators, uses targeted probes only when there is potential long‑run payoff from inducing cooperation, and uses short, decisive punishments to deter opportunistic behavior that would spoil exploitable cooperators. It never sacrifices short‑term private payoff for unconditional niceness (except for calculated, temporary probes intended to generate larger long‑term exploitation gains).

Notation
- n, r, k: game parameters.
- t: current round (1..r). R_rem = r − t + 1 (rounds inclusive remaining).
- history: for each past round s < t we observe c_j,s ∈ {0,1} for every player j (including self). From this we can compute:
  - coop_count_s = Σ_j c_j,s (total cooperators in round s)
  - coop_others_s = coop_count_s − c_self,s (others’ cooperators that round)
  - for each player j ≠ self, coop_rate_j = (Σ_s c_j,s) / (t−1)
  - group_recent_avg = average over last W rounds of (coop_others_s)/(n−1)
- Internal state variables:
  - mode ∈ {Normal, Exploit, Punish}
  - punish_counter (integer ≥ 0)
  - exploit_run_length (how many rounds we have been exploiting consecutively)
  - last_probe_round (round index when we last cooperated as a probe)

Tunable default parameters (deterministic choices; implementer can tune)
- W = min(5, max(1, r−1))          (window for recent behaviour)
- τ_exploit = 0.70                  (threshold fraction of other players cooperating required to regard the group as “exploitable”)
- R_min_for_exploitation = 3        (minimum remaining rounds required to attempt exploitation)
- P = min(3, max(1, floor(r/10)))  (punishment duration in rounds)
- M_probe = max(3, floor(r/6))     (probe frequency while exploiting)
- τ_coop_player = 0.80              (a player-level cooperation rate considered “reliable cooperator”)
- exploit_probe_success_frac = 0.6  (what fraction of others cooperating on the probe constitutes a successful probe)

Decision rules (ordered; applied each round to choose C or D)

1) Last round (t == r)
- Play D. (Backward induction: no future to influence.)

2) If punish_counter > 0
- Play D.
- Decrement punish_counter by 1.
- If punish_counter reaches 0, set mode = Normal and schedule the next probe attempt when conditions allow.
- (Punishment is unconditional defection for P rounds.)

3) If mode == Exploit
- If R_rem < R_min_for_exploitation then abandon Exploit and set mode = Normal; go to step 4.
- exploit_run_length += 1
- If (t − last_probe_round) >= M_probe and R_rem > 1:
    - Play a deterministic probe: C (one round of cooperation to test whether others will sustain cooperation).
    - Set last_probe_round = t.
    - After observing next round (t+1), evaluate probe success: if coop_others_{t+1} / (n−1) ≥ exploit_probe_success_frac then remain in Exploit (continue defecting next rounds); else fail the probe: set mode = Punish, punish_counter = P, exploit_run_length = 0.
- Else:
    - Play D (exploit).
- Additionally, if group_recent_avg (computed over last W rounds) falls below τ_exploit − 0.1 for two consecutive windows, immediately set mode = Punish and punish_counter = P.

4) If mode == Normal
- Compute group_recent_avg (fraction of other players cooperating averaged over last W rounds).
- If group_recent_avg ≥ τ_exploit AND R_rem ≥ R_min_for_exploitation:
    - Enter Exploit mode: mode = Exploit; exploit_run_length = 0; last_probe_round = t − 1 (so a probe will occur after M_probe rounds); Play D (exploit immediately).
- Else:
    - Play D by default (do not cooperate).
- Exception (one-shot opening test):
    - In round 1 only: play D (aggressive default test; do not offer free cooperation).
    - If R_rem is large (≥ 2*M_probe) and you observe a set S of at least q = ceil((n−1)*0.5) players with coop_rate_j ≥ τ_coop_player AND group_recent_avg ≥ 0.5, then perform a single one‑time cooperative offer (play C) in the next round only to see whether you can trigger coordinated cooperation; treat that as a probe (i.e., if the next round shows high cooperation, enter Exploit).

5) Sanity fallback
- If none of the above conditions triggered, play D.

Edge cases handled explicitly
- First round: always D (aggressive test; gather information).
- Last round: always D (no incentive to cooperate).
- Very few remaining rounds (R_rem < R_min_for_exploitation or R_rem small): avoid cooperative probes; default to D to secure immediate payoff.
- Small n: thresholds are fractions of (n−1) so they scale; P and M_probe are min/max bounded so they remain sensible for short games.
- If r is very small (e.g., r = 2 or 3): exploitation is unlikely because R_min_for_exploitation prevents costly probes; the strategy simply defects most rounds (best guarantee).
- If opponents are all unconditional cooperators (or many are), the strategy will detect high group_recent_avg and enter Exploit mode and free-ride repeatedly (with occasional probes to maintain leverage).
- If opponents retaliate to exploitation by reducing cooperation, the strategy imposes a short punishment (P rounds of defection) and then uses a deterministic probe to test if cooperation can be re‑established and exploited again.

Pseudocode (compact, readable)
- Initialize mode = Normal, punish_counter = 0, exploit_run_length = 0, last_probe_round = 0
- For t = 1..r:
    - R_rem = r − t + 1
    - if t == r: action = D; output action; continue
    - if punish_counter > 0:
         action = D; punish_counter -= 1
         if punish_counter == 0: mode = Normal
         output action; continue
    - compute group_recent_avg over last min(W,t−1) rounds (if t == 1, define as 0)
    - if mode == Exploit:
         if R_rem < R_min_for_exploitation: mode = Normal; exploit_run_length = 0
         else:
            exploit_run_length += 1
            if (t − last_probe_round) >= M_probe and R_rem > 1:
                action = C; last_probe_round = t; output action; continue
            else:
                action = D; output action; continue
    - if mode == Normal:
         if group_recent_avg >= τ_exploit and R_rem >= R_min_for_exploitation:
              mode = Exploit; exploit_run_length = 0; action = D; output action; continue
         else:
              action = D
              # try one-time offer only if conditions for profitable long-run exploitation (optional)
              if t == 1: action = D
              output action; continue

After-action bookkeeping (what the strategy computes from observed result next round)
- If you cooperated as a probe on round t:
  - After observing round t+1, compute coop_others_{t+1}/(n−1). If that fraction < exploit_probe_success_frac, then set mode = Punish; punish_counter = P; exploit_run_length = 0. If >= exploit_probe_success_frac, remain or enter Exploit (mode = Exploit).
- If in Exploit mode you see group_recent_avg drop substantially for two consecutive windows, set mode = Punish; punish_counter = P.

Why this is robust
- It uses only game parameters and observed history.
- It exploits unconditional cooperators and conditional cooperators that react to deterministic probes.
- It does not rely on implicit norms or communication.
- It limits punishment duration (P small), preventing endless self‑harm; punishments are decisive enough to deter simple exploiters from sabotaging exploitation opportunities.
- Probing is deterministic (no hidden randomness), so other strategies that respond predictably can be learned and exploited.
- Last-round defection and R_min_for_exploitation logic prevent wasting probes when there is no time to recoup costs.

Parameter tuning guidance
- τ_exploit controls how conservative the strategy is when recognizing exploitable cooperation. Increase to be more conservative (only exploit near-unanimous cooperation); decrease to attempt exploitation earlier.
- P controls punishment severity/duration. Larger P increases deterrence but risks self-harm if opponents do not respond.
- M_probe controls how often we risk a cooperative signal to restore/maintain exploitable cooperation. A smaller M_probe makes the algorithm more willing to invest in maintaining exploitation (more aggressive); larger M_probe is more parsimonious.

Summary of behavior types you will produce in tournaments
- Versus unconditional cooperators: you will generally defect and capture the one-shot gain every round while occasionally probing to sustain exploitation—this yields high payoff.
- Versus unconditional defectors: you defect too (baseline 1 per round), minimizing loss from futile probes.
- Versus conditional cooperators who punish after exploitation: you will try short exploit cycles but will quickly switch to punish+probe cycles; net outcome will depend on their responsiveness, but you bias toward extracting gains whenever possible.
- Versus mixed or stochastic strategies: AEP learns group-level signals and acts to maximize private payoffs, using deterministic probes and short punishments to adapt.

This description contains all rules needed to implement AEP. Parameter values above are defaults intended to be robust; they can be adjusted if empirical tournament results suggest different calibration.
'''

description_AGGRESSIVE_28 = '''
Name: Aggressive Opportunistic Punisher (AOP)

Overview (high-level)
- Default stance: defect. The strategy is aggressive — it starts hostile, only offers cooperation opportunistically when there is clear, sustained evidence that a sizeable group of opponents reliably cooperates. When cooperation is offered, it is limited in length (a “burst”) so the strategy can harvest high public-good returns without committing long-term. Any betrayal during a cooperation burst triggers a long, uncompromising punishment phase (grim-style). After punishment, AOP tests the population with a short probe to see whether cooperation can be re-exploited. Always defect in the final phase of the game to avoid endgame exploitation.

Rationale
- In a single-round public goods game defection strictly dominates. AOP therefore prefers to defect unless there is credible, recent evidence that many players will reliably cooperate.
- The strategy is aggressive: it punishes deviations quickly and harshly and will not be swayed by occasional cooperative gestures unless they are sustained and numerous.
- It is adaptive: it monitors recent history, identifies consistent cooperators, and only cooperates when group-level cooperation is stable enough to produce repeatable gains.

Parameters (derived from n, r)
- n: number of players.
- r: total rounds.
- W: memory window used to detect stability = min(5, max(1, r-1)). (Use up to the last 5 rounds of history; truncated if r is small.)
- q: cooperation frequency threshold to call a player “stable cooperator” = 0.8 (i.e., cooperated in >= 80% of the W most recent rounds).
- M: group-size threshold to open cooperation = ceil(n/2) (a simple majority of players must be stable cooperators).
- B: cooperation burst length = min(4, max(1, floor(r/10 + 1))) — short limited cooperation episodes to harvest gains while limiting exposure.
- P: punishment length = min(r, max(6, ceil(r/3))). (Punishment is long and severe, scaled to game length; at least 6 rounds, or roughly one-third of game.)
- T_end: final-phase length where we always defect = min(r, max(1, ceil(r/10))). (In the final ~10% of rounds, AOP refuses to cooperate to avoid endgame exploitation.)
- Note: implementers may tune small constants (5, 0.8, 4, 6, 1/3, 1/10). They are fixed functions of n and r and do not rely on opponents’ identities beyond history.

State machine / modes
- DEFAULT: default aggressive defecting mode.
- COOP_BURST: cooperating for B rounds because a sufficiently large stable group was detected.
- PUNISH: punishment mode — defect for P rounds after a betrayal.
- TEST: short probe (1 round) after PUNISH to see if cooperation re-emerges.

Initial and terminal behavior
- Round 1: defect (aggressive opening).
- Rounds t where t > r - T_end: always defect (final-phase defection).

Decision rules (precise)
At the start of each round t:
1. If t > r - T_end: play D (always).
2. If currently in PUNISH mode: play D; decrement punishment counter. When counter reaches 0, enter TEST mode.
3. If currently in COOP_BURST: play C; decrement burst counter. If during this burst any player in the population who was previously classified as a stable cooperator (see detection below) defects in any burst round, immediately switch to PUNISH for P rounds (and play D this round and subsequent punishment rounds). If the burst completes with no betrayal, return to DEFAULT.
4. If currently in TEST mode: play C for one probing round. After the probe:
   - If the probe round had at least M cooperators (including me), interpret as renewed collective willingness and enter COOP_BURST for B rounds.
   - Otherwise, enter DEFAULT.
5. Default check (only applies in DEFAULT):
   - Compute stability set: using the last W completed rounds (if less than W exist, use all available past rounds), compute each player j’s cooperation frequency f_j = (# times j played C in the W rounds) / W.
   - Let S = { j : f_j >= q } be stable cooperators.
   - If |S| >= M:
       - Enter COOP_BURST and play C this round (start of a burst).
     Else:
       - Play D.

Detection of betrayals and triggers
- A “betrayal” during a COOP_BURST is: any player j ∈ S (stable cooperators that triggered the burst) plays D in any round of the burst. If this happens, we immediately (in the same round) switch to PUNISH mode and start the P-round punishment (we play D during the round of betrayal and the following P-1 rounds).
- If a previously stable cooperator drops below q in the W-window (due to recent defections), they will no longer be counted in S on the next detection step.

Pseudocode (compact)
Variables:
- mode ∈ {DEFAULT, COOP_BURST, PUNISH, TEST}
- burst_left, punish_left
- history H[t][j] (1=C, 0=D) for t < current_round

Initialize:
- mode = DEFAULT
- burst_left = 0
- punish_left = 0

For each round t from 1..r:
  if t > r - T_end:
    action = D
    continue
  if mode == PUNISH:
    action = D
    punish_left -= 1
    if punish_left == 0: mode = TEST
    continue
  if mode == COOP_BURST:
    action = C
    burst_left -= 1
    /* Immediately inspect current round’s actions (after observing them; if synchronous play is required, define betrayal based on previous round’s moves — see implementation note below). */
    if any j in S played D in this burst round:
      mode = PUNISH
      punish_left = P
      action = D  /* switch to defect this round and during punishment */
    else if burst_left == 0:
      mode = DEFAULT
    continue
  if mode == TEST:
    action = C
    /* After observing this test round */
    if total_cooperators_in_test_round >= M:
      mode = COOP_BURST
      burst_left = B - 1  /* we already did 1 test-cooperation round which doubles as first burst round */
    else:
      mode = DEFAULT
    continue
  /* DEFAULT mode */
  compute f_j for last W rounds
  S = { j : f_j >= q }
  if |S| >= M:
    mode = COOP_BURST
    burst_left = B - 1
    action = C
  else:
    action = D

Implementation note about simultaneity and betrayal detection:
- The game is simultaneous within each round; you must decide before seeing current-round actions. The above text uses “after observing them” language only for describing response logic. In implementation, define betrayal during a burst as: if, in the previous round, any j in S played D while they had been counted as stable cooperators in the detection step that triggered the burst. Concretely, check opponent moves in the last completed round; if at any point during the burst an opponent who was in the triggering S plays D in that round (observed next round), start PUNISH immediately (from the next round). This keeps decisions implementable with the simultaneous-move requirement.

Edge cases and small-r behavior
- r small: learning windows shrink accordingly (W ≤ r-1). B, P, T_end clamp to feasible values. For r=2: Round 1 defect; round 2 (last) defect — full defection, which is aggressively safe.
- n=2: M=1. If the other player has been cooperating sufficiently (f_2 >= q) then AOP will try a cooperation burst; any defection by the opponent triggers punishment.
- If many strategies always defect, AOP remains in DEFAULT and defects — robust.
- If many strategies always cooperate, AOP will detect stable cooperators quickly and undertake repeated cooperation bursts, siphoning high payoffs while always ready to punish any betrayal.

Why this is “aggressive”
- Starts defecting and defaults to defection.
- Only offers cooperation when a clear majority is consistently cooperative.
- Cooperation windows are deliberately short (harvest, then exit) to reduce exposure.
- Betrayal triggers long, strict punishment P (grim-style) to deter exploitation.
- Final-phase defection prevents endgame exploitation.

Adaptive and robust properties
- Adapts to observed cooperation frequencies (not presuming norms).
- Detects and exploits sustained cooperation when present.
- Harsh response to defections reduces vulnerability to opportunistic free-riders.
- Testing after punishment allows re-exploitation of re-formed cooperative groups without naive forgiveness.

Tuning guidance (practical)
- If the tournament historically favors longer cooperation, increase B and decrease P moderately.
- If players respond badly to harsh punishments, reduce P to avoid mutual collapse.
- The constants (W=5, q=0.8, M=ceil(n/2), B small, P large) enforce an aggressive posture by default; adjust only if you want less aggression.

Summary one-liner
Defect by default and in the endgame; opportunistically join limited cooperation bursts only when a clear majority has been consistently cooperating; if any of those cooperators betray during a burst, respond with a long, uncompromising punishment, then probe once and repeat.
'''

description_AGGRESSIVE_29 = '''
Summary (aggressive intent)
- Default stance: defect. Prioritize immediate gain and exploitation of any persistent cooperators.
- Be adaptive: detect when the group punishes defection; if punished, temporarily switch to a short “repair / appease” mode (cooperate for a small number of rounds or mirror the group majority) to stop punishment, then return to exploitation.
- Be unforgiving only when needed: punishers are treated with conditional cooperation to avoid long losses; naive cooperators are milked (you keep defecting).
- Endgame: always defect in the last rounds (no trust in finite horizon).

Notation
- n, r, k: game parameters (given).
- t: current round, 1 ≤ t ≤ r.
- history: for each past round s < t we observe total cooperators totalC_s (including all players). We also know our own actions in prior rounds.
- otherC_s = totalC_s − my_c_s : number of cooperators among the other n−1 players in round s.
- remaining = r − t + 1 (rounds left including current).

Constants (fixed, depend only on r and n)
- M = min(8, r−1)  // memory window size for statistics
- COOP_EXPLOIT_TH = 0.75   // if others cooperated this fraction of time, they are exploitable
- PUNISH_DROP = 0.25       // threshold drop in others' cooperation that counts as punishment
- REPAIR_ROUNDS = min(3, max(1, floor((r)/10)))  // cooperate briefly to repair if punished
- MAJ_THRESHOLD = ceil((n−1)/2)   // majority of others cooperating

High-level decision rules (natural language)
1. First round: defect (aggressive default).
2. Endgame: if remaining ≤ 2, defect (no long-run incentives).
3. Default mode (“Exploit”): defect every round. While exploiting, continuously monitor the behavior of others:
   - If the other players display high, persistent cooperation (others cooperated ≥ COOP_EXPLOIT_TH in the last M rounds) and there is no sign they punish defectors, continue defecting (exploit them).
   - If, after one or more of your defections, the others respond by sharply lowering their cooperation (a drop ≥ PUNISH_DROP of the fraction of others cooperating), classify the group as punishing your defection and switch to a “Repair/Conditional” mode.
4. Repair/Conditional mode (triggered only if punishment is detected):
   - For the next R = min(REPAIR_ROUNDS, remaining−1) rounds, adopt a conciliatory pattern to stop punishment:
     - Simple rule: in each of these rounds cooperate if a majority of others cooperated in the immediately preceding round (i.e., follow group majority). If the group majority defected last round, defect.
   - After R repair rounds, if the group’s cooperation resumes (others’ cooperation fraction in last M rounds rises above COOP_EXPLOIT_TH and there is no further punishing reaction to your defections), return to Exploit (defect).
   - If punishment continues after repair (you defect and others still reduce cooperation), remain in Conditional mode and continue mirroring the majority until the last few rounds when you default back to defection.
5. Ambiguity / tie-breaks: when rules conflict, choose Defect (aggressive tie-breaker).

Why this is aggressive
- You start and stay defecting whenever safe to do so (exploiting cooperators).
- You only concede (cooperate) minimally and strategically: to stop punishments that cause sustained loss, not to build altruistic cooperation.
- You punish back indirectly by refusing to enable stable cooperation when the group is responsive to defection; you do not attempt long cooperative strings unless they are instrumental to stop a costly punishment.
- Endgame defection ensures no end-of-game exploitation of you.

Pseudocode

Inputs: n, r, k, history (for s = 1..t−1 we have totalC_s and my_c_s)
Output: action ∈ {C, D}

function decide_action(t, history):
    remaining = r − t + 1
    if t == r or remaining <= 2:
        return D        // endgame: always defect
    if t == 1:
        return D        // always start aggressive

    // compute recent statistics over last M rounds (excluding future)
    lookback = min(M, t−1)
    sum_other_coop = 0
    for s in (t−lookback) .. (t−1):
        otherC_s = totalC_s − my_c_s
        sum_other_coop += otherC_s / (n−1)   // fraction of others who cooperated that round
    avg_other_coop = sum_other_coop / lookback   // fraction (0..1)

    // detect punishment: did others cut cooperation after one of our defections?
    punished = false
    for s in 2 .. (t−1):
        if my_c_{s−1} == 0:   // we defected in round s−1
            // compare average other cooperation just before vs just after our defection
            prev_other = (totalC_{s−2} − my_c_{s−2})/(n−1) if s−2 >=1 else avg_other_coop
            after_other = (totalC_{s−1} − my_c_{s−1})/(n−1)
            // if others reduced cooperation by at least PUNISH_DROP after our defection, it's punishment
            if prev_other − after_other >= PUNISH_DROP:
                punished = true
                break

    // Mode selection
    if not punished:
        // exploitful: if many others have been cooperating, keep defecting
        if avg_other_coop >= COOP_EXPLOIT_TH:
            return D    // exploit repeatedly
        else:
            return D    // default aggressive defect
    else:
        // in punish mode: perform short repair sequence by mirroring group majority
        // Count how many repair rounds already executed since punishment:
        last_pun_round = s   // from the loop where punished became true
        repair_done = count of rounds u in [last_pun_round .. t−1] where we cooperated as repair
        R = min(REPAIR_ROUNDS, remaining − 1)
        if repair_done < R:
            // mirror last-round majority among others
            last_other = (totalC_{t−1} − my_c_{t−1})
            if last_other >= MAJ_THRESHOLD:
                return C
            else:
                return D
        else:
            // After repair window: if cooperation rebounded, go back to Exploit (defect)
            // compute recent avg again
            if avg_other_coop >= COOP_EXPLOIT_TH:
                return D
            else:
                // continue conditional strategy: mirror majority so as to avoid sustained losses
                last_other = (totalC_{t−1} − my_c_{t−1})
                if last_other >= MAJ_THRESHOLD:
                    return C
                else:
                    return D

Notes and parameter choices
- M = min(8, r−1) gives modest memory, enough to detect patterns but not too slow to adapt.
- COOP_EXPLOIT_TH = 0.75 marks “high cooperation” groups that can be exploited; tune up or down if you want more or less greed.
- PUNISH_DROP = 0.25 declares punishment only when there is a clear reaction to your defection; this avoids overreacting to noise.
- REPAIR_ROUNDS is intentionally short. The goal is stop punishment quickly with minimal concessions; then return to exploitation.
- Always defect in the last two rounds is aggressive and robust in finite-horizon settings.

Robustness remarks
- Against unconditional cooperators: you will defect every round and harvest the per-round advantage (defection strictly dominates in the single-round payoff). You get the maximum exploitation possible.
- Against unconditional defectors: you defect and match them, avoiding loss relative to them.
- Against conditional cooperators that punish defections (grim-like or tit-for-tat-like): you detect punishment and switch to short reparative cooperation to stop long punishment chains, minimizing your total loss while still exploiting opportunities later.
- Against sophisticated strategies that try to signal or coordinate: you never rely on off-path agreements; you react only to observed actions and punishments.
- The decision rule depends only on game parameters and observed history.

Extensions / tunable aggressiveness
- To be even more aggressive: reduce REPAIR_ROUNDS to 0 and PUNISH_DROP larger → no repair, you accept punishment.
- To be slightly less aggressive / safer: increase REPAIR_ROUNDS and reduce COOP_EXPLOIT_TH → cooperate more to maintain group cooperation.
- The above balances short-term exploitation and pragmatic concessions to avoid long retaliation.

Final note
This strategy is intentionally aggressive: default defecting, systematically exploiting cooperation, only conceding a minimal amount to stop coordinated punishment that would otherwise inflict repeated losses. It is simple to implement from observed histories and robust across many opponent types because it only uses counts (no assumptions about others’ internal models).
'''

description_AGGRESSIVE_30 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Short description / intuition
- Default stance: defect. The strategy is aggressively self-interested: it will not give free contributions unless doing so is likely to purchase predictable, ongoing extra returns from others. It actively detects “responsive” (conditional) cooperators and then exploits them: it will seed cooperation only when that seeding reliably causes others to increase cooperation, then harvest by defecting on the next round(s). If exploitation is punished (others reduce cooperation after being exploited), AOE immediately stops seeding and reverts to safe permanent defection. The last round is always defected.

Key design goals:
- Aggressive (exploit cooperators whenever profitable; do not give free rides).
- Adaptive (learns from observed conditional responses of others).
- Robust (falls back to permanent defection if exploitation is not working or is risky).

Notation
- n, k, r: given game parameters.
- t: current round (1..r).
- rem = r - t + 1 (rounds remaining including current).
- history: for each past round s < t we observe vector of contributions c_j,s ∈ {0,1} for all players j.
- I: index of this strategy's player.
- Others = players j ≠ I.

Internal algorithm parameters (suggested defaults; implementer can tune):
- W = min(10, t-1): lookback window in rounds when computing statistics (use up to last 10 rounds for stability).
- E = 1: endgame length in which we always defect (at minimum, always defect in final round). Can be set to a larger small integer (e.g., 2 or 3) in noisy environments.
- resp_threshold = 0.08: minimal per-opponent responsiveness to treat them as “responsive” (see definition below).
- group_resp_threshold = 0.06: minimal average group responsiveness to consider doing an exploit seed.
- min_rem_for_exploit = 2: need at least one future round after a seed to harvest.
- exploitation_cycle = 2: typical cycle is “seed (cooperate) one round, harvest (defect) the next round”.
- punishment_tolerance = 0.02: tolerance for temporary fall in others’ cooperation after exploitation; if cooperation drops more than this persistently, stop exploit strategy.

Definitions computed from history (using last W rounds):
- f_j(C) = fraction of rounds in window in which player j cooperated.
- f_others_given_I_C = average over others of P(j cooperates in round s | I cooperated in round s-1) computed empirically from transitions in history.
- f_others_given_I_D = average over others of P(j cooperates in round s | I defected in round s-1).
- R_group = f_others_given_I_C - f_others_given_I_D (average incremental cooperation by others following my cooperation).
- For each j define R_j similarly; count responsive_count = # of j with R_j ≥ resp_threshold.

High-level decision rules (natural language)
1. Last round(s): If rem ≤ E, defect. (Aggressive endgame.)
2. Safety default: If insufficient history (t == 1) or if there is evidence that attempted exploitation is being punished (see punish detection below), default to defect.
3. Opportunistic exploitation:
   - If rem ≥ min_rem_for_exploit and R_group ≥ group_resp_threshold (i.e., my cooperating reliably raises others’ cooperation rates) then attempt an exploitation cycle:
     a. Seed round (cooperate) when doing so will not be the very last allowed cooperating round to harvest (i.e., ensure rem ≥ exploitation_cycle).
     b. Next round(s): defect to harvest the extra public-good contributions induced by the seed.
   - If multiple responsive players exist (responsive_count large), exploitation is higher-value and more persistent seeding may be profitable; but always stop seeding if exploitation is punished.
4. Otherwise (no sign of responsiveness or exploitation being punished), defect.

Punishment detection and retreat
- After any exploitation (i.e., seed+harvest attempt), measure cooperation levels among others in the rounds immediately after exploitation. If cooperation among others drops by more than punishment_tolerance below its pre-exploit baseline and that drop persists for at least 2 rounds, label exploitation “punished” and stop seeding permanently (revert to permanent defection for the rest of the game, aside from re-checks if the opponent behavior changes drastically).
- This ensures AOE does not keep investing into a population that retaliates or adapts to stop being exploited.

Tie-breaking and noise
- Use smoothed empirical frequencies (Laplace smoothing or small pseudocount) to avoid zero-division or wild swings from single observations.
- When history is too short (t small), be conservative: defect until enough data collected (W≥2) to estimate R_group.

Pseudocode (concise)
Note: uses functions that compute empirical probabilities over last W rounds.

initialize:
  E = 1
  W_window_max = 10
  resp_threshold = 0.08
  group_resp_threshold = 0.06
  min_rem_for_exploit = 2
  exploitation_cycle = 2
  punishment_tolerance = 0.02
  exploited_recently = false
  exploitation_punished = false
  last_action = D (for t=1, treat as D)

for each round t = 1..r:
  rem = r - t + 1
  W = min(W_window_max, t-1)

  if rem <= E:
    play D
    record last_action = D
    continue

  if t == 1:
    play D
    record last_action = D
    continue

  if exploitation_punished:
    // safe permanent defection after punishment
    play D
    record last_action = D
    continue

  // compute empirical conditional cooperation:
  if W < 1:
    // insufficient data
    play D
    record last_action = D
    continue

  compute f_others_given_I_C and f_others_given_I_D using rounds s in {t- W, ..., t-1}:
    - for each s in that window where my action at s was C, note fraction of others who cooperated at s+1 (if s+1 exists)
    - similarly for my action D
    - average across s and across opponents (use smoothing)
  R_group = f_others_given_I_C - f_others_given_I_D

  // Opportunistic exploitation condition
  if R_group >= group_resp_threshold and rem >= min_rem_for_exploit:
    // Decide whether to seed now or wait:
    // If last_action was D (we can seed now). If last_action was C, we are already in a seed -> harvest next.
    if last_action == D:
      // Seed now (cooperate) to trigger others next round
      play C
      exploited_recently = true
      record last_action = C
      continue
    else:
      // last_action == C (we seeded last round) -> harvest now
      play D
      record last_action = D
      // After harvest, evaluate whether exploitation was punished:
      // compute baseline cooperation among others before seed (average cooperation in W window)
      baseline = average cooperation rate of others in window (pre-seed)
      post = cooperation rate of others in the round(s) after the harvest (use 1 or 2 rounds if available)
      if baseline - post > punishment_tolerance and post stays low for next round:
        exploitation_punished = true
      continue

  // default aggressive stance: no reliable exploitation opportunity
  play D
  record last_action = D

Edge cases & special notes
- Very small rem (e.g., t = r-1): Seeding would be wasted if there is no harvest round after it. AOE avoids seeding unless there is at least one harvest round remaining.
- Very small t or W: AOE defects until it has enough history to detect responsiveness.
- High k close to n: All-cooperate socially optimal but defect still strictly dominates single-round. AOE can exploit unconditional cooperators (those who always cooperate regardless of my actions) by simply defecting each round; if many cooperators never retaliate, AOE will score high.
- If opponents are sophisticated conditional punishers who detect exploitation and permanently reduce contributions, AOE will detect the punishment (drop in other cooperation persists) and revert to permanent defection early to avoid further losses.
- If opponents are reciprocators who only increase contribution when they see sustained cooperation from me (not after a single seed), the algorithm’s R_group measure over a window detects this and AOE can switch to a longer seeding schedule (cooperate multiple consecutive rounds to build sustained triggering) before harvesting. Implementation can extend the exploitation_cycle to larger values when past data shows delayed responsiveness (this is an optional tunable).

Why this is “aggressive”
- It defects by default and only cooperates when there is strong, measurable evidence that cooperation will buy predictable extra returns.
- It actively exploits responsive cooperators by seeding cooperation only to harvest on subsequent rounds.
- It punishes quickly by abandoning exploitation permanently when opponents retaliate — i.e., it stops being a target and locks into defection to protect itself.
- It always defects in the final round, removing the last-round altruism trap.

Expected tournament behavior
- Against unconditional cooperators: AOE will defect every round and score very well.
- Against naïve reciprocators (who increase cooperation after my cooperation): AOE will seed and then harvest repeatedly, extracting value.
- Against grim or tit-for-tat-style group punishers: AOE will attempt exploitation, detect punishment, and then stop seeding (revert to safe defection), minimizing long-term losses.
- Against mixed populations: adaptive detection and short seeding cycles will capture exploitable gains while limiting exposure to punishment.

Implementation tips
- Use Laplace smoothing (add pseudocounts) when estimating conditional probabilities.
- Maintain per-player responsiveness R_j and a group average R_group — per-player stats allow targeted exploitation if some subset of players are exploitable.
- Cap how many consecutive seeds you perform without observing harvestable gains (prevents runaway cooperation in presence of false positives).
- Logging thresholds can be tuned by tournament organizers; the defaults above are conservative but aggressive.

This description provides a complete, parameterized decision rule and fallback behavior suitable for direct algorithmic implementation in a tournament.
'''

description_AGGRESSIVE_31 = '''
Strategy name: Aggressive Opportunistic Punisher (AOP)

High-level description (mindset)
- Be aggressive: default to defect to avoid being exploited, exploit any sustained cooperation by others, and punish persistent defectors harshly so cooperating strategies learn that tolerating defectors is costly. Only give cooperation grudgingly and only as a tool to extract future value (periodic/coordinated tests), never as naive unconditional trust. Defect in the final rounds where there is no future to influence.

Key ideas
- Default behavior = D (protect and exploit).
- Detect whether the population contains enough conditional cooperators to make occasional cooperation profitable; if so, exploit by mostly defecting but giving infrequent/coordinated cooperative moves to sustain others’ cooperation.
- If some players are persistent defectors while others cooperate, punish them by sustained defection to lower their payoff.
- Always defect in a short endgame window so you are not exploited by endgame “cooperate-until-the-end” strategies.

Parameters used by the strategy (computed from game parameters)
- endgame_horizon = min(3, r-1). (Always defect in the last endgame_horizon rounds.)
- recent_window W = min(5, t-1) when computing recent behaviour (use full available history when t-1 < W).
- p_high = 0.60 (baseline “high cooperation” threshold among other players). If k is large relative to 1 (i.e., mutual cooperation is more valuable), you may reduce this threshold slightly; implementers can set p_high = 0.60 * max(0.8, 1 - (k-1)/max(1,n-2)), but a fixed 0.60 is fine and robust.
- p_low = 0.15 (very low cooperation).
- periodic_coop_period M = 4 (when exploiting a cooperative population, cooperate once every M rounds to keep conditional cooperators from collapsing).
- punishment_length L = 3 (when punishing, defect for L rounds or until target’s recent cooperation rate improves).
- detection threshold for individual defectors: coop_rate_j < 0.10 over recent_window → label as persistent defector.

Observables maintained
- history matrix H[t'][j] for past rounds t' = 1..t-1 and players j = 1..n (1 for C, 0 for D).
- For each other player j compute recent coop_rate_j = average of H[last W rounds][j].
- group_recent_rate = average_j (coop_rate_j) over j ≠ i.
- m_prev_others = number of other players who cooperated in immediate previous round.

Decision rules (deterministic; small randomized variants optional)

Global edge rules (highest priority)
1. First round (t = 1): play D.
   - Rationale: aggressive start to avoid being a sucker and to probe opponents’ responses.

2. Endgame (t > r - endgame_horizon): play D.
   - Rationale: no future rounds to enforce cooperation → defect.

3. If currently inside an active punishment block (see Punishment state below) then play D.

Main decision flow (applies when not in endgame or first round or punishment)
A. Compute recent statistics:
   - For each j ≠ i: coop_rate_j over last W rounds (or available rounds).
   - group_recent_rate = mean_j coop_rate_j.
   - persistent_defectors = { j | coop_rate_j < 0.10 }.

B. If there is at least one persistent defector AND group_recent_rate ≥ p_high:
   - Enter Punishment mode targeted at the persistent defectors:
     - Set punishment_timer = L (an internal state).
     - Play D this round (and for the next L-1 rounds unless the punished players raise their coop_rate above 0.1 more quickly).
     - While in Punishment mode, keep recomputing coop_rate_j; you may exit early if target(s) raise coop_rate above 0.1.
   - Rationale: aggressive targeted punishment to reduce the benefit of free-riding in otherwise cooperative groups.

C. Else if group_recent_rate ≥ p_high (many others are cooperating recently) — Exploit mode:
   - Default: play D every round (exploit cooperators).
   - To keep conditional cooperators from collapsing you must occasionally return a cooperation signal: if (t mod M) == s where s is a fixed offset chosen at initialisation (e.g., s = 2), then play C. Use a deterministic offset so your pattern is predictable but infrequent.
   - If, after an exploit-round where you defected, group cooperation collapses to below p_low for two consecutive rounds, stop periodic cooperation and switch to the “rebuild/test” branch (below).
   - Rationale: exploit the high cooperation level to maximize your own payoff while maintaining the network of cooperators with only minimal investment.

D. Else if group_recent_rate between p_low and p_high — Conditional reciprocity mode:
   - Play C iff a majority of other players cooperated in the immediately previous round, i.e., m_prev_others ≥ ceil((n-1)/2). Otherwise play D.
   - Rationale: reciprocate recent cooperative majorities but don’t lead; this wins against conditional cooperators while not rewarding solitary defectors.

E. Else (group_recent_rate < p_low) — Rebuild/Test mode:
   - The population appears mostly defecting. Perform cheap tests to look for conditionals:
     - Every T_test = 5 rounds, play C once to probe whether others respond by cooperating. Otherwise play D.
     - If a test cooperation is followed next round by an increase in group cooperation by at least +max(1, floor(0.25*(n-1))) cooperators, treat that as evidence of conditional cooperators and switch to Exploit mode.
   - Rationale: avoid wasting many contributions in a defecting population but probe for opportunity to exploit conditional cooperators.

Punishment mode details (stateful)
- When you detect persistent_defectors while group cooperation is high, set punishment_timer = L and a target set = persistent_defectors.
- During punishment_timer > 0:
  - Play D.
  - Decrement punishment_timer each round.
  - Recompute coop_rate_j each round; if all targets raise coop_rate_j >= 0.10 before punishment_timer expires, exit punishment mode.
- Punishment reduces the target’s short-run payoff and signals aggression; it is deliberately harsh and short (L rounds) to be efficient.

Implementation notes (for algorithm designers)
- All thresholds are tunable; the ones above are chosen for robustness rather than optimality.
- Deterministic periodic cooperation keeps behavior reproducible in tournaments; a small randomized element (e.g., 10% chance to cooperate in Exploit mode instead of strict every-M rule) can sometimes do better against adaptive opponents, but deterministic is simple and robust.
- Keep per-player stats for targeted punishment (persistent_defectors). Because contributions are individually observed, you can punish by withholding contributions (everyone loses some public good) — this is costly but effective against defectors in cooperative populations.
- Avoid “forgiving too early”: punishment should persist for at least L rounds unless the target demonstrably repairs behaviour. But don’t punish forever — you want to remain able to profit from cooperation.

Why this is aggressive and robust
- Aggressive: default D, exploits large cooperative clusters by free-riding most rounds, punishes persistent defectors harshly when they parasitize cooperative groups, and never naively cooperates in the final rounds.
- Robust: adapts to statistical behaviour (group cooperation rate and per-player rates), probes to find conditional cooperators, defends against exploitation, and uses short targeted punishment windows to change opponents’ incentives with limited self-cost.
- Practical: only depends on game parameters (n,r,k) and the observed history; easy to implement state machine with small memory and clear deterministic rules.

Pseudocode (concise)

Initialize:
  endgame_horizon = min(3, r-1)
  W = 5
  p_high = 0.60
  p_low = 0.15
  M = 4
  L = 3
  punishment_timer = 0
  periodic_offset s = 2   # fixed

For each round t from 1..r:
  if t == 1: play D; continue
  if t > r - endgame_horizon: play D; continue
  if punishment_timer > 0:
    play D
    punishment_timer -= 1
    continue

  compute coop_rate_j for each j≠i over last min(W, t-1) rounds
  group_recent_rate = mean_j coop_rate_j
  persistent_defectors = {j | coop_rate_j < 0.10}
  m_prev_others = number of other players who cooperated in round t-1

  if persistent_defectors not empty and group_recent_rate >= p_high:
    punishment_timer = L
    play D
    continue

  if group_recent_rate >= p_high:   # Exploit mode
    if (t % M) == s:
      play C   # occasional maintenance cooperation
    else:
      play D
    continue

  if p_low <= group_recent_rate < p_high:  # Conditional reciprocity
    if m_prev_others >= ceil((n-1)/2):
      play C
    else:
      play D
    continue

  # group_recent_rate < p_low: Rebuild/Test mode
  if (t % 5) == 0:
    play C   # probe
  else:
    play D

End pseudocode

Final remarks
- This strategy is intentionally aggressive: it defects by default, exploits cooperative groups with mostly defection, and uses short, harsh punishments to make defecting costly to others. It is adaptive because it classifies players and population cooperation rates and switches modes accordingly. It is robust across parameter ranges and opponents because it (a) never trusts blindly, (b) periodically probes for opportunities, and (c) refuses to cooperate in the obvious endgame.
'''

description_AGGRESSIVE_32 = '''
Name: Aggressive Opportunistic Grudger (AOG)

Summary (one line):
Be aggressive: avoid being a sucker, exploit reliably identified cooperators via short “cooperate-then-harvest” cycles, and respond to backfire with fast, harsh group-level punishment (a grudging trigger) — always defect in the final round.

Key design principles
- Aggressive payoff objective: maximize my own sum payoff even if that lowers others’ payoffs.
- Opportunistic exploitation: only invest (cooperate) when enough others look reliably cooperative so that intermittent exploitation yields net benefit.
- Fast identification: use a short sliding window to detect cooperators and responsiveness.
- Harsh but bounded retaliation: punish collapses of cooperation quickly and strongly (can escalate to grim-style defection if necessary).
- Backward induction: always defect in the final round (no value in cooperating with no future rounds).

Notation
- t = current round (1..r)
- history: for each player j and each past round s < t we observe c_j,s ∈ {0,1} (1 if j cooperated)
- remaining_rounds = r - t + 1 (including current)
- gamma = k / n (marginal per-capita return)
- Default internal parameters (computed from n,k,r):
  - W = min(5, max(1, floor(r/10))) — sliding window size for reputation (use at most 5 most recent rounds; at small r this shrinks)
  - p_ind = 0.6 — threshold to call a player “cooperative” inside window
  - frac_req = max(0.5, min(0.75, 1 - gamma/2)) — required fraction of players who look cooperative before we attempt exploitation (more willing to try cooperation when gamma is higher)
  - S_coop = 3 — number of consecutive cooperate rounds we use to sustain a cooperative state before a 1-round harvest (if remaining rounds allow)
  - P_punish = min( max(2, ceil(r/10)), remaining_rounds - 1 ) — punishment length (at least 2 rounds, up to ~10% of total rounds, but never use up the final round)
Notes: the constants above are tunable; they are deterministic functions of (n,k,r) via gamma and r.

Decision rules — high level
1. Last round: If t == r, play D (defect).

2. Always avoid being a one-shot sucker:
   - Round 1: play D.

3. Classification (at start of each round t > 1):
   - For each player j (including myself, but use opponents’ rates), compute coop_rate_j = average c_j,s over the last W rounds (or over all past rounds if fewer).
   - Let cooperative_players = set of players with coop_rate_j ≥ p_ind.
   - Let coop_fraction = |cooperative_players| / n.

4. Entering / maintaining exploitation mode:
   - If coop_fraction ≥ frac_req AND remaining_rounds ≥ (S_coop + 1):
     - We treat the group as “sustainably cooperative enough” to try cyclical exploitation:
       - Cycle: cooperate for S_coop rounds to help maintain others’ propensity to cooperate, then play one exploitation round (D) to harvest the private + public good share.
       - Implementation detail: keep an internal counter cycle_pos that resets when entering this mode. On cycle_pos in {1..S_coop} play C; on cycle_pos == S_coop+1 play D, then reset cycle_pos=1.
   - If coop_fraction ≥ frac_req but not enough remaining rounds to run a full cycle:
     - Only cooperate if at least one following round exists where we can exploit (i.e., remaining_rounds ≥ 2); otherwise defect (prevent being last-round sucker).

5. Harsh response (punishment) if exploitation backfires:
   - If we were in exploitation mode and performed an exploitation-defect (the D in the cycle), then observe the immediate response: if the next round(s) show a drop in coop_fraction by ≥ 0.20 (20 percentage points) or the number of cooperative_players drops below frac_req, treat this as “cooperation collapsed due to our exploit.”
   - In that case: switch to Punish Mode — play D for P_punish consecutive rounds (or until the remaining rounds are too few to justify further punishment). This is designed to reduce the deviators’ future payoffs quickly (harsh retaliation).
   - After finishing P_punish rounds, re-assess classification using the sliding window and either re-enter exploitation mode if coop_fraction recovered to ≥ frac_req, or remain in Defect mode if not.

6. Default safe behavior:
   - If coop_fraction < frac_req (not enough reliably cooperative players), play D (defect) — do not give free cooperations when the group is not reliably cooperative.

7. Grudging (escalated) trigger:
   - If punishment does not restore a sufficient fraction of cooperators after a second exploitation attempt (i.e., two times exploitation caused collapse), escalate to Grim: defect for all remaining rounds (permanent defection), since further attempts to sustain cooperation are futile and continuing to punish may waste rounds better spent defecting.

Edge cases and clarifications
- Very small r:
  - If r == 2: round 1 play D, round 2 (last) D.
  - If r == 3: round 1 D, round 2 apply classification on round 1 only then generally defect (no reliable chance to build cycles); round 3 D.
- If W > t-1 (not enough history), compute coop_rate over available rounds (s = 1..t-1).
- Self-cooperation accounting: the strategy tracks others only to decide behavior. You should not condition your classification threshold on your own past moves except that your play influences others’ coop_rate observed in the window.
- Ties and borderline cases: if coop_fraction equals frac_req exactly, treat as “good enough” to attempt exploitation.
- Parameters deterministic: all thresholds are deterministic functions of n,k,r as described; no randomness required. If implementer prefers to add stochasticity to prevent predictability, a small randomization when choosing whether to cooperate in the S_coop rounds is acceptable but not required.

Pseudocode (concise)
Initialize:
  gamma = k / n
  W = min(5, max(1, floor(r/10)))
  p_ind = 0.6
  frac_req = max(0.5, min(0.75, 1 - gamma/2))
  S_coop = 3
  cycle_pos = None
  in_exploit_mode = false
  in_punish_mode = false
  punish_remaining = 0
  punish_failures = 0

For each round t = 1..r:
  remaining = r - t + 1
  if t == r: play D; continue
  if t == 1: play D; continue

  if in_punish_mode:
    play D
    punish_remaining -= 1
    if punish_remaining == 0: in_punish_mode = false
    continue

  Compute coop_rate_j for each player j over last W rounds (s in max(1,t-W)..t-1)
  cooperative_players = {j | coop_rate_j >= p_ind}
  coop_fraction = |cooperative_players| / n

  If in_exploit_mode:
    if coop_fraction < frac_req:
      in_exploit_mode = false
      cycle_pos = None
      play D
      continue
    # perform cycle
    if cycle_pos is None: cycle_pos = 1
    if cycle_pos <= S_coop:
      play C
      cycle_pos += 1
      continue
    else: # harvest round
      play D
      cycle_pos = 1
      # evaluate next round: if coop_fraction next round drops by >=0.20 or falls below frac_req:
      # (this evaluation is done at start of next round — see punishment rule)
      continue

  # Not currently exploiting
  if coop_fraction >= frac_req and remaining >= S_coop + 1:
    in_exploit_mode = true
    cycle_pos = 1
    play C
    continue

  # default
  play D

Punishment trigger (evaluated at start of any round after an exploitation harvest):
  If previous round was an exploitation D by us and coop_fraction now decreased by >= 0.20 OR coop_fraction < frac_req:
    punish_remaining = P_punish (computed as min(max(2, ceil(r/10)), remaining - 1))
    in_punish_mode = true
    punish_failures += 1
    in_exploit_mode = false
    cycle_pos = None
    If punish_failures >= 2: permanently set in_grim = true and defect forever

Rationale and expected behavior
- The strategy never cooperates blindly — it requires an observable, recent, and significant fraction of players to be reliably cooperative.
- When many opponents appear reliably cooperative, AOG invests a short burst of cooperation (S_coop rounds) to sustain their cooperation signal, then defects once to harvest the extra payoff that defectors receive while still allowing the group to remain mostly cooperative. This yields frequent per-cycle excess profit compared with always-cooperating.
- If exploitation causes cooperation to collapse, AOG punishes fast and harshly (P_punish rounds of full defection), which is intended to both reduce their short-run gains and deter other players from punishing the exploiter too softly. If cooperation repeatedly collapses, AOG gives up and defects for the remainder (grudging trigger).
- Always defecting in the final round prevents sucker outcomes from one-sided cooperation when no future rounds remain.

Why this is “aggressive”
- Starts defecting (no trust).
- Actively seeks to exploit reliably cooperative opponents via cyclical harvests instead of sustaining mutual cooperation.
- Uses fast, punishing retaliation to discourage attempts to punish us into cooperative compliance; punishment harms group payoffs and is used to maximize our net advantage.
- Escalates to permanent defection if the group proves unreliable, prioritizing own total payoff over any group welfare.

Implementation notes for tournament coding
- Track per-player cooperation counts for the last W rounds efficiently.
- Keep flags for modes (exploit/punish/grim) and cycle position.
- All thresholds are deterministic functions of n,k,r so the strategy is fully specified before play; no private signals or communication are required.

This strategy is aggressive, adaptive (it adapts to observed cooperative frequencies), and robust (it falls back to defect when cooperation is not sustained and punishes collapse to protect future payoff).
'''

description_AGGRESSIVE_33 = '''
Name: Aggressive Grim-Prober (AGP)

Summary (one line)
- Default: defect; probe briefly to find reciprocators; only cooperate when near-unanimous cooperation has been sustained; punish any betrayal with prolonged permanent-style defection; always defect in the last round. This is explicitly aggressive: it seeks to exploit one-shot gains, tests for exploitable cooperators only briefly, and retaliates harshly and quickly to deter or punish others.

Key derived values (computed from game parameters)
- m = k / n (marginal per-contribution share)
- probe_rounds = max(1, min(3, floor(r/4)))  // a short probing window
- epsilon = min(0.15, 1 / r)                 // small probability to probe by cooperating
- guard = max(1, floor(r/5))                 // avoid risky cooperation near the end
- punish_len = max(2, ceil(2 / (1 - m)))     // punishment duration; scales up when cooperating is almost not costly (m close to 1)

High-level behavior
- First, last-round rule: always defect in the last round (no credible future punishment).
- Probing: in the first few rounds AGP occasionally cooperates with small probability to detect reciprocators.
- Cooperation window: AGP will only switch to cooperation if unanimous cooperation has occurred for at least two consecutive rounds and there remain enough rounds to benefit (i.e., not within the guard period); this protects against one-off cooperators and opportunistic free-riding.
- Punishment: if AGP cooperates and any player defects in the same round (i.e., they “betray” while AGP contributed), AGP enters a punishment phase and defects for punish_len rounds (or until the game ends). Punishment is heavy and immediate — designed to make deviation costly for others.
- Default: in all other situations AGP defects to exploit cooperators and avoid being exploited.

Rationale (aggressive alignment)
- Aggressive: prioritizes exploiting one-shot defection gains whenever cooperation is not reliably enforceable; punishes betrayals loudly and quickly (deterrence by severity); limits forgiveness and limits the window for trusting others.
- Adaptive: probes to discover any reciprocating opponents; only escalates to cooperation if clear, sustained mutual cooperation is observed.
- Robust: because it requires unanimous cooperation before it contributes, AGP minimizes being exploited by mixed groups; punishment duration is calibrated from m so the cost to deviators exceeds typical one-shot deviation gains where possible.

Specification / Pseudocode

State variables maintained from history:
- consecutive_unanimous (integer, initially 0): how many prior rounds in a row had total_cooperators == n
- punishment_counter (integer, initially 0): rounds remaining to stay in punishment mode

On each round t (1-based), given full history (actions of all players for rounds 1..t-1):

1. Compute derived quantities m, probe_rounds, epsilon, guard, punish_len as above.

2. Edge cases:
   - If r == 1: return D (single-shot defection).
   - If t == r: return D (last round defection).

3. Update consecutive_unanimous and punishment_counter from the previous round (if t > 1):
   - Let total_cooperators_prev = number of players who played C in round t-1.
   - If total_cooperators_prev == n then consecutive_unanimous += 1 else consecutive_unanimous = 0.
   - (punishment_counter would already have been set in the moment of betrayal; it decrements each round we remain in that state — see step 6.)

4. If punishment_counter > 0:
   - punishment_counter -= 1
   - return D

5. Probing phase (t ≤ probe_rounds):
   - With probability epsilon: play C (probe).
   - Otherwise play D.
   - If you play C this round and observe any player j play D in this same round, then after the round set punishment_counter = punish_len (see step 6).
   - (Note: probing is randomized to guard against deterministic exploitation.)

6. Main cooperative test (applies when t > probe_rounds and not in punishment):
   - If consecutive_unanimous ≥ 2 AND t ≤ r - guard:
       - Cooperate (play C).
       - After the round: if any player defects in the same round (any D while you played C), immediately set punishment_counter = punish_len.
   - Else:
       - Defect (play D).

7. Maintenance: after each round, if AGP played C and any other player played D in that same round, set punishment_counter = punish_len (overrides any previous smaller counter). If AGP played D, no new punishment is triggered by AGP (we punish only in response to being betrayed while cooperating).

Notes on parameters and edge behavior
- Short games (small r): probe_rounds collapses to 1 and guard is small; strategy essentially defects most rounds and probes once.
- Last-round defect: because unilateral defection dominates in the last round, AGP never cooperates there.
- Punishment scaling: punish_len increases as m approaches 1 (cooperation becomes less costly relative to private payoff), because deviators then gain less per deviation — we lengthen punishment to ensure deterrence. The exact formula is conservative and keeps punishments finite.
- Forgiveness: AGP allows limited forgiveness only via the expiration of punishment_counter; but AGP requires sustained unanimous cooperation (consecutive_unanimous ≥ 2) before resuming cooperation. This is intentionally harsh to deter opportunistic strategies.

Examples of behavior (illustrative)
- Against always-defecters: AGP defects every round (maybe probes once or twice) and obtains the same payoff as other defectors — minimal loss.
- Against naive cooperators (always-cooperate): AGP will try to exploit them in most rounds (because it defects by default); if unanimous cooperation ever occurs twice and not in the guard zone, AGP might switch to C for some rounds — but it will immediately defect if any of those cooperators ever defect while AGP cooperated.
- Against conditional cooperators (Tit-for-Tat style): AGP’s probing can reveal reciprocators; if unanimous cooperation happens for two rounds and there are sufficient rounds left, AGP cooperates to capture mutual gains. If any reciprocator defects while AGP cooperated, AGP punishes harshly.

Why this is robust
- Does not rely on conventions, precommitments or communication.
- Uses observable history only (player actions each round).
- Probing enables discovery of cooperative opponents but is short and random to limit exploitation.
- Requiring consecutive unanimous cooperation is conservative and defends against mixed groups and opportunists.
- Punishment is strong and immediate to make defection costly in a repeated setting (deterrence), aligning with an aggressive goal to maximize payoff relative to others.

Implementation notes for the tournament
- The strategy needs access to the full action history each round.
- It requires randomization only during the probe rounds (epsilon). Use a pseudo-random generator seeded appropriately.
- Keep counters (consecutive_unanimous, punishment_counter) in persistent state across rounds.

Concise decision summary
- Last round → D.
- While in punishment → D.
- During the brief probing window → C with small probability epsilon, else D.
- After probing: cooperate only if there were at least two consecutive unanimous-cooperation rounds recently and you are not too close to the end; otherwise defect.
- If you cooperate and anyone defects that round → enter punish mode for punish_len rounds (defect during punishment).

This strategy is intentionally aggressive: it defects by default to exploit cooperators, uses brief probing to find exploitable reciprocity, requires strong evidence of group-wide cooperation before contributing, and punishes betrayals harshly and promptly.
'''

description_AGGRESSIVE_34 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Short description (mindset): Always prioritize immediate personal gain. Default to defecting and only invest (cooperate) when a short, calculated “test” can create a reliable stream of cooperating opponents to repeatedly exploit. If opponents punish your exploitation, stop investing and revert to permanent defection. Be stingy with forgiveness and never cooperate in the terminal rounds where backward induction makes cooperation futile.

Key design principles
- Aggressive: exploit any reliably cooperative opponents by defecting while they supply the public good.
- Opportunistic: probe rarely to discover exploitable groups; harvest aggressively if probes succeed.
- Adaptive: use recent history to detect exploitable opponents vs retaliators, and switch behavior accordingly.
- Robust: conservative probing (low cost) and fast retreat if exploitation is punished.

Parameters (derived from game parameters n, k, r)
- W = min(10, r-1) — history window for statistics.
- L_end = 1 — number of final rounds where we always defect (set to 1; can be increased to 2 for extra safety).
- T_test = max(2, floor(r / 10)) — interval between scheduled tests (rounds).
- H_harvest = min(5, max(1, floor(r / 6))) — number of rounds to exploit (defect) after a successful test.
- eps = 0.03 — small random exploration probability (very occasional cooperation outside tests).
- alpha_coop = 0.60 — threshold fraction of rounds in W that identifies a generally cooperative opponent.
- alpha_after_defect = 0.50 — threshold fraction of rounds an opponent cooperates after our defection (used to judge non-retaliators).
- punish_drop = 0.30 — if opponents reduce cooperation by this fraction following our exploitation, treat them as retaliators and stop probing.
- q_min = max(1, ceil(0.25 * (n-1))) — minimum number of other players that must look exploitable to justify attempting to seed/exploit.

You can tune these constants. Defaults above are conservative and work across a wide range of n, k, r.

High-level behavior summary
- Round 1: Defect.
- Default: defect every round.
- At scheduled test rounds (roughly every T_test rounds, excluding last L_end rounds), do a cheap probe: cooperate for one round (the test). Observe the response:
  - If many others cooperate in the test round (a successful test) and they appear non-punishing, enter an exploitation phase: play defect for H_harvest rounds to reap elevated payoffs while they continue cooperating.
  - If exploitation provokes a collapse in others’ cooperation (punishment), immediately stop probing/seeding forever and revert to permanent defection.
- Always defect in the last L_end rounds (including final round).
- Occasionally (with probability eps) cooperate outside of tests to check for changes in opponent behavior, but keep eps small.

Detailed decision rules (natural language + pseudocode)

State variables tracked:
- history: list of past rounds with each player’s action.
- my_rounds_D_count_in_window: number of my D rounds in window W.
- for each opponent j:
  - coop_rate_j = (# times j played C in last W rounds) / W
  - coop_after_my_D_j = (# times j played C in rounds immediately after a round where I played D, counted over last W rounds) / (number of my D rounds in the window, if >0)
- global_coop_fraction_in_last_round = (# of players who played C last round) / n

State machine: {PERMANENT_DEFECT, IDLE, TESTING, EXPLOIT, LOCKDOWN}
- Start state: IDLE
- PERMANENT_DEFECT: never probe again; always play D (entered when opponents punish exploitation).
- IDLE: default defecting every round; schedule tests every T_test rounds.
- TESTING: play C for exactly one round (the probe).
- EXPLOIT: after a successful probe, play D for H_harvest rounds to exploit cooperating opponents.
- LOCKDOWN (same as PERMANENT_DEFECT): if punishers detected, switch here.

Pseudocode (conceptual)
1. If current_round > r - L_end: play D (endgame safety).
2. If state == PERMANENT_DEFECT: play D.
3. With small prob eps (random exploration) and not in last L_end rounds: play C as a one-round probe (only if not in PERMANENT_DEFECT). Use these rare probes only to detect environment changes.
4. Otherwise:
   a. If this round is a scheduled test round (round t where t mod T_test == 0) and state==IDLE:
      - state := TESTING
      - play C (one-round probe)
      - after the round, evaluate response (see step 5).
   b. Else if state == EXPLOIT:
      - play D
      - decrement harvest counter; if harvest counter reaches 0, state := IDLE
   c. Else (normal default): play D

5. After a TESTING round, evaluate whether the test succeeded:
   - Let coopers_test = number of other players who played C in the test round (excluding me).
   - Let coopers_fraction_test = coopers_test / (n-1)
   - Compute for each opponent j their coop_rate_j and coop_after_my_D_j over window W (updated after current round).
   - Classify "good_targets" = opponents j with coop_rate_j >= alpha_coop AND coop_after_my_D_j >= alpha_after_defect.
   - Condition to start EXPLOIT:
     - If coopers_fraction_test is high enough: coopers_test >= q_min (i.e., at least q_min other players cooperated in the test), AND
     - There are at least q_min good_targets among opponents (they are both cooperative and not retaliatory), then:
         state := EXPLOIT; set harvest_counter := H_harvest; play D next rounds to harvest.
   - Else: remain in IDLE (no harvesting). Continue default defecting until next scheduled test.

6. After each exploitation phase, check for punishment:
   - Measure average cooperation rate of opponents during (a) pre-test window (W_pre = last W rounds before the test) and (b) the window of rounds immediately after the first exploitation defect (say next min(W, H_harvest) rounds).
   - If average cooperation after exploitation falls by more than punish_drop fraction compared to before, treat this as punishment -> state := PERMANENT_DEFECT (stop probing forever). Rationale: they are retaliators; further attempts to seed/exploit will be costly.

Edge cases and special handling
- First round: play D (fits default aggressive approach; also reveals others' willingness to cooperate).
- Last round and final L_end rounds: always D. Even if others cooperate in last round we exploit them (D) but do not seed because there is no future to harvest.
- If r is very small (r <= 3), reduce testing frequency and set H_harvest small (e.g., 1), or simply never test and always defect.
- If tests never succeed within a certain early fraction of the game (e.g., first r/2 rounds), optionally stop testing early and switch to PERMANENT_DEFECT to avoid wasting rounds.
- If many opponents become highly cooperative and never punish (persistent exploitable pool), AOE will continue exploit cycles (test -> harvest -> test ...) until punished or game ends.

Why this is aggressive and robust
- Aggressive: default D extracts the single-shot dominant action immediately; the strategy deliberately seeds cooperation only when the expected payoff from later repeated exploitation likely exceeds the seed cost. Harvesting (D while others C) yields strictly higher per-round payoff than cooperating.
- Exploitative mechanism: test rounds find pockets of cooperators, then exploitation harvests repeatedly while their cooperation persists.
- Robustness: uses empirical statistics over a window W to identify exploitable players vs punishers. If opponents punish (drop cooperation after exploitation), the strategy immediately ceases costly probing and locks into permanent defection — minimizing further losses.
- Low-cost probing: tests are single-round and infrequent; an optional small eps exploration lets the algorithm detect changing environments while keeping costs low.
- Simple safety: always defect in terminal rounds to avoid backward-induction losses.

Implementation notes for the tournament developer
- All thresholds (alpha_coop, alpha_after_defect, q_min, T_test, H_harvest, W, etc.) should be adjustable. The supplied defaults are conservative but should be fine as a baseline across many tournament populations.
- Keep careful book-keeping of rounds and per-player conditional statistics (especially "cooperate after my D") to detect retaliators.
- Use deterministic scheduling of tests (e.g., rounds 2, 2+T_test, 2+2*T_test, ...) with occasional stochastic exploration (eps) to avoid exploitable predictability in adversarial populations.
- To avoid being exploited by adversaries who fake cooperation just in test rounds to lure a defection and then punish, the strategy requires that exploitable players maintain cooperation after your defection (coop_after_my_D_j >= alpha_after_defect). This filters out “setup-and-punish” strategies.

Typical parameter defaults (copyable)
- W = min(10, r-1)
- L_end = 1
- T_test = max(2, floor(r/10))
- H_harvest = min(5, max(1, floor(r/6)))
- eps = 0.03
- alpha_coop = 0.60
- alpha_after_defect = 0.50
- punish_drop = 0.30
- q_min = max(1, ceil(0.25*(n-1)))

Summary one-liner: Mostly always defect; occasionally and cheaply probe for persistent cooperators; when a probe shows a reliable exploitable cohort, harvest by defecting for several rounds; if opponents punish exploitation, stop probing and permanently defect. This preserves aggressive exploitation where profitable while limiting losses when facing retaliators.
'''

description_AGGRESSIVE_35 = '''
Strategy name: Aggressive Exploit-and-Grim (AEG)

Design principles (what “aggressive” means here)
- Default to defection to avoid being exploited; only tolerate cooperation insofar as it can be exploited.
- Actively probe to detect exploitable cooperators, then harvest by defecting while they keep contributing.
- If cooperation collapses or opponents retaliate, switch to permanent defection (grim) to avoid surrendering long-term advantage.
- Always defect in the known final round (no credible future to sustain cooperation).

What the strategy uses
- Game parameters: n, r, k.
- Full history of past rounds: who contributed each round (the vector of c_j,t for all players including self).
- No private signals, no communication.

High-level behavior summary
- Round 1: Defect (aggressive baseline).
- Throughout play:
  - Monitor recent cooperation rate of the other n−1 players.
  - If others are sufficiently cooperative, exploit them by defecting every round (gain immediate advantage).
  - Periodically probe with a cooperative move to test whether others will reciprocate cooperation (which could create more exploitation opportunities).
  - If cooperation collapses (others stop cooperating after being exploited) or falls below a low threshold, switch to permanent defection (grim) for the rest of the game.
- Final round: Always defect.

Parameters (recommended defaults; implementer can tune)
- W: look-back window for recent behavior. Default W = min(5, r−1) (use fewer rounds when r is small).
- P_probe: probe period (how often we send a cooperation probe). Default P_probe = max(3, floor(r/6)).
- B_exploit: burst length for exploitation after detecting a cooperative environment. Default B_exploit = 1 (we always exploit as long as conditions hold).
- T_high (enter-exploit threshold): fraction of contributions by others in the last W rounds required to classify the group as “cooperative enough” to exploit. Default T_high = 0.6.
- T_low (collapse threshold): if the cooperation fraction falls below this, go to permanent defection. Default T_low = 0.35.
- epsilon: tiny randomization to break deterministic cycles. Default epsilon = 0.02 (2% random flip).
- Note: T_high and T_low can be adjusted based on k and n if desired (see justification below).

Pseudocode (natural-language + structured)

Initialize:
- state ← EXPLOIT (default aggressive state)
- rounds_since_last_probe ← 0
- rounds_in_grim ← 0

At the start of each round t (1 ≤ t ≤ r), decide:

1) If t == r (last round):
   - Play D (defect). End.

2) Compute coop_rate:
   - Let W_t = min(W, t−1). If W_t == 0 then coop_rate = 0.
   - coop_count = total number of C moves made by the other n−1 players in the last W_t rounds.
   - coop_rate = coop_count / ((n−1) × W_t).

3) State-transition rules:
   - If state == PUNISH (grim): remain in PUNISH forever (play D every round).
   - Else (state == EXPLOIT or PROBE):
       a) If coop_rate < T_low: // cooperation collapsed
           - state ← PUNISH
           - Play D (and remain PUNISH from now on).
           - End.
       b) Else if state == PROBE:
           - Evaluate probe response after the probe round (see PROBE behavior below).
           - If probe showed reciprocation (others increased cooperation per rules below), set state ← EXPLOIT; else state ← PUNISH.
       c) Else // state == EXPLOIT:
           - If coop_rate ≥ T_high:
               - Remain EXPLOIT (we have exploitable cooperators). Play D.
           - Else:
               - Remain EXPLOIT but if rounds_since_last_probe ≥ P_probe, schedule a probe next round (enter PROBE on next decision step).

4) PROBING behavior:
   - When a probe is scheduled (enter PROBE for the next round):
       - In the probe round, play C.
       - After that probe round, observe the other players’ actions for M = min(3, r−t) subsequent rounds (or use the look-back window W to evaluate immediate response).
       - If the average cooperation rate of others in those M rounds (or in window W including probe) increases to ≥ T_high_response (set T_high_response = T_high or slightly lower, e.g., 0.55), treat the probe as successful: state ← EXPLOIT (we can exploit them).
       - If not, treat the probe as failed: state ← PUNISH (permanent defection).
   - rounds_since_last_probe resets to 0 after a probe (successful or not). Otherwise rounds_since_last_probe increments each round.

5) Randomization guard:
   - With small probability epsilon, flip the chosen action (C→D or D→C). This prevents overly predictable cyclic patterns that some opponents could exploit.

Example compact decision rule for the round (priority order):
- If t == r ⇒ D.
- Else if state == PUNISH ⇒ D.
- Else if probe scheduled for this round ⇒ play C (enter PROBE), then evaluate next rounds to set state.
- Else if coop_rate ≥ T_high ⇒ play D (exploit).
- Else if rounds_since_last_probe ≥ P_probe ⇒ schedule and perform a probe next round.
- Else ⇒ play D.

Edge cases and special handling
- First round (t = 1): W_t = 0 ⇒ coop_rate = 0 ⇒ state stays EXPLOIT and we play D. Rationale: aggressive baseline; defecting in round 1 immediately extracts any willing contributors.
- Very short games (small r): reduce W, P_probe accordingly. If r ≤ 3, probing is pointless: always defect except you may cooperate only if there is an a priori reason; so in practice always defect for r ≤ 3.
- Near the end: no probing within the last P_probe rounds — avoid probing if remaining rounds cannot sustain exploitation. Always defect in final one or two rounds.
- If all other players are observed to always defect (coop_rate = 0 over W), we immediately remain in PUNISH (permanent defection).
- When deterministic cycles are a risk (e.g., opponents that use long punish windows), the small epsilon randomness allows occasional cooperation to break cycles and detect exploitable strategies.

Why this is aggressive and robust
- Aggressive: Default is defection; the agent proactively exploits observed cooperators instead of trying to sustain mutual cooperation. When exploitation backfires (cooperation collapses or opponents retaliate), the strategy retaliates by permanently defecting (grim), minimizing future losses.
- Robust/Adaptive: The strategy reacts to observed frequencies of cooperation, probes to test for conditional cooperators, and has clear thresholds for switching to permanent defection. It does not assume reciprocity norms, commitments, or communication—only observed behavior.
- Low reliance on delicate balancing: Because defection is a per-round dominant action, the strategy favors secure extraction and uses probing only when it is plausibly profitable to try to trigger cooperation that can be exploited.

Parameter guidance and optional refinement
- Tuning T_high and T_low:
  - If k is relatively high (k closer to n), mutual cooperation produces large total payoff — there may be more value in encouraging and sustaining cooperation to exploit later. Consider raising P_probe (probe more), increasing W (longer memory), and raising T_high slightly to avoid misclassifying noise.
  - If k is low (close to 1), the value of cooperation is small and defection is safer; make T_high higher and T_low higher (be more pessimistic), and rely more on immediate exploitation.
- You can make thresholds functions of k and n if desired. Example heuristics:
  - T_high = clamp(0.55 + 0.4 * (k−1)/(n−1), 0.55, 0.85)
  - T_low = T_high − 0.25
  These increase the entry threshold when k is larger (more to exploit).
- If you expect noisy actions (mistakes), replace permanent PUNISH with a finite-length punishment: after collapse, defect for Q rounds (Q ≈ min(5, remaining rounds)) then revert to EXPLOIT and probe again. That is less “grim” but more forgiving.

Concrete simplified implementation (ready to implement)
- Defaults: W = min(5, r−1), P_probe = max(3, floor(r/6)), T_high = 0.6, T_low = 0.35, M = 3, epsilon = 0.02.
- R1: D.
- For each round:
  - If last round → D.
  - Compute coop_rate over W.
  - If coop_rate < T_low → state=PUNISH → D forever.
  - Else if probe scheduled → play C this round; evaluate next M rounds; if others’ cooperation meets threshold → state=EXPLOIT else state=PUNISH.
  - Else if coop_rate ≥ T_high → D.
  - Else if rounds_since_last_probe ≥ P_probe → schedule probe next round.
  - Else → D.
  - With probability epsilon flip move.

Summary
- The AEG strategy is aggressively exploitative: default defection, opportunistic probing for exploitable cooperators, exploitation while cooperation persists, and permanent defection (grim) if cooperation collapses. It uses only game parameters and the publicly observable history, is simple to implement, and is robust to a wide range of opponent behaviors (including unconditional cooperators, conditional cooperators, and defectors).
'''

description_AGGRESSIVE_36 = '''
Name: Aggressive Grim with Reset (AGR) — an aggressive, history‑dependent enforcer that rewards unanimous cooperation but retaliates swiftly and (by default) permanently against any defection, with an optional limited-forgiveness variant.

High-level idea
- Open cooperatively to probe and capture gains from reciprocators.
- If any player defects even once, deny them future benefit by refusing to contribute thereafter (grim-trigger). That permanent withdrawal is the strongest credible threat you can make unilaterally in this game: by withholding your contribution you both (a) avoid being the sucker and (b) reduce the defector’s future payoff relative to what it would be under mutual cooperation.
- Always defect in the final round (exploit endgame).
- To avoid pathological loss against sophisticated strategies that never recover, a tunable variant uses long but finite punishment and demands a short, clear “all-cooperate” forgiveness sequence to reset.

Precise decision rules

Notation
- n, r, k: game parameters (given).
- t: current round index (1..r), decision is made based only on history up to round t−1.
- history[t’][j]: action of player j in round t’ (C or D). You observe full history.
- ever_defected: Boolean flag = true iff any player (including yourself) has played D in any round so far.
- last_round_all_C(): true iff in round t−1 every player played C.
- consecutive_allC(s): true iff the previous s rounds (t−s … t−1) were all-unanimous-cooperation.
- Variant parameters (tunable):
  - MODE = "grim" (default) or "finite-punish" (forgiving variant)
  - For finite-punish: P = punishment length in rounds (integer ≥ 1)
  - R_reset = number of consecutive unanimous-cooperation rounds required to reset from punishment to cooperating (≥1). (Common sensible choice: R_reset = 2.)

Default (most aggressive) parameter choices
- MODE = "grim" (permanent punishment)
- R_reset ignored (irrelevant)
- First round: start C (to attract cooperators)
- Last round t = r: always play D (exploit endgame)

Core policy (default aggressive / grim)
1. If t == 1: play C.
2. If t == r: play D.
3. If no defection has ever been observed up to t−1 (ever_defected == false):
   - Play C if last_round_all_C() is true OR t == 1 (initial cooperation).
   - Otherwise (someone defected in some earlier round), set ever_defected = true and go to step 4.
4. If ever_defected == true (grim mode):
   - Always play D for every remaining round (never return to C).

Rationale: You cooperate only while the whole group has been perfectly cooperative so far. The moment any defection is observed, you permanently withdraw cooperation; that maximally punishes defectors and protects you from repeated sucker losses. Because you defect in the final round, you gain the exploitive endgame benefit against any unresponsive cooperators.

Forgiving variant (finite punishment + explicit reset)
Use this if you prefer an aggressive-but-adaptive version that sometimes restores cooperation when the group re-establishes unanimous cooperation.

Parameters (choose before play):
- MODE = "finite-punish"
- P = punishment length (suggestion: P = max(2, ceil(r/4)) or P = min(r, ceil(n/(n - k)) + 1)). Larger P is more aggressive.
- R_reset = 2 (require two consecutive unanimous cooperation rounds to trust again)

Policy:
1. If t == 1: play C.
2. If t == r: play D.
3. Maintain a variable pun_until_round (initially 0).
4. On each round t, before choosing:
   - If pun_until_round ≥ t: you are currently punishing; play D.
   - Else (pun_until_round < t), check the last round:
     - If last_round_all_C() and pun_until_round == 0: play C (stay cooperative).
     - If last_round_all_C() and pun_until_round > 0:
       - If consecutive_allC(R_reset) is true: reset pun_until_round = 0 and play C.
       - Else: play C only if you choose to be lenient this round; otherwise play D.
     - If any defection occurred in round t−1 (someone played D):
       - Set pun_until_round = t + P − 1 (punish the group for P rounds starting now) and play D.

Edge cases and explicit rules
- First round: play C (probe).
- Last round (t = r): always D. This is aggressive and exploitative of endgame cooperators; it’s consistent with backward induction for rational opponents but will exploit naïve/cooperative opponents in tournaments.
- If others return to unanimous cooperation:
  - In grim mode you never restore cooperation.
  - In finite-punish mode you require R_reset consecutive unanimous-cooperation rounds to reset; until then you continue to defect.
- If you yourself ever defect (e.g., last-round defection), in grim mode the state is the same for others observing you; you treat any defection anywhere as a trigger for punishment.

Pseudocode (concise, default grim)
initialize ever_defected = false
for t = 1..r:
  if t == 1:
    play C
  else if t == r:
    play D
  else:
    if ever_defected == false:
      if round t−1 had all players play C:
        play C
      else:
        ever_defected = true
        play D
    else:
      play D

Pseudocode (finite-punish variant)
initialize pun_until_round = 0
for t = 1..r:
  if t == 1:
    play C
    continue
  if t == r:
    play D
    continue
  if t <= pun_until_round:
    play D
    continue
  if round t−1 had any D:
    pun_until_round = t + P − 1
    play D
    continue
  # last round unanimous C and not currently punishing
  if pun_until_round == 0:
    play C
  else:
    if consecutive_allC(R_reset):
      pun_until_round = 0
      play C
    else:
      play D

Why this is aggressive and robust
- Aggression: the grim default withdraws cooperation permanently after any defection. That makes defection by others costly because they lose the future benefit of your contributions. In a tournament with many strategies, the credible threat of permanent withdrawal is a primary lever to discipline free-riding strategies.
- Simplicity and clarity: your behaviour depends only on observable history (who cooperated when), which makes the threat transparent and easier for reciprocators to respond to.
- Exploitation of endgame: always defect in the last round to extract one-shot gains.
- Robustness: the finite-punish variant keeps the core aggressive posture but allows re-establishing cooperation when the group clearly re-coordinates (useful in tournaments with many different strategy types). Parameter P and R_reset are tunable depending on how risk-averse you are versus how punitive you want to be.

Parameter tuning guidance
- If you want the absolute most aggressive strategy: MODE = "grim", start C, then any defection → permanent defection; last-round D.
- If you want to balance aggression with opportunity: MODE = "finite-punish", choose P large (e.g., P ≈ r/3..r/2) to make punishment meaningful; choose R_reset small (1 or 2) so reset is possible but difficult.
- If you expect many short tournaments or want to avoid being locked into mutual defection late in play, lower P.

Notes on strategic logic
- One-shot logic: contributing is personally costly in a single round (D dominates C one-shot). Therefore credible future punishment (or reward) is required for sustained cooperation. The AGR strategy maximizes the credibility of your punishment by making it severe and easy for opponents to observe.
- Multi-player limitation: you cannot target punishment to a single defector — a defection triggers withholding of contributions that harms everyone — but that is sufficient as a deterrent in mixed populations because it makes the deviator lose access to your contributions.
- Tournament performance: AGR will exploit unconditional cooperators, discipline conditional cooperators who care about future rounds, and be robust against pure defectors. The forgiving variant is better when many players are noisy or when you expect occasional mistakes and want to restore cooperation.

Summary
- Default aggressive rule: start cooperating, cooperate only while unanimous cooperation persists, the first defection by anyone triggers permanent defection thereafter, and always defect in the final round.
- Tunable forgiving variant: replace permanent defection with a long, explicit punishment window P and require R_reset consecutive unanimous-cooperation rounds to restore cooperation.

This strategy is deterministic, depends only on game parameters and observed history, is simple to implement, and reflects an explicitly aggressive enforcement of cooperation.
'''

description_AGGRESSIVE_37 = '''
Name: Opportunistic Exploiter (aggressive)

Short description (one line)
- Be selfish and exploit cooperators whenever safe; probe rarely to discover exploitable cooperative opponents; avoid long, costly fights with coordinated punishers by conceding briefly if punishment is detected; always defect in the final round.

Design goals and attitude
- Aggressive: prefer immediate gains from defecting, exploit any sustained cooperation by others, and minimize losses from coordinated retaliation.
- Adaptive: use short-window statistics on opponents’ recent behaviour and a simple retaliation detector to switch modes (exploit / probe / retreat).
- Parameterized by game inputs (n, k, r) and only by observed history.

Notation
- t = current round (1..r)
- actions_t = vector of observed actions of all players in round t (C or D)
- my_action_t = our action in round t
- others_coop_t = number of other players (excluding us) who played C in round t
- w = window length for short-term statistics = min(5, t-1) (use up to 5 most recent completed rounds)
- remaining = r - t + 1 (rounds remaining including current)
- clamp(x, a, b) = min(max(x,a),b)

High-level modes
- Exploit mode: defect every round (except possible rare probes). Default because defect dominates immediate payoff.
- Probe mode: play C with small probability in early rounds to detect naive/cooperative strategies.
- Retreat mode: if evidence others punish our defection, temporarily concede (cooperate with modest probability) to restore cooperative flows over the medium run if that is profitable.

Parameter formulas (derived from n, k, r)
- probe_prob = clamp(0.20 * (k - 1)/(n - 1), 0.05, 0.25)
  - Rationale: more incentive to form public good when k is closer to n, so probe more when k is larger
- punish_detect_threshold = 0.15 (absolute drop in average others’ cooperation after our defections)
- punish_window_min = 3 (only detect punishment after at least this many relevant datapoints)
- retreat_coop_prob = clamp(0.6 * (k - 1)/(n - 1), 0.2, 0.8)
  - Rationale: if conceding makes sense, do so with higher probability when public good is more valuable
- probe_rounds = min(3, r - 1) (we never probe in last round)
- last_round_rule: always defect in round r

Decision rules (natural language)
1. Final round:
   - If t == r: Defect (D). (Backward induction: last round cooperation is exploitable.)

2. First few rounds (probing):
   - If t ≤ probe_rounds: play C with probability probe_prob, otherwise D.
   - Purpose: cheaply test whether some opponents are conditional cooperators or naïve unconditional cooperators that can be exploited later.

3. For rounds t in (probe_rounds+1) to (r-1):
   - Compute recent average cooperation among others over the last w rounds:
       p_recent = (1 / ((n-1) * w)) × Σ over last w rounds of others_coop
   - Compute two conditional averages (using all past rounds as available):
       p_after_I_defected = average others’ cooperation in rounds immediately after rounds where I defected
       p_after_I_cooperated = average others’ cooperation in rounds immediately after rounds where I cooperated
     (if there are fewer than punish_window_min data points for these, do not count them as evidence)
   - Punishment detection:
       - If (p_after_I_defected is defined) AND (p_after_I_cooperated is defined) AND
         (p_after_I_cooperated - p_after_I_defected ≥ punish_detect_threshold),
         then treat opponents as likely punishers (they reduce cooperation after our defection).
   - If punishment detected:
       - Enter RETREAT behaviour: to avoid prolonged mutual-loss, cooperate with probability retreat_coop_prob; otherwise defect.
       - Continue retreat until either:
           (a) you have accumulated at least K_recover = min(3, remaining-1) consecutive rounds where others’ cooperation returns to p_recent + 0.05, or
           (b) punishment evidence disappears (p_after_I_cooperated - p_after_I_defected < punish_detect_threshold).
       - Rationale: pay a modest short-term cost to rebuild cooperation only if such rebuilding is likely to give net gains over the remaining rounds.
   - Else (no punishment detected):
       - EXPLOIT: Defect (D).
       - Never voluntarily cooperate in exploitation mode (except probes already done), because immediate payoff from defecting is always higher and the strategy’s aim is to extract value from others’ cooperation, not to altruistically sustain it.

4. Edge cases and special handling:
   - If r is very small (r ≤ 2): never probe; defect every round (including 1) because backward-induction considerations make cooperating pointless.
   - If observed opponents’ cooperation is extremely high (p_recent ≥ 0.9) and remaining rounds are many (remaining ≥ 4): remain in Exploit mode (defect) — aggressive exploitation of near-universal cooperators yields big gains.
   - If the punishment detector yields mixed, low-confidence signals (too few datapoints), default to Exploit mode (defect), except for the pre-planned probes.
   - If n = 2 (a repeated PD-like case), the same logic applies: probe rarely, else defect; punish-detector will capture reciprocators and cause temporary retreat if retaliatory behavior is costly.

Pseudocode (structured, implementable)
- Inputs: n, k, r
- Internal state: history of rounds with own action and others_coop_t

Initialize:
- probe_prob = clamp(0.20 * (k - 1)/(n - 1), 0.05, 0.25)
- retreat_coop_prob = clamp(0.6 * (k - 1)/(n - 1), 0.2, 0.8)
- probe_rounds = min(3, r - 1)
- punish_detect_threshold = 0.15
- punish_window_min = 3
- K_recover = 3

Per-round decision(t):
1. If t == r:
     return D

2. If r ≤ 2:
     return D

3. If t ≤ probe_rounds:
     with probability probe_prob: return C
     else: return D

4. Compute w = min(5, t-1). If w == 0: (no past data) return D.

5. Compute p_recent = (sum_{last w rounds} others_coop) / ((n-1)*w)

6. Build sequences of rounds where our action at round s was D (or C), and look at others_coop_{s+1} (if s+1 ≤ t-1). Compute:
     - list_defected_followers = [others_coop_{s+1}/(n-1) for s where my_action_s == D and s+1 exists]
     - list_cooperated_followers = [others_coop_{s+1}/(n-1) for s where my_action_s == C and s+1 exists]
   If len(list_defected_followers) ≥ punish_window_min and len(list_cooperated_followers) ≥ punish_window_min:
     p_after_I_defected = mean(list_defected_followers)
     p_after_I_cooperated = mean(list_cooperated_followers)
     punish_detected = (p_after_I_cooperated - p_after_I_defected ≥ punish_detect_threshold)
   Else:
     punish_detected = False

7. If punish_detected:
     - If currently in RETREAT and stop conditions not met:
          with probability retreat_coop_prob: return C else D
     - Else (start retreat if punished):
          begin RETREAT mode (track consecutive recovery rounds counter = 0)
          with probability retreat_coop_prob: return C else D

8. Else (no punishment):
     return D

Additional implementation notes
- RETREAT mode should be stateful: once started, continue until recovery or punishment disappears. Recovery is judged by observing restored cooperation levels among others for K_recover consecutive rounds.
- Randomization is used only in probing and retreat-concession. Use a seeded RNG for reproducibility.
- Logging: store last actions and others’ cooperation counts; update the punishment detector each round.

Why this is aggressive
- Default posture is defection every round: that maximizes immediate payoff and is the simplest exploitation baseline.
- Probing is minimal and targeted to discover exploitable cooperators; probes are constrained to early rounds and small probability to limit risk.
- Upon detecting sustained high cooperation by opponents, the strategy exploits by continuing to defect; exploitation is the natural aggressive choice.
- Retreat mode is pragmatic, not moral: it is only to avoid long-term loss from coordinated punishers; it concedes temporarily when the cost of punishment exceeds the benefit of pure exploitation.
- Final-round defection ensures no vulnerability in the last round.

Robustness summary
- Works vs pure defectors (you defect and tie with them).
- Works vs naive unconditional cooperators (you defect and capture higher payoffs).
- Works vs conditional cooperators/tit-for-tat-like punishers (detector finds punishment and triggers a bounded retreat to avoid mutual destruction).
- Works across parameter ranges because probe_prob and retreat_coop_prob scale with k and n (more probing / more concession when the public good is more valuable relative to private endowment).

Tuning recommendations (practical)
- If you find opponents are rarely punished and cooperations are common, increase probe_prob toward its upper bound to exploit earlier.
- If you face frequent coordinated punishers, decrease exploit aggressiveness by increasing sensitivity (decrease punish_detect_threshold) so you retreat earlier.
- For tournaments where absolute worst-case loss must be minimized, raise retreat_coop_prob to repair cooperation quickly; for strictly maximizing own score in mixed pools, keep retreat modest.

Final note
- The strategy is intentionally aggressive: defect by default, exploit cooperators, probe cheaply, and only temporarily concede when evidence shows that punishment would cause larger cumulative losses. It uses only game parameters (n,k,r) and observable history; it contains no assumptions about opponents’ norms or communication.
'''

description_AGGRESSIVE_38 = '''
Strategy name: Aggressive Exploit-and-Punish (AEP)

Intuition (one line)
- Be aggressive: default to defect, systematically exploit any players who reliably cooperate, and respond to betrayal with harsh, simple punishment (grim). Probe rarely to discover exploitable cooperators and try brief, controlled attempts to rebuild cooperation only when the upside is large.

Parameters computed from game parameters (deterministic choices tied to n and r)
- W = min(5, max(1, floor(r/10))) — history window for recent behavior (small when r small, up to 5).
- group_exploit_threshold = 0.7 — if a large fraction of others were cooperating recently, exploit them.
- individual_cooperator_threshold = 0.85 — classify a player as a persistent cooperator if their cooperation rate over last W rounds ≥ this.
- rebuild_window S = min(2, max(1, floor(r/20))) — short, bounded number of rounds to attempt rebuilding cooperation.
- probe_prob = min(0.1, 3/max(1,r)) — small probability to probe for cooperators when group is mostly defecting.
- endgame_defect_rounds E = 1 (defect in the final round). Optionally set E = 2 if you want an even more aggressive endgame (recommended only if r is small).

State maintained from observed history
- For every player j ≠ me: coop_count_j in last W rounds (sliding window), coop_rate_j = coop_count_j / W.
- group_coop_rate = (sum of others' cooperations in last W rounds) / (W*(n-1)).
- flags:
  - in_rebuild_mode (remaining rounds to cooperate to attempt to restore mutual cooperation),
  - grim = False (if True, defect forever).

Decision rules (applied at the start of each round t = 1..r)
1. Endgame:
   - If t > r - E (i.e., in the last E rounds): play D (defect). Rationale: finite horizon backward induction makes last-round cooperation exploitable; be aggressively safe.

2. Grim override:
   - If grim is True: play D.

3. First round / default:
   - If t == 1: play D (start aggressive; probe later if needed).

4. Exploit persistent cooperators:
   - If any opponent j has coop_rate_j ≥ individual_cooperator_threshold: play D (exploit that cooperator).
   - Rationale: persistent cooperators can be harvested repeatedly; always defect to get 1 - k/n immediate advantage.

5. Exploit group cooperation:
   - If group_coop_rate ≥ group_exploit_threshold: play D (exploit a largely-cooperative group).

6. Rebuild cooperation (rare, controlled generosity):
   - If in_rebuild_mode > 0:
     - Play C this round, decrement in_rebuild_mode.
     - If any player defects while in_rebuild_mode (i.e., any c_j == 0 observed for this round), set grim = True and abort rebuild (defect forever afterwards).
     - Rationale: brief, committed attempt to rebuild cooperation, but cut off permanently on betrayal.

7. Probe when group is hostile:
   - If group_coop_rate ≤ 0.15 and with probability probe_prob: enter rebuild mode with in_rebuild_mode = S and play C this round (probabilistic probe).
   - Else: play D.

8. Default:
   - If none of the above triggered, play D.

Update rules after each round observation
- Slide the W-length window for each player j to update coop_count_j and coop_rate_j.
- Update group_coop_rate accordingly.
- If you cooperated during a rebuild window and observed any defection that round, set grim = True (as above).
- If you cooperated outside rebuild mode and are defected on by some players, do not change behavior other than updating coop rates (remain aggressive).

Pseudocode (concise)
- Initialize coop_count_j = 0 for all j ≠ me; in_rebuild_mode = 0; grim = False.
- For t = 1..r:
  - If t > r - E: play D; observe actions; update histories; continue.
  - If grim: play D; observe; update; continue.
  - If t == 1: play D; observe; update; continue.
  - Compute coop_rate_j over last W rounds for all j; compute group_coop_rate.
  - If any coop_rate_j ≥ individual_cooperator_threshold: play D; observe; update; continue.
  - If group_coop_rate ≥ group_exploit_threshold: play D; observe; update; continue.
  - If in_rebuild_mode > 0: play C; observe; if any other player played D in this round: grim = True; in_rebuild_mode = 0; else decrement in_rebuild_mode; update; continue.
  - If group_coop_rate ≤ 0.15 and random() < probe_prob: in_rebuild_mode = S; play C; observe; if any other player played D: grim = True; in_rebuild_mode = 0; update; continue.
  - Else: play D; observe; update; continue.

Comments on aggressiveness and robustness
- Aggressive components:
  - Default is to defect (maximize immediate payoff).
  - Systematically exploits any players who appear reliably cooperative (individual_cooperator_threshold), extracting a consistent edge.
  - Uses a grim mechanism: any explicit betrayal during a rebuild attempt is punished by permanent defection — harsh, simple, and credible.
  - Always defects in the endgame to avoid giving exploitable free payoffs.

- Adaptive components:
  - Uses short-window statistics (W) to adapt to changing behavior.
  - Probes rarely so it can discover exploitable cooperators without giving away persistent generosity.
  - Attempts short, controlled rebuilds (S rounds) only when the potential upside (many cooperators) justifies the risk; this allows the strategy to sometimes unlock larger mutual payoffs to be exploited later.

- Robustness:
  - Works without any assumptions about norms or promises.
  - Handles varying numbers of cooperators, tracks individuals (so it exploits those who habitually cooperate), and avoids being exploited long-term by punishing immediate betrayal with a simple, implementable grim rule.
  - Parameters W, thresholds and S are small functions of r and n so the strategy scales to different games; you can tune them conservatively (smaller S, larger thresholds) to be more aggressively exploitative.

Design notes / possible variants
- If you expect more sophisticated opponents who can target punishment, you can increase probe_prob and S to try to establish mutual cooperation and then exploit; if you expect highly retaliatory opponents, lower probe_prob to avoid being burned.
- If you want even more aggression, increase E to defect for more end rounds (e.g., E = 2 or 3).
- The strategy is intentionally simple and credible: punishment is unambiguous (grim), exploit decisions are transparent (based on measured cooperativeness), and probing is infrequent and controlled.

This description provides all information required to implement AEP in the tournament: it only depends on game parameters (n, r) to set W, S, probe_prob, and on observable history to make decisions.
'''

description_AGGRESSIVE_39 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

High-level idea (aggressive mindset)
- Default stance: defect. Extract immediate benefit whenever possible.
- Only attempt cooperation as a tactical, limited “offer” when many opponents have recently proven themselves to be frequent cooperators (so there is something to exploit).
- If the offer succeeds (others reciprocate during the offer window), switch to exploiting (defect every round) to reap the larger single-round payoff while others keep cooperating.
- If anyone defects during an offer, treat them as a traitor: punish and refuse to cooperate with them again (blacklist). Use a short but credible punishment window to discourage others from defecting during offers.
- Never cooperate in the final round (last-round defection is always safe and aggressive).

Everything depends only on n, k, r and observed history.

Parameters (computed from game parameters; implementer can change constants but these are the recommended deterministic formulas)
- lookback window W = min(5, max(1, floor(r/10))) — how many recent rounds we use to estimate opponents’ recent cooperation rates.
- offer length S = min(r-1, max(2, ceil(2 / (1 - k/n)))) — number of consecutive rounds we will cooperate when making a cooperation "offer" (scales up when exploitation advantage per round 1 - k/n is small).
- punishment length P = min(r-1, 2*S) — number of rounds we punish after a betrayal during an offer.
- individual cooperation threshold T_ind = 0.8 — a player is considered a frequent cooperator if they cooperated at least T_ind fraction in the last W rounds.
- group threshold for offering: require at least M = floor(0.6*(n-1)) other players to meet T_ind (i.e., a clear pool of likely cooperators). (If n small, interpret as "a majority of others".)
Notes: these parameter choices are deterministic functions of n,k,r + fixed constants (0.8, 0.6) so the strategy depends only on game parameters and history.

State maintained
- For every other player j: coop_rate_j (fraction of j’s contributions in last W rounds), blacklisted_j (boolean, initially false).
- mode ∈ {Default, Offering, Exploiting, Punishing}
- rounds_left_in_mode (integer)
- t = current round index (1..r)

Top-level decision rules (natural language + pseudocode)

Initialisation:
- mode = Default
- blacklisted_j = false for all j
- compute W, S, P from n,k,r as above
- t := 1

At start of each round (before choosing action):
- If t == r (last round) -> action = D (defect). End.
- Update coop_rate_j using the last min(W, t-1) rounds (if t=1 coop_rate_j undefined or 0).
- Count coopful = number of other players j with coop_rate_j ≥ T_ind and blacklisted_j == false.

Mode logic and actions:

1) Default mode
- Default action: D (defect).
- If (t ≤ r - S) and (coopful ≥ M):
    - We have identified a large pool of frequent cooperators. Start a cooperation offer:
    - mode := Offering
    - rounds_left_in_mode := S
    - action := C (cooperate) this round (start the S-round offer).
  Otherwise:
    - action := D

2) Offering mode (we are attempting to create a cooperation cluster)
- action := C (cooperate) for this round.
- After observing everyone’s actions in this round:
    - If any other player j defected in a round where we cooperated:
        - Mark those defectors j with blacklisted_j := true (they betrayed the offer).
        - Immediately switch to Punishing mode:
            - mode := Punishing
            - rounds_left_in_mode := P
            - Next round will be D.
    - Else (no one defected this round while we cooperated):
        - rounds_left_in_mode := rounds_left_in_mode - 1
        - If rounds_left_in_mode == 0:
            - The S-round offer completed without betrayal => assume a cooperation cluster formed among non-blacklisted players that we can exploit.
            - mode := Exploiting
            - rounds_left_in_mode := r - t  (exploit for the remainder or until collapse)
            - Next round will be D (we defect to exploit them).

3) Exploiting mode (we have a cooperating pool; we defect each round to free-ride)
- action := D (defect) every round.
- After each round observe responses and update coop_rate_j:
    - If group cooperation (fraction of other players cooperating) falls below 0.5 for two consecutive rounds (they reacted to exploitation by widespread collapse), or if remaining rounds are ≤ floor(S/2), then give up exploitation:
        - mode := Default
    - If a specific player j who had cooperated during the offer later defects while we were cooperating in the offer, they should already be blacklisted (this was handled in Offering); if anyone defects while we were exploiting and their behavior shows persistent defection and no willingness to return to cooperation, we mark them blacklisted permanently to avoid future offers including them.
- Continue defecting while in Exploiting mode.

4) Punishing mode
- action := D (defect) every round for rounds_left_in_mode rounds.
- rounds_left_in_mode := rounds_left_in_mode - 1 each round.
- If rounds_left_in_mode reaches 0:
    - After punishment window, re-evaluate coopful (with updated coop_rate_j excluding blacklisted players).
    - If coopful ≥ M and enough rounds remain to try another offer (t ≤ r - S):
        - Start a fresh Offering as in Default.
    - Else:
        - mode := Default

Edge cases (explicit)
- First round (t == 1): Default => defect.
- Final round (t == r): always defect.
- If r is very small and S >= r: will never offer (safe — always defect).
- If coop_rate_j cannot be computed because no prior rounds, treat as 0 for threshold checks (so we will not make an offer in round 2 unless proven cooperators exist).
- Blacklisting is permanent: once blacklisted_j == true, we never treat j as part of a cooperative pool and we never cooperate when that person is present (that reduces the chance of being exploited by traitors in future offers).

Why this is aggressive and robust
- Aggressive extraction: default is defect, and when a clear pool of cooperators is detected we deliberately initiate an offer only to create an exploitable cluster and then exploit it (defect every round while they keep cooperating). The single-round advantage of defection over cooperation (1 - k/n) is what ACE tries to capture repeatedly once the cluster is formed.
- Targeted probing: ACE does not randomly try to cooperate early; it requires empirical evidence (recent high cooperation rates) before making any costly offers. This avoids wasting cooperation where opponents are mostly defectors.
- Credible punishment: any betrayal during an offer triggers an immediate punishment window and permanent blacklisting of traitors. That raises the cost for opportunistic opponents who might try to betray only when extracting the last advantage.
- Adaptive: parameters scale with k (via S) so when the per-round exploitation advantage (1 - k/n) is small (k close to n), the strategy requires a longer offering window S to justify the investment; when exploitation advantage is large, S is small and the strategy exploits quickly.
- Endgame-safe: never cooperates in the final round, preventing naive endgame losses.
- Memory-limited and implementable: uses only sliding-window statistics and simple state variables; fits a tournament where many heterogeneous AIs play.

Pseudocode (compact)

Initialize: mode = Default; for all j: blacklisted_j = false; compute W,S,P,M;
for t in 1..r:
  if t == r: play D; continue;
  update coop_rate_j over last min(W, t-1) rounds (0 if none);
  coopful = count of j with coop_rate_j >= T_ind and not blacklisted_j;
  if mode == Default:
    if (t ≤ r - S) and (coopful ≥ M):
      mode = Offering; rounds_left = S; play C;
    else:
      play D;
  else if mode == Offering:
    play C;
    observe others’ actions this round:
      if any j defected this round:
        for each such j: blacklisted_j = true;
        mode = Punishing; rounds_left = P;
      else:
        rounds_left -= 1;
        if rounds_left == 0:
          mode = Exploiting; rounds_left = r - t; // exploit remainder
  else if mode == Exploiting:
    play D;
    observe group cooperation over recent rounds:
      if group cooperation collapsed (below 0.5 two rounds in a row) or remaining rounds small:
        mode = Default;
      // optionally mark persistent defectors blacklisted
  else if mode == Punishing:
    play D;
    rounds_left -= 1;
    if rounds_left == 0:
      recompute coopful;
      if (t ≤ r - S) and (coopful ≥ M):
        mode = Offering; rounds_left = S; // start another offer next round
      else:
        mode = Default;
  t = t + 1

Final notes and implementer tips
- Parameter constants (0.8 for T_ind, 0.6 for M fraction, lookback W minimums) are tunable; they trade off risk of false offers vs missed exploitation opportunities. The choice above errs on safety (don't offer unless many clear cooperators).
- Blacklisting is powerful but costly if misapplied — the design above only blacklists players who defect during an offer when we cooperated (clear betrayal).
- This strategy is explicitly aggressive: it prioritizes extracting immediate gains from cooperators, punishing betrayals, and refusing to cooperate in the final round. It is also adaptive: it only offers cooperation when history suggests there is an exploitable cooperative subset, and it scales the offer length with the per-round exploitation advantage (1 - k/n).

This description gives a complete, implementable decision rule set and handles first/last rounds, offering/punishment/exploitation dynamics, and edge cases.
'''

description_AGGRESSIVE_40 = '''
Name: Exploit-and-Punish (Aggressor)

High-level idea
- Default to defect to harvest the safe private payoff and to exploit unconditional cooperators.
- Probe early (briefly) to identify reliable cooperators, then free-ride on any sufficiently large, predictable cooperative subset when doing so is profitable over the remaining horizon.
- If I cooperate to form a coalition and someone “betrays” (does not reciprocate when I expected them to), I punish severely (short, high-impact punishments that are targeted by reputation), then forgive if they re-establish reliable cooperation.
- Always defect in the final round.

This strategy only depends on n, r, k and the full history of players’ actions; it does not depend on any off-game coordination.

Notation
- t: current round (1..r)
- remaining_rounds_after_current = r - t
- history: for each previous round s < t we observe every player's action a_j,s ∈ {C,D}
- coop_count_j = number of times player j cooperated in rounds 1..t-1
- coop_rate_j = coop_count_j / max(1, t-1)  (so defined even in t=1)
- others = set of players j ≠ me
- group_coop_rate = average_j∈others coop_rate_j
- k_over_n = k / n
- cost_of_cooperating_now = 1 - k_over_n  (this is always positive under game spec since k < n)

Core parameters (interpretable from game parameters; defaults below can be used by implementer)
- T_hi = 0.80  (threshold to call a player a “reliable cooperator”)
- T_lo = 0.30  (threshold to call a player “unreliable”)
- P_max = ceil(0.5 * r)  (maximum punishment length)
- beta = 0.5  (punishment length fraction of remaining game when triggered)
- M_min = 1  (minimum cooperative partners needed for a profitable coalition; actual threshold computed below)
- Probe_rounds = min(2, r-1)  (short initial probing window)
- Forgiveness_condition: player’s coop_rate_j since punishment <some small window> ≥ T_hi (rejoin)

Decision rule (natural-language + pseudocode)

Top-level:
- If t == r: play D (final round defect).
- Else if t == 1: play D (aggressive opening → exploit naïve cooperators).
- Else:
    - Update per-player coop_rate_j.
    - Check punishment state for any players (see Punishment rules below). If in active punishment phase, play D.
    - Otherwise compute whether cooperating this round can create a profitable coalition over the remainder of the game. If yes and coalition partners are sufficiently reliable, play C to try to form/maintain the coalition. Otherwise play D.

Coalition profitability test (concrete, parameter-free calculation using game parameters and history)
1. Let s_j be the set of opponents with coop_rate_j ≥ T_hi.
   Let s = |s_j| (number of reliably cooperative opponents).
2. If s = 0, do not cooperate.
3. Compute minimal s_needed such that cooperating now to encourage those s players to keep cooperating for the remaining rounds repays the immediate cooperation cost:
   - cooperating now vs defecting now gives immediate payoff difference: Δ_immediate = (k_over_n) - 1 = -cost_of_cooperating_now (negative)
   - if I cooperate now and those s players indeed continue to cooperate for the remaining_rounds_after_current rounds because of my cooperation, my extra per-future-round gain (compared to them falling to D) is roughly k_over_n * s.
   - Require: remaining_rounds_after_current * k_over_n * s > cost_of_cooperating_now
   - Rearranged threshold: s_needed = ceil( cost_of_cooperating_now / (remaining_rounds_after_current * k_over_n) )
   - If remaining_rounds_after_current = 0 then s_needed = ∞ (won’t cooperate because last round is already excluded).
4. If s ≥ max(M_min, s_needed) and coop_rate_j for those s players is consistently high (e.g., they cooperated last round and coop_rate_j ≥ T_hi), then play C (attempt coalition formation).
   - Rationale: cooperating only when there are enough reliable partners and enough remaining rounds to recoup the one-time cost.
5. Otherwise play D.

Probe policy (identification of cooperators)
- If during the initial Probe_rounds I have only defected so far and I observe players cooperating frequently, I keep defecting to exploit them (aggressive opening).
- If there are few rounds left but enough history to identify some partners (coop_rate_j ≥ T_hi), apply the coalition test above using the remaining rounds.

Punishment rules (aggressive, targeted, and measurable)
- Definition of betrayal: in any round t' where I cooperated expecting some player j to cooperate (j had previously coop_rate_j ≥ T_hi and cooperated in the previous few rounds), if j plays D at t', I mark j as betrayer.
- Punishment trigger: when a player j is marked betrayer, I enter a punishment phase:
   - Punishment_length = min(P_max, max(1, ceil(beta * (r - t')))). Intended to be a short but significant portion of remaining rounds.
   - While punishment phase is active, I play D unconditionally. (Punishment is public—everyone sees it.)
   - After Punishment_length rounds I reassess j’s behavior:
       - If since the end of punishment j’s local coop_rate_j (over a short window) ≥ T_hi, I remove the punishment label for j (forgive).
       - Otherwise, I reapply a punishment (escalate up to P_max) or move to permanent defection if endgame nears.
- Note: the punishment is not targeted in action (I cannot individually target one player's payoff), but the reputation label and the public D signal generally lower group cooperation; that harms betrayers as well as cooperators. The goal is to make betrayal unattractive relative to cooperating with me.

Selective exploitation vs mass cooperation
- If a large majority of players are reliably cooperative and the coalition test is satisfied, I will cooperate to exploit them repeatedly (I cooperate just enough to keep them cooperating under the assumption they are conditional).
- If reliable cooperators are few or remaining rounds are small, I defect.

Last-round consistency
- Final round (t == r): always D (no future to recoup cooperation cost).

Edge case handling
- t = 1: play D (start aggressively).
- r = 2: very limited horizon: only probe 1 if desired, but coalition test usually fails because remaining_rounds_after_current is tiny; so strategy will mostly D, D (defect both rounds) and punish seldom.
- n = 2: reduces to a 2-player repeated PD. Pseudocode still applies: coalition test becomes 1 opponent; if opponent reliably cooperates and many rounds remain such that cooperating is profitable, cooperate; otherwise defect. Punishment becomes classical revenge.
- If coop_rate_j are undefined (t = 1), treat all as coop_rate_j = 0 for safety (i.e., assume D until proven otherwise).
- If multiple players betray simultaneously, punishment is applied but the strategy retains the same rules—punish short and harsh then reassess.

Reasoning why this is aggressive and robust
- Aggressive: default defect, exploit unconditional cooperators, punish betrayals decisively, use short probes only when there is a mathematically justified chance to form a profitable coalition.
- Robust: no reliance on norms; decisions are computed from observed cooperation rates and the remaining horizon using only game parameters (n, r, k). It handles unconditional defectors (ignore them), unconditional cooperators (exploit them), conditional cooperators (try to form coalitions if profitable), and retaliators (punish and re-forge cooperation if they resume cooperating).
- Adaptive: reacts to changes in opponents’ behavior via coop_rate_j and recent rounds; punishment is temporary with a clear forgiveness pathway to avoid endless wars when opponents return to cooperation.

Pseudocode (concise)

Inputs: n, r, k, history up to round t-1
Constants: T_hi=0.8, T_lo=0.3, beta=0.5, P_max=ceil(0.5*r), Probe_rounds=min(2, r-1)

function decide_action(t, history):
    if t == r:
        return D
    if t == 1:
        return D
    compute coop_rate_j for each opponent j from history
    if in_active_punishment_phase_for_any_j:
        return D
    remaining = r - t
    if remaining == 0:
        return D
    S = { j in others | coop_rate_j >= T_hi and j cooperated in last round }
    s = |S|
    cost = 1 - k/n
    if s == 0:
        return D
    s_needed = ceil( cost / (remaining * (k/n)) )
    if s >= max(1, s_needed):
        return C
    else:
        return D

Punishment management (run after each round update):
- For any player j we expected to cooperate (coop_rate_j ≥ T_hi and I cooperated) who played D, mark betrayal and start punishment of length min(P_max, ceil(beta * remaining_after_betrayal))
- While punishment active: play D; decrement punishment timer each round. After timer 0, if player j’s recent coop_rate ≥ T_hi → forgive; else possibly reapply punishment (with diminished remaining rounds) or go to permanent D.

Implementation notes / parameter tuning
- T_hi, beta, P_max, Probe_rounds can be tuned for the tournament meta-population. A higher T_hi makes the strategy conservative (only trusts very steady cooperators); a smaller beta makes punishment shorter. Aggressive defaults above favor exploitation (high T_hi, significant beta).
- If implementers want lower variance, require S to include players who not only have coop_rate_j ≥ T_hi but who also cooperated in the last two rounds.
- The coalition test is deliberately conservative and uses the provable recoup condition derived from k, n, and remaining rounds; this avoids naïve cooperation that cannot be recovered.

Summary
- Always defect in the last round; open aggressively by defecting; exploit and free-ride on reliably cooperative players when there are enough of them and enough future rounds to recoup the cost of cooperating; punish betrayals with short, strong public defection phases and forgive if cooperation resumes. The decision uses only n, r, k and observed coop rates (history), is adaptive, and aims to maximize individual payoff aggressively while remaining robust to a wide variety of opponent types.
'''

description_AGGRESSIVE_41 = '''
Name: Adaptive Aggressor

High-level intent (aggressive mindset)
- Seek high personal payoff even at the expense of group payoff when profitable:
  - Aggressively punish defectors to deter long‑run free‑riding.
  - Opportunistically exploit persistent cooperators when retaliation risk is low (especially near the end).
  - Be adaptive: use observed histories to differentiate naïve cooperators, reciprocators and persistent defectors; respond differently.
- Do not rely on implicit coordination. All decisions use only the public history and the known parameters (n, k, r).

State / internal variables (kept from history)
- t: current round (1..r)
- rem = r − t + 1 (rounds remaining including current)
- history: for each past round s (< t) we know each player’s action c_j,s
- For each player j ≠ me, compute cooperation rate p_j = (# times j played C in history) / (t − 1) (if t=1, p_j undefined; treat as 0.5 for initialization)
- avg_coop = average of p_j over j ≠ me
- last_round_cooperators m_last = number of cooperators in round t−1 (if t=1, set m_last = 0)
- punishment_timer (integer ≥0): number of remaining rounds in which I must defect as punishment (0 means not currently punishing)
- recent_window W = min(6, max(1, t−1)) (used for short-term rates)
- short_term_p_j: cooperation rate of j in last W rounds
- Parameters derived from game parameters:
  - INITIAL_ROUNDS = min(2, r − 1)   // probe/cooperate briefly at start
  - PUNISH_BASE = max(1, ceil(3 / (1 − k/n)))   // base punishment length, scaled by how small 1−k/n is
  - PUNISH_MAX = r
  - P = min(PUNISH_MAX, PUNISH_BASE)   // punishment length (capped by remaining rounds when used)
  - MAJOR_DEFECT_FRACTION = 0.25  // if ≥ this fraction defected last round, treat as mass defection
  - PERSISTENT_DEFECTOR_THRESH = 0.4  // p_j < this => treat j as persistent defector
  - NAIVE_COOP_THRESH = 0.95  // p_j ≥ this => treat j as “naïve cooperator”
  - LATE_EXPLOIT_REMAIN = 3  // in last rem ≤ this rounds, exploit high cooperation
  - LATE_EXPLOIT_AVG = 0.8   // threshold avg_coop to consider late exploitation
  - FORGIVE_WINDOW = max(1, ceil(r/10)) // window to judge if cooperation has returned after punishment
  - EPSILON = 0.02 // tiny randomization to probe / avoid deterministic cycles

Decision rules (exact order; first applicable rule determines action)
1. Last round: if t == r => play D (always defect in final round; no future punishment possible).

2. If punishment_timer > 0:
   - Play D.
   - Decrement punishment_timer by 1 after the round (i.e., punishment_timer ← punishment_timer − 1).
   - Rationale: punish quickly and strongly.

3. Opening probe: if t ≤ INITIAL_ROUNDS:
   - Play C (cooperate to build a baseline and identify types).
   - Rationale: cooperating early induces information and may elicit cooperation from reciprocators.

4. Late-stage opportunistic exploitation:
   - If rem ≤ LATE_EXPLOIT_REMAIN and avg_coop ≥ LATE_EXPLOIT_AVG:
       - Play D (exploit a cooperative population near the end).
       - Rationale: low future retaliation risk; seize one-shot gains.

5. Immediate mass-defection trigger:
   - If m_last < n (someone defected last round) and fraction_defected_last = (n − m_last)/n ≥ MAJOR_DEFECT_FRACTION:
       - Set punishment_timer ← min(P, rem − 1) (never punish beyond final round).
       - Play D now (start punishment immediately).
       - Rationale: strong, fast punishment to deter cascades of defection.

6. Targeted retaliation for persistent defectors (less disruptive):
   - If m_last < n but fraction_defected_last < MAJOR_DEFECT_FRACTION:
       - Identify defectors in last round D_set.
       - If any j in D_set has short_term_p_j ≤ PERSISTENT_DEFECTOR_THRESH (i.e., they are persistent defectors):
           - Set punishment_timer ← min(P, rem − 1).
           - Play D now.
           - Rationale: punish persistent defectors aggressively while avoiding provoking the whole group when defection was isolated/rare.
       - Else (defection was isolated and by mostly otherwise cooperative players):
           - Play C (forgive isolated lapses; don’t start a destructive punishment cycle).
           - Rationale: be reluctant to punish occasional, likely accidental or endgame defections.

7. Exploit naïve cooperators (opportunistic, limited):
   - If exists j with p_j ≥ NAIVE_COOP_THRESH and rem ≤ ceil(r/4) and random() < 0.5:
       - Play D (one-shot exploit a near-unconditional cooperator when remaining rounds are limited to reduce retaliation risk).
       - After such an exploit, if many players reduce cooperation next round, proceed to punishment following rule 5 or 6.
       - Rationale: exploit unreactionary opponents but limit frequency (50% chance) to avoid predictable exploitation that fosters coordinated retaliation.

8. Cooperation maintenance:
   - If none of the above triggered:
       - Play C if avg_coop ≥ 0.5 (others are more cooperative than not).
       - Otherwise play D (if the group is predominantly defecting, avoid being repeatedly exploited).
   - Rationale: maintain cooperation if reciprocators are present; otherwise protect self.

9. Small stochastic probing:
   - With probability EPSILON, flip the chosen action (C↔D) to test opponents / avoid predictability.

Post-punishment forgiveness rule (how to end punishment and resume cooperation)
- After punishment_timer reaches 0, do not immediately return to unconditional cooperation. Observe the next FORGIVE_WINDOW rounds:
  - If average cooperation among others in that window ≥ 0.6, resume cooperative default (follow rule 8).
  - If not, remain defensive (play D except in late-exploit conditions).
- Rationale: reward genuine return to cooperation; avoid being exploited by temporary compliance followed by defection.

Implementation notes (pseudocode outline)
- Input: n, k, r, history
- Maintain punishment_timer across rounds.
- On each round:
  1. Compute t, rem, p_j, avg_coop, m_last, short_term_p_j.
  2. Follow decision rules 1→9 above.
  3. Update punishment_timer and history after the simultaneous round outcome.

Why this is aggressive and robust
- Aggressive:
  - Strong, immediate punishment (multi-round) for mass or persistent defection.
  - Opportunistic exploitation of naïve cooperators and late-stage exploitation when retaliation is unlikely.
  - Low forgiveness for persistent defectors: punishments are long (P scaled up when 1 − k/n is small).
- Robust:
  - Uses clear, history-derived statistics (per-player rates, short-term behavior, last-round snapshot).
  - Distinguishes isolated/one-off defections from persistent or mass free-riding — avoids over-reacting.
  - Stochastic probing prevents being exploited by deterministic extortioners and enables detection of changes in opponent behavior.
  - Always defects in final round — avoids being sucker in known-endgame.

Tuning guidance
- PUNISH_BASE and thresholds (MAJOR_DEFECT_FRACTION, PERSISTENT_DEFECTOR_THRESH, NAIVE_COOP_THRESH, LATE_EXPLOIT_REMAIN) can be tuned empirically against the tournament population. The provided defaults make the strategy noticeably aggressive but not suicidally punitive.
- If many opponents are highly retaliatory, reduce P and lower exploitation probabilities; if many opponents are naïve cooperators, increase opportunistic exploitation near the end.

Edge cases summary
- First round: cooperate (probe).
- Last round: defect (always).
- If punishment would extend into last round, cap it to avoid wasting punishment rounds.
- If history is empty for p_j, treat p_j as 0.5 for initial estimates to avoid early over-commitment.
- Very small r: INITIAL_ROUNDS min/max ensure we do not waste all rounds on probes; late exploitation logic handles very short games.

This strategy is implementable from public history only, is explicitly aggressive (punish early and hard; exploit cooperators late), and adapts to opponent behavior classes (naïve cooperator, reciprocator, persistent defector).
'''

description_AGGRESSIVE_42 = '''
Strategy name: Aggressive Exploiter with Conditional Punishment (AECP)

High-level description
- Default stance: defect (D). Aggressively exploit any detectable/cooperative opponents by defecting while they (still) cooperate.
- Learn from history using short windows of observed cooperation by others. If many opponents are cooperating, keep defecting to harvest the extra private payoff.
- If opponents clearly respond to your exploitation by reducing cooperation (retaliating), switch to a short, harsh punishment phase (still defecting) to signal that retaliation is costly, then go back to the default aggressive stance.
- Occasionally probe (small, controlled probability of cooperating) early on to detect latent conditional cooperators who only reveal themselves if someone cooperates first.
- Endgame: defect in the last rounds where retaliation is impossible.

This strategy only uses game parameters (n, r, k) and the full observable history of all players' past actions.

Parameters (suggested default values, adjustable)
- w: observation window for estimating recent cooperation among opponents = min(5, r-1) (use fewer rounds when r is small)
- T_exploit: fraction threshold of opponents cooperating to trigger exploitation = 0.5
- delta_retaliation: fraction drop in opponents' cooperation that counts as retaliation = 0.20
- L_punish: punishment length in rounds when retaliation is detected = min(3, r)
- p_probe: small probability to cooperate in probing rounds = 0.10 (used early/occasionally)
- probe_phase_len: number of initial rounds where probing may be used = min(3, r-1)
- final_defect_rounds E: number of final rounds to always defect = min(3, r) (ensures endgame defection)
- eps_random: tiny randomization to avoid full predictability = 0.02 (optional)

Rationale for parameter choices
- Window w keeps responsiveness to recent behavior while smoothing noise.
- T_exploit = 0.5 means we exploit when a clear minority or plurality of opponents are cooperating — exploitation is profitable when others contribute often.
- L_punish short but harsh; punishing longer wastes return and risks lowering our own payout.
- Probing lets us detect conditional cooperators who will cooperate only after seeing cooperation; without probing they never reveal and we miss exploitation opportunities.
- Endgame defecting prevents being suckered in final rounds when there's no future to leverage.

Decision rules (natural language)
1. Endgame: If t > r - E, play D (defect). Always defect in final E rounds.
2. Compute f_t = fraction of opponents (others) who cooperated on average over the last w rounds (if t=1 or no past rounds, treat f_t = 0).
   - Precisely: f_t = (1 / ((n-1) * min(w, t-1))) × sum_{s = t-min(w,t-1)}^{t-1} sum_{j≠i} c_j(s).
3. If we are currently in a punishment phase (remaining_punish > 0): play D and decrement remaining_punish.
4. Otherwise:
   - If t ≤ probe_phase_len:
       - With probability p_probe (independent draw), play C to probe; otherwise play D.
   - Else:
       - If f_t ≥ T_exploit: exploit — play D (we expect many opponents cooperating; defecting yields higher own payoff).
           - If we have just started exploiting (we switched to exploitation this round), record baseline f_baseline := f_t.
           - Monitor f in subsequent rounds: if f falls by more than delta_retaliation relative to f_baseline within the next 1–2 rounds, take that as retaliation and enter punishment phase: set remaining_punish = L_punish and play D this round (punishment starts immediately).
       - Else (f_t < T_exploit): default aggressive posture — play D (we don't invest in cooperation; wait for good exploitation opportunities).
5. Add small randomization: with probability eps_random, flip the chosen action (C↔D) to avoid being fully predictable (optional).

Edge cases and clarifications
- First round (t = 1): We recommend defect with high probability (1 - p_probe). The small p_probe chance to cooperate can uncover unconditional or conditional cooperators who will cooperate even when others defect.
- Short games (small r): shorten w, probe_phase_len and L_punish automatically by the min(...) rules to avoid wasted punishment at the end.
- If multiple opponents are cooperating but they are conditional on mutual cooperation (they require someone else to cooperate), our probing rounds (cooperating occasionally early) give a chance to reveal them. Once they reveal and continue cooperating, we exploit by defecting.
- Punishment design: punishment is pure defection for L_punish rounds (we do not invest in cooperation to fight). This is harsh (aggressive) but short to limit own losses while signaling retaliation is costly. After punishment we return to default defect+probe behavior; do not grant long-term forgiveness that sacrifices payoff.
- Final rounds: Because the game is finite and with perfect information, cooperating near the end is exploitable; therefore always defect in the last E rounds. E can be increased if you expect many slow-learning cooperators and want an extra safety buffer, but that reduces opportunities for cooperative exploitation earlier.

Pseudocode (compact)

Initialize:
  remaining_punish = 0
  f_baseline = 0
  in_exploit = false

For each round t = 1..r:
  if t > r - E:
    action = D
    play action; continue

  compute f_t = fraction of opponents cooperating in past min(w,t-1) rounds
  if remaining_punish > 0:
    action = D
    remaining_punish -= 1
    if remaining_punish == 0:
      in_exploit = false
    play action; continue

  if t <= probe_phase_len:
    if random() < p_probe:
      action = C
    else:
      action = D
    # If we cooperated and many others cooperated, that will be observed next round
    play action; continue

  # Main aggressive logic
  if f_t >= T_exploit:
    # exploit
    if not in_exploit:
      in_exploit = true
      f_baseline = f_t
      exploit_start_round = t
    action = D
    # check for retaliation signal: if f_t < f_baseline - delta_retaliation (observed in later rounds)
    # Implementation note: check after you observe opponents in the next round(s).
    play action; continue
  else:
    # default aggressive posture
    in_exploit = false
    action = D
    play action
  # optional tiny randomness:
  with probability eps_random flip action before playing

Retaliation detection (monitoring):
  After you have been exploiting, on each subsequent round recompute f_t. If within 1–2 rounds f_t drops below f_baseline - delta_retaliation:
    remaining_punish = L_punish
    in_exploit = false
    (start punishment immediately next round)

Why this is aggressive and robust
- Aggressive: default behavior is to defect and to exploit any detected cooperation by others. When many opponents cooperate we always defect to reap the larger immediate payoff. Punishment is short but forceful to discourage opponents from trying to retaliate or drive cooperative equilibria that exclude us.
- Robust: relies only on observed frequencies; it tolerates a wide range of opponent types (always-defectors, conditional cooperators, unconditional cooperators, tit-for-tat, probabilistic strategies). Probing reveals latent cooperators; exploitation capitalizes when present; punishment responds to retaliation. Short window and configurable parameters make it responsive to different r and n.
- Adaptive: uses recent history (w) and responds to changes (retaliation drop delta). Small randomized probing and eps_random prevent deterministic exploitation by meta-strategies that could manipulate fully predictable play.

Tuning tips
- Increase p_probe or probe_phase_len to find conditional cooperators in populations that rarely cooperate unless someone initiates.
- Increase T_exploit to be more conservative (only exploit when a larger fraction cooperate).
- Increase L_punish to deter stubborn retaliators, but beware it costs you payoff if opponents don't change.
- Decrease E (final rounds) only if you believe many opponents will cooperate in final rounds in ways you can exploit without credible retaliation.

Summary
- Always defect by default, probe occasionally early, exploit whenever a clear fraction of opponents are cooperating, punish briefly and harshly if exploitation triggers retaliation, and always defect in final rounds. This gives an aggressive, adaptive, history-dependent algorithm that focuses on maximizing individual payoff in mixed populations of strategies.
'''

description_AGGRESSIVE_43 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Intuition (one-sentence): Default to defecting, but probe briefly to learn opponents, then exploit any significant cooperative tendency by free‑riding and by triggering cooperation when that will be immediately profitable; retaliate strongly against strategies that punish defection, and always defect in the final round.

Key design principles
- Aggressive: Prioritize immediate and total personal payoff; actively exploit others’ cooperation rather than try to sustain altruistic equilibria.
- Adaptive: Use observed history to detect (a) large numbers of consistent cooperators, (b) conditional cooperators who respond to my cooperation, and (c) punishers who retaliate when I defect; switch exploitation tactics accordingly.
- Robust: Conservative probes only early; revert to always-defect if environment is hostile or data are insufficient; always defect in final round.

Parameters (computed from game parameters and history; constants may be adjusted by tournament implementer)
- W = min(5, r-1) : history window for short-term statistics
- probe_prob = min(0.15, 3/r) : small probing probability in early rounds
- high_coop_frac = 0.6 : threshold fraction of other players cooperating considered “many cooperators”
- cond_delta_thr = 0.15 : minimum increase in an opponent’s cooperation probability after my cooperation to call them “conditional”
- cond_count_thr_frac = 0.25 : fraction of opponents required to be conditional cooperators to trigger “manipulation” mode
- punishment_drop = 0.20 : if opponents reduce cooperation by this much after my defection, treat them as punishers
- bait_length = 1 : number of consecutive cooperations used to trigger others
- exploit_pattern = after baiting (cooperate), immediately defect the next round to free-ride
(These are tunable; all strategy decisions are computed using only n, r, k and the public history.)

Overview of behavior
1. First round and final round: defect.
2. Exploration phase (very early rounds): mostly defect but occasionally cooperate with small probability to gather behavioral statistics.
3. After gathering data, classify the environment:
   - If many players are unconditional/high cooperators, switch to permanent exploitation (always defect).
   - Else if a nontrivial set of players are conditional cooperators (they cooperate more after I cooperate), use a “bait-and-exploit” cycle: occasionally play C to induce their cooperation, then play D immediately after to free-ride.
   - If many players punish my defection (their cooperation drops substantially after I defect), reduce exploitation frequency (avoid baiting/exploitation) and revert to mostly defect with fewer probes.
4. Always defect in final round.

Decision rules (explicit, step-by-step)

Variables tracked from public history:
- For each round t and each player j ≠ me: record c_j,t ∈ {0,1}.
- For each opponent j, compute:
  - P_j(C | I_C) = fraction of rounds (in the chosen window) that j cooperated when I cooperated in previous round.
  - P_j(C | I_D) = fraction of rounds j cooperated when I defected in previous round.
  - delta_j = P_j(C | I_C) − P_j(C | I_D).
- avg_other_coop = average fraction of other players cooperating over the window.
- avg_coop_after_myD and avg_coop_after_myC = average fraction of others cooperating on rounds after I defected vs after I cooperated.

Routine used every round t (1..r)

Pseudocode (natural-language / structured):

Initialize:
- last_action = None
- history window W = min(5, r-1)
- If r ≤ 3: always play D (insufficient horizon to justify risk)

For round t = 1..r:
  If t == 1:
    action = D                // aggressive opening
  Else if t == r:
    action = D                // last-round defection (backward induction)
  Else:
    // compute recent statistics using last min(W, t-1) rounds
    Use the most recent H = min(W, t-1) rounds to compute:
      - For each opponent j: P_j(C | I_C), P_j(C | I_D) (estimate over those H rounds where that conditioning applies)
      - delta_j = P_j(C | I_C) - P_j(C | I_D)
      - cond_j = 1 if delta_j ≥ cond_delta_thr and P_j(C | I_C) ≥ 0.5 else 0
      - cond_count = sum_j cond_j
      - avg_other_coop = average across those H rounds of (sum_{j≠me} c_j / (n-1))
      - avg_coop_after_myD = average other cooperation in rounds immediately after I played D in the window
      - avg_coop_after_myC = average other cooperation in rounds immediately after I played C in the window

    // Detect punishers
    If avg_coop_after_myD ≤ avg_coop_after_myC - punishment_drop:
      punishers_present = true
    Else:
      punishers_present = false

    // Classification
    If avg_other_coop ≥ high_coop_frac:
      // Many cooperators overall -> pure exploitation
      action = D
    Else if cond_count ≥ ceil(cond_count_thr_frac * (n-1)) and not punishers_present:
      // Enough conditional cooperators and they don't punish -> manipulate
      // Manipulation pattern: bait (C) occasionally, then exploit (D) immediately
      If last_action == C:
        // We bated previous round and likely induced cooperation: defect to exploit
        action = D
      Else:
        // choose to bait with small probability tuned to expected gain
        // If recent average cooperation by others when I cooperated is high, bait more often
        p_bait = 0.5 * min(1, avg_coop_after_myC + 0.2)   // tunable: increase baiting if it yields responses
        action = C with probability p_bait, else D
    Else:
      // Default aggressive posture with conservative probing
      If t ≤ 1 + ceil(sqrt(r)):   // brief exploration window (scale with r)
        action = C with probability probe_prob, else D
      Else:
        // environment not cooperative enough or punishing -> mostly defect, but keep tiny probe
        action = C with probability (probe_prob / 2), else D

  Execute action; record last_action = action; update history statistics.

Remarks and rationale
- Why mostly defect? In one-shot and given k < n, defect strictly dominates cooperate for immediate payoff. Aggression seeks to exploit that by free-riding whenever others cooperate.
- Why probe? A tiny, bounded probing phase detects exploitable opponents (high unconditional cooperators or conditional cooperators). Probes are deliberately rare to limit losses to punishers/retaliators.
- Bait-and-exploit cycle: When a subset of opponents raises their cooperation after I cooperate, a single cooperation (bait) can induce a cluster to increase contributions next round; defecting next round yields the larger private gain from free-riding. This pattern is short (bait_length = 1) so the window for punishment is limited.
- Detecting punishers: If others systematically lower cooperation after my D, exploitation is costly (they punish me). In that case AOE backs off—fewer bait attempts and conservative behavior—because repeated exploitation reduces my long-run payoff.
- Final-round defect: No incentive to cooperate in final round, so always defect.

Edge cases
- Very small r (r ≤ 3): The horizon is too short to recover probing losses; always defect.
- Insufficient statistics for conditional estimates: Use smaller H; if conditional estimates are unreliable (too few conditioning occurrences), the strategy falls back to conservative default (mostly defect + probes).
- All opponents always defect: AOE will be D forever (no probes necessary after a few rounds).
- All opponents always cooperate (or large majority cooperate): AOE will detect high avg_other_coop and switch to permanent exploitation (always D).
- Strong punishers (many players reduce cooperation after I defect): AOE reduces or stops baiting and defaults to cautious defecting, minimizing losses to punishment.

Concrete example behaviors
- Against unconditional cooperators: after a short probe window sees avg_other_coop high → always defect thereafter and reap free-rides every round.
- Against tit-for-tat–like punishers: probes reveal punishment; AOE will avoid baiting/exploitation and mostly defect with minimal probing to avoid severe retaliation.
- Against conditional cooperators (they cooperate when you did last round): AOE will use 1‑round baits (C) and then D to exploit the induced cooperation; it repeats this opportunistically as long as punishment is not detected.

Implementation notes for algorithm designers
- Maintain counts with smoothing (Laplace +1/ +2) to avoid divide-by-zero and overfitting when data are sparse.
- Tune p_bait and thresholds cond_delta_thr, high_coop_frac, punishment_drop to the tournament environment.
- All decisions use only n, r, k and the public action history; no private signals required.

Summary
AOE is an aggressive, adaptive strategy that defaults to defection, performs small probes to learn opponents’ tendencies, and then either (a) permanently free-rides on mass cooperators, (b) performs short “bait-and-exploit” cycles on conditional cooperators, or (c) backs off if opponents punish defection heavily. It always defects in the final round. This yields an aggressive but data-driven approach that maximizes personal payoff while avoiding repeated costly punishments.
'''

description_AGGRESSIVE_44 = '''
Name: Aggressive Conditional Exploiter (ACE)

Goal: Maximize my cumulative payoff by (1) exploiting unconditional cooperators, (2) opportunistically inducing cooperation from responsive opponents only when the expected future payoff gain exceeds the immediate cost of cooperating, and (3) otherwise defecting. The strategy only uses game parameters (n, k, r) and the full public history of actions.

Summary of intuition
- In one-shot, Defect strictly dominates Cooperate; cooperation can only pay off if it changes future behaviour of others enough to recoup the immediate loss. ACE therefore defaults to Defect, but it measures opponents’ responsiveness and occasionally “pays” the immediate cost of cooperation only when doing so is likely to produce a net future gain.
- ACE is aggressive: it prioritizes immediate payoff, exploits predictable cooperators, punishes/responds to retaliatory behaviour, and keeps cooperation pulses rare and conditional.

Constants / tunable defaults (these are part of the strategy and may be set before tournament)
- window w = min(5, r-1) — use recent up-to-5 rounds to estimate behaviour
- min_samples = 3 — minimum number of observed “after C” or “after D” transitions to trust responsiveness estimates
- probe_epsilon = 0.08 — baseline small probability to probe (cooperate) when data are insufficient
- exploitable_individual_threshold = 0.9 — mark a player as unconditional cooperator if their coop rate > 0.9 in window
- exploit_group_threshold = 0.6 — if a large fraction of players are cooperating consistently, exploit by defecting
- noise_smoothing: use Laplace smoothing (add 1 pseudo-count) for proportion estimates to avoid divide-by-zero

Key derived quantity
- Immediate cost of cooperating in a single round (vs defecting) = cost = 1 - k/n (always > 0 because k < n).
- Remaining future rounds after current decision = Rf = r - t (where t is current round number, starting at 1). If Rf = 0 (last round), no future benefit possible.
- Future benefit per additional cooperating other per future round = k/n.
- Threshold in expected additional cooperating others (delta_num) required to justify cooperating now:
  delta_threshold = (cost) / (Rf * (k/n)) = (n - k) / (k * Rf)
  (If Rf = 0, delta_threshold = +infty → never cooperate on last round.)

Measurements from history (computed each round before choosing action)
- For each round s in the last w rounds, we can observe:
  - my_action_s ∈ {C, D}
  - others_coop_count_s = total cooperators excluding me (0..n-1)
- p_after_C = average fraction of other players cooperating in rounds that immediately followed a round where I played C (empirical probability of others cooperating after my C), using smoothing.
- p_after_D = same but after rounds where I played D.
- delta_p = p_after_C - p_after_D
- delta_num = delta_p * (n - 1) = expected additional cooperating others (count) induced by my playing C (vs D) next round.
- coop_rate_j for each opponent j = fraction of rounds (in window) where j played C.
- group_coop_rate = average fraction of players (excluding me) cooperating per round in window.

Decision rules (top-level)
1. If t = 1: play D (first round defect).
2. If t = r (last round): play D (no future to influence).
3. If group_coop_rate >= exploit_group_threshold or ANY opponent j has coop_rate_j >= exploitable_individual_threshold:
   - Play D (exploit predictable cooperators).
4. Compute Rf = r - t and delta_num and delta_threshold (see above).
   - If Rf > 0 and we have at least min_samples observations of “after C” and min_samples of “after D”:
     - If delta_num > delta_threshold and delta_num > 0:
       - Play C (the expected induced future cooperation justifies the immediate cost).
     - Else:
       - Play D.
   - Else (insufficient samples):
     - With probability probe_epsilon: play C to collect data (probe).
     - Otherwise: play D.
5. Robustness to punishment/retaliation:
   - If after any recent defection by me, group_coop_rate drops sharply versus overall baseline (for example p_after_D is substantially lower than my baseline p_overall_by_others by more than 0.15), treat as active punishment. In presence of detected punishment, avoid cooperating except possibly for a long-term “truce” only if delta_num >> delta_threshold and Rf is large. Default under punishment: play D.

Pseudocode (concise)

Inputs: n, k, r; public history H up to round t-1
function ACE_Action(t, H):
  if t == 1:
    return D
  if t == r:
    return D
  w = min(5, r-1)
  consider last_w_rounds = last min(w, t-1) rounds from H
  For each of those rounds s, record my_action_s and others_coop_fraction_s = (cooperators_excl_me)/(n-1)
  Compute for rounds s where my_action_s == C: list Lc of others_coop_fraction_s
  Compute for rounds s where my_action_s == D: list Ld of others_coop_fraction_s
  p_after_C = (sum(Lc) + 1) / (len(Lc) + 2)  // Laplace smoothing
  p_after_D = (sum(Ld) + 1) / (len(Ld) + 2)
  delta_p = p_after_C - p_after_D
  delta_num = delta_p * (n - 1)
  group_coop_rate = average of others_coop_fraction_s over window
  For each opponent j compute coop_rate_j over window
  // Exploitable check
  if group_coop_rate >= exploit_group_threshold or exists j: coop_rate_j >= exploitable_individual_threshold:
    return D
  Rf = r - t
  if Rf <= 0:
    return D
  delta_threshold = (n - k) / (k * Rf)
  if len(Lc) >= min_samples and len(Ld) >= min_samples:
    if delta_num > max(delta_threshold, 0):
      // Play C only if it is expected to induce enough future cooperators
      return C
    else:
      return D
  else:
    with probability probe_epsilon:
      return C
    else:
      return D

Behavioral notes and examples
- Aggressive default: ACE defects almost always, especially early and late in the game, so it captures the single-round advantage of defection.
- Exploitation of unconditional cooperators: If an opponent tends to always cooperate, ACE will consistently defect and extract the larger one-shot benefit.
- Opportunistic cooperation: If cooperating now reliably increases others’ cooperation in many future rounds (i.e., delta_num is estimated high and many rounds remain), ACE will cooperate to trigger a profitable sequence. This happens only when the measured responsiveness and remaining rounds makes it worthwhile.
- Probing: Probes are rare (probe_epsilon small) so the strategy does not give away cooperation indiscriminately, but still gathers data to detect responsive opponents.
- Punishment handling: If opponents organize to punish (others’ cooperation plunges after my defection), ACE avoids cooperating and sustains defection; it can only agree to a long-term truce if the responsiveness and remaining rounds are clearly in its favor.

Why this is aggressive and robust
- Aggressive: The baseline is always Defect, so ACE extracts immediate gains and exploits unconditional cooperators. It cooperates only when a measured, quantifiable future payoff gain compensates the immediate loss. It retaliates (by withholding cooperation) when punished.
- Robust/adaptive: ACE uses direct empirical responsiveness estimates from full public history to adapt to many opponent types: unconditional cooperators, unconditional defectors, reciprocal strategies, and even mixed/noisy strategies. It uses smoothing and minimal-sample rules to avoid being misled by noise, and a small probing rate to learn.
- Parameter-free in core decision: All thresholds are derived from game parameters (n, k, r) via delta_threshold; the other constants (window, min_samples, probe_epsilon, exploitable thresholds) are implementation-level tuning that keep the strategy stable in tournaments.

Implementation notes
- Use counts (not just fractions) for better precision when n is small. Laplace smoothing prevents division-by-zero.
- If you want a more exploratory variant, increase probe_epsilon or the window; if more ruthless exploitation is desired, reduce probe_epsilon and reduce min_samples.
- The core decision rule (compare delta_num against delta_threshold) is mathematically justified by comparing immediate cost vs expected future benefit per additional cooperating other.

This description is ready to be implemented: it specifies the exact computations, rounding/smoothing choices, first/last-round rules, and how to react to group behaviour.
'''

description_AGGRESSIVE_45 = '''
Name: Aggressive Conditional Exploiter (ACE)

High-level idea
- Be aggressively exploitative: defect whenever it is immediately profitable or when cooperation is unreliable.
- Probe early (briefly) to detect whether many opponents are willing to cooperate.
- If a clear cooperative majority is detected and there are enough future rounds to reap benefits, switch to exploitation (defect repeatedly to take the private gain while others keep cooperating).
- Detect coordinated retaliation against your defections; if opponents appear to coordinate punitive responses, switch to permanent defection (grim) to avoid being repeatedly punished and to maximize relative payoff.
- Always defect in the final round (no future to influence).

The strategy depends only on game parameters (n, r, k) and the complete history of past rounds.

Parameter choices (fixed, but can be tuned)
- Probe length P = min(3, r-1). (A short early probe to estimate cooperativeness.)
- Smoothing window W = min(5, r-1). (For short-term behavior estimates.)
- High-cooperation threshold rho_high = 0.60 (i.e., 60% of OTHER players cooperating is “many”).
- Low-cooperation threshold rho_low = 0.20 (i.e., <20% of others cooperating is “rare”).
- Exploit safety margin M = max(1, floor(r/10)). (Don’t start long exploitation if few rounds remain.)
- Soft cooperative probability p_soft_coop = 0.20 (when cooperation is ambiguous, cooperate only occasionally).
- Punishment-detection drop fraction delta = 0.50 (if others’ cooperation drops by >= 50% from recent baseline after your defection, treat as coordinated retaliation).
- Small-r rule: if r <= 3, always defect (no durable enforcement possible; maximize immediate payoff).

State variables
- grim (boolean) — if true, defect forever (permanent punishment mode).
- history of rounds: for each past round t we observe full vector of contributions c_j,t.

Decision rules (natural language)
1. If r <= 3: play D every round (edge-case fast games).
2. If grim is true: play D.
3. If current round t is the final round (t = r): play D (no future, so defect).
4. First round (t = 1, when r > 3): play C (short probe to reveal cooperators).
5. For rounds t > 1 and not final and not grim:
   a. Compute coop_rate = (total number of contributions by other players over the last L rounds) / (L*(n-1)), where L = min(P, t-1). (i.e., fraction of other players who cooperated on average in the probe window).
   b. If coop_rate >= rho_high AND there are at least M future rounds left (t <= r - M):
        - Switch to/exploit: play D (exploit the cooperative majority).
   c. Else if coop_rate <= rho_low:
        - Play D (cooperation is rare — do not give up private endowment).
   d. Else (ambiguous cooperation):
        - Play D with high probability (1 - p_soft_coop). With small probability p_soft_coop play C to probe and attempt to re-create cooperation if profitable.
6. After each round where you played D, check for punishment signals:
   - Compute baseline_other_coop = average number of other players cooperating per round over the last W rounds before your defection (if available). Let current_other_coop be the number of other cooperators observed in the round immediately after your defection.
   - If current_other_coop <= baseline_other_coop × (1 − delta) (i.e., at least a delta fraction drop), interpret this as coordinated retaliation and set grim = true (permanent defection from now on). The idea: opponents coordinated to punish you sharply; respond with permanent defection to stop repeated exploitation and maximize relative payoff thereafter.

Pseudocode (concise)
Note: c_j,t ∈ {0,1}. We track others’ contributions only.

initialize grim = false
if r <= 3:
  always play D
for t = 1..r:
  if grim:
    action = D
  else if t == r:
    action = D
  else if t == 1:
    if r <= 3: action = D else action = C
  else:
    L = min(P, t-1)
    coop_count_others = sum over last L rounds of (sum_{j≠i} c_j,round)
    coop_rate = coop_count_others / (L*(n-1))
    if coop_rate >= rho_high and t <= r - M:
      action = D   # exploit
    else if coop_rate <= rho_low:
      action = D
    else:
      action = D with probability 1 - p_soft_coop; else C
  play(action)
  # punishment detection (only meaningful if previous round you defected)
  if t >= 2 and my_action_{t-1} == D:
    Wcur = min(W, t-2)  # rounds before the defection we can use as baseline
    if Wcur >= 1:
      baseline = (sum over those Wcur rounds of (sum_{j≠i} c_j,round)) / Wcur
      current_other_coop = sum_{j≠i} c_j,t
      if current_other_coop <= baseline * (1 - delta):
        grim = true

Rationale and aggressive alignment
- Aggression is realized by:
  - Defaulting to defection in almost all circumstances, thereby avoiding the “sucker” loss and capturing the private gain whenever others cooperate.
  - Opportunistically exploiting clear cooperative majorities after a short probe (so you gain the one-shot advantage repeatedly while others still cooperate).
  - If opponents appear to coordinate harsh punishments in response to your defections, switching to permanent defection (grim) protects you from further exploitation and avoids being trapped in repeated punishment cycles — this is an aggressive, self-interested safeguard.
  - Only a small and tactical amount of cooperation is used (first-round probe and occasional cooperative noise) to identify and, when profitable, maintain exploit opportunities.

Robustness notes
- Works for arbitrary opponent mixes: unconditional defectors, unconditional cooperators, TFT-style reciprocators, or stochastic strategies. The early probe identifies cooperative majorities; the exploitation rule extracts value from them. The grim switch prevents repeated, coordinated punishment from eroding your payoff.
- Uses only observed history and parameters n, r, k (k enters only indirectly in strategic reasoning; recall single-round incentive to defect is always positive because k < n).
- Avoids being “nice” in final rounds: always defects on the last move (standard backward-induction aggression).

Tuning suggestions (if you later implement and test)
- Increase P and W when r is large to get more reliable estimates of opponents’ tendencies.
- Adjust rho_high/rho_low and delta to tune sensitivity: higher rho_high makes the strategy require a stronger cooperative signal before exploitation; higher delta makes punishment detection stricter (less likely to trigger grim on noisy drops).
- Adjust p_soft_coop to trade off between seeding cooperation (larger p) and being exploitative (smaller p).

Summary
ACE is an aggressive, history-conditioned defect-first strategy with a short probing phase and an exploit regime that defects to extract gains from cooperating opponents. It is adaptive (probes and monitors others’ cooperation rates), robust (detects and neutralizes coordinated retaliation via grim), and tuned to maximize relative payoff in varied opponent environments.
'''

description_AGGRESSIVE_46 = '''
Name: Aggressive Exploit-and-Punish (AEP)

High-level description
- Baseline: defect. Only cooperate when there is a clear, exploitable stream of cooperative opponents that I can free-ride on while keeping them willing to cooperate occasionally. If any opponent who has been classified as "reliable cooperator" betrays that status, punish harshly (permanently for the remainder of the game). The strategy is adaptive because it classifies opponents from observed history and updates behavior; it is aggressive because it prefers defection, free-rides when profitable, and applies irreversible, group-level punishment to deter or exploit cooperation by others.

Parameters (derived from game parameters and history)
- r: remaining total rounds (input).
- n: number of players.
- W: look-back window for estimating behaviour = max(1, min(5, r-1)). (Short window to react quickly; scales down near end.)
- tau = 0.8: threshold cooperation rate to call a player a "reliable cooperator" (i.e., cooperated in >= tau × W of last W rounds).
- exploit_prob = 0.95: probability with which we defect when exploiting a set of reliable cooperators (we mostly defect to free-ride).
- signal_prob = 0.05: occasional cooperation probability to keep reliable cooperators from abandoning cooperation (small trembling-hand signal).
- last_round: boolean indicator whether this is the final round (t == initial_r).
- permanent_punish: boolean flag set once we detect a betrayal by a previously reliable cooperator; when set, we defect for all remaining rounds (aggressive grim).
- minimal_group_coop = 0.7: if a large fraction of group (≥ minimal_group_coop) has cooperated recently, we may attempt a short probe cooperation to try to re-establish a cooperative stream to exploit.

State maintained (from observed history)
- For each opponent j: recent_coop_count_j = number of times j played C in the last W rounds.
- reliable_j = (recent_coop_count_j / W) ≥ tau.
- total_recent_group_coop_rate = average cooperation rate across all opponents over last W rounds.
- permanent_punish flag as above.

Decision rules (deterministic outline + small-probability randomness where specified)
1) First round (t = 1)
   - Defect.
   - Rationale: aggressive baseline avoids first-round exploitation.

2) Last round (final round t = r_initial)
   - Defect.
   - Rationale: no future to leverage cooperation; play my dominant action.

3) If permanent_punish flag is set
   - Defect for all remaining rounds (no forgiveness).
   - How the flag gets set: when any player who was classified reliable_j at time of classification plays D in a subsequent round while most other reliable players continued to play C in that same round (i.e., clear betrayal). See betrayal detection below.

4) Otherwise (not first or last round and no permanent punishment)
   - Update recent_coop_count_j for each opponent j from last W rounds and compute reliable_j.
   - Let R = {j | reliable_j is true}. Let r_count = |R| (number of reliable opponents).
   - Betrayal detection (before acting): if some j ∈ R played D in last round while at least (max(0, r_count − 1)) of the other R cooperated that last round (i.e., j broke a prior pattern), set permanent_punish = true and Defect this round.
     - Rationale: targeted betrayal by a previously reliable cooperator triggers permanent punishment (aggressive deterrent).
   - If r_count ≥ 1 (there is at least one reliable cooperator)
       - Exploit mode:
           - With probability exploit_prob: Defect (free-ride).
           - With probability (1 − exploit_prob) = signal_prob: Cooperate (a small, intermittent cooperation to signal continued willingness to support the stream and reduce the chance that all cooperate-starters abandon cooperation).
           - Rationale: mostly defect to capture immediate gains; occasionally cooperate to maintain the stream of cooperative opponents at low personal cost.
   - Else if total_recent_group_coop_rate ≥ minimal_group_coop
       - Probe mode: (many players have been cooperating recently but no one meets the strict "reliable" threshold)
           - Cooperate with small probability p_probe = 0.2 to try to help create a small cooperating nucleus to exploit in future rounds; otherwise Defect.
           - Rationale: try to seed a cooperative cluster that can be exploited later while keeping exposure limited.
   - Else
       - Defect.
       - Rationale: no reason to invest when cooperation is rare.

Pseudocode (compact)
- Inputs: n, r_initial, t (current round index, 1..r_initial), history of all players' actions for past rounds
- Internal constants: W = max(1, min(5, r_remaining)), tau = 0.8, exploit_prob = 0.95, signal_prob = 0.05, minimal_group_coop = 0.7, p_probe = 0.2
- State: permanent_punish (initially false)

Algorithm:
1. If t == 1: action = D; return action.
2. If t == r_initial: action = D; return action.
3. If permanent_punish: action = D; return action.
4. Compute W = max(1, min(5, r_initial − t + 1)).
5. For each opponent j: recent_coop_count_j = count of C in last W rounds for j.
6. reliable_j = (recent_coop_count_j / W) ≥ tau.
7. R = set of j with reliable_j true; r_count = |R|.
8. If r_count ≥ 1:
     - If any j ∈ R played D in last round while at least max(0, r_count−1) other members of R played C that last round:
         - permanent_punish = true; action = D; return action.
     - Else:
         - With probability exploit_prob: action = D; else action = C; return action.
9. Else:
     - total_recent_group_coop_rate = (sum over j of recent_coop_count_j)/(n×W)
     - If total_recent_group_coop_rate ≥ minimal_group_coop:
         - With probability p_probe: action = C; else action = D; return action.
     - Else:
         - action = D; return action.

Edge cases and clarifications
- Window shrink near end: W cannot exceed remaining rounds; when only 1 round left, we already default to defect in last round.
- Stochasticization: exploit_prob and signal_prob introduce small randomness. Implementations may use deterministic approximations (e.g., cooperate on every K-th exploitation round) if pure randomness is not allowed.
- Betrayal detection is deliberately strict: only a reliably cooperative opponent who defects while other reliable opponents still cooperated is labeled betrayer. This avoids punishing mis-coordination when cooperation is collapsing generally.
- Permanent punishment is group-level: once triggered, I defect against everyone forever (rest of game). This is aggressive and simple to implement. If you prefer less absolute severity, replace permanent_punish with a long finite punishment length P = min(r_remaining, ceil(0.6 × r_remaining)), but the strategy provided uses permanent punishment as the aggressive default.

Why this is aggressive and robust
- Aggressive: default action is D; the strategy exploits identified cooperators to obtain higher immediate payoffs; betrayal triggers irreversible, harsh punishment to deter opportunistic cooperation among opponents; occasional small cooperation is only used to sustain exploitability of cooperators, not for altruism.
- Adaptive: the strategy infers reliable cooperators from recent behavior (sliding window), adjusts to changing environments (window adapts near the end), and probes when the group shows a high baseline cooperation rate.
- Robust: it does not assume norms or communication; it reacts to each player's empirical behavior; it is resilient to noise via thresholds and small-probability signals; it will not unconditionally cooperate and will not allow repeated exploitation by supposed cooperators without retaliation.

Tuning suggestions (implementer can experiment)
- Shorter W makes the strategy more reactive; longer W makes it more stable.
- Increase tau to be more selective about who counts as reliable; decrease tau to accept less consistent cooperators.
- Increase exploit_prob to be more purely exploitative; increase signal_prob if experiments show cooperators abandon too quickly.
- Replace permanent_punish with a long finite punishment if permanent punishment is too brittle in your tournament.

Example intuition (n=6, r large)
- If a couple of opponents repeatedly contribute, AEP will mostly defect and capture the higher payoff from free-riding; it will occasionally cooperate 5% of the time to keep those opponents contributing. If one of them unexpectedly defects while the others keep cooperating, AEP flips to permanent punishment (defect forever), deterring fragile cooperation among opponents and inflicting group-level losses to vindicate its aggressive stance.

This strategy depends only on game parameters (n, r) and the full public history of moves. It is implementable as described and intentionally biased toward aggressive exploitation and severe deterrence.
'''

description_AGGRESSIVE_47 = '''
Summary (one line): Aggressive, stateful trigger strategy that defects by default, only cooperates after seeing unanimous cooperation (and then for a short handshake), punishes any deviation harshly for multiple rounds proportional to the size of the deviation, and requires clear clean signals to resume cooperation. It exploits high-but-not-unanimous cooperation and always defects in the last round.

Intuition / aggressive mindset
- Default to self-interest (defect) unless opponents prove near-perfect reliability.
- If opponents ever give you a clean unanimous cooperation signal, reward briefly (to harvest future cooperation) but punish any deviation severely and for multiple rounds.
- Exploit large but imperfect cooperation clusters by defecting once (to grab extra private payoff) and then triggering a long punishment.
- Be unforgiving enough to deter casual cooperators but allow re-entry only after clear evidence of reform. Always defect in the final round.

Parameters used by the strategy (all derived from game parameters n, r or fixed small constants)
- S (handshake length) = 2 rounds of cooperation once trust is earned.
- R_min = 3 (minimum number of future rounds required to attempt cooperation).
- Forgiveness requirement: need 2 consecutive “clean” rounds (all others cooperated) after punishment to resume handshake.
- Punishment length function P(deviation_count, rem_rounds) = min(rem_rounds, 1 + 2 × deviation_count).
  - deviation_count = number of other players who defected in the triggering round = (n-1) - m_prev.
  - This punishes larger deviations more strongly.
- Threshold for opportunistic exploitation: opportunistic_exploit_threshold = ceil(0.8 × (n-1)) (if many others cooperated but not unanimous, exploit once then punish).

State variables maintained (computed from history)
- state ∈ {BASELINE, COOP_STREAK, PUNISH, WAIT_FOR_REENTRY}
- coop_streak_remaining (if in COOP_STREAK)
- punish_remaining (if in PUNISH)
- clean_rounds_count (counts consecutive rounds where all others cooperated, used while in WAIT_FOR_REENTRY)
Note: All of these are deterministic functions of past rounds’ actions and the round index.

Decision rules (plain language)
1. Terminal round rule
   - If this is the final round t = r, choose D (defect).

2. Short-game rule
   - If remaining rounds rem = r − t + 1 < R_min, choose D (too near the end to sustain/recoup cooperation).

3. First round
   - t = 1: choose D (probe / signal toughness).

4. Otherwise follow the finite-state policy:

Base definitions each round
- Let m_prev = number of cooperators among the other n−1 players in the previous round (if t = 2 use round 1; if no history treat m_prev = 0).
- deviation_count = (n−1) − m_prev.
- rem = r − t + 1.

State transitions and actions
- If state = PUNISH:
  - Action: D.
  - Decrement punish_remaining each round. When punish_remaining hits 0, set state = WAIT_FOR_REENTRY and clean_rounds_count = 0.

- Else if state = COOP_STREAK:
  - Action: C (cooperate).
  - Decrement coop_streak_remaining each round.
  - If during COOP_STREAK in the previous round we observe any other-player defection (m_prev < n−1):
    - Immediately abandon streak and set punish_remaining = P(deviation_count, rem) and state = PUNISH. (That defector is punished.)
  - If coop_streak_remaining reaches 0 without any deviation, set state = BASELINE (we gave the brief reward; revert to baseline until we get a fresh clean signal).

- Else if state = WAIT_FOR_REENTRY:
  - If m_prev == n−1 (all other players cooperated last round):
    - increment clean_rounds_count.
    - If clean_rounds_count >= 2 and rem >= R_min:
      - Enter COOP_STREAK: coop_streak_remaining = S; set state = COOP_STREAK.
    - Action: D while waiting (we remain defensive until handshake begins).
  - Else:
    - reset clean_rounds_count = 0 and Action: D.
    - If m_prev < n−1 then remain in WAIT_FOR_REENTRY (we require consecutive clean rounds).

- Else if state = BASELINE:
  - Default Action: D.
  - Reacting to previous round:
    - If m_prev == n−1 and rem >= R_min:
      - (Perfect clean signal) Enter COOP_STREAK: coop_streak_remaining = S; Action for this round = C (start handshake).
    - Else if m_prev >= opportunistic_exploit_threshold and m_prev < n−1:
      - Opportunistic exploit: Action = D this round (exploit many cooperators). Immediately set punish_remaining = P(deviation_count, rem) and set state = PUNISH starting next round (we punish after exploiting).
    - Else remain defecting.

Pseudocode (concise)
Initialize state = BASELINE; coop_streak_remaining = 0; punish_remaining = 0; clean_rounds_count = 0.
For each round t = 1..r:
  rem = r - t + 1
  if t == r or rem < R_min: play D; continue
  if t == 1: play D; continue
  compute m_prev from the actions in round t-1 (number of other players who played C)
  deviation_count = (n-1) - m_prev
  if state == PUNISH:
    play D
    punish_remaining -= 1
    if punish_remaining == 0: state = WAIT_FOR_REENTRY; clean_rounds_count = 0
    continue
  if state == COOP_STREAK:
    # If previous round had a defection by others, punish immediately
    if m_prev < n-1:
      punish_remaining = min(rem, 1 + 2 * deviation_count)
      state = PUNISH
      play D
      punish_remaining -= 1
      if punish_remaining == 0: state = WAIT_FOR_REENTRY; clean_rounds_count = 0
      continue
    # Otherwise continue cooperation streak
    play C
    coop_streak_remaining -= 1
    if coop_streak_remaining == 0: state = BASELINE
    continue
  if state == WAIT_FOR_REENTRY:
    if m_prev == n-1:
      clean_rounds_count += 1
      if clean_rounds_count >= 2 and rem >= R_min:
        state = COOP_STREAK; coop_streak_remaining = S
        play C
      else:
        play D
    else:
      clean_rounds_count = 0
      play D
    continue
  # state == BASELINE
  if m_prev == n-1 and rem >= R_min:
    state = COOP_STREAK; coop_streak_remaining = S
    play C
    continue
  if m_prev >= ceil(0.8 * (n-1)) and m_prev < n-1:
    # opportunistic exploit
    punish_remaining = min(rem, 1 + 2 * deviation_count)
    state = PUNISH
    # play D this round (exploit)
    punish_remaining -= 1
    if punish_remaining == 0: state = WAIT_FOR_REENTRY; clean_rounds_count = 0
    continue
  # otherwise
  play D

Notes and rationale details
- Unanimous cooperation as trigger: requiring all other players to have cooperated in the previous round is a high bar; it's aggressive because it only trusts near-perfect evidence. It ensures our brief cooperation streak is not wasted on unreliable partners.
- Short handshake (S = 2): a short reward period limits our downside; cooperating briefly signals willingness to cooperate while limiting exposure.
- Punishment proportional to deviation_count: larger numbers of defectors are punished longer. This is aggressive and deters groups from free-riding en masse.
- Opportunistic exploitation: when many others cooperate but not all, we defect once to harvest extra private payoff, then punish. This directly extracts value from cooperators and discourages sloppy cooperation.
- Forgiveness only after clear signals: to re-enter cooperation we require two consecutive clean rounds. This prevents re-entry after a single fluke.
- Final-round defection, and automatic defection near the end, avoid being exploited by endgame reasoning.

Robustness considerations
- Works without communication and only uses public history (all previous actions).
- Handles a mix of strategies: against pure defectors it never loses (it defects); against unconditional cooperators it exploits them via opportunistic exploitation; against conditional cooperators that seek mutual cooperation it can enforce cooperation if those players are willing to sustain unanimous cooperation and withstand our harsh punishment.
- Not noise-robust: the strategy is intentionally aggressive (punishments are long and re-entry requires clear signals). If the tournament includes noisy mistakes, this strategy may sustain long punish cycles; if noise is expected, reduce punishment multipliers and shorten forgiveness windows (e.g., require only one clean round) — but that softens aggressiveness.

Practical tuning knobs (optional)
- Reduce punishment severity (lower the multiplier 2 → 1) to get more robustness to noise.
- Relax trigger from unanimity to high fraction (e.g., >= 95%) if n is large and unanimous rounds are unlikely even with cooperative groups.
- Tune S, R_min and opportunistic_exploit_threshold to trade off aggressiveness vs. ability to sustain cooperation.

This specification is fully algorithmic and uses only game parameters (n, r, k via choosing no action that requires k explicitly—k only indirectly affects whether cooperation is sensible; the strategy remains valid for all allowed k) and public history.
'''

description_AGGRESSIVE_48 = '''
High-level description (aggressive mindset)
- Default stance: be a free-rider. Defect by default to secure the dominant single-round advantage. Only cooperate when cooperating is very likely to buy back larger future gains (i.e., when enough opponents reliably reward cooperation).
- Exploit naive or unconditional cooperators whenever safe. Probe rarely to find exploitable cooperators. If opponents punish your defection, respond decisively (escalating defection / withholding cooperation) so punishers do not keep extracting value from you.
- Never trust that cooperation can be enforced in the final rounds: defect in the last round (and normally the penultimate) to extract immediate gains.

The strategy depends only on game parameters (n, r, k) and observed history of actions. Below I give decision rules, edge-case handling, and pseudocode with recommended internal parameters. The pseudocode is written so it can be implemented directly.

Key notation
- t: current round index (1..r)
- H: full history up to round t-1; H[t'] = vector of c_j,t' for j=1..n (including my past moves)
- me: index of this player
- others: set of players excluding me
- c_j,s ∈ {0,1}: whether player j cooperated in round s

Internal statistics maintained
- CR_j = empirical cooperation rate of player j over last L rounds (Laplace-smoothed)
- P_group_next_given_my_C = empirical average fraction of others who cooperated in the next round after I cooperated in the previous round
- P_group_next_given_my_D = empirical average fraction of others who cooperated in the next round after I defected in the previous round
- baseline_group_rate = average recent fraction of others cooperating (excl. my action)
- punish_score_j = a running score for j indicating how often j defected when I cooperated (higher => j is an exploiter or punisher)

Recommended internal parameters (tunable)
- L = max(5, min(20, r)) — window length for most recent statistics
- epsilon_probe = 0.03 — small probability to randomly cooperate for probing (helps detect unconditional cooperators)
- alpha_persist = 0.6 — how long a change in group behavior is assumed to persist (used in benefit calculations)
- punish_threshold = 0.25 — fraction drop in group cooperation interpreted as punishment following my defection
- max_punish_len = ceil(0.25 * r) — maximum punishment length to deploy against a group that punishes me
- benefit_margin = 1.05 — require future benefits exceed immediate gain by this multiplier (avoid noisy flips)

Decision rules (plain language)
1. Terminal rounds
   - If t == r (last round): Defect (D).
   - If t == r-1 (penultimate): Defect (D). (Reason: last-round defection cannot be credibly punished, so cooperating in penultimate rarely pays.)

2. Default
   - Default action: Defect (D).

3. Rare probing
   - With small probability epsilon_probe in early rounds (t <= ceil(0.5*r)), cooperate (C) to probe for unconditional cooperators. Do not probe if recent data indicates many punishers are present (see below).

4. Profit-driven cooperation ("Profitable Investment" test)
   - Compute immediate gain from defecting over cooperating in one round:
       Δ0 = 1 - (k / n)  (> 0 because k < n)
   - Estimate the expected increase in future per-round group cooperation caused by cooperating now:
       Δp = alpha_persist * (P_group_next_given_my_C - P_group_next_given_my_D)
     (If insufficient data to estimate these probabilities, treat Δp = 0.)
   - Expected future rounds remaining after t: Rf = r - t
   - Expected future benefit to me from an extra cooperating other-player in each future round = (k / n)
   - Estimated future payoff gain from cooperating now:
       ΔF = (k / n) * (Rf) * Δp
   - If ΔF > benefit_margin * Δ0 (i.e., expected future gains sufficiently exceed immediate loss), then cooperate this round.
   - If cooperating passes the test, enter a cooperative streak: keep cooperating for S = min(ceil(Rf/2), L) consecutive rounds or until the conditional statistics change (others stop responding), to harvest the benefit.

5. Exploitative behavior once cooperative streak detected
   - If we detect a subgroup of players whose CR_j >= 0.9 across many rounds and who do not punish our defection (their conditional P_j(C | my D) ≈ P_j(C | my C)), treat them as exploitable: default to defect while you repeatedly probe how much their cooperation continues. Continue exploiting as long as they remain > 80% cooperative and don't coordinate punishment.

6. Punishment detection and escalation (aggressive retaliation)
   - If after I defect in some round s, the fraction of others cooperating in s+1 drops by more than punish_threshold relative to baseline_group_rate, infer group punishment.
   - If group punishes me, escalate by switching to permanent defection for a punishment duration punish_len = min(max_punish_len, ceil(2 * observed_drop_fraction * r)). While punishing, never cooperate and do not probe.
   - Maintain punish_score_j for individuals: if a particular j frequently defects when I cooperated, treat j as an exploiter and ensure you never attempt to rehabilitate them solely — punish by withholding cooperation (but note punishment is only meaningful if others coordinate).

7. Forgiveness/reset
   - After punishing for punish_len rounds, check whether P_group_next_given_my_C has improved; if so, cautiously attempt to cooperate once to test reset; otherwise continue defecting.

8. Edge/no-data rules
   - In round 1 (no history): Defect (D). Optionally probe with probability epsilon_probe to find unconditional cooperators.
   - If data is sparse (t small), be conservative: require larger Δp to trigger cooperation (i.e., increase benefit_margin).

Pseudocode

Initialize:
  CR_j = 0 for all j ≠ me
  punish_score_j = 0 for all j ≠ me
  history window length L = max(5, min(20, r))
  epsilon_probe = 0.03
  alpha_persist = 0.6
  punish_threshold = 0.25
  max_punish_len = ceil(0.25 * r)
  benefit_margin = 1.05
  current_coop_streak = 0
  punish_until_round = 0

On each round t, given H:
  if t == r:
    return D
  if t == r - 1:
    return D

  If t <= ceil(0.5 * r):
    # occasional probing early
    if random() < epsilon_probe and not (t <= punish_until_round):
      return C

  # update statistics using last L rounds of history
  compute CR_j for each j over min(L, t-1) most recent rounds (Laplace smoothing)
  compute baseline_group_rate = average over last min(L, t-1) rounds of (sum_{j≠me} c_j / (n-1))
  compute P_group_next_given_my_C and P_group_next_given_my_D:
    - For each past round s where I cooperated at s, look at fraction of others cooperating at s+1 (if s+1 ≤ t-1), average to get P_group_next_given_my_C (use smoothing)
    - Similarly for my defection rounds to get P_group_next_given_my_D
  compute Δp_raw = P_group_next_given_my_C - P_group_next_given_my_D
  Δp = alpha_persist * Δp_raw
  Δ0 = 1 - (k / n)
  Rf = r - t
  ΔF = (k / n) * Rf * Δp

  # Check if currently punished by group
  if there exists recent s in [t-L, t-1] where I defected and the fraction of others cooperating at s+1 <= baseline_group_rate - punish_threshold:
    observed_drop = baseline_group_rate - (fraction at s+1)
    punish_len = min(max_punish_len, ceil(2 * observed_drop * r))
    punish_until_round = max(punish_until_round, t + punish_len - 1)
    # Immediately escalate: defect until punish_until_round
  if t <= punish_until_round:
    return D

  # Profitable Investment test
  if ΔF > benefit_margin * Δ0:
    # enter cooperative streak
    streak_len = min( max(1, ceil(Rf/2)), L )
    current_coop_streak = streak_len
    return C

  # If inside an active cooperative streak, continue until it ends unless punish_detected
  if current_coop_streak > 0:
    current_coop_streak -= 1
    return C

  # Exploitation of unconditional cooperators:
  if exists many j with CR_j >= 0.9 and average P_j(C | my D) ≈ P_j(C | my C):
    # exploit them
    return D

  # Otherwise default aggressive action
  return D

Notes and rationale
- The per-round single-shot advantage of defecting is constant (Δ0 = 1 - k/n). Any cooperation decision must be supported by expected future benefits large enough to outweigh that fixed cost. The algorithm therefore makes cooperation conditional on a data-driven estimate of future benefit, not on blind reciprocity.
- The small random probing (epsilon_probe) serves two aggressive purposes: (1) reveal which opponents are unconditional cooperators or very forgiving, (2) make it possible to initiate profitable cooperative streaks when opponents reliably reward cooperation.
- Punishment detection is aggressive: if the group collectively punishes you after you defect, the strategy retaliates with a prolonged defection block (punish_until_round). This prevents repeated small extractions by cooperative punishers who would otherwise continue to punish you and extract value.
- The strategy refuses to trust cooperation in the final rounds (defects in last and usually penultimate rounds) to avoid being exploited by backward-inducting opponents.
- This policy is adaptive (uses empirical conditional probabilities), robust (works when opponents are random, naive cooperators, or retaliators), and aggressive (defaults to defection, exploits cooperators, and imposes harsh retaliation on punishers).

Parameter tuning
- Larger epsilon_probe => faster discovery of cooperators but more risk.
- Larger alpha_persist assumes one cooperation influences many future rounds (suitable when r is large).
- benefit_margin prevents noisy data from flipping behavior; increase it in short games.

Summary of aggressive behavior in simple bullets
- Default: always defect.
- Occasionally probe early to find exploitable cooperators.
- Cooperate only when empirical evidence indicates cooperating will produce enough extra future cooperation to repay the immediate cost.
- If group punishes you for defecting, retaliate with extended permanent defection for a calculated duration.
- Always defect in the last round (and normally in the penultimate round).

This strategy is easy to implement, depends only on n, r, k and the observed history, and embodies an aggressive, exploitative, adaptive approach that is robust against a wide variety of opponent behaviours.
'''

description_AGGRESSIVE_49 = '''
Name: Aggressive Exploiter with Responsive Probing (AERP)

High-level idea (aggressive mindset)
- Default to defection to capture the dominant one-shot advantage and to exploit any cooperative opponents.
- Only cooperate when there is clear, measurable evidence that a single cooperation now will raise others’ future cooperation enough to more than offset the immediate cost of 1.
- Gather short, targeted probes early and continue intermittent probes when data are sparse. If opponents are highly cooperative overall, exploit them by defecting consistently.
- Always defect in the last round (backward induction).

Intuition behind the decision rule
- In one round, cooperating always reduces your immediate payoff by (1 - k/n) > 0. The only reason to cooperate is to change opponents’ future behavior. Estimate that effect from history, convert it into expected future benefit, and cooperate only when the estimated future benefit > immediate cost.
- Be aggressive: default = D; cooperate rarely and only when the measured, data-driven expected return is convincingly positive.

Definitions and history bookkeeping
- n, r, k: game parameters (given).
- For each past round t (1..current-1) record:
  - my_action[t] ∈ {C, D}
  - others_coop_count[t] = number of other players (out of n-1) who cooperated that round
  - others_coop_rate[t] = others_coop_count[t] / (n-1)
- For convenience, define rounds s where we can observe a "response" in the next round: for s < current_round, others_coop_rate[s+1] exists.

Core statistics (computed from history)
- A = average others_coop_rate in the round after I cooperated:
    A = mean( others_coop_rate[s+1] | s < t and my_action[s] == C )
- B = average others_coop_rate in the round after I defected:
    B = mean( others_coop_rate[s+1] | s < t and my_action[s] == D )
- delta = A − B (interpreted as per-round increase in other cooperation probability triggered by my cooperating)
  - If either A or B is undefined (no examples), treat that as “insufficient data”.

Estimate of future benefit from cooperating now
- Remaining rounds after this round = H = r − t.
- Conservative expected per-round increase in number of other cooperators if I cooperate now ≈ delta × (n − 1).
- Conservative expected total future increase in cooperators across remaining rounds ≈ delta × (n − 1) × H.
- Monetary value to me per extra other cooperator in each future round = k/n.
- So expected future benefit ≈ (k/n) × delta × (n − 1) × H = delta × k × (H) × (n−1)/n.
  - For simplicity and robustness we use simplified scalar: expected_gain = (k/n) × delta × H (measures cooperators-per-round effect normalized to my payoff).

Decision rules (natural language)
1. Last round: Always Defect.
2. Early probing phase (rounds 1..T_probe, where T_probe = min(3, r−1)):
   - Round 1: Defect (aggressive start).
   - Round 2: Cooperate once as a probe (if r>1). This gives at least one data point linking my cooperation to others’ immediate reaction.
   - Round 3: Defect (unless r==3 then Round 3 is last round and we already defect).
   - Purpose: get minimal, intentional data without surrendering many rounds.
3. After probes (t > T_probe and t < r):
   - If there is sufficient data to estimate delta:
     - Compute expected_gain = (k/n) × delta × H.
     - Cooperate this round if and only if expected_gain > 1 + margin, where margin is a small safety buffer (suggested margin = 0.05).
       - Rationale: you pay 1 immediately; require expected future benefits strictly exceed that cost.
   - If there is insufficient data (no or too few observations of either A or B):
     - Default to Defect, but do deterministic periodic probing: once every P rounds (e.g., P = max(6, ceil(r/10))) attempt a probe cooperation to collect data. (This probe should be rare so as not to be easily exploitable but frequent enough to learn.)
4. Mass-exploitation override:
   - If recent observed average others_coop_rate (across the last L rounds, e.g., L = min(5, t−1)) ≥ coop_exploit_threshold (suggested 0.75), then always Defect (exploit persistent cooperators).
     - Rationale: when the population is largely cooperative, immediate exploitation is best; no need to risk cooperating.
5. Retaliation detection and reaction:
   - If you detect that after you defect others reduce their cooperation rate by a sizable amount compared to baseline (i.e., B is noticeably below overall baseline cooperation by > punish_threshold, e.g., 0.25), then opponents are punishing defectors.
   - In that case you will cease exploitation attempts (i.e., you will stop periodic probes) and revert to cautious long-term optimization:
     - Compute whether limited cooperation (using expected_gain rule above) would yield better long-run payoff. If not, default to Defect but avoid repeated exploitation attempts that incur costly punishments.
     - This prevents you from repeatedly trying to exploit a population that retaliates effectively.
6. Forgiveness and adaptation:
   - If you were in a mode that stopped probing because of retaliators, you still perform rare probes (once per P rounds) to detect whether retaliation has subsided; if responsiveness delta changes to make cooperation profitable, resume exploited cooperation as per expected_gain rule.
7. Deterministic behavior:
   - All decisions depend only on game parameters, history and deterministic periodic probing schedule; randomness is optional but not required.

Edge cases
- r = 2: T_probe = min(3,1) = 1, so round 1: Defect, round 2 (last): Defect. Never cooperate.
- r small (≤3): probing is minimal; last-round defection dominates.
- No data about A or B (early stages): use the fixed small probing schedule to collect data; otherwise Defect.
- Many players (large n): thresholds scale through the delta term and k/n factor so the rule remains valid.
- Opponents use mixed/erratic strategies: AERP estimates responsiveness from observed transitions; noisy opponents increase variance but the margin and conservative gain estimate reduce false positives.
- If estimated delta is negative (cooperating makes others cooperate less): never cooperate unless future horizons are long and you have reason to switch modes (very unlikely).

Pseudocode (concise)
Note: “mean(S)” returns average of set S if non-empty, otherwise returns undefined/null.

Initialize:
  history = empty lists for my_action[t] and others_coop_rate[t]
  T_probe = min(3, r - 1)
  P = max(6, ceil(r / 10))  # periodic probe interval
  margin = 0.05
  coop_exploit_threshold = 0.75
  punish_threshold = 0.25

For each round t = 1..r:
  if t == r:
    action = D
    record and continue
  if t <= T_probe:
    if t == 1:
      action = D
    else if t == 2 and r > 1:
      action = C   # single early probe
    else:
      action = D
    play action, observe others_coop_rate[t], record history, continue

  # Non-probe rounds
  # Compute baseline recent cooperation
  recent_L = min(5, len(history)) 
  recent_avg = mean( others_coop_rate for last recent_L rounds ) if recent_L>0 else 0

  if recent_avg >= coop_exploit_threshold:
    action = D   # exploit mass cooperators
    play, record, continue

  # Compute A and B (responses)
  set_A = []
  set_B = []
  for s in 1..(t-1):
    if my_action[s] == C and s+1 <= t-1:
      set_A.append( others_coop_rate[s+1] )
    if my_action[s] == D and s+1 <= t-1:
      set_B.append( others_coop_rate[s+1] )
  A = mean(set_A) if set_A not empty else undefined
  B = mean(set_B) if set_B not empty else undefined

  if A defined and B defined:
    delta = A - B
    H = r - t
    expected_gain = (k/n) * delta * H
    # Detect punitive response to my defection
    baseline_overall = mean( others_coop_rate for all past rounds ) if history nonempty else 0
    punish_detected = (B < baseline_overall - punish_threshold)
    if expected_gain > 1 + margin and not punish_detected:
      action = C
    else:
      # not profitable (or punishers present)
      # periodic probing to update estimates
      if (t mod P) == 2:   # deterministic periodic probe (offset so not at t=1)
        action = C
      else:
        action = D
    play action, record, continue

  else:
    # insufficient data: default defect with rare deterministic probe
    if (t mod P) == 2:
      action = C   # probe
    else:
      action = D
    play, record, continue

Why this is aggressive and robust
- Aggressive: Default action is defect; last-round defection enforced; if mass cooperation is observed, exploitation is immediate and persistent. Cooperations are rare and performed only when a data-driven future-return calculation justifies them.
- Robust/adaptive: The strategy measures how opponents respond to my cooperation vs defection (delta) and uses a conservative horizon calculation to avoid being fooled by noise. Periodic probes keep the strategy adaptive to opponent changes. A retaliation detector stops repeated exploit attempts if opponents punish defection effectively.
- Parameter-free intuition: although there are small auxiliary parameters (probe frequency P, margins, thresholds), they are set as conservative defaults and can be derived from r, n, k if implementer wishes; the decision rule itself depends only on measurable historical reactions and game parameters.

Implementation notes
- Use at least one probe early (round 2) to create a minimal causal test of responsiveness.
- Use moving averages and smoothing if opponents’ behavior is noisy.
- If implementer prefers randomness over deterministic periodic probing, you may replace the periodic probe with a small probability epsilon probe, but make epsilon small (e.g., 0.05) to preserve aggressiveness.
- The margin can be tuned upward in noisy environments to reduce false-positive cooperation.

Summary (one-line)
Default to defect, probe minimally to estimate if your cooperation triggers others to cooperate, compute the conservative expected future payoff of cooperating (using delta × (k/n) × remaining rounds), and cooperate only when that expected future gain convincingly exceeds the immediate cost; otherwise keep defecting and exploit persistent cooperators — always defect in the final round.
'''

description_AGGRESSIVE_50 = '''
Strategy name: Aggressive Exploiter with Conditional Probing (AECP)

High-level idea
- Default stance is aggressive: defect (D) by default to capture the one-shot advantage.
- Use occasional, targeted cooperative probes (C) to detect exploitable opponents who will keep cooperating even when you defect.
- If a probe shows exploitable cooperation, exploit it by defecting for a short block to extract extra payoff.
- If opponents punish your defections (they cut cooperation substantially after you defect), stop probing and switch to permanent defection for the rest of the game (accept mutual low payoff rather than be repeatedly punished).
- In the final small number of rounds, always defect (no time to repair cooperation).

This strategy depends only on game parameters (n, r, k) and the observed history (actions of all players each round). It is deterministic in its rule set except for the probes which are scheduled deterministically; implementers may introduce randomness if desired but it is not required.

Parameters (derived from given game parameters)
- P = max(1, floor(r / 10))  // probe every P rounds
- H = max(1, floor(r / 10))  // exploit-block length after a successful probe
- B = max(1, floor(r / 5))   // blackout: suspend further probes for B rounds after an unsuccessful probe
- EndDefect = min(3, r - 1)  // final rounds in which we always defect
- SuccessThreshold = ceil(0.60 * (n - 1))  // number of other players cooperating after a probe that counts as “exploitably cooperative”
- PunishDrop = 0.40  // if many players reduce cooperation by >= 40% after your defection, consider them punitive
- PunishMajority = ceil(0.40 * (n - 1))  // number of opponents whose cooperation drop triggers “punitive environment” detection

Notes on these choices:
- P, H, B scale with r so the schedule adapts to short vs long games.
- SuccessThreshold and PunishMajority scale with n.
- The constants (0.60, 0.40) are aggressive: they favor exploitation and quick abandonment if punished.

State and statistics kept
- For every opponent j ≠ i: counts of rounds played, count of times j cooperated.
- Global short-run windows used for comparisons: track cooperation counts for last relevant rounds (e.g., last probe and the round immediately after).
- A flag PunitiveEnvironment (initially false) that, once set true, causes permanent defection for the rest of the game.

Detailed decision rules (natural language + pseudocode)

Initialization:
- Set PunitiveEnvironment = false.
- Maintain history arrays; initialize probe timer next_probe = P (we will probe on rounds t where t == next_probe).
- If r ≤ 3: play D every round (too short to try to manipulate others).

Round t decision:
1. If t > r - EndDefect:
   - Play D (final-defect window). Rationale: no time to repair future cooperation; always take the immediate one-shot advantage.

2. Else if PunitiveEnvironment == true:
   - Play D (permanent defection after detection of punishment).

3. Else (normal regime):
   - If this round is a scheduled probe round (t == next_probe):
       - Play C (probe).
       - Record that we probed on this round and mark we will evaluate the responses in the next round(s).
       - Set next_probe = t + P (unless set to blackout below).
   - Else if we are in an exploit-block that immediately followed a successful probe and still within that block:
       - Play D (exploit while others are likely to cooperate).
   - Else:
       - Play D (default aggressive choice).

Evaluation after each round (update statistics and adjust mode):
- After each round t, update per-opponent cooperation counts and overall cooperation numbers.
- If we probed on round t0 = t (i.e., we played C at t0), then in round t0+1:
   - Let coop_after_probe = number of other players who played C at t0+1.
   - If coop_after_probe ≥ SuccessThreshold:
       - Mark that the probe succeeded.
       - Enter exploit-block for the next H rounds (play D each of those rounds). (Exploit-block starts at t0+1 and lasts H rounds; if it overlaps final-defect window, final-defect takes precedence.)
   - Else:
       - Probe unsuccessful. Suspend further probes for the next B rounds by setting next_probe = t0 + B + 1 (blackout).
- Punishment detection (run every round after t ≥ 2):
   - For each opponent j, compute baseline cooperation rate p_j_baseline from earlier rounds (e.g., their cooperation frequency in the rounds before your most recent defection; implementation can use a simple moving average or the entire history prior to your last defection).
   - Compute p_j_after = cooperation rate of j in the rounds immediately after your defections (e.g., the one or two rounds after you defected).
   - If the number of opponents j for which p_j_baseline - p_j_after ≥ PunishDrop is ≥ PunishMajority:
       - Set PunitiveEnvironment = true and stop probing (permanently play D thereafter).
       - Rationale: a significant group punishes your defection by cutting cooperation sharply; aggressive response is to stop trying to manipulate them and remain defecting.

Edge cases and clarifications
- Very short games (r ≤ 3): always defect every round. There is no time to benefit from trying to induce cooperation.
- If probes happen near the end so that exploit-block would overlap the final-defect window, final-defect (always D) overrides exploitation (we do not start exploitation when only EndDefect or fewer rounds remain).
- If opponents are perfectly unconditional cooperators (never punish), probes will consistently succeed and lead to repeated exploit-blocks that maximize your payoff.
- If opponents punish aggressively and collectively, you will detect punishment quickly and switch to permanent defection (accepting mutual low-payoff but preventing repeated costly punishment cycles).
- The strategy requires only observation of other players’ actions and payoffs from past rounds (both are available by the spec). All decisions are based on those observations and the game parameters (n, r, k).

Why this is "aggressive" and why it is robust
- Aggressive aspects:
  - Default behavior is defection (never cedes the one-shot advantage).
  - When exploitable cooperation is found, the strategy immediately and repeatedly defects to extract the maximal personal windfall.
  - If opponents punish, the strategy abandons attempts at cooperation and locks into permanent defection rather than being cowed into sustained low payoff.
- Robustness:
  - Probing is explicit and limited in frequency to limit losses against non-exploitable opponents or punishers.
  - The punishment-detection rule prevents long cycles of costly mutual retaliation; it recognizes collective punishment and responds by minimizing further losses.
  - Parameter choices scale with r and n so the schedule adapts to short vs long games and small vs large groups.
  - The policy does not assume norms, communications, or pre-arranged plans; it learns from observed behavior and adapts.

Pseudocode (compact)
- If r ≤ 3: play D every round.
- Initialize next_probe = P, PunitiveEnvironment = false, exploit_remaining = 0.
- For each round t = 1..r:
   - If t > r - EndDefect: action = D.
   - Else if PunitiveEnvironment == true: action = D.
   - Else if exploit_remaining > 0: action = D; exploit_remaining -= 1.
   - Else if t == next_probe: action = C; record probe_round = t; next_probe = t + P (subject to blackout adjustments).
   - Else: action = D.
   - Observe others’ actions this round and update per-opponent stats.
   - If we probed at round p = t - 1 (i.e., we cooperated at p and are now observing p+1):
       - Let coop_after_probe = #others who played C at t.
       - If coop_after_probe ≥ SuccessThreshold: exploit_remaining = H (start exploitation at t, counted in the logic above).
       - Else: next_probe = t + B (blackout).
   - Run punishment detection: if #opponents whose cooperation dropped by ≥ PunishDrop after our defections ≥ PunishMajority, set PunitiveEnvironment = true.

Implementation notes
- Exact windows for measuring baseline and after-defection cooperation (for punishment detection) can be implemented as simple averages over the last few relevant rounds (e.g., baseline = average cooperation in the 5 rounds before you first defected or before a given defection, p_after = cooperation in the 1–2 rounds after your defection). Use consistent windows; the strategy’s thresholds are tuned for aggressive behavior so they are tolerant of noisy estimates.
- All thresholds (0.60, 0.40, fractions of r) are tunable; the ones provided reflect a strongly exploitative, robust stance.

Summary
AECP is a simple, aggressive, history-dependent strategy: start by defecting, probe occasionally by cooperating to find exploitable cooperators, exploit them in short blocks when found, and immediately halt all attempts at cooperation and switch to permanent defection if many opponents punish your defections. In the final rounds always defect. This produces a high-exploit, deterrence-aware policy that is easy to implement and robust to a wide range of opponent behaviors.
'''

description_AGGRESSIVE_51 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

Summary (one line)
- Be opportunistically cooperative only when doing so is likely to create sustained mutual contributions; otherwise defect. Lure cooperation early if profitable, exploit guaranteed cooperators, and respond to defections with fast, harsh, but reversible punishment. Always defect in the final round.

Key ideas (rationale)
- In a single round defection strictly dominates cooperation (k/n < 1), so cooperation is only instrumentally valuable if it changes future behavior. ACE therefore only cooperates when historic behavior shows a high chance of future return.
- ACE is aggressive: it (a) starts by probing to detect reciprocators, (b) defects to exploit clear cooperators, and (c) punishes defectors decisively to deter them from exploiting ACE in future rounds. Punishment is reversible but sufficiently long to matter.
- ACE uses only the game parameters (n, r, k) and the complete history of observed actions; no assumptions about opponents’ strategies or shared norms are required.

Parameters ACE uses internally (fixed heuristics; can be tuned)
- L = min(5, r) — lookback window size for short-term assessments
- S_probe = min(2, r-1) — number of opening probing rounds (cooperate unless early evidence of defection)
- θ_start = 0.7 — fraction of other players cooperating in probe window required to consider entering cooperative mode
- θ_continue = 0.5 — continuing cooperation requires recent group cooperation ≥ θ_continue
- τ_punish = max(3, floor(r/6)) — length of punishment once initiated (capped by remaining rounds)
- s_redeem = 3 — number of consecutive cooperative rounds by a punished player to remove punishment
- R_safe = 1 — number of final rounds where ACE always defects (always use R_safe=1 so final round defect is guaranteed)

High-level decision rules
1. Final rounds
   - If current round t > r - R_safe: Defect. (Always defect in the final round; if R_safe>1 treat final R_safe rounds as endgame and defect.)

2. Opening / probing
   - Rounds 1..S_probe: Cooperate by default to test other players’ willingness to cooperate, unless you already observe that the majority of others defected in the first observed round(s) — in that case switch immediately to defection-permanent (see “failed probe” below).

3. Post-probe mode selection
   - After S_probe rounds compute the fraction P_probe of other players’ cooperations observed in the probe window.
     - If P_probe ≥ θ_start and remaining rounds > 1: enter Cooperative-Conditional mode.
     - Else: enter Defect-Default (exploit/lockdown) mode.

4. Cooperative-Conditional mode (attempt to sustain cooperation but exploit when profitable)
   - In each round t (not final):
     a. If you are currently punishing (see punishment rules) then Defect.
     b. Else compute recent group cooperation rate P_L = fraction of other players who cooperated across the last min(L, t-1) rounds (if t-1 = 0, use P_L = P_probe).
     c. If P_L ≥ θ_continue and remaining rounds ≥ 2 then:
        - If previous round all other players cooperated (i.e., they looked like guaranteed cooperators), Defect this round to exploit (a single-round free-ride). This is aggressive: occasional single-round defections against fully cooperative groups to harvest the large private margin.
        - Else Cooperate.
     d. If P_L < θ_continue then defect (to punish/avoid being exploited) and enter punishment mode targeted at any player who defected while you cooperated in the immediately preceding round.

5. Defect-Default mode (if probing failed)
   - Defect every round, but re-run a fresh probe after a window of t_gap = max(3, floor(r/6)) rounds if the environment changes (i.e., if you see unexpectedly many cooperations by others). This allows opportunistic re-entry to Cooperative-Conditional mode if the population shifts.

6. Punishment (aggressive deterrence)
   - Maintain a set Punished of players whom you will punish.
   - Trigger: if in any round you played C and some other player j played D, add j to Punished.
   - Behavior: while Punished is non-empty you Defect (this reduces the public good and lowers punished players’ payoffs).
   - Duration: each punished player j remains in Punished for τ_punish rounds from their most recent triggered defection, unless they produce s_redeem consecutive cooperative actions in subsequent rounds (in which case remove them early).
   - Punishment is targeted in bookkeeping (you track who triggered it) but executes via your own defection; this is enough to reduce punished players’ future payoffs and signals consequences.
   - After punishment period ends for all members you resume Cooperative-Conditional or Defect-Default mode depending on recent rates.

7. Forgiveness & recovery
   - After a punished player shows s_redeem consecutive Cooperates (and group cooperation becomes respectable again, P_L ≥ θ_continue), remove them from Punished and allow cooperation toward them again.
   - Forgiveness is limited and conditional — ACE does not forgive immediately.

Decision summary (concise pseudocode)
- Inputs: n, r, k, history actions[t'][player] for t'<t
- Track: mode ∈ {Probe, CoopConditional, DefectDefault}, Punished dictionary {player: remaining_punish_rounds}, last_actions
- For round t:
  1. If t > r - R_safe: action = D
  2. Else if mode == Probe and t ≤ S_probe:
       if (observed majority of others defected in rounds so far) then mode = DefectDefault; action = D
       else action = C
  3. Else if mode == Probe:
       compute P_probe
       if P_probe ≥ θ_start and remaining_rounds ≥ 2: mode = CoopConditional
       else mode = DefectDefault
       (then evaluate action under new mode)
  4. If any Punished entry has remaining_punish_rounds > 0: decrement counters; action = D
     (allow early removal if punished player has s_redeem consecutive C)
  5. If mode == CoopConditional:
       compute P_L (past L rounds cooperation fraction of others)
       if P_L ≥ θ_continue and remaining_rounds ≥ 2:
           if all other players cooperated in previous round: action = D   (exploit)
           else action = C
       else:
           add to Punished any player who defected while you cooperated in previous round
           action = D
  6. If mode == DefectDefault:
       action = D
       if you observe a run of unexpectedly high cooperation from others (P_L ≥ θ_start for a lookback) then switch mode to Probe and conduct new probe sequence
  7. After determining action, update Punished (start timers, check s_redeem streaks), record action.

Edge cases and special handling
- Very small r (r=2 or r=3): Cooperate only in probe rounds if r≥3 and only if probe shows strong reciprocation; otherwise defect (because there is little opportunity to recoup cooperation).
- If r is large: ACE becomes more willing to cooperate conditionally because there is more future to realize reciprocal benefits.
- If k is very close to n (k/n ≈ 1): private marginal cost of cooperating is small; ACE’s thresholds θ_start and θ_continue implicitly make ACE more willing to cooperate because it needs less evidence to expect returns. (You can optionally lower θ_start when k/n is large.)
- If many opponents always cooperate: ACE will exploit them periodically (single-round defections when others are guaranteed to cooperate) before rejoining cooperation to avoid being permanently ostracized.
- If many opponents always defect: ACE defaults to always defect (DefectDefault) and will not waste payoffs on fruitless cooperation.

Aggressiveness checklist (how this is “aggressive”)
- Probes to detect cooperators rather than blindly cooperating.
- Exploits fully cooperative groups by deliberately free-riding when safe (maximizes one-shot gains while minimizing long-term retaliation risk).
- Fast, long punishments for defectors who exploit ACE when ACE cooperated (aimed to reduce their payoff and deter future exploitation).
- Limited forgiveness: requires a sustained cooperative signal before revoking punishment.
- Endgame defection: always defects in the last round (standard aggressive endgame).

Robustness
- Works against pure defectors (you defect and avoid losses), pure cooperators (you exploit occasionally to raise your payoff), conditional cooperators (you mirror and punish appropriately), and mixed/random players (you adapt thresholds based on observed frequencies).
- Uses only history and parameters; no outside coordination necessary.

Implementation notes for engineers
- Maintain per-player cooperation counters and recent streaks; update Punished timers each round.
- Use simple moving-window averages for P_L and P_probe.
- Tune θ_start, θ_continue, τ_punish, s_redeem if empirical tournament results suggest different tradeoffs between exploitation and sustaining cooperation.
- Optionally make θ_start a decreasing function of k/n (more willing to cooperate when public return is larger).

Examples of behavior
- If many players cooperate in probes, ACE will enter CoopConditional, cooperate most rounds but occasionally defect in a round after seeing everyone else cooperate to harvest a high payoff, then resume cooperation. If any player defects while ACE cooperated, ACE punishes for τ_punish rounds by defecting.
- If probes reveal mostly defectors, ACE locks into DefectDefault and defects every round.
- In last round ACE defects regardless.

This strategy balances aggressive immediate payoffs (defection/exploitation) with instrumental cooperation when past evidence indicates future reciprocal gain; aggression is implemented via targeted, sustained punishment and periodic exploitation of reliable cooperators.
'''

description_AGGRESSIVE_52 = '''
Name: Aggressive Conditional Punisher (ACP)

Short summary (mindset)
- Start aggressively (probe by defecting), exploit any players who keep cooperating, and otherwise try to coerce group cooperation by conditional cooperation when the group looks reliably cooperative. 
- If anyone defects against my cooperation, respond with fast, severe, targeted punishment (short but intense) and only forgive slowly. 
- Near the end of the finite horizon, abandon cooperation and defect (endgame).  
This strategy only uses the public history (who played C/D each past round), the known parameters (n, k, r), and the current round index.

Core intuition and parameter definitions
- α := k / n (the per-player return from a single contribution). 0 < α < 1.
- Cooperation is never a strict immediate best response in a single round (α < 1), so cooperation is only worth doing to induce future cooperation. We therefore:
  - Probe early by defecting to detect unconditional cooperators we can exploit.
  - If the group demonstrates a reliably high cooperation rate while I defect, exploit them.
  - If the group appears to be conditional cooperators, try conditional cooperation to encourage mutual cooperation — but punish hard and quickly any betrayal.
- Concrete internal parameters (all computed from game parameters for implementability):
  - ProbeLength L = min(3, max(1, floor(r/5))) — small number of opening probing rounds (at least 1).
  - Window W = min(5, max(1, r-1)) — how many recent rounds to average others’ behavior.
  - q_coop = clamp(0.6 - 0.3*α, 0.4, 0.8) — required recent fraction of other players cooperating to justify cooperating.
  - exploit_threshold = 0.8 — if many others cooperate while I defect, switch to permanent exploitation (keep defecting).
  - punishment_base = 2 — base punishment length in rounds.
  - punishment_duration P = min(remaining_rounds - 1, punishment_base + number_of_defectors_in_trigger) — punishment length scales with how many players defected when I was betrayed (capped by remaining rounds).
  - q_heal = min(0.95, q_coop + 0.1) — criterion for returning to cooperation after punishment.
  - Endgame cutoff R_thresh = 3 — if ≤ R_thresh rounds remain, always defect.

State maintained (derived from public history)
- history of all players' actions by round (so we can compute number of cooperators each round).
- punishment_mode flag and punishment_end_round (if set, we are currently punishing).
- exploit_mode flag (if set, permanently defect except last-round default).
- last cooperation window averages to compute f_recent: the fraction of other players who cooperated averaged over the last W rounds (excluding my own actions when appropriate).

Decision rules (in natural language)
1) Endgame:
   - If current round t is the final round (t == r) or remaining rounds <= R_thresh: play D.

2) Exploit detection (aggressive exploitation):
   - During any round (including probe rounds), if I played D and the fraction of other players who played C that round ≥ exploit_threshold, set exploit_mode := TRUE and from then on always play D (except final-round rule is already D). Exploitation means I exploit naive cooperators for the rest of the game.

3) Probing phase (rounds 1..L):
   - Default play: D (aggressive probe).
   - Purpose: observe whether many players are unconditional cooperators who will keep cooperating even if I defect (to exploit).
   - If exploit condition (2) triggers while probing, switch to exploit_mode.

4) Normal/coercion phase (after probing, when not in exploit_mode and not in punishment_mode):
   - Compute f_recent := average fraction of other players cooperating over last W rounds.
   - If f_recent ≥ q_coop and remaining_rounds > R_thresh: play C (attempt to build mutual cooperation).
   - Else: play D.

   Rationale: only cooperate when the group looks reliably cooperative (my cooperation has a decent chance of being reciprocated). The q_coop depends on α: when α is larger, cooperating is relatively more valuable, so q_coop is lower.

5) Triggering punishment:
   - If in any round I played C and one or more other players chose D that same round (i.e., someone defected while I cooperated), then:
     - Enter punishment_mode.
     - Set number_of_defectors_in_trigger := number of players who defected that round.
     - Set punishment_end_round := current_round + P where P = min(remaining_rounds - 1, punishment_base + number_of_defectors_in_trigger).
     - While in punishment_mode (current_round ≤ punishment_end_round), always play D.
   - Also trigger punishment if the recent cooperation rate f_recent drops abruptly below a previous window by a large margin (optional robust trigger): if f_recent decreases by > 0.25 versus the prior W-round average, treat that as a coordinated betrayal and trigger the same punishment.

   Rationale: punishment must be clear, fast and sufficiently long to make defection costly for others. Punishment length scales with how many defected.

6) Forgiveness / exit punishment:
   - After punishment_end_round passes, do not immediately resume cooperation.
   - Require that f_recent ≥ q_heal for two consecutive windows (or over the last W rounds and the W rounds before that) to resume normal cooperative behavior. Until healing is observed, continue to play D.
   - Exception: if exploit_mode was set earlier, remain in exploit_mode.

7) Exploit-mode behavior:
   - If exploit_mode is true: always play D (this is pure exploitation). Because exploit_mode was set only after seeing others cooperate heavily while I defected, exploitation is profitable.

8) Edge cases:
   - Very small groups (n = 2): q_coop clamps (due to clamp limits) still work; punishment duration P will be small; the endgame logic ensures rational defection near end.
   - If r is very small (e.g., r=2 or 3): L will be 1, so the strategy probes only briefly and quickly goes to defect; the endgame cutoff R_thresh ensures no wasteful late cooperation.
   - If many strategies in population always cooperate regardless, exploit_mode will trigger early and you will always defect hence capturing immediate gains.
   - If population is mostly harsh punishers, you will be forced into cooperation only when f_recent surpasses q_coop; otherwise you will defect or punish, minimizing losses.
   - No communication or coordination beyond public actions is required.

Compact pseudocode (outline)

Given n, k, r
α := k / n
L := min(3, max(1, floor(r/5)))
W := min(5, max(1, r-1))
q_coop := clamp(0.6 - 0.3*α, 0.4, 0.8)
exploit_threshold := 0.8
punishment_base := 2
R_thresh := 3
q_heal := min(0.95, q_coop + 0.1)

state: punishment_mode=false, punishment_end_round=0, exploit_mode=false

for each round t = 1..r:
  remaining := r - t + 1
  if remaining <= R_thresh: play D; continue
  if t == r: play D; continue
  if exploit_mode: play D; continue
  if punishment_mode and t <= punishment_end_round:
    play D
    continue
  compute f_recent := average fraction of other players who cooperated over last W rounds (if fewer than W rounds exist, use all available)
  if t <= L:   // probing
    play D
    observe other players' actions this round
    if fraction_of_others_cooperating_this_round >= exploit_threshold:
      exploit_mode := true
    continue
  // Normal/coercion behavior
  if f_recent >= q_coop:
    play C
    observe others this round
    if any other defected this round:
      // trigger punishment
      d := number_of_defectors_this_round
      P := min(remaining - 1, punishment_base + d)
      punishment_mode := true
      punishment_end_round := t + P
  else:
    play D
    // optionally detect sudden drop in cooperation and trigger punishment:
    if recent_drop > 0.25:
      d := number_of_defectors_in_last_round
      P := min(remaining - 1, punishment_base + d)
      punishment_mode := true
      punishment_end_round := t + P

After punishment_end_round, do not immediately cooperate: require f_recent >= q_heal in two consecutive windows to resume cooperation. Exploit_mode persists once set.

Why this is "aggressive" and robust
- Aggressive: initial defecting probes seize easy gains from unconditional cooperators; exploit_mode locks in exploitation when profitable. Punishments are fast and scaled to the severity of the betrayal, discouraging opportunistic defection in future rounds.
- Robust/adaptive: decisions are based on empirical recent cooperation rates and scale with observed defections; the strategy adapts to populations that are naive (exploit), forgiving (sustain cooperation), or retaliatory (avoid cooperating or punish back).
- Finite-horizon aware: the endgame cutoff prevents wasted attempts to coerce cooperation when threats are no longer credible.

Implementation notes and tuning
- The numeric constants (probe length, window size, thresholds) are choices that trade exploration/exploitation and severity of punishment. They are computed from n, k, r so implementers can adjust them if desired.
- The strategy is simple to implement from public history: it only needs to count cooperators per round and maintain a few flags and counters.
- If you want more or less aggression: increase exploit_threshold (less likely to exploit) or decrease it (more likely); increase punishment_base for harsher punishment; shorten healing requirement for more forgiveness.

This strategy is designed to (1) capture gains from naive cooperators, (2) coerce conditional cooperators into cooperation by credible, rapid punishment, (3) be conservative near the end of the game, and (4) adapt to a wide range of opponent behaviors with minimal assumptions.
'''

description_AGGRESSIVE_53 = '''
Name: Aggressive Gatekeeper (AG) — an adaptive, punitive, exploitation-capable strategy for the repeated public-goods game.

Intuition (short):
- Probe early to identify who cooperates.
- Form a “trusted/cooperator set.” Try to sustain cooperation with that set when it is large enough, but exploit it opportunistically to raise your own payoff.
- Treat any betrayal by a trusted player as a serious offense: punish with a multi-round defection that escalates with repeat betrayals.
- Default to defection unless there is a credible, sufficiently-large cooperative coalition to make cooperation worth the long-run investment.
- Always defect in the final round (standard backward-induction aggression).

Key constants (tunable; set from n, r, k or fixed small integers):
- probe_rounds = min(2, max(1, floor(r/10))) — initial probing rounds.
- window L = min(10, max(3, floor(r/4))) — history window for estimating recent behavior.
- p_good = 0.65 — threshold cooperation rate (in window L) to label a player a “cooperator”.
- majority_frac = 0.5 — fraction of others who must be “cooperators” to consider coordinated cooperation.
- p_exploit = 0.25 — probability (or fraction of opportunities) to defect opportunistically when coalition looks solid.
- base_punish = 3 — minimal punishment length (in rounds).
- punish_scale = 2 — punishment escalation multiplier per repeat betrayal.
- forgive_window = L — re-evaluate forgiveness using the same window.
- endgame_horizon = min(3, floor(r/10)) — last rounds where we switch to guaranteed defection.

State maintained:
- For each player j ≠ i: coop_count_j (in last L rounds), betray_count_j (number of times j betrayed after being trusted).
- punishment_counter (global counter of rounds left in a punishment phase where AG will defect unconditionally).
- last_actions for all players (including self).
- round t.

Decision rules (natural language, then concise pseudocode):

1) First round(s) — probing
- For t ≤ probe_rounds: play C. (Signal willingness to cooperate to identify cooperators quickly.)
Rationale: an aggressive program still needs to know who will reciprocate; a short probe is worth detection.

2) Last rounds / endgame
- If t > r - endgame_horizon or t == r: play D. (Do not cooperate in the last few rounds; in final round always defect.)

3) If currently in a punishment phase
- If punishment_counter > 0: play D and decrement punishment_counter each round.
Rationale: punishments must be credible and multi-round to deter.

4) Maintain a rolling estimate of each opponent’s cooperation rate over the most recent min(t-1, L) rounds:
- coop_rate_j = coop_count_j / window_size.

5) Identify trusted set and evaluate coalition
- Trusted set G = { j : coop_rate_j ≥ p_good }.
- If |G| ≥ majority_threshold where majority_threshold = ceil((n-1) * majority_frac), we consider a credible coalition exists.

6) Cooperation vs Defection when not punishing and not in endgame
- If credible coalition exists:
  - If last round saw universal cooperation (all players cooperated) then:
    - Opportunistic exploit: with probability p_exploit (or deterministically every floor(1/p_exploit)-th time) play D to free-ride once; otherwise play C to sustain cooperation.
      - If you exploit (play D while many others cooperated), you must be ready to enter a punishment phase next round if those players retaliate.
  - Else (no universal cooperation last round), play C to help restore and preserve the coalition.
- If no credible coalition exists:
  - Play D (default aggressive behavior).

7) Triggering punishments (aggressive enforcement)
- If any player j ∈ G defects in a round when a coalition was expected (i.e., most of G cooperated that round but j defected), treat j as a betrayer:
  - betray_count_j ← betray_count_j + 1
  - Set punishment_counter = max(punishment_counter, base_punish * (punish_scale)^(betray_count_j - 1))
  - Immediately begin collective punishment by defecting for punishment_counter rounds (punish the group by refusing contribution).
Rationale: escalate punishment for repeat betrayers to make cooperation credible.

- Also trigger a community punishment if global cooperation suddenly collapses (e.g., global coop rate drops below 50% after sustained high cooperation ≥ 70%): set punishment_counter = base_punish.

8) Forgiveness and re-entry
- After punishment_counter expires, re-compute coop_rate_j over forgive_window:
  - If coop_rate_j ≥ p_good, restore j to G (remove further escalation for that player).
  - Else leave betrayal count intact and remain skeptical (future infractions increase punishments).
- Small randomized forgiveness: with small probability f (e.g., 0.05) allow reentry earlier to break cycles if it appears everyone is defecting permanently.

9) Practical notes on p_exploit and randomness
- If deterministic implementations are required, replace probabilistic exploit by a deterministic schedule: defect on the first cooperative-opportunity after the probe, then wait K rounds of unconditional cooperation with the coalition before next exploit, or defect every M-th time universal cooperation occurs.
- The exploitation frequency should be low enough to avoid making cooperation unsustainable, but frequent enough to raise AG’s absolute payoff.

Pseudocode (outline):

Initialize coop_count_j = 0, betray_count_j = 0 for all j ≠ i
punishment_counter = 0

for t in 1..r:
  window = min(t-1, L)  # number of past rounds we can inspect
  if t <= probe_rounds:
    action = C
  else if t == r or t > r - endgame_horizon:
    action = D
  else if punishment_counter > 0:
    action = D
    punishment_counter -= 1
  else:
    for each j ≠ i compute coop_rate_j = coop_count_j / window (if window > 0, else 0)
    G = { j : coop_rate_j >= p_good }
    if |G| >= ceil((n-1)*majority_frac):
      if all players cooperated last round:
        # opportunistic exploit with probability p_exploit
        if random() < p_exploit:
          action = D
          # mark that we exploited this round; we will check for retaliation next rounds
        else:
          action = C
      else:
        action = C
    else:
      action = D

  Play action; observe others' actions; update coop_count_j (sliding window bookkeeping)
  # Punishment detection:
  if not in punishment (punishment_counter == 0):
    if there existed a prior coalition (|G_old| big) and some j in G_old defected this round while most of G_old cooperated:
      betray_count_j += 1
      new_punish = base_punish * (punish_scale)^(betray_count_j - 1)
      punishment_counter = max(punishment_counter, new_punish)

  # Post-punishment forgiveness: after punishment_counter reaches 0, recompute coop rates over forgive_window and restore trust where appropriate
  (Implement small random forgiveness to escape persistent mutual defection)

Why this is aggressive and robust
- Aggressive: defaults to defection, exploits stable cooperators, and applies escalated, multi-round punishments for betrayal. It does not tolerate recurrent exploitation and will raise the cost of betrayal via sustained group defection.
- Robust/adaptive: probes early to identify cooperators, uses sliding-window statistics to adapt to changing opponent behaviour, escalates punishment only when warranted (so punishments are credible), and uses controlled forgiveness to avoid endless wars.
- Tournament-minded: avoids getting trapped by naive cooperators forever (exploits them opportunistically), but can cooperate with reciprocators when it is profitable to secure higher long-run payoff.

Edge cases summary
- First round(s): cooperate for a short probe window (signal and detect cooperators).
- Last round: defect.
- Endgame window (few last rounds): switch to assured defection to avoid being exploited.
- If punishment is in effect: defect until punishment_counter expires; punishment length escalates with repeat betrayals.
- If many players all defect persistently (mutual defection spiral), use occasional random forgiveness to attempt re-establishing a cooperative core (but infrequently, to maintain aggression).

Tuning tips
- Increase base_punish or punish_scale to be more punitive; decreases your own cooperation losses at the cost of more repeated defection cycles.
- Increase p_good to be stricter about whom you trust; decrease p_good to be more willing to try coalition-building.
- Increase p_exploit if you prefer to extract more short-term gains at risk of more retaliation.
- Longer windows L smooth noise but slow adaptation; short windows react faster but can be noisy.

Implementation note
- This strategy requires tracking past actions by player identity (allowed by the game spec).
- Randomness is used only for occasional exploitation and forgiveness; a deterministic schedule can replace it if desired.

Overall: AG is designed to be clearly aggressive — it punishes betrayal harshly, exploits stable cooperators, and cooperates only when a sufficiently large, reliable coalition exists and the payoff from doing so is likely to outweigh the risk of exploitation. It is adaptive (sliding-window statistics), scalable with n and r (uses horizon-aware endgame rules), and robust to a wide variety of opponent behaviours.
'''

description_AGGRESSIVE_54 = '''
Strategy name: Aggressive Exploit-and-Punish (AEP)

High-level idea (aggressive mindset)
- Default to defection to avoid being exploited; only give cooperation as a short, controlled “bait” when there is a clear, persistent signal that many others are willing to contribute. Use that bait to elicit cooperative behavior and then immediately exploit (defect) to collect free-rides. If the group responds by punishing me (reducing cooperation to deny my exploitation), respond with permanent, uncompromising defection (grim) to avoid being repeatedly exploited by any “nice” strategies or to punish retaliators harshly. Never cooperate in the final round(s) where there is no prospect of future reward.

This strategy (1) protects against pure defectors, (2) extracts surplus from naive or reciprocating cooperators, and (3) retaliates strongly against attempts to impose cooperative norms on me.

Parameters derived from (n, r, k)
- r: number of rounds (r > 1). If r is very small the strategy simplifies to always defect.
- n: number of players.
- Window and thresholds:
  - w (lookback window) = min(3, max(1, floor(r/10))) — small window of recent rounds to detect emerging cooperation.
  - H (high-cooperation threshold among others) = ceil((n-1)/2). Require a majority of the other players to have cooperated in the measured rounds to count as a cooperative signal.
  - C_seed (consecutive-high rounds to seed) = 2 (two consecutive rounds with at least H others cooperating).
  - cooldown = max(1, floor(r/10)) — minimum rounds between successive bait attempts.
  - exploit_length E = min(max(1, floor(r/4)), remaining rounds − 1) — how many rounds to exploit after a successful bait (never exhaust final round).
  - final_safety_rounds F = 1 (always defect in last round); optionally F = 2 if you want stricter end-game exploitation avoidance.

Behavioral modes
- SEEK (default): defect every round; monitor others for persistent high cooperation.
- BAIT (seed): play one cooperative move to elicit cooperation from others.
- EXPLOIT: after a successful BAIT, defect for E rounds to collect free-rides while others keep cooperating.
- PUNISH (grim): permanent defection for the rest of the game after being punished or if exploited unexpectedly.
- SAFE-END: in the last F rounds, always defect.

Decision rules (natural-language)
1. First round: Defect. (No trust is given initially.)
2. Default behavior (SEEK): Defect. Keep a running count of how many of the other players cooperated in each recent round. If for C_seed consecutive rounds at least H of the other players cooperated in each of those rounds, and there are at least 2 rounds remaining, transition to BAIT. Enforce a cooldown between bait attempts.
3. BAIT (single round): Play C for exactly one round (the bait) to encourage cooperative responses. Observe the immediate reaction of others in the next round.
   - If, in the round immediately following your BAIT, at least H other players cooperate (they accepted the bait / are continuing cooperation), treat the BAIT as successful and go to EXPLOIT.
   - If cooperation falls below H (they punished you or refused to cooperate), treat this as punishment and switch to PUNISH (permanent defection).
4. EXPLOIT: Play D for E consecutive rounds (or until end-game safety). During EXPLOIT:
   - If others’ cooperation collapses sharply (cooperation among others falls below H for 2 consecutive rounds), assume the group is trying to punish or adapt; immediately switch to PUNISH.
   - Otherwise continue exploiting for E rounds. After exploit period completes, return to SEEK but enforce cooldown before the next BAIT attempt.
5. PUNISH: Defect for the remainder of the game (grim trigger).
6. End-game safety: In the last F rounds (including final round), always defect (no cooperation where no future benefit is possible).

Edge cases and special rules
- Very short games (e.g., r ≤ 4): almost always defect every round. If r ≤ 2, defect all rounds.
- If n = 2 (pairwise public good is equivalent to 2-player PD): thresholds reduce to H = 1 (other player cooperated). The same pattern applies but behavior is naturally simpler (bait if the other cooperates repeatedly).
- If a BAIT round is triggered but remaining rounds ≤ 1 (i.e., no time to exploit), do not BAIT — continue defecting.
- If multiple players attempt to BAIT simultaneously (observed many cooperators including yourself), the rules above still apply—the success condition is based purely on observed response (cooperation among others).
- If you detect targeted deviations (e.g., one player cooperates while everyone else defects to try to be exploited): that is fine—this strategy exploits such unilateral cooperators when baiting; otherwise you remain defecting.
- Randomization: to avoid perfect predictability and to exploit strategies that attempt to second-guess deterministic play, you may optionally randomize a small fraction (ε) of SEEK defections to cooperate with tiny probability (ε = 0.01–0.05). However, the core design is deterministic and aggressive.

Pseudocode (clear, implementable)
(Variables kept across rounds: mode, consecutive_high, last_bait_round, cooldown_counter)

Initialize:
  if r <= 2: always play D every round.
  mode = SEEK
  consecutive_high = 0
  last_bait_round = -infty
  cooldown_counter = 0

For each round t = 1..r:
  remaining = r - t + 1
  Observe actions of others in past w rounds (or fewer if t small). Let m_prev = number of other players who cooperated in round t-1 (if t>1).
  If t > 1:
    if m_prev >= H: consecutive_high += 1 else consecutive_high = 0
  else:
    consecutive_high = 0

  If remaining <= F:
    play D (SAFE-END). If mode != PUNISH, set mode = PUNISH and continue.

  If mode == PUNISH:
    play D
    continue

  If mode == SEEK:
    If (consecutive_high >= C_seed) AND (t - last_bait_round >= cooldown) AND (remaining >= 2):
      mode = BAIT
      play C
      last_bait_round = t
      continue
    Else:
      play D
      cooldown_counter = max(0, cooldown_counter - 1)
      continue

  If mode == BAIT:
    // BAIT lasts exactly one round and we already played C this round.
    // Next round we must observe reaction.
    play C (only happens if we entered BAIT at start of this round)
    // After this round, on the next iteration we will evaluate m_prev to decide
    // whether to go to EXPLOIT or PUNISH:
    continue

  If mode is to be set after BAIT evaluation (on the next round t'>t):
    // Evaluate based on round t results
    If m_prev >= H (i.e., after the BAIT, others cooperated in response):
      mode = EXPLOIT
      exploit_remaining = min(E, remaining - 1)  // keep at least one round safe at end
      play D
      exploit_remaining -= 1
      continue
    Else:
      mode = PUNISH
      play D
      continue

  If mode == EXPLOIT:
    play D
    If exploit_remaining > 0:
      exploit_remaining -= 1
    // Monitor others: if others’ cooperation falls below H for 2 consecutive rounds, set mode = PUNISH
    If detect_sharp_collapse_of_cooperation():
      mode = PUNISH

    If exploit_remaining == 0:
      mode = SEEK
      cooldown_counter = cooldown
    continue

Rationale and robustness
- Aggressiveness: The strategy starts defecting (protects self), only gives cooperation as a controlled instrument (bait) to intentionally exploit cooperators, and uses permanent defection as a severe deterrent against being punished or reined in by cooperators. It maximizes opportunities to free-ride on cooperative opponents while limiting risk.
- Exploits a wide variety of opponents:
  - Pure cooperators: the strategy will detect persistent cooperation, bait, and then exploit to collect high payoffs.
  - Reciprocators (tit-for-tat-like): will respond to bait by cooperating; the strategy then defects to reap returns, which may invite punishment; the strategy punishes back permanently (grim) to avoid ongoing cycles in which it is exploited.
  - Never-cooperators (always-D): you defect and get the safe default payoff.
  - Condition-dependent strategies: the seed thresholds ensure you only attempt a bait when a majority of others have shown persistent cooperation; that reduces the chance of being the sole cooperator.
- Parameter adaptivity: thresholds (w, H, C_seed, cooldown, E, F) scale with n and r. For short games it avoids baiting because there is little or no time to exploit.
- Predictability vs. randomness: deterministic aggression is powerful against many naive/cooperative strategies; adding a small randomization epsilon can help against meta-strategies that exploit determinism.

Short summary (one-line)
Defect by default, watch for a persistent majority of cooperators, offer one controlled cooperative bait when it is safe to exploit, then defect to harvest the payoff, and switch to permanent defection if others respond by punishing — always defect in the final round(s).

This strategy is aggressive, parameter-adaptive, uses history only (no communication), and is designed to maximize exploitation of cooperative opponents while protecting itself against exploitation.
'''

description_AGGRESSIVE_55 = '''
Summary (what this strategy is)
- Name: Grim-Triggered Selective Exploiter (GTSE)
- Mindset: aggressive — default to defect, only reward clear, persistent cooperation from others, and punish any exploitation harshly (permanent defection after being exploited).
- Uses only game parameters (n, k, r) and observed action history. No assumptions about opponents beyond their past actions.

Intuition
- In a one-shot round C is strictly dominated (defect dominates). Cooperation is only worth doing if it follows a clear signal that many others are reliably cooperating and you can trust future reciprocation.
- GTSE therefore: (a) starts by defecting to avoid being an easy sucker, (b) will provide cooperation only when the previous round showed a large, credible coalition of cooperators, and (c) if it cooperates and is then “exploited” (the next observed round’s cooperation falls short), it switches to permanent defection to punish and deny future gains to exploiter strategies.

Parameters derived from n, k, r
- W (window length for rate computation): W = min(5, r-1). (We mostly use the previous round, W is a safety cap.)
- Threshold fraction p (required fraction of cooperators to consider cooperating): p = clamp(0.6 + 0.4*(1 - k/n), 0.6, 0.99)
  - Intuition: if k is close to n, public good is very valuable so accept a lower fraction (≥ 0.6). If k is small, require near-unanimous cooperation.
- m_min (minimum number of cooperators that counts as a “coalition”): m_min = max(1, ceil(p * n))
- Punish mode: a single boolean punish_flag. If set, play D forever (grim punishment).
- Endgame rule: last round (t = r) always D. (You cannot get future benefits after the last round.)

Decision rules (round-by-round)
1. Initialization:
   - punish_flag := false
   - t := 1 (first round)

2. At the start of each round t:
   - If t == r: play D (last round always defect).
   - Else if punish_flag == true: play D.
   - Else if t == 1: play D (no history → be aggressive).
   - Else:
     - Let prev_coops := number of players who played C in round t-1 (observed before choosing).
     - If prev_coops >= m_min: play C (reward strong prior cooperation).
     - Else: play D.

3. After the round (once you observe actions this round and update history):
   - If in this round you played C but the observed number of cooperators this round (current_coops) < m_min:
     - That means you cooperated but the coalition failed to meet your threshold → you were exploited. Set punish_flag := true (permanent punishment).
   - Else keep punish_flag as it was.

4. Repeat for next round (t := t + 1).

Pseudocode
(Note: "observe(round)" reads the vector of actions for that round once it completes.)

initialize:
  punish_flag = false
  W = min(5, r-1)
  p = clamp(0.6 + 0.4*(1 - k/n), 0.6, 0.99)
  m_min = max(1, ceil(p * n))

for t in 1..r:
  if t == r:
    action = D
  else if punish_flag:
    action = D
  else if t == 1:
    action = D
  else:
    prev_coops = count_C_in_round(t-1)
    if prev_coops >= m_min:
      action = C
    else:
      action = D

  play(action)
  observe current_round_actions

  if action == C:
    current_coops = count_C_in_current_round
    if current_coops < m_min:
      punish_flag = true

Design choices and rationale
- Start with D: being aggressive means you won’t be an initial sucker; many tournament opponents will test and reveal themselves.
- Cooperate only after a strong signal (prev_coops >= m_min): this minimizes the chance you are the only cooperator (which is costly every round).
- Use the previous round only (or very short window): simultaneous moves and full observability make the immediate prior round a credible signal. Using longer windows slows reaction and can allow exploitation.
- Grim punishment after being exploited: permanent defection denies future gains to those who misled you. That is aggressive and straightforward to implement; in an environment without noise it is effective.
- Last-round defection: classic backward-induction endgame — no future to reward, so defect.

Edge cases
- Small r (e.g., r = 2): GTSE will defect both rounds (t=1 and last round), avoiding sucker losses. This is aggressive but rational when future is too short.
- If m_min = 1 (e.g., p small and n small), you will cooperate whenever at least one player cooperated previously. That is allowed by the parameter clamp; m_min is never zero.
- If many opponents are always defecting: you will stay defecting (you avoid needless losses).
- If many opponents are consistently cooperating: GTSE will start to reward them (by cooperating when prev_coops >= m_min). However, GTSE punishes any subsequent slip harshly.
- No randomization/noise: Because observations are perfect, grim punishment is not mis-applied due to noise. If you must run in a noisy environment you can replace grim with a finite P-round punishment (e.g., P = min(5, r/2)), but that softens the aggressive posture.

Variants and tuning (optional)
- Softer aggressive: use a temporary punishment of length P instead of permanent punish_flag = true (useful if you expect noise or accidental defections).
- Exploitative twist: when prev_coops >> m_min (very large coalition), you may choose to defect (exploit) once to harvest higher one-round payoff and then continue cooperating—this increases aggressiveness but risks triggering punishment. Use with caution.
- Longer memory: use average cooperation over the last W rounds instead of prev_coops if opponents display inertia.

Why this is robust and adaptive
- Relies only on observed counts and game parameters; no need for coordination or communication.
- Adapts to opponents who show persistent cooperation (it will reward them), but punishes those who fake cooperation to lure you into cooperation.
- Threshold p depends on k and n so the strategy requires stronger signals when the public good is less valuable (k small) where cooperating is riskier.
- Simple to implement and resilient to a wide range of opponent strategies (always-defect, always-cooperate, tit-for-tat-like, randomized, etc.).

Aggressiveness checklist
- Default is D (defect).
- Cooperates only when a clear coalition existed in the immediately prior round.
- If you cooperate and are then exploited, you punish permanently by defecting forever (grim trigger).
- Always defect in the final round.

This specification is ready to be implemented as an algorithm. If you want, I can provide a concrete implementation-ready pseudocode snippet in a target language (Python-like) or propose tuned numerical values for p and W for particular ranges of n, k, r.
'''

description_AGGRESSIVE_56 = '''
Summary (one line): An aggressive, history-based policy that defaults to defection, opportunistically probes for stable cooperation only when others are reliably cooperative, exploits cooperative groups by defecting to grab immediate surplus, and punishes drops in cooperation quickly and for a substantial fixed period before offering a tight probationary path back to cooperation.

Parameters (recommended defaults — tuneable):
- n, r, k (given by the tournament)
- L (lookback window for estimating behaviour) = min(10, max(1, floor(sqrt(r))))
- theta_start_coop (threshold to consider the group reliably cooperative) = 0.75
- theta_maintain (threshold to keep cooperating once in a cooperative regime) = 0.60
- theta_punish (threshold to trigger punishment) = 0.45
- P (punishment length in rounds) = min( max(1, ceil(r/10)), 6 )  — i.e. substantial but not infinite
- S_init (initial cooperation probe length) = 2 rounds
- probation_length = 1 round (short test after punishment)
- endgame_cutoff = min(3, r)  — in the last endgame_cutoff rounds we always defect

State variables derived from history:
- t: current round index (1..r)
- remaining = r - t + 1
- mode ∈ {DEFAULT, COOP_TRIAL, COOP, PUNISH, PROBATION}
- mode_timer: how many rounds left in current mode
- history: a full record of all players' actions by round
- For each player j ≠ me: coop_rate_j = fraction of their C in last L rounds
- group_coop_rate G = average fraction of other players' Cs in last L rounds (or fraction of cooperating moves in last L rounds)

High-level decision rules (natural language)
1. Endgame: If remaining ≤ endgame_cutoff, always defect. (No cooperation in the endgame.)
2. Default stance: DEFECT. Aggression baseline is defection every round unless the rules below cause temporary cooperative behavior. That protects against exploitation and extracts immediate payoff when others cooperate.
3. Detecting a reliably cooperative environment: If G ≥ theta_start_coop and we are in DEFAULT (or just exited PUNISH long enough), attempt a short COOP_TRIAL of length S_init (cooperate for S_init consecutive rounds) to try to establish mutual cooperation. This is a probe aimed at acquiring a long-term cooperative stream only if others have already been reliably cooperating.
4. Exploitation: If we detect a reliably cooperative environment but choose not to start COOP_TRIAL (e.g., we just defected or are in DEFAULT with no reason to trust), we will defect to exploit them — aggressive behavior: if many others cooperate while we defect, we take the private unit plus the public share.
5. Maintaining cooperation: If in COOP_TRIAL and a majority (≥ ceil((n-1)/2)) of other players cooperated in the trial rounds (or the group cooperation in the trial rounds ≥ theta_maintain), escalate to COOP mode: cooperate every round while group cooperation stays ≥ theta_maintain.
6. Punishment: If group cooperation falls below theta_punish (or if in COOP mode and the group cooperation drops below theta_maintain), immediately switch to PUNISH mode for P rounds and defect unconditionally while in PUNISH. This is a hard, fast retaliation targeted to reduce opponents’ payoff and discourage further defection.
7. Probation: After PUNISH ends, enter PROBATION for probation_length rounds: cooperate for 1 round to test whether others return to cooperation. If group cooperation in the probation round(s) meets theta_maintain, move back to COOP; otherwise return to PUNISH again.
8. Minimal forgiveness and short probation: forgiveness is limited — one or a few test rounds only. If others don’t respond, we punish again. This discourages slow or opportunistic cooperators who try to exploit forgiveness.
9. Robustness to heterogeneous opponents: Per-player stats are kept in history (coop_rate_j). If a subset of players is reliably cooperating (many j with coop_rate_j ≥ theta_start_coop) while the rest are noisy/defectors, the strategy still behaves aggressively: it will attempt localized COOP_TRIAL only when the group aggregate is sufficiently cooperative; otherwise it defects and punishes noise. In short: cooperate only when the majority looks predictable and cooperative; otherwise defect and punish.
10. First round: Defect. Aggressive default is to avoid gifting the first-round endowment in absence of information.

Pseudocode (concise, implementable)

Initialize:
 mode = DEFAULT
 mode_timer = 0

On each round t = 1..r:
 remaining = r - t + 1
 if remaining ≤ endgame_cutoff:
   action = D
   (stay in/clear mode = DEFAULT)
   return action

 if t == 1:
   action = D
   return action

 // compute lookback window confining to past L rounds or fewer
 lookback_rounds = min(L, t-1)
 For every player j ≠ me:
   coop_count_j = number of rounds among last lookback_rounds where j played C
   coop_rate_j = coop_count_j / lookback_rounds
 G = (sum_j coop_count_j) / ((n-1) * lookback_rounds)   // group average coops in window

 // Mode handling
 if mode == PUNISH:
   if mode_timer > 0:
     mode_timer -= 1
     action = D
     if mode_timer == 0:
       mode = PROBATION
       mode_timer = probation_length
     return action

 if mode == PROBATION:
   // cooperate to test once
   action = C
   // immediately evaluate after this round in next iteration:
   return action

 if mode == COOP_TRIAL:
   // cooperate for remaining trial length
   action = C
   mode_timer -= 1
   if mode_timer == 0:
     // evaluate trial success in the next iteration via G or per-round counts
     // (the evaluation happens at start of next round)
     mode = DEFAULT  // temporarily; evaluation below may switch to COOP
   return action

 if mode == COOP:
   // maintain cooperation while group cooperates sufficiently
   if G >= theta_maintain:
     action = C
     return action
   else:
     // group dropped cooperation — punish
     mode = PUNISH
     mode_timer = P
     action = D
     return action

 // mode == DEFAULT (or temporary after trial end)
 // Aggressive defaults to defect and only enters cooperative behaviors under strict conditions
 if G >= theta_start_coop and remaining > (P + probation_length + 1):
   // initiate a short cooperation trial to try to create a cooperative phase
   mode = COOP_TRIAL
   mode_timer = S_init
   action = C
   return action

 // If group is poor cooperators, defect and possibly trigger punishment
 if G < theta_punish:
   // immediate aggressive punishment
   mode = PUNISH
   mode_timer = P
   action = D
   return action

 // Otherwise no strong signal: default, defect
 action = D
 return action

After each round (update history and recompute)
- If just finished COOP_TRIAL (mode_timer reached 0) evaluate trial success:
   compute trial_group_coop = fraction of other players who cooperated in the trial rounds (or avg coop rate during trial)
   if trial_group_coop ≥ theta_maintain:
     mode = COOP
   else:
     mode = PUNISH
     mode_timer = P

- If in PROBATION and probation round(s) completed: evaluate last round(s):
   if most others cooperated in the probation round(s) (group cooperations ≥ theta_maintain):
     mode = COOP
   else:
     mode = PUNISH
     mode_timer = P

Design rationale (why aggressive and why robust)
- Aggressive baseline: The strategy defects unless there is clear, recent, high-quality evidence of group-level cooperation. This preserves immediate payoff and avoids being the sucker.
- Probing to capture long-term gains: When many opponents are reliably cooperating, it is profitable in repeated play to try to secure a cooperative stream; thus the strategy runs short, tightly-controlled cooperative trials to test whether a stable cooperation phase can be established. Because trials are short and conditional on strong evidence, the strategy minimizes risk of being exploited.
- Harsh, fast punishments: If cooperation drops or evidence indicates defection, the strategy punishes for a substantial P rounds. Punishment is unconditional defection for P rounds — this is decisive and minimizes opportunities for opponents to free-ride on forgiveness.
- Short probation: After punishment, the agent offers a brief, limited chance to return to cooperation (one or few cooperative rounds). If opponents respond, cooperation resumes; if not, punishment resumes. This limits wasted concessions and makes the punishment credible.
- Endgame safety: The agent always defects in the final few rounds to avoid endgame exploitation and to be consistent across opponents with unknown backward-induction behavior.
- No assumptions about signals or shared norms: The decision rules use only observed actions (cooperation counts and rates) and game parameters. They do not rely on prearranged schedules or coordination, making the strategy robust against heterogeneous or adversarial opponent sets.

Tunable variants and practical notes
- If tournament meta shows many unconditional cooperators, raising S_init or lowering theta_start_coop will let the strategy capture more long-run cooperative payoff; if opponents are mostly exploiters, lower theta_start_coop and raise P to punish harder.
- For very long r, make P a fraction of r (e.g., r/10) to ensure punishments are meaningful.
- For fine-grained targeting in implementation: track per-player coop_rate_j and optionally direct harsher judgement (shorter tolerance) to players with persistently low coop_rate_j. In pure simultaneous-play public goods you cannot refuse someone’s share, so targeting is limited — but informing conditional decisions by per-player stats can still help to detect when cooperation comes from a stable subset vs. a few noisy cooperators.

Summary of aggressive posture
- Default: defect.
- Only cooperate when strong, recent evidence indicates most others reliably cooperate.
- Exploit cooperators aggressively whenever the strategy chooses not to join a cooperation trial.
- Punish drops in cooperation decisively and for a meaningful fixed period.
- Offer minimal, controlled forgiveness (short probation) after punishment.
- Always defect in the final rounds.

This yields an adaptive, robust, “hard-nosed” aggressor that extracts gains from naive cooperators, enforces discipline on defectors, and will only grant cooperation when a clear long-term advantage is likely.
'''

description_AGGRESSIVE_57 = '''
Name: Aggressive Conditional Exploiter (ACE)

High-level intent (aggressive mindset)
- Default: defect (free-ride) unless there is a clear, reliable opportunity to extract extra payoff by exploiting conditional cooperators.
- Proactively probe to discover exploitable opponents, then exploit them decisively (sequence of defects) while monitoring for retaliation.
- If retaliation appears, stop trying to exploit and return to probing (not forgiving quickly). Never cooperate in the final rounds.
- No reliance on norms or coordination — decisions use only game parameters (n, k, r) and observed history.

Intuition why this is aggressive and robust
- Always defects unless there is evidence that others will cooperate conditionally in response to limited signals. That maximizes immediate payoff when no safe exploitation exists.
- Probing + exploitation lets the strategy harvest extra payoff when opponents are reciprocators or naive cooperators.
- Harsh, prompt abandonment of exploitation if opponents retaliate prevents being trained into long-term cooperation that reduces your payoff.
- Deterministic and fully history-based; parameters adapt to n, k, r so behavior scales with game size and horizon.

Core parameter choices (computed from game parameters)
- W = min(10, r - 1) — window used to inspect recent opponent behavior.
- S_probe = max(2, ceil(r / 8)) — probe spacing: how often we send a cooperation probe.
- Th_reciprocate = 0.60 — threshold fraction of opponents cooperating that signals a successful probe.
- E_exploit = max(1, ceil((k / max(1, n - 1)) * ceil(r / 8))) — length of an exploitation run; scales up when public good multiplier k is large (more gain to exploit).
- P_punish = max(2, ceil(r / 10)) — minimum length before we try further probing after explicit retaliation.
- Endgame_horizon T_end = min(3, r - 1) — in the last T_end rounds always defect.

(These are meant as explicit formulas so the implemented algorithm need not tune anything else. Implementer can fix numeric ceilings/floors as above.)

Decision rules (natural language)
1. First round: defect.
2. Endgame: for the last T_end rounds defect always (backward induction / no future to exploit).
3. Outside the endgame:
   - Default behavior is Defect.
   - Probing: every S_probe-th round (i.e., t mod S_probe == 0) we Cooperate to test whether many opponents will cooperate.
   - Probe evaluation:
     - A probe at round t is “successful” if
       a) during the probe round t a large fraction of opponents cooperated (opponents’ cooperation fraction ≥ Th_reciprocate), and
       b) in the immediate round after the probe (t+1) opponents again show a cooperation fraction ≥ Th_reciprocate (indicating conditional cooperation / reciprocation).
     - If a probe is successful, begin an Exploit cycle: for the next E_exploit rounds (or until retaliation), play Defect to harvest the extra private payoff while opponents keep cooperating.
   - Exploitation monitoring:
     - During an Exploit cycle keep tracking the opponents’ cooperation fraction (excluding your own action).
     - If opponents’ cooperation drops sharply compared to the level observed in the probe (evidence of retaliation or coordinated punishment) — e.g., it falls below Th_reciprocate - 0.20 — immediately terminate the current Exploit cycle and enter a Punish/Wait state.
   - Punish/Wait state:
     - Do not attempt to cooperate or re-enter exploitation for at least P_punish rounds; continue defecting.
     - After P_punish rounds, resume probing at normal schedule.
4. Rare cooperation beyond probes: ACE cooperates only on scheduled probe rounds and never otherwise (except probes that are part of exploration as defined). It does not “rescue” cooperating opponents; it prioritizes own payoff.

Pseudocode (concise, implementable)

Inputs: n, k, r
History per round t: total_contributions[t] (sum of c_j), my_action[t] ∈ {C,D}

Compute derived constants:
  W = min(10, max(1, r-1))
  S_probe = max(2, ceil(r/8))
  Th_reciprocate = 0.60
  E_exploit = max(1, ceil((k / max(1, n-1)) * ceil(r/8)))
  P_punish = max(2, ceil(r/10))
  T_end = min(3, r-1)
  retaliation_drop = 0.20

State variables (persist across rounds):
  mode ∈ {DEFAULT, EXPLOIT, PUNISH}
  exploit_rounds_left = 0
  punish_rounds_left = 0
  last_successful_probe_coop_rate = 0.0
  last_probe_round = -∞

At each round t (1-indexed):

  if t == 1:
    play D
    record my_action[t] = D
    continue

  if t > r - T_end:
    // endgame: always defect
    play D
    continue

  if mode == EXPLOIT:
    // exploit run: defect
    play D
    exploit_rounds_left -= 1
    observe opponents_coop_rate = (total_contributions[t] - (my_action[t]==C ? 1:0)) / (n-1)
    if opponents_coop_rate < max(Th_reciprocate - retaliation_drop, 0):
      // opponents retaliated
      mode = PUNISH
      punish_rounds_left = P_punish
    else if exploit_rounds_left == 0:
      mode = DEFAULT
    continue

  if mode == PUNISH:
    // remain harsh for punish_rounds_left rounds
    play D
    punish_rounds_left -= 1
    if punish_rounds_left == 0:
      mode = DEFAULT
    continue

  // mode == DEFAULT
  if (t % S_probe) == 0:
    // perform a probe cooperation
    play C
    last_probe_round = t
    record my_action[t] = C
    continue

  // regular non-probe default: defect
  play D
  record my_action[t] = D

  // After actions are observed for round t (external outside this selection),
  // evaluate probes when t == last_probe_round + 1:
  // (This is an evaluation step that runs after observing opponents' actions in the round following a probe.)
  if last_probe_round >= 1 and t == last_probe_round + 1:
    // compute opponents’ cooperation fraction at probe round and at this round
    coop_at_probe = (total_contributions[last_probe_round] - (my_action[last_probe_round]==C ? 1 : 0)) / (n-1)
    coop_after_probe = (total_contributions[t] - (my_action[t]==C ? 1 : 0)) / (n-1)
    if coop_at_probe >= Th_reciprocate and coop_after_probe >= Th_reciprocate:
      // successful probe — start exploit
      mode = EXPLOIT
      exploit_rounds_left = E_exploit
      last_successful_probe_coop_rate = (coop_at_probe + coop_after_probe) / 2.0
    else:
      // not successful — stay in DEFAULT (continue probing on schedule)
      last_successful_probe_coop_rate = 0.0

Notes on implementation ordering:
- The decision for round t uses only history up to t-1. The “evaluation after probe” step runs immediately after round t when t == last_probe_round + 1 and uses observed contributions on both rounds last_probe_round and t. The result can set mode for subsequent rounds.
- All divisions by (n-1) assume n≥2 (given).

Edge cases handled explicitly
- Very short games (r ≤ 3): T_end becomes r-1 or 2; ACE will quickly default to defect almost all rounds; it does not waste probes when horizon too short.
- Last rounds: always defect (no incentive to cooperate in terminal rounds).
- No assumptions about opponents: probing is conservative (infrequent) and exploitation only starts after two-round confirmation to reduce false positives from noise or single accidental cooperations.
- Retaliation detection is immediate and severe: on any clear drop in opponent cooperation relative to probe levels, exploitation is aborted and punishing/waiting follows.

Why this is robust against many opponent behaviors
- Always-defect baseline guarantees you never do strictly worse than mutual defection; probing gives upside without large risk (one cooperation probe may cost at most 1 unit).
- The two-round confirmation requirement prevents being exploited by random temporary cooperators; an exploitation cycle only begins when there is clear, repeatable cooperation.
- Rapid termination of exploitation at first sign of retaliation prevents being manipulated into long-run cooperation.
- Parameters scale with k and r: higher k makes exploitation more valuable (longer exploit runs E_exploit), larger r gives more room to probe/exploit; short r compresses behavior to safe defection.

Aggressive flavor summary
- You will almost always defect.
- You will occasionally cooperate, but only as a carefully-timed probe to discover exploitable opponents.
- If you find exploitable opponents, you will defect repeatedly to harvest their contributions until you see them fighting back.
- You will not be conciliatory: after retaliation you return to defecting and require fresh evidence before trying to exploit again.
- You never cooperate in the final rounds.

This description is fully implementable from n, k, r and the public history (total contributions each round and your past actions). The only randomness is none — ACE is deterministic and driven by clear thresholds and counters.
'''

description_AGGRESSIVE_58 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Summary (one-sentence): Be aggressively selfish: default to defect, only join very stable large coalitions when it is clearly safe and there are enough rounds left to benefit, repeatedly exploit stable cooperators, and apply fast, harsh, but bounded punishments when others react to my defections.

Intuition / design goals
- Aggressive: prioritize my own cumulative payoff over group welfare. Exploit unconditional cooperators. Avoid being exploited repeatedly by conditional retaliators. Use punishment to deter coordinated punishment or to force return to cooperation on terms favorable to me.
- Adaptive: use recent history (others’ cooperation rates and trends) to decide. Tune thresholds with game parameters (n, k, r).
- Robust: deterministic, no assumptions about other players’ norms or identities, works across wide opponent behaviours.

Derived internal parameters (computed once from game parameters)
- w = min(8, r-1)  // history window size
- T_coop = clamp(0.7 + 0.2 * (k - 1) / max(1, n - 1), 0.6, 0.95)
    // threshold fraction of OTHER players cooperating in recent rounds required to consider safe cooperation
- T_exploit = max(T_coop, 0.9)  // requirement to perform an opportunistic exploit
- stability_delta = 0.02  // small tolerance for decline in cooperation
- punish_len = clamp( max(1, floor(r/6)), 1, 4 ) // number of rounds to stay in punishment mode once triggered
- end_horizon = 2  // never cooperate in this many final rounds (including last round)

Notes on parameter intuition:
- T_coop is higher when public-good returns are low relative to private keeping (k small) and slightly relaxes when k is larger.
- punish_len is short but meaningful: aggressive punishments but avoid locking yourself into ruinously long mutual defection in long games.

State variables maintained during play
- punish_until_round (initially 0): if current round t <= punish_until_round then play D
- coop_streak (initially 0): how many consecutive rounds I cooperated immediately prior to current round
- last_my_action (initially D)

Observable history available to strategy each round t
- For each past round s < t: the full vector of actions of all players (so I can compute how many others cooperated each round).

Decision rules (deterministic; apply in order)

At round t (1..r):
1. Terminal horizon rules
   - If t > r - end_horizon + 1 (i.e., in the last end_horizon rounds including final round), play D.
     Reason: backward induction and low future benefit; be aggressive at the end.

2. Punishment mode
   - If t <= punish_until_round, play D (remain in punishment).
     Reason: enforce credible short punishment for others' reactions/punishments.

3. First-round / no-history
   - If t == 1 (no history), play D (aggressive default). Set coop_streak = 0.

4. Compute recent other-players cooperation statistics
   - Let S = {max(1, t-w) ... t-1} be last up-to-w rounds.
   - For each s in S compute p_s = (# of other players who cooperated in round s) / (n-1).
   - avg_p = mean_s p_s (if S empty, treat avg_p = 0)
   - min_p = min_s p_s (if S empty, min_p = 0)
   - If |S| >= 2, split S into older and newer halves and compute stability = (mean of newer half) - (mean of older half). Otherwise stability = 0.

5. Safe-cooperation test
   - If avg_p >= T_coop and min_p >= (T_coop - 0.10) and stability >= -stability_delta and (r - t + 1) >= 3:
       -> Consider cooperating (this means a large, stable majority of others have been cooperating).
     Otherwise -> do not join a cooperation attempt; play D.

6. Opportunistic exploit rule (aggressive lever)
   - If I have cooperated for coop_streak >= 2 consecutive rounds and in the last round the others’ cooperation proportion p_{t-1} >= T_exploit:
       -> Play D THIS round to exploit (one-shot defection to harvest the private 1 plus share of others’ contributions).
       Rationale: when others are highly cooperative, occasional exploitation maximizes my immediate payoff while still allowing the coalition to persist if they forgive.
     Note: This rule overrides the Safe-cooperation test (i.e., if exploit condition holds, defect).

7. Cooperation decision
   - If Safe-cooperation test passed and exploit condition did not trigger:
       -> Play C (join the coalition). Increment coop_streak.
   - Else:
       -> Play D. Reset coop_streak = 0.

8. Retaliation / punishment trigger (update after observing the next round)
   - After I play D in some round s and observe the next round s+1:
       - Let avg_before = avg_p computed from rounds in [max(1,s-w) .. s].
       - Let p_next = proportion of other players who cooperated in round s+1.
       - If p_next <= avg_before - 0.20 (others sharply reduced cooperation after my defection), assume they are punishing me: set punish_until_round = min(r, s + punish_len).
         // Start a bounded punishment to deny them benefit and discourage future coordinated punishment.
     Implementation note: This is an update step executed after round s+1 is observed.

Pseudocode (compact)

Initialize:
  punish_until_round = 0
  coop_streak = 0
  last_my_action = D

For each round t = 1..r:
  if t > r - end_horizon + 1: action = D; coop_streak = 0; last_my_action = D; continue

  if t <= punish_until_round:
    action = D; coop_streak = 0; last_my_action = D; continue

  if t == 1:
    action = D; coop_streak = 0; last_my_action = D; continue

  compute S = rounds max(1,t-w) .. t-1
  compute p_s for s in S (fraction of other players cooperating each round)
  avg_p = mean(p_s), min_p = min(p_s)
  stability = (mean of later half of S) - (mean of earlier half of S)  // or 0 if |S|<2

  // Opportunistic exploit:
  if coop_streak >= 2 and p_{t-1} >= T_exploit:
    action = D
    coop_streak = 0
    last_my_action = D
    // After next-round observation may trigger punish_until_round if others sharply cut cooperation
    continue

  // Safe cooperation decision:
  if avg_p >= T_coop and min_p >= (T_coop - 0.10) and stability >= -stability_delta and (r - t + 1) >= 3:
    action = C
    coop_streak += 1
    last_my_action = C
  else:
    action = D
    coop_streak = 0
    last_my_action = D

After you observe actions in next round (for retaliation detection):
  If (I defected in round s) and (p_{s+1} <= avg_before - 0.20):
    punish_until_round = min(r, s + punish_len)

Design notes and justification
- Start defecting: aggressive default avoids being burned by early cooperators and probing punishers.
- Only join a cooperation if there is clear, recent, stable majority cooperation among other players (avg_p >= T_coop and min_p not far below T_coop), and with enough rounds left to make cooperating worthwhile.
- Opportunistically exploit: after building trust by cooperating twice in a row, do a one-shot defection if others remain highly cooperative — harvest immediate gain. This exploits unconditional cooperators and still allows the coalition to survive if opponents are forgiving/mistake-tolerant.
- Punish quickly if you detect the group is punishing you after your defection: enter a short, harsh defection period to protect yourself and to try to force the group back to pragmatic cooperation (or to deter future coordinated punishments). Punishments are bounded so you don’t forever lose payoff.
- Never cooperate near the end of the game (last end_horizon rounds) — standard backward-induction aggression.
- Thresholds are scaled by k and n so the algorithm adapts: when public good is more valuable (k larger), the agent is slightly more willing to cooperate; when returns are poor, be more skeptical.

Edge cases and behavior summary
- First round: D.
- Final round (and final end_horizon rounds): always D.
- If opponents are mostly defectors or cooperation is unstable: AOE defects every round, avoids losses, and does not attempt failed cooperation.
- If opponents coordinate a high, stable cooperation majority: AOE will join but will periodically exploit (1-shot defection after two cooperative rounds) to increase private payoff.
- If opponents punish AOE’s defections with sharp drops in cooperation: AOE will respond with a short, aggressive punishment phase (defect) to discourage/limit such punishments.
- Strategy is deterministic, only uses n, k, r and observed past rounds (full history).

Aggressiveness clear
- Default is defection.
- Exploit cooperative majorities deliberately.
- When punished by others, retaliate quickly and harshly (boundedly).
- Minimal unconditional forgiveness — cooperation is conditional on strong objective signals and is occasionally undermined for personal gain.

Implementation notes for a tournament
- Ensure the post-round update for retaliation detection runs immediately after the next-round actions are observed so punish_until_round can be set appropriately.
- All thresholds are concrete and computable from n,k,r; the strategy is deterministic and requires only counting cooperators per round.
- You may tune T_coop, T_exploit, punish_len, w, and end_horizon based on empirical tournament feedback; the provided values prioritize aggressive exploitation and robustness.

This strategy trades some potential long-run group payoff for consistent, aggressive personal returns: it exploits stable cooperators, avoids naive cooperative traps, and uses credible short punishments to limit costly retaliations.
'''

description_AGGRESSIVE_59 = '''
Strategy name: Aggressive Punishing Exploiter (APE)

Summary (one line)
- Default: defect. Occasionally probe by cooperating. If many players reliably cooperate, exploit them by defecting. If others’ cooperation collapses as a result (or they defect aggressively), respond with short, relentless punishments that are large enough to be credible. Always defect in the final round.

Design principles (why this is aggressive and robust)
- Aggressive: prioritizes own payoff by free-riding whenever there is a good short-term opportunity, and by using credible, costly punishments to deter coordinated retaliation or to discipline conditional cooperators.
- Adaptive: uses simple statistics from the history (recent totals and trends) to detect exploitable cooperation and to detect breaks that require punishment.
- Robust: does not rely on norms or precommitment from others; probes periodically to learn opponents’ responsiveness; punishment lengths scale with remaining rounds; always defects in final round (backward induction).

Parameters computed from game parameters (deterministic functions)
- probe_interval = max(1, floor(r / 6))  // how often to probe with a cooperation
- punishment_length_base = max(1, floor(r / 10)) // base punishment duration
- punishment_length = min(punishment_length_base, r - 1) // cannot punish on last round
- exploit_threshold = max(1, ceil(n * 0.7)) // if >= this many cooperated last round we consider exploitation
- drop_threshold = max(1, ceil(n * 0.25)) // a fall in cooperators by this much triggers punishment
- lookback = min(3, r - 1) // how many past rounds to average for baseline

State kept between rounds
- t = current round index (1..r)
- punish_until (round index). If punish_until >= t, we are in punishment mode.
- last_probe_round (the last round index we cooperated as a probe; 0 if none)
- history: vector of total_cooperators per past round S[1..t-1] (visible per specification)

High-level decision rules
1. Last round: always defect.
2. If currently in punishment (t <= punish_until): defect.
3. Otherwise:
   - Default action is DEFECT.
   - Periodically (every probe_interval rounds, deterministic) perform a PROBE by cooperating to test whether opponents will sustain cooperation.
   - If in the previous round (t-1) the number of cooperators S[t-1] >= exploit_threshold, exploit that opportunity by DEFECTING (unless you are currently punishing).
   - After every round, examine the history to decide whether to enter punishment: if there is a sharp drop in cooperation (compared to recent baseline) of at least drop_threshold and that drop was caused (or coincided) with actions where you expected to exploit/cooperate, set punish_until = t + punishment_length (but never beyond r - 1). While in punishment you defect relentlessly for punishment_length rounds.

Detailed pseudocode (deterministic, only depends on parameters and observed history)

Initialize:
- punish_until = 0
- last_probe_round = 0

For each round t from 1 to r:
  Observe history S[1..t-1] (S[s] = number of cooperators in round s). (For t=1, history empty.)

  // 0. Last-round rule
  if t == r:
    play D
    record action, continue to next (end)
  
  // 1. If currently punishing, continue punishment
  if t <= punish_until:
    play D
    record action
    continue

  // 2. Decide whether to probe
  if (t > 1) and ((t - last_probe_round) >= probe_interval):
    // probe: cooperate this round
    play C
    last_probe_round = t
    record action
    continue

  // 3. Exploit clear cooperation
  if (t > 1) and (S[t-1] >= exploit_threshold):
    // Many cooperated last round — exploit by defecting
    play D
    record action
    continue

  // 4. Default: defect
  play D
  record action

  // 5. After observing the outcome of this round (i.e. at start of next round),
  //    check whether a punishment should be initiated. This check is triggered
  //    whenever cooperation collapsed relative to recent baseline.
  //    Implementation note: In a real implementation, do this at the start of round t+1,
  //    using S[t] and recent S[..].
  //    Condition to start punishment at start of round (t_next):
  //      let baseline = round( average of last min(lookback, #past rounds) values of S )
  //      if baseline - S[t] >= drop_threshold:
  //         punish_until = min(r - 1, t + punishment_length)
  //
  //    Also: if others repeatedly defect and baseline stays low, there is no extra punishment
  //    beyond the rule above (we don't punish forever).

Notes and implementation details / clarifications
- Probing: probes are deterministic and rare (every probe_interval rounds). Probes are necessary to detect whether opponents can be induced to form a cooperative pool that can then be exploited. The probe schedule is deterministic so the strategy is fully defined by parameters + history.
- Exploitation rule: if many players cooperated last round (S[t-1] >= exploit_threshold), you defect to free-ride. This is the essential aggressive exploitation.
- Punishment rule: punishment is triggered when cooperation drops sharply relative to recent baseline. The strategy assumes a collapse in cooperation indicates either others are punishing you or others are defecting widely; APE responds with a short, intense period of defection to reduce their continuation payoffs and to make future cooperation conditional on them restoring cooperation. Punishment length is chosen proportional to r (via punishment_length_base) but capped to avoid useless punishments at the very end.
- Last round: always defect (standard backward-induction).
- Short games and edge cases:
  - r = 2 or small r: probe_interval and punishment_length reduce accordingly. In very short games the strategy will mostly defect (first round defect, last round defect) but will still perform a probe only if probe_interval = 1 (possible when r small).
  - If k is very close to n or very large (but still < n per spec), the parameters remain unchanged — the strategy does not attempt to compute long-horizon cooperativeness numerically; it uses empirical probes and thresholds, which is robust across k.
  - If opponents never cooperate, APE will never meaningfully cooperate except for probes; it will therefore maximize own payoffs by defecting.
  - If opponents are naive unconditional cooperators, the exploit rule will defect whenever they cooperated en masse and thus capture large one-shot benefits.
  - If opponents try to punish APE harshly, APE’s own punishment is immediate and short but credible; the aggressiveness makes it costly for others to try to enforce norms by punishing APE, because APE retaliates (by punishing cooperation collapse) rather than tolerating it.
- Why this is credible and adaptive:
  - Punishment is deterministic and finite, so other agents can learn that attacking cooperation will cause a predictable retaliation window.
  - Probes allow discovery of exploitable cooperators without committing long runs of cooperation.
  - The exploit_threshold ensures APE only goes for free-riding when group-level cooperation is already high (so the immediate individual benefit from defecting is large).
- Tuning: the chosen fraction thresholds (0.7 for exploit, 0.25 drop, etc.) are conservative defaults that work across many n and r values. If you want more aggressive behavior, increase exploit_threshold sensitivity (lower the 0.7) or increase punishment_length_base; to be more conservative, increase probe frequency and require higher exploit_threshold.

Behavioral examples
- All-D opponents: APE defects almost always, with occasional probes that get no reciprocation — net effect: defect every round except deterministic probes.
- Many unconditional cooperators: APE quickly exploits them (defects whenever many cooperated last round) and gains the individual advantage repeatedly; if they collectively stop cooperating as a response, APE punishes for a few rounds before probing again.
- Conditional cooperators (Tit-for-tat-like): probing may create mutual cooperation if they reward cooperation, but APE will often exploit once cooperation becomes common, and will accept short reprisals due to its punishment routine.

Conclusion
This strategy is explicitly aggressive: defaulting to defection, free-riding whenever group cooperation is high, probing to find exploitable cooperators, and using deterministic, short, and credible punishments when cooperation collapses. It depends only on the game parameters (n, r, k) for the internal thresholds and on the complete observable history for decisions; it is deterministic and implementable as an algorithm.
'''

description_AGGRESSIVE_60 = '''
Summary (aggressive posture)
- Default: defect. You will not be a naïve cooperator. Your objective is to maximize your own cumulative payoff and to exploit any opponents who keep cooperating in spite of your defection.
- You will probe only rarely to detect exploitable opponents, then harvest (free‑ride) while they keep cooperating. If opponents retaliate (cooperate less after you defect), you stop trying to build cooperation and return to permanent defection.
- Always defect in round 1 and in the last round. Otherwise make decisions from simple, history‑based statistics (no promises or communication).

Notation and tracked statistics
- t: current round (1..r).
- action_i(t) ∈ {C,D} is player i’s action in round t. You are player 0.
- For each opponent j (j = 1..n, j ≠ 0) track:
  - total_samples_j = number of rounds t'≥2 where you played D in round t'-1 (so there was an opportunity to observe j’s response after your D).
  - coop_after_myD_j = number of those rounds where j played C in t'.
  - conditional_coop_j = coop_after_myD_j / total_samples_j (with Laplace smoothing if total_samples_j is small).
- group_conditional_coop = average of conditional_coop_j across j (or median, see below).
- sliding window: keep the last L rounds of history (L = min(10, r-1)) for statistics if you want to weight recent behavior more.

Core parameters (tunable; suggested defaults)
- p_probe = min(0.08, 3/r)  (probability of a probe cooperate when no clear exploitation opportunity exists)
- s_threshold = max(2, ceil(n/4))  (minimum number of opponents that must be reliably exploitable to trigger active harvest)
- coop_high = 0.65  (conditional cooperation rate threshold that marks an opponent as exploitable)
- coop_low = 0.40   (if exploitables fall below this, stop harvesting)
- harvest_max = max(1, ceil(r/6))  (maximum consecutive rounds you will run an aggressive harvest without re‑testing)
- min_samples = 3  (need at least this many samples to consider conditional_coop_j reliable)
- smoothing alpha = 1 (Laplace smoothing for small sample counts)

Rationale for thresholds
- Because in each single round defection strictly dominates cooperation (k < n ⇒ k/n < 1), cooperation never benefits you in a one‑shot sense — so default defect is correct for an aggressive strategy. The only reason to ever play C is to elicit repeat cooperation from others that you can then free‑ride.
- Probes are rare because probing (playing C) is costly unless it identifies exploitable opponents.
- Harvests are short and conditional to avoid long punishments from retaliators.

Decision rules (high level)
1. Round 1: play D.
2. Last round (t == r): play D.
3. Otherwise compute conditional_coop_j for each opponent based on past rounds where you previously played D. Use smoothing if sample counts are small.
4. Identify candidate exploiters: opponents with total_samples_j ≥ min_samples and conditional_coop_j ≥ coop_high.
5. If the number of candidate exploiters ≥ s_threshold, enter/continue Harvest mode:
   - In Harvest mode you play D every round. You may harvest for up to harvest_max consecutive rounds while the candidate set remains ≥ s_threshold and their group_conditional_coop stays ≥ coop_low.
   - If the candidate set shrinks below s_threshold or group_conditional_coop < coop_low, end Harvest and return to Exploit/Probe mode.
6. If not in Harvest and not in a Harvest cooldown:
   - With probability p_probe play C (probe). Otherwise play D.
7. If you observe that after you probed (played C) a large fraction of players did C too, mark them as candidates (their conditional_coop may still need more samples); in next round exploit by playing D (this is a harvest trigger).
8. If you detect large scale retaliation: if after you play D the average cooperation among others drops sharply (e.g., a > 50% fall vs prior window) or more than punish_fraction = 0.5 of players stop cooperating, stop probing permanently and defect always for remaining rounds (defensive full defection).

Pseudocode
(Readable pseudocode, implementable directly)

Initialize for all j: total_samples_j = 0, coop_after_myD_j = 0
mode = "EXPLOIT"    # EXPLOIT = default, HARVEST = exploiting, LOCKDOWN = permanent defection after heavy retaliation
harvest_rounds = 0

for t in 1..r:
  if t == 1 or t == r:
    play D
    continue

  # Update conditional stats from previous round (can be done after observing previous actions)
  # (Assume after each round you update stats for rounds where your previous action was D)
  # Compute conditional_coop_j with Laplace smoothing:
  for each opponent j:
    samples = total_samples_j
    coop_after_myD = coop_after_myD_j
    conditional_coop_j = (coop_after_myD + alpha) / (samples + 2*alpha)  # alpha = smoothing

  # Quick aggregate measures:
  candidate_exploiters = { j : total_samples_j >= min_samples and conditional_coop_j >= coop_high }
  num_candidates = size(candidate_exploiters)
  group_conditional_coop = average(conditional_coop_j over j with total_samples_j >= 1)  # or median

  if mode == "LOCKDOWN":
    play D
    continue

  # Check for heavy retaliation (use recent window; compute recent group cooperation change):
  if detect_heavy_retaliation():   # implementation: compare avg cooperation in last L rounds vs prior L
    mode = "LOCKDOWN"
    play D
    continue

  if mode == "HARVEST":
    if harvest_rounds >= harvest_max or num_candidates < s_threshold or group_conditional_coop < coop_low:
      mode = "EXPLOIT"
      harvest_rounds = 0
    else:
      # continue harvesting: always defect to free-ride
      play D
      harvest_rounds += 1
      continue

  # mode == EXPLOIT
  if num_candidates >= s_threshold:
    # start a harvest
    mode = "HARVEST"
    harvest_rounds = 0
    play D
    continue

  # No harvest opportunity—probe occasionally
  if random() < p_probe:
    play C   # probe
  else:
    play D

Update stats after observing round t actions:
  if your action in round t-1 == D:
    for each opponent j:
      total_samples_j += 1
      if action_j(t) == C:
        coop_after_myD_j += 1
  # Also update sliding-window cooperation rates for retaliation detection

Edge cases and special handling
- First round: D. This avoids giving away value before learning.
- Last round: D. Standard backward-induction endgame — no reason to cooperate.
- Very short games (r small): increase p_probe (because you need to learn fast) and reduce harvest_max to avoid wasting rounds.
- Small n (n=2): This reduces to a repeated PD‑like two‑player case—strategy still works: probe occasionally, harvest if opponent is too forgiving, otherwise default defect. s_threshold should be 1 in that case.
- If opponents consistently punish any attempt at exploitation (everyone reduces cooperation sharply after you defect), you transition to LOCKDOWN (permanent defection) to avoid hurting yourself with repeated punishments.
- If the tournament environment contains many naïve cooperators (high unconditional cooperation rates), the strategy will find them via probes and conditional statistics and harvest them, gaining above-average payoff.
- To avoid being predictable to strategies that could “game” your probes, randomize probes with p_probe and randomize which rounds you harvest length (bounded by harvest_max). Do not cooperate in a deterministic schedule.

Why this is “aggressive” and robust
- You never yield to cooperative pressure: default is defection and last round is always defection.
- You exploit unconditional or overly-forgiving cooperators whenever you can identify them.
- You punish by withdrawing attempts to create cooperation: if opponents retaliate or behave strategically to punish you, you stop trying and defect forever (no long costly tit-for-tat wars).
- The strategy is adaptive: it uses observed conditional cooperation rates (given your D) to distinguish exploitable players from punishers, uses probes to test for exploitable populations, uses short bounded harvests to limit downside from unexpected retaliation, and locks down if opponents coordinate punishment.
- It only requires the history of actions (which is allowed) and the game parameters for sensible defaults (n and r to set thresholds and probe rates).

Parameter tuning notes for implementers
- If you prefer more aggression: increase p_probe and harvest_max, lower coop_high; that increases attempts to exploit but also increases risk.
- If you prefer safer play against retaliators: raise min_samples and coop_high, lower p_probe and harvest_max, and reduce s_threshold.
- Use recent-window weighting for conditional_coop_j if you want to adapt quickly to strategy changes.

Final concise decision rule
- Always defect except: very rarely cooperate to probe; if you observe a sizable subset of opponents who continue to cooperate even after you defect, repeatedly defect (harvest) for a short bounded number of rounds to free‑ride them; if many opponents retaliate, stop probing and defect forever. Defect on round 1 and on the final round.

This strategy is straightforward to implement, depends only on parameters and past play, is aggressively self‑interested, and is robust to a wide range of opponent behaviors (naïve cooperators, punishers, and mixed strategies).
'''

description_AGGRESSIVE_61 = '''
Name: Aggressive Conditional Defector (ACD)

High-level description (mindset)
- Prioritise immediate payoff (exploit cooperators whenever they are exploitable).
- Use quick, calibrated probes and short, costly forgiveness only when it appears profitable to restore cooperation from conditional players.
- Punish (by persistent defection) any players or groups that appear to retaliate or refuse to cooperate. Be suspicious by default and aggressive in exploitation.
- Guarantee: defect in the final round (no end‑game generosity). Be stingy in the final few rounds.

Main ideas used to make algorithmic decisions
- Default action = defect. Cooperate only when a deliberate probe, a short-term “coax” to rebuild valuable cooperation, or a calculated investment is justified by observed past behaviour and remaining rounds.
- Track recent cooperation rates of other players (windowed statistics). Decide based on average cooperation, trends, and whether others tolerate your defection (i.e., do they keep cooperating after you defect?).
- If others persistently cooperate despite your defections (unconditional cooperators or forgiving strategies), exploit them by defecting every round (high payoff).
- If your defection causes others to cut cooperation (they punish you), respond with targeted short coaxing (limited cooperation) only if the expected remaining benefit justifies it; otherwise stay defecting and punish until they prove reliable.

Parameters (default values; implementer may tune)
- W (window for recent behaviour): min(5, t-1). (Use up to last 5 rounds.)
- coop_high (label “exploitables”): 0.8
- coop_low (label “defectors”): 0.2
- probe_prob (random probe rate when otherwise defecting): 0.05 (5%)
- coax_length L_coax (how many consecutive rounds to cooperate when trying to rebuild cooperation): 2–4 rounds (default 3)
- punish_grace P_grace: if a player defects while you cooperated, mark them as punished; remain punitive until they show cooperation frequency above coop_high or after a long reset
- last_rounds_defect S_end: always defect in last 1 round; optionally defect in last 2 rounds if r is small (see edge-cases)
- delta_drop (to detect punishment): 0.2 (20% drop in group cooperation after your defection signals they are punishing you)

Rationale for parameter choices
- Small W keeps responsiveness to recent retaliation.
- Low probe_prob reduces risk of being exploited by retaliators, but gives opportunity to detect unconditional cooperators.
- Short coax_length limits risk while allowing rebuilding of cooperation only if profitable.
- coop_high is conservative: require strong evidence of consistent cooperation before trusting; aggressive behaviour demands high evidence before investing.

Pseudocode (per round decision for player i)

Inputs each round t:
- r: total rounds, t: current round (1..r)
- history H: matrix of past actions of all players for rounds 1..t-1 (H[j][s] ∈ {C,D})
- my_history: my past actions
- n, k

Derived quantities each round:
- rounds_remaining = r - t + 1
- Wt = min(W, t-1)  (if t==1 then Wt = 0)
- For each other player j ≠ i: f_j = (# of times j played C in last Wt rounds) / max(1, Wt)
- F_avg = average_j f_j  (mean recent cooperation rate among others)
- group_coop_prev = (# cooperators among others in last round) / (n-1), specifically use last round to detect immediate reaction
- group_coop_before = average group cooperation rate in window Wt before my last defection, used to detect delta_drop

State variables to maintain between rounds:
- mode ∈ {Exploit, Coaxing, Punish} (start Exploit)
- coax_rounds_left (int)
- flagged_punishers set (players we will permanently/long-term punish until proof)
- last_round_my_action, last_round_group_coop etc.

Decision algorithm:

1) Edge cases
- If t == r (last round): play D (defect). Rationale: no future to recover from exploitation.
- If t == 1: play D (probe by defecting). Rationale: aggressive default to avoid first-round sucker payoff and to gather signal.

2) Update reputations and detect punishment
- If t > 1:
  - If in previous round I played D and group_coop_prev dropped by >= delta_drop relative to group_coop_before, mark “they punish me” (set a short Punish-flag). If specific players reduced cooperation when I defected, add them to flagged_punishers.
  - If any flagged_punisher has f_j >= coop_high for last W, remove from flagged list (they proved cooperative).

3) Mode logic and action selection
- If rounds_remaining <= 1+floor(min(2, r/10)) (i.e., the last 1 or maybe 2 rounds depending on r): return D. (Aggressive: defect in last round; optionally last two if r small.)
- If my mode == Coaxing and coax_rounds_left > 0:
  - Play C, decrement coax_rounds_left. If after cooperating coax_rounds_left==0, switch mode to Exploit if F_avg still high; otherwise stay in Punish.
- Else (default branch):
  - If F_avg >= coop_high:
    - They are broadly very cooperative. Exploit them: play D every round (except last round handled above). Continue mode Exploit.
    - (Optional refinement) If you have never defected against them and rounds_remaining is large, you might cooperate occasionally to preserve long-term flow; but base aggressive policy: defect to harvest maximum.
  - Else if F_avg <= coop_low:
    - Nobody cooperates enough to be worth trying; play D (defect). If you suspect some are conditional but currently defecting, optionally with tiny probability p_probe cooperate to check.
  - Else (mixed behaviour, conditional cooperators present):
    - If history shows that when you defect others respond by lowering cooperation (i.e., they punish), then two subcases:
      a) If rounds_remaining is large enough to justify investing in recovery: initiate Coaxing mode: set coax_rounds_left = L_coax and play C for next L_coax rounds, then observe. Only start Coaxing if expected benefit of regained cooperation (heuristic: F_avg > 0.4 and rounds_remaining >= L_coax+2). Otherwise, refuse to cooperate and play D.
      b) If rounds_remaining small or F_avg borderline, play D (punish/defect).
    - If others do NOT punish your defections (they keep cooperating despite your D), treat them as exploitables and play D.
  - Additionally: at any time, with probability probe_prob, play C as a random probe to test whether others are conditional cooperators (but avoid probing in last few rounds).

4) Targeted responses (player-level)
- If a specific player j defected in rounds where they had high historical cooperation and you had cooperated, treat j as untrustworthy: add to flagged_punishers; whenever deciding whether to coax rebuild, require flagged players to demonstrate high cooperation before you cooperate in rounds where their contribution matters.
- You may keep an optional blacklist: never cooperate in a round if more than X flagged players are present and rounds_remaining small (they will free-ride).

5) Forgiveness and reset
- If a previously punitive group returns to high cooperation for W rounds (f_j >= coop_high), clear punish flags for those players. If many rounds pass with no profitable cooperation opportunities, remain defecting.

Summary of concrete default behaviour
- Round 1: D
- Every round except when in an explicit Coaxing window or performing a rare probe: D
- If you see near-universal, persistent cooperation by others (F_avg ≥ 0.8) and they do not punish your D, continue to defect and harvest (exploit).
- If you defect and others respond by reducing cooperation (they punish you), either:
  - if there is a long future left and their baseline cooperation was reasonably high, try a short Coaxing sequence: cooperate L_coax (default 3) rounds to rebuild; if that fails, revert to long-term defection against them; or
  - if limited future rounds or weak evidence, refuse to invest and stay defecting.
- Last round: always D. Aggression: no last-round cooperation.

Notes on adaptivity and robustness
- This strategy is conservative but adaptive: the agent never blindly cooperates; it tests and exploits unconditional cooperators, and will only pay to rebuild cooperation when the observed cooperation rate and remaining rounds make that plausible.
- It guards against permissive reciprocators by probing rarely; guards against punitive reciprocators by detecting retaliation quickly and switching to punishment.
- Parameters (W, coop_high, L_coax, probe_prob) can be tuned to the tournament environment; the defaults prioritize exploitation with limited, targeted forgiveness.

Edge cases and special handling
- Small r (e.g., r = 2 or 3): be more aggressive in early rounds because there is little horizon to recoup cooperation investments. Concretely, for r ≤ 3: defect from round 1 through r (never coax), except if you detect an opponent who always cooperates across rounds (then still defect to exploit).
- Very small n (n = 2): public goods becomes pairwise; the same logic applies but tracking individuals becomes easier—target punishment/coax at the single partner.
- If W = 0 (first round): use default first-round rule D.
- If one finds themselves in long sequences where F_avg flips around (oscillatory opponents), rely on short coax windows and low probe probability to avoid being trapped in cycles of being punished.
- If an opponent set shows near-100% cooperation even after your defections, treat them as permanent exploit targets and defect every round (maximal exploitation).

Why this is “aggressive”
- Default is to defect and harvest immediate gains.
- Will exploit unconditional cooperators indefinitely (unless you detect that exploitation triggers punitive loss).
- Punishes retaliators swiftly by switching to permanent or long-term defection.
- Forgiveness is short, costly, and only used when expected future gains warrant it.
- No naïve end-game cooperation: last-round defection guaranteed.

Implementation checklist for programmers
- Maintain per-player recent cooperation frequencies f_j over a sliding window W.
- Maintain group average F_avg and last-round group cooperation to detect delta_drop.
- State machine with modes {Exploit, Coaxing, Punish} and coax_rounds_left counter.
- Random probe: small probability per round when in Exploit to do a single C.
- Configurable parameters: W, coop_high, coop_low, probe_prob, L_coax, delta_drop, S_end.

This strategy is easy to implement, parameter-flexible, explicitly aggressive, and reacts to opponents’ behaviour rather than assuming norms. It will do well against unconditional cooperators (exploit them), avoid being repeatedly exploited by retaliatory strategies (detect & punish), and will attempt limited, measured rebuilding of cooperation when long-run benefits appear feasible.
'''

description_AGGRESSIVE_62 = '''
Name: Aggressive Conditional-Grim with Probes (ACG-Probe)

Short summary (mindset)
- Aggressive: begin by protecting yourself (defect), only join cooperation when the group has shown strong, recent commitment to cooperate, and punish any collapse of that commitment with permanent defection (a “grim” reaction). To avoid being trapped forever by noisy behaviour, periodically probe the group for renewed high-cooperation; if a probe shows the group can sustain cooperation again, resume conditional cooperation. This exploits unconditional cooperators, deters casual defectors, and only gives you cooperation when it is safe and profitable in repeated-play terms.

Core ideas used
- Cooperate only when past evidence shows a strong majority cooperated (so your cooperation is likely to be reciprocated).
- If you cooperate and cooperation collapses, treat that as exploitation and enter a permanentpunish mode (defect forever) except for occasional probes.
- Always defect in the final round (backward induction for the terminal move).
- All decisions depend only on game parameters (n, r, k) and observed history of actions.

Parameters derived from game inputs
- tau = ceil(0.8 * n)  // “strong majority” threshold (choose a high fraction to be aggressive)
- P_probe = max(2, floor(sqrt(r)))  // probe interval when punished
- last_round = r

(These can be tuned: 0.8 and sqrt(r) are recommended aggressive defaults. Higher tau = more cautious; lower tau = less aggressive.)

State maintained
- punish_mode (boolean) — false initially; if true you defect except when executing probes
- last_probe_round (integer) — the last round in which you executed a probe (initially 0)
- history: for each past round t, C_t = number of cooperators observed that round (including your own action and others’)

Decision rules (natural language)
1. Round r (final round): defect.

2. Round 1: defect. (Aggressive opening: don’t be first-mover cooperator.)

3. For rounds t = 2 .. r-1:
   - If punish_mode == true:
       - If (t - last_probe_round) >= P_probe:
           - Execute a probe: play C this round (cooperate) and set last_probe_round = t.
           - After the round, if C_t >= tau, set punish_mode = false (exit punishment), else stay in punish_mode.
       - Else:
           - Defect this round.
   - Else (not in punish_mode):
       - If C_{t-1} >= tau (strong majority cooperated last round):
           - Cooperate this round.
           - After the round, if C_t < tau (cooperation collapsed despite your cooperation), set punish_mode = true (enter permanent punishment mode) and set last_probe_round = t.
             (That is, if you cooperated but group did not sustain strong cooperation, treat you as exploited and switch to punish.)
       - Else:
           - Defect this round.

Edge / special cases
- If r = 2: round 2 is final -> defect in both rounds (you defect in round 1 per rule, and always defect in last round). This is consistent with aggressive, risk-averse play in very short games. (If you want to be less severe for very small r, decrease tau or enable a single cooperative test in round 1; but the aggressive default is defect.)
- If n is very small (e.g., n = 2), tau = ceil(0.8*2) = 2: you only cooperate when opponent cooperated previously. With this aggressive policy you will normally defect first; if an opponent cooperates repeatedly you will eventually join only after observing a full-cooperation round (rare if you open with D). This is intentionally aggressive.
- If multiple players are cooperating but just under tau, you still defect; tau can be reduced if you prefer a less aggressive variant.

Pseudocode

Initialize:
  tau = ceil(0.8 * n)
  P_probe = max(2, floor(sqrt(r)))
  punish_mode = false
  last_probe_round = 0
  For bookkeeping: store C_t for each completed t

For each round t = 1..r:
  if t == r:
    action = D
  else if t == 1:
    action = D
  else: // 2 <= t <= r-1
    if punish_mode:
      if (t - last_probe_round) >= P_probe:
        action = C
        last_probe_round = t
      else:
        action = D
    else:
      if C_{t-1} >= tau:
        action = C
      else:
        action = D

  Play action (C or D), observe full round outcome and update C_t.

  // After observing the round:
  if action == C and t < r:
    if C_t < tau:
      // exploited (cooperation collapsed) -> aggressive punishment
      punish_mode = true
      last_probe_round = t

  if punish_mode and action == C and C_t >= tau:
    // probe succeeded: group willing to sustain cooperation
    punish_mode = false

Rationale and properties
- Exploitation: Opening with D and generally defecting unless you see strong cooperation lets you exploit naive/unconditional cooperators for immediate high payoffs.
- Enforcement: The grim-like punishment (switch to permanent defection after being “suckered”) is harsh and credible: once you have evidence that your cooperation is not reciprocated, you stop cooperating. This deters strategies that try to get you to pay while they free-ride.
- Flexibility: Periodic probes let the strategy recover from transient noise or one-off mistakes: it can rejoin cooperation if the group reestablishes strong cooperation.
- Safety: Cooperating only when almost everyone cooperated previously reduces the risk of being the sole cooperator. The aggressive threshold (tau) means you cooperate only when group reciprocity is likely.
- Last-round logic: Always defect in the final round (no future to sustain reciprocation).

Variants (if you want slightly less or more aggression)
- Less aggressive: lower tau (e.g., 0.6*n), or require two consecutive rounds with C_t >= tau before you start cooperating.
- More aggressive: immediate punish_mode activation on a single shortfall (as above) and increase P_probe to test less often.
- Smarter dynamic tau: make tau depend on k and n (e.g., lower tau if k is close to n so cooperation is more valuable). Aggressive default uses fixed high tau for robustness.

Why this is robust in tournament play
- Against unconditional cooperators: you exploit them (you defect, collect private 1 plus public share from their contributions).
- Against unconditional defectors: you defect and obtain the safe baseline payoff; you don’t waste resources trying to coax cooperation.
- Against conditional cooperators (e.g., TFT-like): you will join only if you see strong mutual cooperation; if they reward you when you probe, you can earn the mutual-cooperation stream; if they defect opportunistically you punish and stop cooperating.
- Against complex, trying-to-manipulate strategies: the high threshold and grim punishment reduce opportunities for being gamed into repeatedly paying while others free-ride. Periodic probes prevent permanent lockout due to occasional noise.

Implementation note
- The strategy uses only observed counts of cooperators per round (C_t) and its own past actions — information available under the game assumptions.
- tau and P_probe are computed from (n, r) only; no other external assumptions are required.

This design produces an aggressive, history-dependent policy that (1) exploits naive cooperators, (2) only participates in cooperation when it is safe (strong majority observed), (3) punishes exploitation decisively, and (4) retains a path back to cooperation via occasional probes.
'''

description_AGGRESSIVE_63 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Summary (one sentence)
- Play to maximize my own cumulative payoff by (1) exploiting observable clusters of cooperators, (2) only cooperating when doing so is likely to create sustained cooperative returns I can later harvest, and (3) punishing/abandoning groups that do not respond to exploitation or that repeatedly betray me. The strategy is parameterized and adaptive to history; it never trusts blindly and always defects in the final round.

Intuition and aggressive mindset
- Cooperating is never the myopically best action in a single round (defection strictly dominates given k < n), so any cooperation must be an investment to change others’ future behavior. I will only invest (cooperate) when there is credible evidence that a critical mass of others are reliably cooperative and that enough rounds remain to recoup the investment. When that evidence appears I will “exploit” them by defecting to take the short-term extra payoff; if the population collapses in response, I will switch to punishment/abandonment to avoid being repeatedly exploited. I will probe cautiously and unpredictably to find exploitable cooperators, then harvest them.

Parameters (computed from game parameters or chosen defaults)
- n, k, r: given
- lookback window L (rounds) for estimating others’ behavior: L = min(5, max(1, floor(r/10)))
- probe length S_probe (rounds of cooperative probing to test responsiveness): S_probe = 1 (short is safer)
- exploit length S_exploit (rounds to defect when exploitation conditions met): S_exploit = max(1, floor(0.15 * r_remaining)) when triggered (bounded)
- high cooperation threshold p_high to trigger exploitation: p_high = 0.6 (default; see adaptive variant below)
- low cooperation threshold p_low to abandon cooperation: p_low = 0.25
- trust forgiveness threshold betray_count_limit: 2 (number of recent betrayals tolerated before entering permanent defect)
- small randomization epsilon for probing/testing: epsilon = 0.05
- minimum rounds left to attempt any cooperative investment: R_min = 1 + S_exploit (only invest if r_remaining > S_exploit)

Adaptive adjustment of p_high
- If k is closer to n (k/n near 1), cooperators produce more public good; we can be slightly more willing to exploit, so reduce p_high slightly: p_high = 0.6 - 0.2*(k/n - 0.5) clipped to [0.5,0.8]. (This is a simple rule; implementer can tune.)

State machine (high-level)
- States: Neutral, Probing, Exploiting, Punish/Abandon (Grim).
- Start in Neutral.
- Transitions are driven by fraction of cooperators observed in recent rounds and by whether my last cooperative investments were “rewarded” (others cooperated) or led to betrayal.

Definitions used in decision rules
- H[t] = vector (c_1,...,c_n) of contributions in round t (including my own).
- For estimating others, consider others’ contributions only: for round t, others_coop_frac[t] = (Σ_{j≠me} c_j) / (n-1).
- p_recent = average of others_coop_frac over last L rounds (excluding fraction computed only from rounds available).
- betray_count = number of rounds in a recent window (e.g., last L rounds) in which I cooperated but the fraction of others cooperating was < p_low (i.e., my cooperation was not reciprocated).

Decision rules — When do I cooperate vs defect?
1. Final round (round r): Always defect. (Backward induction and no future benefit.)
2. If currently in Exploiting state: defect for remaining exploit rounds, then reevaluate.
3. If in Punish/Abandon (Grim) state: defect for all remaining rounds.
4. Neutral default:
   - If r_remaining <= 1 + S_exploit: defect (not enough future rounds to recoup an investment).
   - Compute p_recent (others’ average cooperation rate over last L rounds).
   - If p_recent >= p_high and r_remaining > R_min:
       - Enter Exploiting state: defect for S_exploit rounds (harvest). (Do not cooperate until exploit ends.)
   - Else if p_recent <= p_low:
       - Remain/enter Punish/Abandon if betray_count >= betray_count_limit (I stop cooperating permanently); otherwise remain Neutral and defect.
   - Else (intermediate cooperation observed):
       - Play mixed probing: cooperate with small probability epsilon (to test responsiveness); otherwise defect.
       - If I choose to cooperate in probing and in the next few rounds others raise cooperation noticeably (p_recent increases consistently), then switch to Exploiting when p_recent >= p_high.
5. Probing state: when launched, cooperate for S_probe rounds (S_probe = 1) to test for conditional cooperators. If other players respond by cooperating in proportion p_recent >= p_high after probing, switch to Exploiting; else revert to Neutral and defect (or escalate punishments if repeatedly betrayed).

Punishment rules (aggressive)
- If I detect repeated betrayals of my probing cooperation (betray_count >= betray_count_limit), enter Punish/Abandon (permanent defection for the rest of the match). This prevents me from being repeatedly exploited.
- If I exploit (defect for S_exploit rounds) and the cooperation rate among others collapses below p_low, then I enter Punish/Abandon (I will not cooperate again unless there is a later sustained signal of coordinated cooperation).
- Optional escalation: if a pattern emerges where others repeatedly restore cooperation after I punish, I may return to Neutral and resume opportunistic probing (but only after a long inactivity of at least S_exploit rounds).

Ties, randomness and unpredictability
- Use epsilon-randomization in Neutral borderline cases to avoid being exploited by perfectly correlating opponents. E.g., when p_recent in (p_low, p_high), cooperate with probability epsilon and defect with probability 1-epsilon.
- Slightly randomize S_exploit (±1 round) to avoid cycles.

Edge cases
- First round (t = 1): Probing. Cooperate on round 1 with probability 0.5 (or deterministic cooperate if you prefer a more aggressive lure). Rationale: cooperating first lets me identify whether there are unconditional or conditional cooperators to exploit. Alternative first-round default: defect if you want strictly safe aggressive approach; but cooperating gives opportunity to harvest early if many others cooperate.
- Last round (t = r): Always defect.
- Near-last rounds: if r_remaining ≤ S_exploit + 1, do not initiate a cooperation-investment cycle (I defect).
- Very short games (r small, e.g., r = 2 or 3): tend to defect almost always except optional first-round probe; respond aggressively (punish quickly).
- If L > number of past rounds available, compute p_recent using available rounds only.
- If n = 2 (pairwise public good akin to PD), the same rules apply; be particularly aggressive in short games.

Pseudocode (concise)

Inputs: n, k, r, history H (rows: rounds 1..t-1, columns: players; missing rounds ignored)
State variables (persist across rounds): state ∈ {Neutral, Probing, Exploiting, Punish}, exploit_timer, betray_count

At the beginning of each round t:
  r_remaining = r - (t - 1)
  if t == r:
    action ← D; return

  compute L = min(5, max(1, floor(r/10)))
  compute others_coop_frac for each past round (exclude my c_i)
  p_recent = average of others_coop_frac over last min(L, available_rounds)

  if state == Punish:
    action ← D; return

  if state == Exploiting:
    if exploit_timer > 0:
      exploit_timer -= 1
      action ← D; 
      if exploit_timer == 0: state ← Neutral
      return

  # Neutral / probing logic
  if t == 1:
    # first-round probing choice (aggressive: lure)
    with prob 0.5: action ← C; else action ← D
    if action == C: record this as a probe (optional)
    return

  # Do not invest if not enough rounds left
  if r_remaining <= 1 + max(1, floor(0.15 * r_remaining)):
    action ← D; return

  # Trigger exploitation if cooperation is reliably high
  if p_recent >= p_high and r_remaining > (1 + S_exploit):
    state ← Exploiting
    exploit_timer ← S_exploit
    action ← D
    return

  # If cooperation is very low, consider abandoning
  if p_recent <= p_low:
    if betray_count >= betray_count_limit:
      state ← Punish
      action ← D; return
    else:
      action ← D
      return

  # Borderline region: probe with small epsilon
  with prob epsilon:
    action ← C
    # if I cooperated and others didn't reciprocate, increment betray_count after observing next round
  else:
    action ← D
  return

After each round resolves, update:
  - update betray_count: if I cooperated in a recent probing round and others' coop_frac in that round < p_low then betray_count += 1, else possibly decay betray_count by 1 over time to allow forgiveness if desired.
  - update p_recent on next decision.

Notes and rationale for parameter choices
- S_exploit: choose a fraction of remaining game so exploitation yields material gains but is short enough to limit retaliation costs.
- p_high = 0.6 is intentionally demanding: I only exploit when a clear majority of others are cooperating.
- p_low = 0.25 ensures I do not throw away cooperation into clear defector crowds.
- The strategy is aggressive because it (a) exploits when group cooperation is high, (b) seldom cooperates except to probe, and (c) abandons cooperation (punishes) permanently after repeated betrayals to avoid being used.

Variants and tuning
- More aggressive: increase p_high to 0.7 and reduce first-round probe probability to 0.25 (less initial trust), or make exploit length longer.
- More cautious: require multi-round confirmation before exploiting (e.g., p_recent sustained L rounds), reduce betray_count sensitivity.
- If you want to be less predictable, randomize S_exploit and probing frequency.

Why this is robust
- Uses only observable history and parameters.
- Balances exploration (probing) and exploitation; does not rely on others following norms.
- Quickly abandons cooperation if not profitable, preventing long-run exploitation by defectors.
- Takes advantage of exploitable cooperators when they exist and extracts value aggressively.

Implementation notes for programmers
- Keep the state machine (Neutral / Probing / Exploiting / Punish) and timers small and efficient.
- Update p_recent with an exponentially-weighted moving average if you prefer smoother responsiveness.
- Keep epsilon small to avoid being manipulated by opponents that can detect deterministic patterns.

End result
- A compact, implementable strategy that is aggressively self-maximizing, adaptive to observed cooperation rates, protects against repeated exploitation, and exploits cooperation clusters when doing so yields short-term gain and sufficient expected future payoff.
'''

description_AGGRESSIVE_64 = '''
Name: Aggressive Conditional Exploiter (ACE)

Intuition (short)
- Default posture: defect. The one-shot incentive is to defect; ACE is aggressive so it will not give away payoff unless doing so buys more payoff later. ACE exploits unconditional cooperators, opportunistically defects when many others cooperate, and only pays the cost of cooperating when there is strong, empirical evidence that doing so will raise future incoming contributions enough to repay the cost. When betrayed after cooperating ACE responds with harsh, long punishments (to deter being used).

Key constants (computed from game parameters)
- n, r, k are given.
- one-shot cooperation cost L = 1 - (k/n) (this is the immediate loss to you from cooperating vs defecting in a single round).
- lookback window W = min(10, max(1, r-1)). (History window for statistics.)
- endgame rounds E = 1 (always defect on final round). Optionally set E = min(3, r-1) for more conservative play in short games; default: E = 1.
- exploitation_threshold_high = 0.75 (fraction of players cooperating in a round that counts as a "coop-rich" round).
- unconditional_cooperator_rate = 0.9 (player whose cooperation rate ≥ 0.9 over window W is treated as effectively unconditional cooperator).
- min_detect_delta = 0.10 (minimum detectable increase in others’ cooperation you require to believe your cooperation causally increases their cooperation).
- punishment_length M = min( max(3, floor(r/5)), r-1 ) (punish for a significant number of rounds but not longer than remaining game).
- recoup_condition: Cooperate only if expected future gain from inducing extra cooperators exceeds the present cost L:
    R_remain * (k/n) * delta_est > L
  where R_remain = remaining rounds after current round, delta_est is estimated increase in average number of other cooperators per round attributable to your cooperating (estimated from history). We require delta_est ≥ min_detect_delta to trust it.

Decision rules — full description

At the start of each round t (1 ≤ t ≤ r) the agent computes statistics from history H up to round t-1:

- For each player j ≠ me, compute coop_rate_j = (# times j played C in last W rounds) / W.
- Group_coop_rate_last = average over j of coop_rate_j (i.e., mean fraction of others cooperating in last W rounds).
- For causal responsiveness: compute two averages over the history window (or full history if short):
    - Others_after_my_C = average fraction of other players who cooperated in the round immediately after rounds where I played C (count only rounds where a next round exists and is within history).
    - Others_after_my_D = average fraction of other players who cooperated in the round immediately after rounds where I played D.
  Then estimate delta_est = Others_after_my_C − Others_after_my_D (this is the observed change in others’ cooperation associated with my cooperating).
  If not enough samples exist (fewer than 2 occurrences of my C or my D in window), use a conservative delta_est = 0.

Now rules in order (priority):

1) Endgame: if t = r (the final round) — defect. (No future to induce cooperation.)

2) Exploit unconditional cooperators: if there exists at least one player j with coop_rate_j ≥ unconditional_cooperator_rate, then defect (exploit them every round). Rationale: they will give you extra payoff; aggressive behavior is to harvest that.

3) Opportunistic harvest of coop-rich rounds: if in the immediately previous round (t-1) the total number of cooperators ≥ floor(exploitation_threshold_high × n), then defect in this round (exploit the high cooperation). Explanation: when many others are cooperating, defecting now gives a strictly higher payoff than cooperating and is pure exploitation.

4) Conditional cooperation to induce reciprocation:
   - Compute R_remain = r − t (remaining rounds after current round).
   - If delta_est ≥ min_detect_delta AND R_remain * (k/n) * delta_est > L:
       - Cooperate in this round (C) as an investment to try to raise others’ cooperation in the remaining rounds.
     Otherwise: go to next rule.

   Notes:
   - This condition ensures ACE only pays the immediate cost of cooperating when historical evidence suggests that cooperating raises others’ cooperation by at least delta_est, and the expected cumulative benefit in remaining rounds outweighs the immediate loss.
   - ACE uses the empirical causal test (Others_after_my_C vs Others_after_my_D) to infer responsiveness; this prevents wasted “generous” cooperation against non-responsive opponents.

5) Punishment when betrayed: if I cooperated in the previous round (t-1 == C by me) and the number of other cooperators in that same round ≤ floor( (exploitation_threshold_high - 0.25) × n ) (i.e., a significant betrayal where many others did not reciprocate), then enter punishment mode: defect for the next M rounds (or until the end of the game if fewer rounds remain). This is a harsh deterrent (grim-like for a while) to prevent being used.

6) Default fall-back: defect.

Edge cases and clarifications

- First round (t = 1): There is no history. By the rules above:
  - It is not the final round (unless r = 1, but r > 1 by spec).
  - No unconditional coop detected.
  - No prior coop-rich round.
  - No delta_est available (set to 0).
  => Default: defect on round 1.

- Short game (small r): Because R_remain will be small, the recoup condition in rule 4 will rarely be met; ACE will almost always defect in short games. This is appropriate because the window to earn back a cooperation cost is small.

- Very long game and responsive opponents: If you observe that when you cooperate others subsequently increase cooperation (delta_est large enough) and many rounds remain so the recoup condition holds, ACE will cooperate as a strategic investment. But it will defect to harvest when many others cooperate in the short term (rule 3) and will punish betrayals harshly (rule 5). This combination allows ACE to exploit unconditional cooperators and to sustain mutual cooperation only when the evidence strongly suggests others are reciprocity-sensitive and there are enough rounds left to profit.

- Last few rounds: ACE always defects in the final round. You can optionally extend E to 2–3 rounds (set E > 1) if you want to be extra safe; the default is E = 1.

- Parameter adaptivity: The core thresholds (W, exploitation_threshold_high, unconditional_cooperator_rate, min_detect_delta, M) are tunable constants. They are set to reasonable defaults above; implementers can adjust them for more or less aggression. The decision rule explicitly depends on n, r, k (via L and R_remain), and on empirical history.

Pseudocode

Inputs: n, r, k, history H (list of past rounds; each round lists each player’s C/D)
Constants set as above.

At start of round t:
  if t == r:
    play D
    return

  compute W, window = last W rounds of H (or entire history if shorter)
  for each player j ≠ me:
    coop_rate_j = (# C by j in window) / length(window)
  Group_coop_rate_last = average_j coop_rate_j

  if exists j with coop_rate_j ≥ unconditional_cooperator_rate:
    play D
    return

  if t > 1:
    total_coops_prev = number of players who played C in round t-1
    if total_coops_prev ≥ ceil(exploitation_threshold_high * n):
      play D
      return

  compute Others_after_my_C and Others_after_my_D from history (use only pairs of consecutive rounds where my previous action is C or D)
  if insufficient samples then delta_est = 0 else delta_est = Others_after_my_C - Others_after_my_D

  R_remain = r - t
  L = 1 - (k/n)

  if delta_est ≥ min_detect_delta and R_remain * (k/n) * delta_est > L:
    play C
    return

  if I played C in round t-1:
    total_others_coop_in_that_round = (# of other players who played C in round t-1)
    if total_others_coop_in_that_round ≤ floor((exploitation_threshold_high - 0.25) * n):
      enter punishment mode for next M rounds (set internal counter punish_until = t + M - 1)
  if in punishment mode (current round ≤ punish_until):
    play D
    return

  play D
  return

Aggressive properties (why this is aggressive and robust)
- Default defect maximizes single-round payoff and avoids needless generosity.
- Exploits unconditional cooperators directly (rule 2).
- Opportunistically defects to harvest when the group is temporarily cooperative (rule 3).
- Only pays cooperation cost if there is empirical evidence that doing so raises future receipts enough to pay back (rule 4) — prevents being suckered into “nice” cooperation.
- Uses strong punishments after being betrayed to deter exploitation; punishment length is nontrivial (M) so opponents learn not to defect after you try to cooperate.
- Adapts to opponent behavior (delta_est, per-player coop rates, group rates) and to game parameters (r and k enter the recoup calculation).
- Robust to a wide set of opponent types:
  - Exploits unconditional cooperators.
  - Can sustain cooperation with conditional cooperators who are responsive to your cooperation.
  - Refuses to waste payoff on naive or anti-social strategies in short games or near the end.

Implementation notes
- Use smoothing for delta_est to avoid noise (e.g., exponentially-weighted moving average) if opponents are noisy.
- If history sample sizes are tiny, be conservative (delta_est = 0) — ACE will defect rather than be generous.
- To reduce oscillation of punishments with noisy betrayals, you may require the betrayal condition to hold in two out of the last two rounds before punishing.
- Tunable thresholds (W, exploitation_threshold_high, unconditional_cooperator_rate, min_detect_delta, M) let you modulate aggression vs. opportunistic cooperation.

Summary
ACE is an aggressive, adaptive strategy: it defects by default and attacks exploitable cooperators, but it will invest in cooperation only when there is credible, empirical evidence that such investment will raise future payoffs sufficiently. It punishes betrayals harshly to avoid being used, and it opportunistically defects to harvest high-cooperation rounds. The rules depend only on the game parameters n, r, k and the public history, and are implementable as a straightforward algorithm.
'''

description_AGGRESSIVE_65 = '''
Name: Aggressive Conditional Coercer (ACC)

Intuition (short)
- Be willing to cooperate only when almost everyone has demonstrated reliable cooperation; otherwise refuse to be exploited.  
- If a unanimous/near-unanimous cooperative opportunity appears, take a one-round exploitation (defect once) to extract extra payoff, then (if needed) punish any players who retaliate/defect.  
- Punish defectors harshly and for a long enough interval to make cooperation more attractive than continuing to defect.  
- Always defect on the last round.

This strategy is aggressive: it rarely gives free contributions, it exploits clear cooperative majorities opportunistically, and it uses long, credible punishments to coerce future cooperation. It depends only on parameters (n, r, k) and observed history (who cooperated each past round).

Parameters derived from game parameters
- lookback window w = max(1, floor(r / 10)) — use recent behavior over ~10% of the game (at least 1 round).
- trust_threshold q = 0.9 — a player is “trusted” if they cooperated ≥ 90% of times in the lookback window.
- unanimity_fraction φ:
  - if r ≤ 10 then φ = 0.8 (allow a little slack in short games),
  - else φ = 1.0 (require essentially unanimous recent cooperation).
- punishment_length P = max(1, floor(0.3 * r)) — punish for about 30% of the game (bounded by remaining rounds).
- exploit_cooldown X = max(1, floor(r / 8)) — after exploiting once, wait X rounds before next exploitation attempt (prevents continuous alternating exploitation/punishment loops).
- safety: always defect on final round (backward induction).

State kept (based on history)
- For every other player j: history of their last actions (C or D) — we only need last w rounds of each player.
- round index t (1..r)
- last_exploit_round (initially -infty)
- currently_punishing_until (initially 0)
- last_round_cooperator_count (for last round observed)

Decision rules (natural language)
1. If t == r (last round): play D.
2. If t ≤ currently_punishing_until: play D (we are in punishment mode).
3. Compute for each other player j their recent_coop_rate_j = (# of C by j in last min(w, t-1) rounds) / min(w, t-1). (If t-1 = 0, treat as 0.0 for safety.)
4. Let trusted_fraction = fraction of other players j with recent_coop_rate_j ≥ q.
5. If trusted_fraction ≥ φ:
   a. If (t - last_exploit_round) ≥ X and last_round_cooperator_count ≥ ceil(φ * n) (i.e., the previous round had almost everybody cooperating), then:
      - Exploit: play D this round (take the one-shot extra gain).
      - Set last_exploit_round = t.
      - Observe reactions next round(s); if any player responds by defecting when previously cooperative, begin punishment (see step 6).
   b. Else:
      - Cooperate (play C) — join mutual cooperation.
6. If in the immediately previous round some players defected while a large fraction of the group had cooperated (i.e., opportunistic defection observed), then initiate punishment:
   - Set currently_punishing_until = t + min(P, r - t) (start punishment immediately; last until P rounds or end of game).
   - During punishment, play D always. After punishment ends, allow redemption only if players meet the trust threshold in the lookback window.
7. Otherwise (trusted_fraction < φ): play D.

Redemption and forgiveness
- After punishment expires, players can “redeem” by demonstrating high recent cooperation: only if recent_coop_rate_j ≥ q (computed with the lookback window) will they be counted as trusted. The strategy will resume cooperation only when the trusted_fraction condition holds again.

First-round rule (edge-case)
- If t = 1:
  - If r ≥ 6: play C (probe for cooperative opponents — aggressive strategies often benefit from an initial probe to detect cooperators).
  - If r < 6: play D (short games: too risky to open).

Behavior on short games
- Because endgame effects dominate, ACC is conservative in short games (starts by defecting and rarely grants cooperation); for medium/long games it is willing to establish cooperation but only under almost-unanimous, sustained evidence.

Pseudocode

Initialize:
  last_exploit_round = -INF
  currently_punishing_until = 0
For each round t = 1..r:
  if t == r:
    play D; continue
  if t <= currently_punishing_until:
    play D; continue
  if t == 1:
    if r >= 6: play C else play D
    continue
  w_actual = min(w, t-1)
  for each player j != me:
    recent_coop_rate_j = (# of C by j in last w_actual rounds) / w_actual
  trusted_fraction = (# of j with recent_coop_rate_j >= q) / (n-1)
  last_round_cooperator_count = number of players (including me if known) who played C in round t-1
  if trusted_fraction >= φ:
    if (t - last_exploit_round) >= X and last_round_cooperator_count >= ceil(φ * n):
      // exploit once
      play D
      last_exploit_round = t
      // detect possible betrayal next rounds; if others reduce cooperation, trigger punishment
    else:
      play C
  else:
    play D

When to trigger punishment (immediately upon observing history):
  After any round where a large cooperative majority existed (last_round_cooperator_count >= ceil(φ * n)) and one or more players defected in a way that looks like opportunistic exploitation, set:
    currently_punishing_until = t + min(P, r - t)
  During punishment play D always. After punishment ends, allow redemption per trust_threshold rules.

Why this is aggressive and why it’s robust
- Aggressive criteria: cooperation is only granted when nearly every opponent has proven reliably cooperative; otherwise ACC refuses to give away endowments. This reduces suckers’ payoffs and deters freeloaders. It exploits near-unanimous cooperation by defecting once to collect an above-baseline payoff, then uses long punishments if opponents retaliate, making defection an unattractive long-run strategy for them.
- Robustness: ACC does not assume norms or prearranged schedules. It uses only observed history and thresholds. It is forgiving only when players demonstrably earn it (high recent coop rates). It is adaptive: it probes early in long games, can build cooperation, can exploit when opportunity opens, and can switch to long punishments to enforce compliance.
- Parameter ties to n, r, k: thresholds do not need k explicitly (public-goods incentives are general), but the length of punishment P and lookback w scale with r so decisions are sensitive to horizon. The last-round defection is always applied (backward induction).

Tuning notes (for implementer)
- Increase q and φ to be stricter (more aggressive). Decrease them to be more conciliatory.
- Increase P to make punishments longer and thus more coercive; decrease P to be more risk-averse.
- Adjust w so the trust metric is stable (larger w for noisy opponents).
- The exploit_cooldown X prevents the strategy from continuously alternating exploit/punish cycles that harm its own payoff.

Summary statement
ACC is an aggressive, history-dependent rule: it cooperates only when near-unanimous, exploits a clear cooperative majority once, then punishes defectors for a long interval. It defects on the last round and otherwise uses recent cooperation rates to decide whether to reward (cooperate) or refuse (defect). This produces high exploitation of naive cooperators, strong deterrence against persistent defectors, and adaptivity to many opponent behaviors.
'''

description_AGGRESSIVE_66 = '''
Summary (one line): Be aggressively self-interested: default to defect every round, probe rarely to detect exploitable cooperators, exploit any persistent unconditional cooperators, and never cooperate in the last round.

Rationale (brief): In a one-shot public-goods round D strictly dominates C (for any fixed behavior of others), and finite common-knowledge repetition implies strong incentives to defect. The aggressive objective is to maximize my own total payoff against arbitrary opponents, so the safe baseline is to defect always. To be adaptive and extract extra payoff when opponents are naïvely or habitually cooperative, add a small, controlled probing mechanism and a fast, permanent exploitation mode when a large cohort of opponents is identified as reliably cooperative. Harsh, immediate defection is used as punishment and permanent protection once opponents turn non-beneficial.

Decision rules (natural language then pseudocode):

Notation
- n, r: given
- t: current round index (1..r)
- history H: for each past round s < t we observe each player's action c_j,s ∈ {0,1}
- r_remain = r - t + 1 (rounds including current)
- coop_rate_j = (# times player j played C so far) / (t-1) if t>1, otherwise undefined
- avg_other_coop = average of coop_rate_j over j≠me (use 0 if undefined)
- last_round_total_C = total cooperators observed in round t-1 (0 if t=1)
- Parameters (fixed, depend only on n,r,k; choose defaults below):
  - p_probe (small probe probability) = min(0.05, 5/r)  (small and decreases with long tournaments)
  - exploitable_rate = 0.80  (threshold to tag a player as persistent cooperator)
  - exploitable_fraction = max(0.25, 2/n)  (fraction of other players that must be persistent cooperators to trigger full exploitation)
  - burn_threshold = 0.5  (if a large collapse in cooperation follows our exploitation, stop any future cooperation attempts)
  - probe_window = min(5, r)  (how many recent rounds to consider for short-term signals)
  - min_exploit_horizon = 2  (need at least this many remaining rounds to justify exploitation)

State variables maintained
- persistent_cooperators = set of players j with coop_rate_j ≥ exploitable_rate (computed each round)
- exploitation_mode (boolean): once triggered becomes true for remainder unless burned
- burned (boolean): if true, refuse to cooperate forever

Initial values: exploitation_mode = false, burned = false.

High-level rules
1. Last round (t == r): Defect unconditionally. (Backward induction + exploitation in last round impossible.)
2. Burned state: If burned == true, Defect unconditionally.
3. Exploitation detection:
   - Each round recompute persistent_cooperators based on full history.
   - If exploitation_mode == false and
     - |persistent_cooperators| ≥ exploitable_fraction × (n−1) and
     - r_remain ≥ min_exploit_horizon,
     then set exploitation_mode = true and switch to permanent Defect (this exploits them immediately and forever).
   - Exploitation_mode simply means “defect every round” (same as baseline) but signals we identified exploitable cohort. It also suppresses probes/attempts at cooperation.
4. Probing and testing (only when not exploitation_mode and not burned):
   - Default action is Defect.
   - With small probability p_probe (independent each round), play Cooperate to test whether any players are conditional cooperators who will respond by increasing cooperation subsequently.
   - If we probe and observe next round(s) that a large fraction (> exploitable_fraction) of others increase cooperation by a significant amount (e.g., their short-term coop rate increases by ≥ 0.3), consider those players for persistent_cooperators.
5. Burn (permanent stop at attempts to induce cooperation):
   - If after we try to exploit (i.e., we have defected while others had been cooperating) cooperation collapses across the population to below burn_threshold for several consecutive rounds (use probe_window), set burned = true (we stop all future cooperation attempts).
   - Burn protects us from wasting rounds trying to coordinate on cooperation that will no longer appear.
6. First round (t == 1): Defect, except with probability p_probe cooperate to discover naïve or unconditional cooperators.
7. Edge of game (few rounds left): If r_remain <= 1, defect. If r_remain == 2, do not initiate any attempt to induce cooperation (defect, perhaps probe with tiny prob), because there is insufficient horizon to benefit.

Compact pseudocode

Initialize:
  exploitation_mode = false
  burned = false
  p_probe = min(0.05, 5/r)
  exploitable_rate = 0.80
  exploitable_fraction = max(0.25, 2/n)
  burn_threshold = 0.5
  probe_window = min(5, r)
  min_exploit_horizon = 2

On each round t:
  r_remain = r - t + 1

  if t == r:
    action = D
    return action

  if burned:
    action = D
    return action

  compute coop_rate_j for all j≠me (use history length t-1; if none, treat as 0)
  persistent_cooperators = { j | coop_rate_j >= exploitable_rate }

  if not exploitation_mode:
    if |persistent_cooperators| >= exploitable_fraction*(n-1) and r_remain >= min_exploit_horizon:
      exploitation_mode = true
      action = D
      return action

  if exploitation_mode:
    // we have identified exploitable cohort; extract payoff by always defecting
    action = D
    // monitor population: if average cooperation collapses below burn_threshold (for probe_window rounds), set burned = true
    return action

  // Not exploitation_mode and not burned: probing regime
  // Default is defect, with occasional probes to detect exploitable cooperators
  draw u ∼ Uniform(0,1)
  if u < p_probe:
    action = C   // probe
  else:
    action = D

  return action

Burn detection (run after observing results of round t):
  compute avg_coop_recent = average total_cooperators / n over the last probe_window rounds (including t)
  if exploitation_mode and avg_coop_recent < burn_threshold for probe_window consecutive rounds:
    burned = true
    exploitation_mode = false

Notes and parameter guidance
- p_probe is small because cooperation is individually costly; probing is only for discovering exploitable opponents, not to promote cooperation as a norm.
- exploitable_rate and exploitable_fraction control aggressiveness: lower exploitable_rate or exploitable_fraction makes the strategy easier to trigger exploitation (more aggressive toward detecting and exploiting marginal cooperators). The defaults are already aggressive (need high per-player cooperativeness but only 25% of players required).
- burn_threshold ensures the strategy quickly abandons any attempt to create higher cooperation if the population becomes hostile or punishing.
- The strategy never relies on cooperation to punish or enforce; punishment is just defection (which also protects us).

Why this is "aggressive"
- The baseline is unconditional defection every round (maximally selfish).
- It probes rarely (minimal cost) to find exploitable opponents who are reliably cooperative; once such a cohort is detected it switches to persistent exploitation (always defect) to maximize immediate exploitation payoff each round.
- It is quick to stop attempts (burn) when exploitation reduces aggregate cooperation (so it does not waste rounds trying to re-build cooperation).
- It never cooperates in the final round and avoids long cooperative gambits that can be exploited by others.

Robustness
- Works against:
  - All-defect strategies (matches them; no loss).
  - All-cooperate or high-cooperation opponents (exploits them by defecting every round).
  - Conditional cooperators: probes detect them; if large enough, exploit cohort quickly.
  - Random/noisy strategies: probing is rare so noise has little effect; burn protects against unstable environments.
- Depends only on observed history and known parameters n, r, and fixed thresholds derived from them.

Variants (tunable aggressiveness)
- More aggressive: increase p_probe, lower exploitable_rate and exploitable_fraction to trigger exploitation more often.
- More cautious: set p_probe = 0 and exploit only if you observe persistent_cooperators purely from their actions (no probing).

Final practical rule for implementation
- Default action: D.
- Occasionally (very rarely) play C only to test whether many others are unconditional cooperators.
- If you can identify a sizable group of opponents who almost always play C, defect every round to exploit them; never cooperate when r_remain == 1; if cooperation collapses after your exploitation, stop attempting any cooperation forever.

This strategy is simple to implement, aggressive in payoff-seeking, adaptive to opponent behaviour via light probing and detection, and robust in finite repeated play.
'''

description_AGGRESSIVE_67 = '''
Name: Aggressive Conditional Punisher (ACP)

Short description (mindset):
- Start by offering cooperation to capture gains from any willing cooperators.
- If anyone defects, hit back immediately and severely: stop contributing for a long, pre‑committed punishment period and probe for reconciliation only after punishments. Be willing to convert to permanent defection against repeat offenders.
- Occasionally and opportunistically defect once when everyone has been reliably cooperative to grab extra payoff (I accept the risk of retaliation). This combination is explicitly aggressive: it seeks to extract gains from cooperators, deters free‑riding by imposing high future costs, and is unforgiving toward repeated offenders.

Key internal parameters (computed from n, r):
- P_default (punishment length): ceil(0.5 * r). (Long, but not necessarily forever; scales with horizon.)
- Forgive_required: F = 2 consecutive rounds of universal cooperation (after a punishment) to restore normal cooperation mode.
- Repeat_offense_threshold: S = 2 (if a player is punished S times for separate offenses, convert to permanent punishment against everyone).
- Probe_frequency_for_exploit: E = max(3, ceil(r/10)). (Attempt opportunistic exploitation at most about every E rounds of sustained cooperation.)
- Window for “reliably cooperative” (used to decide opportunistic exploitation): W = min( max(3, ceil(r/10)), t-1 ) at turn t. (Small window.)

State variables (maintained each round):
- mode ∈ {COOP, PUNISH, PROBE, PERM_DEFECT}
- punished_players: set of players being punished (updated when offenses occur)
- punishment_timer: integer countdown
- offense_count[j]: number of separate punishments applied historically to player j (starts at 0)
- last_unanimous_coop_round: last round index when all players cooperated
- last_exploit_round: last round index when the strategy performed an opportunistic defection

Decision rules (natural language + pseudocode):

High-level rule order (evaluated each round t):
1. If t == r (last round) → play D (defect). (Standard endgame aggression.)
2. If mode == PERM_DEFECT → play D.
3. If mode == PUNISH:
   - play D.
   - decrement punishment_timer; when it reaches 0, switch to PROBE.
4. If mode == PROBE:
   - play C for one round (this is the reconciliation probe).
   - If the probe round saw unanimous cooperation (everyone played C in the probe), increment forgiveness counter; if forgiveness counter ≥ F, clear punished_players, reset counters, switch to COOP.
   - If any punished player defects in probe (or any new defection occurs), identify defectors and:
       a) increment their offense_count (if this is a separated offense),
       b) set punished_players = set of current defectors,
       c) set punishment_timer = P_default,
       d) if any player's offense_count ≥ S, switch mode → PERM_DEFECT immediately; else switch to PUNISH.
5. If mode == COOP:
   - Opportunistic exploitation: if the last W rounds (including t-1) show near‑unanimous cooperation (all players cooperated in each of those rounds), and (t - last_exploit_round) ≥ E, then play D this round (do a single-shot exploit), set last_exploit_round = t. If that exploit causes any defection response from others next round, it will trigger punishment rules (we accept that).
   - Otherwise play C.
6. Monitoring: after each round, if any player defected when we were in COOP mode (i.e., a new defection happened), identify the set of defectors Dset:
   - For each j in Dset, increment offense_count[j] by 1 (counting this as a separate offense).
   - Set punished_players = Dset
   - Set punishment_timer = P_default
   - Switch mode → PUNISH
   - If any offense_count[j] ≥ S after increment, immediately switch to PERM_DEFECT.

Pseudocode (concise):

Initialize:
  mode = COOP
  punished_players = {}
  punishment_timer = 0
  offense_count[j] = 0 for all j
  last_unanimous_coop_round = 0
  last_exploit_round = -Inf
  forgiveness_streak = 0

Each round t (players simultaneously choose action):
  if t == r:
    action = D
    return action

  if mode == PERM_DEFECT:
    action = D
    return action

  if mode == PUNISH:
    action = D
    punishment_timer -= 1
    if punishment_timer == 0:
      mode = PROBE
      forgiveness_streak = 0
    return action

  if mode == PROBE:
    action = C
    // after observing round outcome:
    if (everyone played C in this probe round):
      forgiveness_streak += 1
      if forgiveness_streak >= F:
        punished_players = {}
        mode = COOP
    else:
      // some defection occurred during probe
      Dset = {players who played D in probe}
      for j in Dset: offense_count[j] += 1
      punished_players = Dset
      punishment_timer = P_default
      if any offense_count[j] >= S: mode = PERM_DEFECT else mode = PUNISH
    return action

  // mode == COOP
  // check for opportunistic exploit
  if (last W rounds are unanimous C) and (t - last_exploit_round >= E):
    action = D
    last_exploit_round = t
    // After the round, if any defection occurred (others defected) we will handle it by switching to PUNISH
    return action
  else:
    action = C
    return action

After each round (observation step):
  // If we were in COOP mode and saw any defection:
  if mode == COOP and there exists at least one player j who played D this round:
    Dset = {j | j played D}
    for j in Dset: offense_count[j] += 1
    punished_players = Dset
    punishment_timer = P_default
    if any offense_count[j] >= S: mode = PERM_DEFECT else mode = PUNISH

Notes and rationale (why this is aggressive, adaptive, robust):

- Aggression: immediate, long punishments (P_default ~ half the game) deter exploitation; repeat offenders trigger permanent defection. That threatens large future cost to anyone who defects against me, which reduces the incentive to free‑ride against my contributions.
- Initial cooperation signal: cooperate on round 1 and in COOP mode to capture gains if others are willing to cooperate; this allows exploitation of pure cooperators when safe.
- Opportunistic exploitation: if a group has been reliably cooperative, I will occasionally defect once to boost my immediate payoff; this extracts value from naive cooperators (aggressive extraction). If they retaliate, I punish hard.
- Adaptiveness: punishment is targeted to the observed defectors (we mark the set of defectors and punish after infractions); after punishment I probe to see whether cooperation can be restored (forgiveness after demonstrated consistent cooperation). The strategy therefore can re‑establish cooperation with reciprocators but is unforgiving to repeat offenders.
- Robustness to different opponents: 
  - Against Always-Defect: after observing their defection in round 1, I punish/defect for remainder (limit my losses).
  - Against conditional cooperators/TFT-like: the threat of long punishments encourages those strategies to cooperate with me (I will cooperate while they do so).
  - Against always-cooperate (naive): I exploit them occasionally to increase my payoff (I accept a chance of retaliation).
  - Against stochastic/noisy opponents: this strategy is not noise‑tolerant (it is intentionally harsh). If you expect noise, reduce P_default and increase forgiveness F to avoid catastrophic unwinding. But the user asked for aggressiveness, so the design favors severity.
- Endgame handling: last round always defects (standard and essential for finite horizon). That prevents being exploited on last-round contributions.

Parameter recommendations:
- For a purely aggressive entry, use P_default = ceil(0.5 * r), F = 2, S = 2, E = max(3, ceil(r/10)). If you need slightly less brittle behavior (if you expect noisy mistakes), reduce P_default to ceil(0.25 * r) and increase F to 3.

Summary of behaviour in plain terms:
- I start cooperative and try to reap mutual benefits.
- If anyone defects, I immediately stop contributing for a long punishment period targeted at the observed defectors; if they repeat, I defect forever.
- After punishment I probe with a cooperative round and require a couple of rounds of universal cooperation before trusting again.
- While trust is stable I will opportunistically defect once every so often to increase my relative payoff.
- Last round: defect.

This is an explicit, parameterized, aggressive strategy that depends only on the game parameters (r) and observed history; it is easy to implement and will both extract payoff from naive cooperators and impose strong costs on defectors.
'''

description_AGGRESSIVE_68 = '''
Name: Aggressive Bully with Probing and Targeted Punishment (ABPP)

High-level idea
- Be aggressively self-interested: exploit any clear pool of cooperators by defecting to gain the highest immediate payoff, but do this strategically so the group does not permanently collapse into everyone-defect (which would reduce long-run profit).
- Use probing cooperations occasionally to detect exploitable cooperators and to lure the group back toward cooperation when it is beneficial.
- Use fast, targeted punishment against players who repeatedly defect when others cooperate, and use group-level punishment when exploitation by us causes a large collapse of cooperation.
- Never cooperate in the last round (backward-induction endgame).

The strategy uses only: (i) the game parameters n, k, r and (ii) the observed history of who cooperated in previous rounds. It is deterministic (no private communication needed) and adaptive.

Main parameters (derived deterministically from n,k,r; you can tune constants if desired)
- Memory window H = min(10, max(1, floor(r/3))) — how many past rounds we use to estimate recent behaviour.
- Personal forgiveness window L = min(H, 5)
- Personal reliable threshold θ_personal = 0.6 (a player is “reliable” if their cooperation rate in the window ≥ θ_personal).
- Group cooperation thresholds: θ_high = 0.60, θ_low = 0.30 (fractions of other players cooperating in recent window).
- Punishment length P = max(1, ceil(r/6)) rounds.
- Probe cadence S = max(3, ceil(r/6)) — every S rounds we attempt a single cooperation as a probe unless in punishment or it’s the last round.
- Collapse sensitivity Δ_collapse = 0.30 (a large drop in group cooperation triggers group punishment).

State variables (maintained from history)
- punish_timer (integer ≥ 0): if >0 we defect and decrement; punishment is group-level temporary hard punishment.
- personal_blacklist: set of players marked as “cheaters” (we will defect against them until they show sustained cooperation).
- last_probe_round, last_exploit_round: round indices to control probing/exploitation bookkeeping.

Decision rules (round t; rounds numbered 1..r)

1) Last round (t == r)
- Always defect. (Standard backward-induction.)

2) First round (t == 1)
- Defect. (Aggressive safe start: avoid being a first-round sucker.)

3) For rounds 2 ≤ t ≤ r−1, determine action as follows:

Preparation: compute recent statistics
- Let H_t = min(H, t−1) (the actual number of past rounds available).
- For each other player j ≠ me, compute cooperation_rate_j = (number of times j played C in the last H_t rounds) / H_t.
- Let reliable_j = (cooperation_rate_j ≥ θ_personal).
- Let reliable_fraction = (# reliable_j) / (n−1).
- Let group_coop_last = (number of players (including me) who cooperated in round t−1) / n.
- Let group_coop_recent = average fraction of cooperators (including me) across the last H_t rounds (i.e., average over those rounds of (cooperators_in_round)/n).

A) If punish_timer > 0:
- Action: Defect.
- Decrement punish_timer by 1.
- Rationale: execute and finish the group-level punishment phase.

B) Else, check and update personal blacklist:
- For each other player j:
  - If in any of the last L rounds player j defected while at least ceil( n*θ_high ) players cooperated in that round (i.e., j defected when most players cooperated), mark j in personal_blacklist.
  - If a blacklisted player has cooperated in every round of the most recent L rounds (cooperation_rate_j in last L rounds = 1), remove j from blacklist (forgiveness on sustained good behaviour).

C) Targeted reaction to immediate last-round pattern
- If group_coop_last ≥ θ_high:
  - There is a clear pool of cooperators right now. Aggressive response: Defect this round (exploit).
  - After the round ends, if (in the next round) group cooperation falls by at least Δ_collapse compared to group_coop_recent (i.e., exploitation caused a sharp collapse), then set punish_timer = min(P, r−t) to forcibly punish the group for P rounds. (This is a brief hard punishment to deter reckless collapse and to allow us to re-gain a better future stream.)
  - Also mark as blacklisted any player who appears to exploit the situation by defecting in multiple of the last L rounds when group cooperation was high.
  - Rationale: exploit while cooperators exist, but be ready to deter collapse that would harm our long-run payoff.

D) If not (C), check targeted cooperation towards reliable players
- If reliable_fraction ≥ θ_high:
  - There is a majority of reliable cooperators in recent history. We will cooperate to sustain that profitable environment:
    - Action: Cooperate, except:
      - If any of the reliable players are on personal_blacklist (should not happen because blacklist implies unreliability, but check defensively), then Defect instead.
  - Rationale: If many players have proven reliable, cooperating now helps sustain a larger public good from which we can periodically extract (via exploitation steps).

E) If not (D), opportunistic probe to discover cooperators
- If (t − last_probe_round) ≥ S and not in personal_blacklist-related punishment and t ≤ r−2:
  - Action: Cooperate this round as a probe.
  - Set last_probe_round = t.
  - Rationale: occasionally cooperate to test whether others will respond with cooperation and to discover new reliable partners to exploit or partner with later.

F) Default fallback (when no other rule triggers)
- Action: Defect.

Additional details and clarifications
- Targeted blacklisting: blacklisting is per-player and allows targeted retaliation. Against a blacklisted player we defect regardless of other rules until they show sustained cooperation in the forgiveness window. This focuses punishment where it is most effective and reduces the collateral damage of group punishments.
- Group punishment mechanism: the brief punish_timer phase is a hard group-level punishment intended to deter cooperative players from letting exploitations repeatedly collapse cooperation (i.e., it signals that we will respond strongly to severe swings). The length P is calibrated to be significant but not so long that everyone moves to permanent all-defect (P = roughly r/6).
- Probing cadence S ensures we do not continually give free benefits; probes are rare but regular to detect exploitable cooperative tendencies.
- Exploitation behavior: whenever the immediate preceding round shows a clearly cooperative group (group_coop_last ≥ θ_high), we defect to extract the best short-run payoff. This is the main “aggressive” move.
- Robustness: Because we both exploit and punish, ABPP is robust across opponent types:
  - Against unconditional cooperators: repeated exploitation yields high payoff.
  - Against conditional cooperators (reciprocators): the probes and cooperation-when-reliable sustain cooperation and enable regular extractions.
  - Against persistent defectors: ABPP largely defects (protects itself) and targets blacklists to avoid wasting cooperation on them.
  - Against sophisticated opponents: the mix of exploitation, short punishment and forgiveness deters being milked and adapts to shifting group behaviour.
- Edge cases:
  - Small r (e.g., r=2 or r=3): H and P shrink. First round defect; last round defect; possibly one probe on t=2 if r≥3 and the cadence permits, but otherwise behavior collapses to defection because endgame dominates.
  - Small n (n=2): reduces to a 2-player repeated PD-like behavior. ABPP reduces to: defect first, probe occasionally, exploit when partner cooperates, punish partner if they defect frequently while you were cooperating — an aggressive bullying/tit-for-tat-flavored policy.
  - If everyone defects permanently, ABPP will defect too (minimizes losses).
  - If most players become reliably cooperative, ABPP cooperates to sustain a cooperative basin (so it can exploit punctually), rather than permanently destroying cooperation.

Pseudocode (concise)

Initialize:
  H = min(10, max(1, floor(r/3)))
  L = min(H, 5)
  θ_personal = 0.6; θ_high = 0.6; θ_low = 0.3
  P = max(1, ceil(r/6))
  S = max(3, ceil(r/6))
  punish_timer = 0
  personal_blacklist = {}
  last_probe_round = 0

For each round t = 1..r:
  if t == r: action = D; continue
  if t == 1: action = D; continue

  compute H_t = min(H, t-1)
  for each other player j:
    cooperation_rate_j = cooperations_by_j_in_last_H_t / H_t
    if cooperation_rate_j ≥ θ_personal: mark reliable_j

  reliable_fraction = #reliable_j / (n-1)
  group_coop_last = (# cooperators in round t-1) / n
  group_coop_recent = average fraction cooperators over last H_t rounds

  if punish_timer > 0:
    action = D
    punish_timer -= 1
    continue

  update personal_blacklist:
    for each j:
      if j defected in any of last L rounds when that round had ≥ ceil(n*θ_high) cooperators:
        add j to personal_blacklist
      if j in personal_blacklist and j cooperated in all of last L rounds:
        remove j from personal_blacklist

  if group_coop_last ≥ θ_high:
    action = D      # exploit the immediate pool
    last_exploit_round = t
    (after seeing next-round history: if group_coop drops by ≥ Δ_collapse relative to group_coop_recent set punish_timer = min(P, r-t))
    continue

  if reliable_fraction ≥ θ_high and no reliable players are blacklisted:
    action = C
    continue

  if (t - last_probe_round) ≥ S and t ≤ r-2:
    action = C
    last_probe_round = t
    continue

  action = D

Why this is “aggressive”
- It defects by default and immediately exploits clear pools of cooperators. It uses short but meaningful punishments to deter reckless collapses or freeloaders who defect when others largely cooperate. It probes so as not to miss opportunities to discover exploitable cooperators. The mix of exploitation, targeted punishment and occasional cooperation is designed to maximize the strategy’s cumulative payoff while deterring others from purely opportunistic behaviors that would undermine long-run gains.

Implementation notes
- All thresholds and windows are deterministic functions of n and r (and are independent of hidden variables). Tuning θ_personal, θ_high, P, H, S and Δ_collapse is possible; the values above are conservative defaults that balance aggression and long-run gain.
- The “after seeing next-round history” clause for detecting collapse means the implementation must compute whether the prior exploitation produced the large drop on the next step and then set punish_timer. This slightly delays the group punishment by one round (we exploit immediately, observe the response, then punish if collapse occurred).

Summary
ABPP is a predominantly defecting, probing and exploitative strategy that cooperates only when it detects a stable, reliable cooperative environment (so cooperation is a means to maintain a valuable long-run basin) and punishes sharply and selectively when players defect opportunistically. It is aggressive in exploitation, merciless towards repeat exploiters, but adaptive and forgiving enough to rebuild cooperation when that is strategically profitable.
'''

description_AGGRESSIVE_69 = '''
Name: Opportunistic Exploiter (OE)

High-level description (aggressive mindset)
- Default: defect. Defection is the safe, exploitative baseline because a unilateral contribution never increases your immediate payoff (k/n < 1). Use cooperation only as a tactical instrument: (1) probe to discover exploitable, forgiving opponents; (2) briefly “pardon” when widespread punishment would otherwise lock in low payoffs; (3) never give up an exploitative steady-state when safe to do so.
- Aggression is explicit: maximize my rounds’ payoffs by free-riding whenever others are reliably cooperating or when opponents will not punish; minimize the cost of retaliation by detecting punishers and using short, targeted conciliatory moves rather than long unconditional cooperation.

State and bookkeeping
- For each round t we observe the public vector of actions. From history we maintain:
  - coop_frac[t] = fraction of other players (excluding me) who cooperated in round t.
  - I_played[t] ∈ {C,D}.
- Derived quantities (computed each decision step):
  - L = min(5, t-1) — recent window length.
  - p_recent = average coop_frac over the most recent L rounds (or over available rounds if t-1 < L).
  - retaliation_rate = fraction of my past defections after which coop_frac dropped significantly in the next round. More precisely:
    - For each past round s where I_played[s] = D and s < last_round, compute delta_s = coop_frac[s+1] − coop_frac[s].
    - Count punish_cases = number of those s with delta_s <= −0.15 (i.e., at least 15 percentage points drop after my defection).
    - retaliation_rate = punish_cases / number_of_my_defections_seen (if no prior defections, set retaliation_rate = 0).
  - rounds_remaining = r − t + 1.

Tunable internal thresholds (deterministic functions of parameters)
- probe_interval = max(3, floor(r/6)) — how often we run a cooperation probe if safe.
- R_threshold = 0.25 — retaliation_rate above this means “significant punishers exist”.
- p_high = 0.65 — recent cooperation fraction considered “high”.
- p_low = 0.20 — recent cooperation fraction considered “low”.
- delta_punish = 0.15 — used in retaliation detection.
- pardon_length = min(3, max(1, floor(rounds_remaining/6))) — number of consecutive cooperative rounds to attempt reconciliation when we decide to pardon.

Deterministic decision rules (when to cooperate vs defect)
1. Last round (t == r):
   - Always defect. (Backward induction; last-round cooperation is exploitable and cannot be enforced.)

2. First round (t == 1):
   - Defect. (Aggressive opening: exploit any unconditional cooperators and collect information about how opponents react in subsequent rounds.)

3. General case (2 ≤ t < r):
   Evaluate the following in order; the first applicable rule determines action.

   A. Safety check: If p_recent is extremely low (p_recent ≤ p_low)
      - Action: Defect.
      - Rationale: There is no cooperation to exploit; cooperation is costly and unlikely to induce large-scale reciprocity.

   B. Punishment-avoidance: If retaliation_rate > R_threshold and rounds_remaining is small (rounds_remaining ≤ max(3, floor(r/10)))
      - Action: Cooperate for pardon_length consecutive rounds (including current), unless you observe other players immediately return to defect-heavy behavior—then revert to defection.
      - Rationale: When punishers exist and few rounds remain, it can be better to make a short conciliatory investment to avoid immediate low-payoff mutual defection in the remaining rounds. This is a tactical, short-term concession only.

   C. Exploit-when-safe: If retaliation_rate ≤ R_threshold
      - Action: Defect.
      - Rationale: Opponents do not reliably punish my defections; keep exploiting consistently.

   D. Probe-for-forgiving-cooperators: Otherwise (punishers exist, but not in a way that forces immediate pardon, and not extreme low cooperation)
      - If (t mod probe_interval == 0) and rounds_remaining > 1:
         - Action: Cooperate this round as a probe.
         - Rationale: Test whether a brief cooperation leads to increased cooperation by others (i.e., find out if there exist forgiving strategies that will allow me to later exploit stabilized cooperation).
      - Else:
         - Action: Defect.

Additional operational details (how to interpret results and adapt)
- Interpreting a probe:
  - After a cooperative probe at round s, observe coop_frac[s+1]. If coop_frac[s+1] increases (by at least delta_punish ) compared with coop_frac[s], classify that group as "responsive-forgiving". In that case leave retaliation_rate unchanged but consider reducing probe_interval (become slightly more willing to probe or tolerate brief cooperations) because there is opportunity to establish a cooperative pattern to later exploit by alternating strategies.
  - If coop_frac[s+1] drops sharply after my probe (others punish cooperators too), treat them as punishers: increase the effective retaliation_rate calculation (it will be reflected in future deltas).

- Returning after a pardon:
  - After performing the ordered pardon_length cooperations, immediately resume defect-by-default unless subsequent data (p_recent rising and retaliation_rate falling) indicates profitable, enduring cooperation patterns.

- Tie-breaker:
  - If two options look equal (rare given deterministic rules), choose defect (aggression prioritizes immediate safe payoff).

Edge cases and parameter scaling
- Very small r (e.g., r = 2 or 3):
  - The rules shrink probe_interval and pardon_length appropriately; final-round defection ensures no naive last-round cooperation.
  - With r = 2: Round 1 defect (default); Round 2 defect (last round).
- Very small n (n = 2):
  - coop_frac is just the other player's action; retaliation_rate directly measures whether the other punishes. Strategy reduces to aggressive defection with occasional probes/pardons as above.
- High k relative to n:
  - k enters only indirectly; since k/n < 1 always, immediate best reply is still to defect. However, when k is large (cooperation gives high group payoff), opponents may value reciprocity more; OE’s probes and pardon behavior are designed to discover such dynamics and exploit them if opponents are forgiving, or to avoid punishment if opponents retaliate.

Pseudocode summary (concise)
- Initialize history arrays.
- For t = 1..r:
  - If t == r: play D.
  - Else compute p_recent, retaliation_rate, rounds_remaining and other derived variables.
  - If t == 1: play D.
  - Else if p_recent ≤ p_low: play D.
  - Else if retaliation_rate > R_threshold and rounds_remaining ≤ max(3, floor(r/10)):
      - play C for up to pardon_length rounds (unless evidence shows no effect).
  - Else if retaliation_rate ≤ R_threshold:
      - play D.
  - Else: // punishers exist but not immediate-short-run case
      - If (t mod probe_interval == 0) and rounds_remaining > 1: play C (probe).
      - Else: play D.

Why this is robust and adaptive
- Robust to many opponent types: default defection exploits unconditional cooperators and punishes naive cooperative strategies. Probing lets OE discover forgiving strategies to exploit further. Retaliation detection protects OE from being locked into mutual defection by reactive punishers: when punishers are detected and few rounds remain, OE pays short low-cost conciliations rather than suffer cycles of mutual punishment.
- Adaptive: all decisions are based only on observed history and game parameters (n, r, k). No external assumptions about opponents’ norms or private agreements are needed.
- Aggressive: the strategy maximizes opportunistic free-riding, returns punishments with tactical concessions only when it increases expected remaining payoff, and refuses long-run unconditional cooperation.

Implementation notes for the tournament coder
- Implement coop_frac[t] excluding the agent’s own action.
- Use deterministic arithmetic for thresholds (no randomness), or if randomness is allowed, one can add low-probability randomized probes to avoid being exploited by perfectly predicting opponents; but the base strategy above is fully deterministic and tournament-ready.
- Tune internal thresholds (R_threshold, p_high, p_low, probe_interval) if the tournament environment has characteristics you can estimate a priori. The defaults above are conservative and intended to be broadly effective.
'''

description_AGGRESSIVE_70 = '''
Name: Aggressive Grim-Exploit (AGE)

Intuition (short)
- Start by cooperating to attract cooperative opponents.
- Opportunistically exploit rounds where every other player cooperated in the previous round (defect once to grab the one-shot advantage).
- If any other player ever defects (i.e., you observe any opponent not contributing), respond with a full, irrevocable punishment: defect for all remaining rounds (Grim trigger).  
- Always defect in the final round (no future to be punished).

This strategy is simple, aggressive, and fully specified from the game parameters and the observed history. It is adaptive (reacts to how others behave) and harsh (permanent punishment for any observed defection), and it opportunistically exploits unanimous cooperation.

Decision rules (natural language)
1. Initialization:
   - State variable Punished := false.
2. First round (t = 1):
   - Play C (cooperate).
3. For each subsequent round t = 2, ..., r:
   - If t == r (final round): play D (defect).
   - Else if Punished == true: play D (permanent punishment mode).
   - Else (not in punishment and not final round):
     - If in the previous round every other player cooperated (i.e., all opponents chose C in t-1): play D this round (opportunistic exploit).
     - Otherwise play C (cooperate).
4. After observing actions in each round:
   - If any opponent j (j ≠ you) played D in that round, set Punished := true (enter permanent punishment for all remaining rounds).

Pseudocode
- Inputs: n, r, k (strategy uses only n and r to set behavior; k does not change decision rules here)
- History: for each past round s < t we have the vector of actions (C/D) for all players

Initialize:
  Punished := false

For round t = 1 to r:
  if t == 1:
    action := C
  else if t == r:
    action := D
  else if Punished:
    action := D
  else:
    others_prev_coop := number of opponents who played C in round t-1
    if others_prev_coop == n - 1:
      action := D   # exploit unanimous cooperation last round
    else:
      action := C

  Play action

  Observe opponents' actions in this round:
    if (exists opponent j with action_j == D):
      Punished := true

Notes and rationale
- Why cooperate first? Cooperating at the start signals willingness to cooperate and lets AGE collect high payoffs from reciprocating cooperators. Starting with D is purely exploitative and risks losing the potential large cooperative surplus early.
- Why exploit unanimous cooperation? If all other players cooperated last round, you can temporarily defect to obtain the one-shot advantage versus cooperating (this is the “aggressive” exploitation). That gain is attractive versus continuing to cooperate when others are reliably cooperating.
- Why Grim trigger (permanent punishment) when you see any defection? In a public goods game, single-player defections reduce the group return but are hard to punish individually without coordination. A permanent defection by you ensures defectors are punished harshly: future rounds you withholding contribution lowers their future payoffs and reduces incentive to defect against you. It is the simplest, strongest deterrent and fits an aggressive mindset. Because everyone observes all actions, Grim is credible and simple to implement.
- Why always defect in the final round? Backward induction: no future rounds to be punished, so defecting in the last round extracts the one-shot advantage.
- Robustness: AGE balances exploitation (when it’s safe) with harsh deterrence (when someone defects). It adapts to observed opponent behavior and is parameter-free except for using r and n to compute unanimous cooperation. It does not rely on others sharing norms or on coordinated retaliations.
- Edge cases:
  - r = 2: Round 1: C; Round 2 (final): D. This extracts the final-round advantage while still signaling cooperation once.
  - If an opponent defects in the very first round, AGE sees it and switches to permanent defection immediately (it will defect in the remaining rounds).
  - If a noisy or infrequent defector appears, AGE still responds with permanent defection (no forgiveness). That is by design: the strategy is intentionally harsh to deter any defection.
  - If multiple opponents punish you after you opportunistically defect, AGE’s permanent punishment may be triggered (if you treat others' occasional retaliation as defection—note your own opportunistic D will be observed by others, but AGE sets Punished only when opponents defect, not when you defect). Your own opportunistic defection will often invite retaliation and thus may provoke mutual defection thereafter; that is an acceptable aggressive tradeoff.

Tradeoffs and behavior summary
- Strengths: Simple, easy to implement, aggressive deterrence (permanent punishment), opportunistic exploitation of highly cooperative groups, clear endgame behavior. Works well against strategies that try to build mutual cooperation without harsh enforcement and against naive cooperators.
- Weaknesses: No forgiveness — a single observed defection (even accidental) dooms cooperation for the rest of the game. That can yield low group payoff against strategies that make isolated mistakes. Also, exploiting unanimous cooperation risks provoking mass retaliation; AGE accepts that cost for the short-term gain.

Optional tunings (if implementer wants variants)
- Softer variant: replace permanent Punished with punishment length P proportional to remaining rounds (e.g., Punished for max(ceil(0.7 * remaining_rounds), 1) rounds) to allow eventual return to cooperation. That makes the strategy less brutally aggressive and more tolerant of occasional errors.
- More exploitative variant: defect whenever at least (n - 2) others cooperated previously (exploit even if one other defected), but that is riskier.

This specification suffices to implement AGE in a tournament: it only needs the game parameters (n, r) and the full action history to date.
'''

description_AGGRESSIVE_71 = '''
Name: Aggressive Conditional Exploiter (ACE)

High-level idea (aggressive mindset)
- Default to exploiting the one-shot incentive to defect unless there is credible, recent evidence that cooperating will produce sustained mutual cooperation that is worth “investing” in.
- Use an initial short test to see who will cooperate. If enough players show a reliable cooperative pattern, cooperate selectively to earn future group gains — but keep exploiting whenever possible and punish quickly and harshly when others defect.
- Punish defectors (targeted and/or group-wide) to make defection costly to them. Forgive only after clear, sustained re-cooperation.
- Always defect in the final round(s) (no credible future to enforce cooperation).

This strategy depends only on n, r, k and the full history of players’ actions (who played C/D each past round).

Tunable internal parameters (suggested defaults; can be adjusted for different tournament meta)
- w = min(5, max(1, floor(r/4))) — sliding-window length for estimating recent cooperation rates.
- init_grace = 1 — initial “test” round(s) where we cooperate to probe opponents.
- q_high = 0.75 — recent cooperation rate threshold for classifying a player as “reliably cooperative”.
- q_low = 0.20 — recent cooperation rate threshold for classifying a player as “reliable defector”.
- alpha = 0.70 — fraction of others cooperating in the immediately previous round required to trigger willingness to cooperate (when other conditions hold).
- punish_length = min(max(2, floor(r/6)), r) — rounds to punish a detected defector (can be made infinite for a “grim” variant).
- endgame = 1 — number of last-rounds to unconditionally defect (always defect in last round; optionally last few rounds).

Decision rules (concise)
- Round 1: Cooperate (init_grace = 1). Purpose: probe and establish an initial willingness to cooperate so conditional cooperators reveal themselves.
- Any round t where t > r - endgame: Defect unconditionally (final-stage defection).
- For t in {2 .. r - endgame}:
  1. Compute for every other player j their cooperation rate over the last min(w, t-1) rounds (call it rate_j).
  2. Classify players:
     - Reliable cooperators: rate_j ≥ q_high.
     - Reliable defectors: rate_j ≤ q_low.
     - Uncertain: otherwise.
  3. If any player is a reliable defector, initiate punishment against them:
     - Add them to PunishSet and defect for punish_length rounds (targeted punishment; but since action is global, punishing reduces your return too — use because aggressive).
     - If punish_length expires and that player’s recent cooperation rate rises above q_high, remove from PunishSet (forgive).
  4. If PunishSet is non-empty, defect.
  5. Else (no current punishments):
     - Let last_round_cooperators = number of players (including self if you cooperated then) who played C in round t-1.
     - If fraction last_round_cooperators among others ≥ alpha AND a majority (≥ ceil((n-1)/2)) of other players are classified as reliable cooperators, then cooperate (we accept short-term loss to sustain a profitable cooperative stream).
     - Otherwise defect.
  6. If a surprising mass defection occurs (e.g., in a single round the number of cooperators among others falls by more than 40 percentage points from previous round), switch to permanent defection for the remainder of the game (aggressive escalation). This is the “burn the village” response — harsh but deters unstable cooperative opponents.
- Randomization: To avoid deterministic exploitability by highly adaptive opponents, optionally flip action with tiny probability ε (e.g., ε = 0.02) when cooperating condition holds; but keep ε small since the strategy is explicitly aggressive.

Rationale behind thresholds and rules
- Cooperating in the first round is a low-cost probe that reveals conditional cooperators. If everyone defects in round 1, there is little reason to trust cooperation later.
- We cooperate only when both (a) recent individual behavior is reliably cooperative and (b) the immediate previous round shows high group cooperation. This avoids being a long-term soft target for unconditional defectors.
- Punishment is aggressive and quick (punish_length rounds). The goal is to impose a future cost on defectors to make cooperation an equilibrium for conditional cooperators. Because our action is global (C/D), punishment harms everyone — that’s an intended aggressive leverage to deter defectors.
- We always defect in the last round(s) because there is no future to enforce reciprocity (backward induction).
- The surprise-mass-defection switch to permanent defection is an escalation: it avoids extended tit-for-tat wars and exploits the fact that some opponents will not sustain cooperation when challenged. It also communicates a credible threat of harsh consequences for breaking widely established cooperation.

Edge cases and special handling
- Small number of rounds (r = 2 or 3):
  - If r = 2: round 1 can be used as a probe by cooperating once, but round 2 is the last round so defect. If you want maximum aggression, defect both rounds. Recommended: cooperate round 1 only if you expect many conditional cooperators; otherwise defect both rounds.
  - If r = 3: cooperate round 1 to test, then round 2 follows standard rule, round 3 defect.
- If k is very close to n (public good large): group returns are large and cooperation is more attractive. The strategy already adapts because reliable cooperation rates will be detected and alpha/q_high thresholds cause cooperation when a majority are reliable cooperators. If you want more cooperative play when k is high, lower alpha or q_high slightly.
- If many opponents are random/noisy: the sliding window w and q_high/q_low thresholds prevent overreacting to noise. Punishments only kick in for persistent defectors.
- If opponents target you specifically (punish-you strategies): the small randomization ε and limited punish_length reduce cycles of mutual destruction; you can choose longer punish_length if you want maximal aggressiveness.

Pseudocode

Inputs: n, r, k, history of actions H (H[t][j] ∈ {C,D} for rounds 1..t-1, players j=1..n)
Internal state: PunishSet (players currently being punished) with timers

Initialize:
  w = min(5, max(1, floor(r/4)))
  init_grace = 1
  q_high = 0.75
  q_low = 0.20
  alpha = 0.70
  punish_length = min(max(2, floor(r/6)), r)
  endgame = 1
  ε = 0.02  // optional tiny noise
  PunishSet = empty

Function action(t):
  if t == 1:
    return C  // probe

  if t > r - endgame:
    return D  // final-round defection

  // Update punish timers and remove expired
  for each player j in PunishSet:
    decrement PunishSet[j].timer
    if PunishSet[j].timer <= 0:
      remove j from PunishSet

  // Compute recent cooperation rates
  window = min(w, t-1)
  for each player j != me:
    rate_j = (# of C by j in rounds t-window .. t-1) / window

  // Identify reliable defectors/cooperators
  ReliableCooperators = { j | rate_j >= q_high }
  ReliableDefectors = { j | rate_j <= q_low }

  // Detect new persistent defectors and start punishment
  for j in ReliableDefectors:
    if j not in PunishSet:
      PunishSet[j] = { timer: punish_length }

  if PunishSet not empty:
    return D

  // Detect surprise mass-defection
  if t >= 3:
    coop_prev = number of others who played C in round t-1
    coop_prevprev = number of others who played C in round t-2
    if (coop_prevprev > 0) and ((coop_prevprev - coop_prev)/ (n-1) >= 0.40):
      // aggressive escalation to permanent defection
      return D for all remaining rounds

  // Decide to cooperate only if strong evidence of group cooperation
  fraction_cooperators_last = (number of others who played C in round t-1) / (n-1)
  if fraction_cooperators_last >= alpha and |ReliableCooperators| >= ceil((n-1)/2):
    with probability (1 - ε): return C
    else: return D
  else:
    return D

Notes on implementer choices
- The above thresholds are suggestions; the core idea is the structure: probe → conditional cooperation given strong evidence → aggressive, possibly targeted punishment → permanent defection in last round(s).
- To be more aggressive, increase punish_length (or make it permanent/“grim”), increase the surprise-defection threshold to trigger permanent defection on smaller shocks, and lower init_grace to 0 (never cooperate first).
- To be more robust against noise, increase w and require longer recovery before dropping players from PunishSet.

Behavioral summary (what opponents will experience)
- If many opponents are conditional cooperators who reciprocate, ACE will cooperate after the probe and sustain mutual gains (but will sometimes defect if signals weaken).
- If opponents are unconditional defectors, ACE largely defects and punishes persistently, denying them benefit.
- If opponents try to exploit ACE by cooperating long then defecting late, ACE’s punishment escalation and endgame defection protect against long-run exploitation (and ACE will pick off the late-round exploitation by defecting near the end as well).

This strategy is aggressive: it prefers defection by default, uses cooperation strategically as an investment to gain a subsequent cooperative stream, punishes defectors quickly and harshly, and abandons cooperation permanently if the group shows sustained or large unexpected defections. It adapts to behavior observed in history (individual rates, last-round cooperation, and surprise deviations) and only requires the game parameters and history.
'''

description_AGGRESSIVE_72 = '''
Name: Aggressive Exploit‑and‑Grim (AEG)

Idea (short): Always defect by default and only join cooperation when (1) every other player just showed unanimous cooperation and (2) there are enough remaining rounds that sustained mutual cooperation is strictly more profitable than a one‑shot exploitation. If any cooperator deviates after you start cooperating, punish permanently (grim). Always defect in the final round.

Why this is aggressive:
- It probes/starts with defection (no trust given).
- It only cooperates to exploit very clear, strong signals of willingness to cooperate (unanimous cooperation by all others).
- When it does cooperate it does so only if future gains justify it; any deviation by others triggers permanent, harsh punishment (defection forever).
- It exploits one‑shot gains when safe (last round and most rounds), and never allows indefinite exploitation by others.

Decision rules (verbal)
1. Round 1: Defect.
2. Last round (round r): Defect.
3. At any round t (1 < t < r), compute:
   - others_prev = number of cooperators among the other n−1 players in round t−1
   - R_remain = number of rounds remaining including this one = r − t + 1
   - T_min = smallest integer T > 0 satisfying T*(k − 1) > 1 − k/n. Equivalently:
        T_min = ceil( (1 − k/n) / (k − 1) )
4. State variables:
   - coop_mode (boolean): true iff we have committed to a cooperative run
   - punished (boolean): true iff we have observed a deviation after cooperatively committing and now punish permanently
   Initialization: coop_mode = false, punished = false
5. Action selection:
   - If punished == true: play D.
   - Else if t == 1 or t == r: play D.
   - Else if coop_mode == true:
       - If others_prev == n − 1 (all other players cooperated last round): continue cooperating (play C).
       - Else (some other player defected after we entered coop_mode): set punished = true; play D from now on (punish permanently).
   - Else (not punished and not currently in coop_mode):
       - If others_prev == n − 1 (every other player cooperated last round) AND R_remain >= T_min:
           - Enter coop_mode = true and play C (begin the cooperative run).
       - Else:
           - Play D.

Pseudocode

Inputs: n, r, k
Derived: T_min = ceil( (1 - k/n) / (k - 1) )   // positive integer

State:
  coop_mode = false
  punished = false

For each round t = 1..r:
  if t == 1:
    action = D
  else if t == r:
    action = D
  else if punished:
    action = D
  else if coop_mode:
    if (num_cooperators_among_others_in_round[t-1] == n-1):
      action = C
    else:
      // Someone defected after we cooperated -> punish forever
      punished = true
      action = D
  else:
    // not in coop_mode and not punished
    if (num_cooperators_among_others_in_round[t-1] == n-1) and ((r - t + 1) >= T_min):
      coop_mode = true
      action = C
    else:
      action = D

Return action

Notes, justification and edge handling
- The T_min formula: cooperating for T rounds when others also cooperate yields T*(k − 1) extra payoff (per T rounds) versus mutual defection (1 per round). A single one‑shot defection by you while others cooperate gives you an immediate advantage of (1 − k/n). To make sustained cooperation worth more than a one‑shot exploitation, you need T*(k − 1) > (1 − k/n). So the strategy only initiates cooperation when there are at least T_min rounds left.
- Unanimous signal requirement (others_prev == n−1): This is strict but robust. Only when every other player just cooperated do we trust there is a stable cooperation opportunity worth joining. This protects against partial cooperators or strategies that try to trick you with noisy/coincidental cooperation.
- Punishment is permanent (grim): Aggressive and simple. Any betrayal after you commit triggers defection forever; this deters exploitation attempts and is optimal for an “aggressive” mindset in a setting with perfect monitoring and no noise.
- Last round: Always defect. In a finite horizon, defecting in the last round gives a one‑shot advantage and there are no future rounds to sustain cooperation, so an aggressive strategy never cooperates there.
- First round: Defect (probe). Starting with D avoids being exploited by strategies that always cooperate.
- Robustness: The strategy adapts to observed behavior. If a large, unanimous cooperative cohort forms and persists, AEG will join them only when it is strictly profitable (by the T_min test), extract the higher mutual payoffs thereafter, but immediately revert to permanent defection if betrayed. If no trustworthy unanimous coalition appears, AEG simply defects every round—safe and aggressive.

Variants / tuning
- Shorter or finite punishment: If you prefer not to punish permanently (to allow re‑entry), replace the permanent punished = true with a punishment counter of length L (e.g., L = floor(r/4)) and reset coop_mode only if you observe S consecutive unanimous cooperation. That is less aggressive but more forgiving.
- Less strict entry rule: require a large majority (e.g., >= 90%) rather than unanimous. Unanimous is simplest and safest; majority variants are riskier.
- If noise or mistakes are expected, add contrition logic (e.g., forgive after one round of unanimous cooperation) — but the specification assumes perfect observation and no noise, so the grim approach is appropriate for aggressiveness.

Example behavior (interpretation)
- Against persistent cooperators that always play C: AEG will wait until it observes unanimous cooperation, check remaining rounds R_remain ≥ T_min, then switch to cooperating to reap the high mutual payoff k per round (which is worse than defecting once but better long run). Because it will cooperate, it also gains the option to defect in the final round (it will defect there).
- Against mixed or retaliatory strategies: AEG will mostly defect; if it ever tries to cooperate and someone defects, it punishes forever—deterring repeat exploitation.
- Against always‑defect strategies: AEG will always defect (no worse than opponents).

This strategy is fully defined by the game parameters (n, r, k) and the public history of past actions, and it is aggressive, adaptive and robust across a wide range of opponent behaviors.
'''

description_AGGRESSIVE_73 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Short description
- AOE is an aggressive, history-aware rule: it defects by default, probes opponents with occasional/coordinated cooperation to discover exploitable cooperators, exploits when the group is reliably cooperative, and switches to harsh permanent punishment if it ever gets “suckered” (i.e., it cooperates but most others defect). It also respects backward induction near the end of the finite horizon (defect in the final rounds). Small randomized probing prevents lock-step exploitation by clever opponents.

Design goals satisfied
- Depends only on game parameters (n, r, k) and observed history.
- Adaptive: tracks recent cooperation rates and adjusts behavior (probe, build, exploit, punish).
- Robust: includes detection of exploitation, harsh punishment, and randomized probing to avoid being gamed.
- Aggressive: actively seeks to exploit cooperative groups and punishes others permanently upon being exploited.

High-level logic (natural language)
1. Default posture: defect every round unless there is a clear, repeatable opportunity to either (a) probe to find cooperators, (b) build a cooperative group that can later be exploited, or (c) exploit a reliably cooperative group.
2. Probing: small-probability cooperations early and occasionally thereafter to test whether opponents will sustain cooperation.
3. Exploitation: when recent history shows a very high fraction of others cooperating, defect to harvest high payoffs.
4. Building (opportunistic cooperation): if cooperation among others is moderate and there is time to both build trust and later exploit, occasionally cooperate to increase future exclusive gains.
5. Punishment (grim): if you ever cooperate and the round’s outcome shows you were heavily outnumbered by defectors (you got the “sucker” outcome), flip to permanent Defect for the rest of the game (harsh deterrent).
6. Endgame: in the last two rounds (t = r and t = r−1) always defect. (Backward induction.)

Parameters (recommendation + how they scale)
- Window W for estimating recent cooperation: W = min(5, r−1). (Short window to be responsive.)
- Probe probability p_probe (first round & occasional probes): p_probe = min(0.25, 2/(n)). Smaller for large groups.
- Build cooperation probability p_build when others moderately cooperate: p_build = 0.6 (can be lowered if k/n is large — see adaptation below).
- Exploit threshold tau_exploit: cooperate_rate ≥ 0.85 → treat group as reliably cooperative → Exploit (Defect).
- Build threshold tau_build: cooperate_rate ∈ [0.55, 0.85) → attempt to build / test cooperation (cooperate with p_build).
- Punish threshold: if you cooperated and # of cooperators including you ≤ ceil(n/2) (i.e., majority defected), trigger permanent PunishMode = True.
- Adaptation to k: compute delta = 1 − k/n (immediate private gain from defecting).
    - If delta is small (k/n near 1), sustain cooperation is more valuable. In that case lower tau_exploit by factor (1 − delta) and increase p_build.
    - If delta is large (k/n small), be more aggressive: raise tau_exploit and reduce p_build.

Decision rules — concrete pseudocode

Initialize:
- PunishMode = False
- History H = empty list of rounds; for each past round t we have vector of contributions per player
- W = min(5, r−1)
- delta = 1 − k/n
- p_probe = min(0.25, 2/n)
- tau_exploit = 0.85 * (1 − 0.5*max(0, (k/n) − 0.8))   // slightly reduce threshold if k/n large
- tau_build = 0.55

At each new round t (1..r):
1. If t ≥ r−1: return D (always defect in last two rounds).
2. If PunishMode == True: return D.
3. If t == 1:
   - With probability p_probe: return C (probe).
   - Else: return D.
4. Compute coop_rate = average fraction of other players who cooperated in the last min(W, t−1) rounds:
   - If there are no past rounds (t==1) coop_rate = 0.
   - Otherwise coop_rate = (sum over last L rounds of number_of_other_cooperators_in_that_round) / (L*(n−1)).
5. Decision by ranges:
   - If coop_rate ≥ tau_exploit:
       - // High-confidence exploitation: opponents reliably cooperate — defect to exploit
       - return D
   - Else if coop_rate ≥ tau_build:
       - // Moderate cooperation: try to build trust to later exploit
       - With probability p_build: return C
       - Else: return D
   - Else (coop_rate < tau_build):
       - // Low cooperation: do not cooperate
       - With small probability p_probe (sporadic re-check): return C
       - Else: return D
6. After the round outcome is observed and appended to H:
   - If the action you actually played this round was C and total_cooperators_in_round ≤ floor((n−1)/2) (i.e., you cooperated but a strict majority of others defected this round):
       - Set PunishMode = True (permanent defect thereafter).
   - Optional softer rule: if you cooperated and total_cooperators_in_round is unusually low compared to coop_rate expectation (e.g., drop by > 0.3), set PunishMode = True.

Rationale / explanation of each element
- Probe small and early: needed to detect exploitable cooperative opponents quickly; randomization prevents becoming predictable to opponents that might try to manipulate you.
- Exploit at very high cooperation rates: when most others cooperate, defecting yields a substantially higher payoff than cooperating (immediate gain delta). Aggressive strategy should exploit that.
- Build only when cooperation is moderate and there's time: cooperate sometimes to try to raise future cooperation, but remain cautious — only attempt building when you can later exploit it (i.e., not close to the end).
- Grim punishment when suckered: permanent defection is an aggressive deterrent to prevent being repeatedly exploited by strategies that try to entice you into cooperating while defecting themselves.
- Endgame defection: finite horizon equilibrium logic — avoid being exploited in the last rounds.

Edge cases handled explicitly
- First round: small prob probe to learn; otherwise defecting default prevents immediate exploitation.
- Last two rounds: always defect (t = r and t = r−1) to avoid backward induction exploitation.
- If history is too short to compute coop_rate, behavior uses default probe probability or defects.
- Persistent minority cooperators: exploitation rule (defect when coop_rate high) ensures you reap benefit; if exploited by majority defectors in any probe, punish permanently.
- Very small or very large groups: p_probe scales with n; tau_exploit slightly adapts with k/n to be less aggressive when k is close to n (cooperation relatively more valuable).
- Random seed tie-breaking: use randomness for probes and builds to avoid cycles.

Practical tuning suggestions
- More aggressive play: increase tau_build and tau_exploit, lower p_build/p_probe.
- More cooperative/long-term play (if k/n large and group cooperation yields big returns): lower tau_exploit, increase p_build, and consider a shorter or reversible punishment (instead of permanent) to re-enter cooperation if others change.
- PunishMode can be softened to a long but finite punishment (e.g., punish for P rounds) rather than permanent if you want to attempt recovery; but pure aggression recommends permanent punishment.

Why this is “aggressive”
- AOE actively seeks to exploit cooperative groups (defect when most others cooperate), uses harsh permanent punishment when it detects being suckered, and defaults to defection so as not to be a soft target. It only cooperates when it sees a clear path to a profitable manipulation (probe/build/exploit cycles), not out of blind reciprocity.

Example behaviors
- Against an always-cooperator: after a probe or build, AOE will detect high coop_rate and then defect every round → large gain every exploited round.
- Against an always-defector: AOE will mostly defect and stay neutral (no loss).
- Against conditional cooperators: AOE will try to build/test; if reciprocated it will exploit; if reciprocated but then retaliated, AOE will enter PunishMode and lock to defection.

Implementation notes for programmers
- Keep the window W short (responsive) and update coop_rate each round.
- Use random draws for p_probe and p_build decisions; seed RNG to avoid deterministic exploitation by opponents that observe you across tournaments.
- Make PunishMode persistent and check it at the start of each round.
- Adapt tau_exploit/p_build with k/n for better results across wide parameter regimes.

Summary
AOE: defect-by-default, probe occasionally, build cautiously, exploit decisively when many others cooperate, punish permanently if you’ve been suckered, and defect near the end. This combination is aggressive, adaptive, and robust in tournaments of independent strategies.
'''

description_AGGRESSIVE_74 = '''
Name: Aggressive Conditional Cooperator (AggroCC)

Intuition (one-line): Start by probing for partners, cooperate when a clear cooperative norm exists, exploit stable cooperation periodically, and respond to any betrayal with fast, strong, but sometimes-forgiving mass punishment that is calibrated to the severity of the betrayal and the remaining horizon.

Design goals satisfied:
- Adaptive: reacts to recent aggregate cooperation rates and to size/severity of defections.
- Robust: never relies on secret coordination; decisions depend only on public history and game parameters (n, r, k).
- Aggressive: will exploit stable cooperators periodically, punish betrayals decisively, and (when necessary) escalate to long punishments to make threats credible.

Parameters (derived from game parameters and fixed defaults):
- memory m = 2 (use last up to 2 rounds to estimate recent cooperation).
- coop_threshold theta = 0.66 (if recent cooperation fraction ≥ theta, treat group as mainly cooperative).
- punish_fraction alpha = 0.25 (base fraction of total rounds used as punishment length when punishment is proportional).
- punish_min = 1 (minimum punishment length).
- exploit_interval E = 4 (if there has been stable full-cooperation, exploit once every E rounds).
- short_game_cutoff R_short = 4 (if r ≤ R_short, the game is too short to sustain cooperation reliably; adopt always-defect).
All of these are determined before play from n, r, k and are constants during the match. They can be tuned but are fixed for the strategy.

State variables (maintained from history):
- punish_timer (integer ≥ 0): remaining rounds in Punish mode; when >0, always defect.
- consecutive_fullcoop (integer): how many consecutive past rounds had all players cooperating.
- last_exploit_round (integer): round index of last exploit (initially -∞).

High-level finite-state description:
- Normal/Seeking: cooperate when recent cooperation is strong; otherwise defect.
- Punish: defect for punish_timer rounds; allow early forgiveness only under strong signals.
- Exploit: when cooperation is very stable (full-cooperation for ≥ 2 consecutive rounds), take a deterministic one-round exploit periodically (every E rounds).

Edge cases:
- First round: If r ≤ R_short (short game), play D. Otherwise play C (probe for cooperation).
- Last round (t = r): always play D (defect).
- If a large/majority betrayal occurs, punishment escalates to a very long (possibly remaining-horizon) punishment.
- Forgiveness: punishment can be stopped early only after very clear repentance (two consecutive rounds of full-cooperation observed).

Detailed decision rules (natural-language + pseudocode-style):

At initialization:
- If r ≤ R_short: set mode AlwaysDefect (will play D every round).
- Else: punish_timer = 0; consecutive_fullcoop = 0; last_exploit_round = -10^9.

On each round t (1..r):
1) If r ≤ R_short:
     Play D (Always defect for short games).
     Continue.

2) If t == r (last round):
     Play D (always defect in final round).
     Continue.

3) Update basic statistics from history (rounds 1..t-1):
     - Let S_{t-1} = number of cooperators in round t-1 (0..n).
     - Let f_prev = S_{t-1} / n (fraction in last round).
     - Let f_recent = average fraction of cooperators over last up to m rounds (use available history if < m rounds).
     - If f_prev == 1 then consecutive_fullcoop += 1 else consecutive_fullcoop = 0.

4) If punish_timer > 0:
     - Play D.
     - punish_timer -= 1.
     - (Exception: if during punishment you see two consecutive rounds of full-cooperation, clear punish_timer = 0 and resume Normal next round — this is forgiveness but requires a strong signal.)
     - Continue.

5) Exploit check:
     - If consecutive_fullcoop >= 2 and (t - last_exploit_round) >= E:
         - Play D this round (exploit stable cooperators).
         - last_exploit_round = t.
         - consecutive_fullcoop = 0 (reset so we require another streak to exploit again).
         - Continue.

6) Normal decision:
     - If f_recent >= theta:
         - Play C (cooperate to sustain cooperation).
     - Else:
         - Play D.

7) Trigger punishment when betrayed:
     - After playing (or when evaluating history at end of round), if you cooperated in round t' (previous round) and S_{t'} < n (someone defected while you cooperated), then on the next round enter punish mode as follows:
         - Let d = number of defectors in that round (d = n - S_{t'}).
         - If d > n/2 (majority defected): set punish_timer = remaining_rounds - 1 (i.e., defect for the rest of game) — escalate to near-grim if betrayal was large.
         - Else (minority betrayal): set punish_length = max(punish_min, ceil(alpha * r * (d/n))) — i.e., punish proportional to betrayal severity and total horizon.
           Then set punish_timer = min(punish_length, remaining_rounds - 1).
     - Implementation detail: always apply this punishment immediately on the first post-betrayal move (so the first punitive round is the round after the betrayal is observed).

Notes and rationale for key choices:
- First-round probe: cooperating on round 1 (for sufficiently long games) allows discovery of conditional cooperators and opens the possibility of extracting higher payoffs across many rounds. Short games (r ≤ 4) typically cannot sustain reciprocity: we therefore defect to avoid being suckered.
- Last round defect: standard backward-induction insight; defecting in final round is strictly dominant, and being aggressive we will exploit anybody who cooperates there.
- Cooperation when recent cooperation ≥ theta: this is how we sustain beneficial mutual contributions when the group norm exists; theta prevents premature cooperation in noisy or exploitative environments.
- Strong, calibrated punishment: aggression manifests in quick and sometimes long punishment. If many players betray, escalate to near-permanent defection to make punishment credible. If only a few betrayed, punish for a number of rounds proportional to betrayal magnitude. These punishments are costly to us too, but are intended to deter future exploitation and to extract a better share from conditional cooperators.
- Periodic exploitation: when full-cooperation is stable, we periodically defect to harvest the short-term gain (we accept the risk of sparking punishment). We keep this deterministic (every E rounds) so the pattern is predictable and the strategy doesn't rely on randomness.
- Forgiveness rule: permit forgiveness only on strong signals (two consecutive full-coop rounds). This avoids being easily trolled back into cooperation by single-round noise.

Concrete pseudocode (compact):

Parameters:
- m = 2; theta = 0.66; alpha = 0.25; punish_min = 1; E = 4; R_short = 4

State:
- punish_timer = 0; consecutive_fullcoop = 0; last_exploit_round = -inf

Each round t:
1. If r ≤ R_short: action = D; return action
2. If t == r: action = D; return
3. Observe history up to t-1:
   S_prev = cooperators in round t-1 (if t>1 else undefined)
   f_prev = S_prev / n (if t>1)
   f_recent = average cooperation fraction over last up to m rounds
   If t>1 and f_prev == 1: consecutive_fullcoop +=1 else consecutive_fullcoop = 0
4. If punish_timer > 0:
     action = D
     punish_timer -= 1
     If (in last two rounds both were full-cooperation): punish_timer = 0
     return action
5. If consecutive_fullcoop >= 2 and (t - last_exploit_round) >= E:
     action = D
     last_exploit_round = t
     consecutive_fullcoop = 0
     return action
6. If f_recent >= theta:
     action = C
   else:
     action = D
7. After executing the action and observing results for round t (or at start of t+1):
   If (you cooperated in round t) and (S_t < n):
      d = n - S_t
      rem = r - t
      If d > n/2:
         punish_timer = rem (defect for rest)
      Else:
         punish_length = max(punish_min, ceil(alpha * r * (d/n)))
         punish_timer = min(punish_length, rem)
End.

Performance characteristics and tournament rationale:
- Against unconditional cooperators: AggroCC will cooperate, but will periodically defect to extract extra profit; that yields high exploitation returns.
- Against mild conditionals (e.g., Tit-for-Tat-like): initial cooperation invites mutual cooperation which AggroCC sustains until betrayed; punishment will deter many conditional types from defecting and thus generate long-run cooperative gains.
- Against pure defectors: AggroCC quickly defects and avoids being exploited.
- Against exploitative/oscillatory opponents: punishments and the threshold logic reduce susceptibility to chronic exploitation; punishments can be long when betrayals are large to maintain credibility.
- Robustness: only uses aggregated public observables; no need for communication or identity labels.

Final note: The strategy balances short-term exploitation (periodic defection when cooperation is stable) with credible, severity-scaled retaliation. It is aggressive: it extracts gains from cooperators and punishes betrayals decisively. It is adaptive: the severity of punishment scales with the severity of the betrayal and with the remaining horizon, and the agent will return to cooperation only upon strong evidence of repentance. Implementation only requires knowledge of n, r, and the full action history.
'''

description_AGGRESSIVE_75 = '''
Name: Aggressive Exploit-and-Grim (AEG)

Short description (mindset)
- Default is non-cooperation (defect). Be ready to exploit any rounds where many others cooperated. If you ever get exploited after cooperating, shift to permanent defection (grim) to stop being prey. Probe rarely and only when there is a realistic chance of setting up a cooperative clique you can later exploit. Never cooperate in the final round.

Intuition
- In a single round defection strictly dominates cooperation (because k < n). Long-run benefit of cooperating requires high, sustained reciprocity from others. AEG only offers cooperation when others have already shown near-universal, recent cooperation; otherwise it defects and exploits. If you cooperate and some players exploit you, you punish permanently (grim) — harsh but clear and aggressive.

Parameters used by the strategy (internal; chosen deterministically from n and r)
- M = min(10, max(1, r-1))  // memory window (how many past rounds to inspect)
- P_probe = max(3, floor(r/10))  // probe interval: at most once every P_probe rounds
- T_exploit = ceil(0.60*(n-1))  // exploit threshold: if at least this many other players cooperated last round, defect to exploit
- T_build = ceil(0.90*(n-1))    // build/cooperate threshold: require near-universal cooperation among others in recent history before you consider cooperating to establish cooperation
- P_last = r  // last round index for clarity

All thresholds are deterministic functions of n and r and require no communication or assumptions about opponents beyond observed history.

Definitions from history available each round t
- c_j,t ∈ {0,1}: action of player j in round t (1 = C, 0 = D)
- coop_count_last = Σ_{j≠me} c_j,t-1  // number of other players who cooperated last round (if t=1, undefined)
- coop_recent_avg = (Σ_{s=max(1,t-M)}^{t-1} Σ_{j≠me} c_j,s) / (M*(n-1))  // fraction of cooperation by others over last M rounds (0..1)
- I_cooperated_last = my action in last round (1 or 0)

State variable (maintained by the strategy)
- GRIM (boolean) — starts false. If GRIM becomes true, you defect for the rest of the game.

Decision rules (ordered; first applicable rule determines action)
1. Last-round rule (backward induction)
   - If t == r (final round): Play D (defect). (No future to enforce cooperation; be aggressive.)

2. Grim rule
   - If GRIM == true: Play D.

3. Immediate exploitation rule
   - If t > 1 and coop_count_last ≥ T_exploit: Play D.
     - Rationale: Many others just cooperated → one-shot exploitation yields a large payoff. Be aggressive and grab it.

4. Cautious build rule (attempt to form a cooperative clique very rarely)
   - If coop_recent_avg ≥ T_build/(n-1) AND (t mod P_probe) == 0:
       - Play C (cooperate this round) to try to sustain a near-universal cooperation pattern.
       - If you play C and any other player j plays D in that same round (i.e., you are exploited), set GRIM := true for all subsequent rounds.
     - Rationale: Only when almost everyone has shown very high cooperation do you concede 1 unit now to potentially create many future cooperative rounds. Even then, the probe is rare; exploitation triggers a permanent switch to defection.

5. Probe rule (detect possible reciprocators if no clear pattern)
   - If t > 1, coop_recent_avg ≥ 0.5 AND (t mod P_probe) == 0:
       - Play C once as a probe. If you are exploited in that probe (i.e., you play C and any j plays D in that round), set GRIM := true.
     - Rationale: A less expensive, less committed probe to find reciprocators if the population shows mixed behavior.

6. Default rule
   - Otherwise: Play D.

Edge cases and clarifications
- First round (t = 1): There is no history. Follow rules:
  - Since t != r and GRIM is false, rules 3–5 depend on past rounds and will not trigger. So you follow Default rule → Defect in round 1.
  - (Rationale: aggressive default, avoid being first cooperater unless near-universal cooperation is guaranteed.)

- If M > t-1, the recent averages use the available earlier rounds only (divide by the number of examined rounds times (n-1)). Implementation note: compute coop_recent_avg over the actual number of past rounds ≤ M.

- When you cooperate in a build or probe round, you must observe the same-round actions before updating GRIM. If any other player defects in that round while you cooperated, you consider yourself exploited and set GRIM := true.

- GRIM is permanent: once set, it is never reset. This is intentionally aggressive — it signals very strongly that exploiting you will eliminate future cooperative benefits.

- Determinism: All thresholds and intervals are deterministic functions of n and r. If you want to include randomized tie-breaking, you can, but the base version is deterministic.

Pseudocode (concise)

initialize GRIM := false
set M := min(10, max(1, r-1))
set P_probe := max(3, floor(r/10))
set T_exploit := ceil(0.60*(n-1))
set T_build := ceil(0.90*(n-1))

for each round t = 1..r:
  if t == r:
    action := D
    play action; continue
  if GRIM:
    action := D
    play action; continue

  if t > 1:
    coop_count_last := sum over j≠me of c_j,(t-1)
    coop_recent_total := sum over s=max(1,t-M)..(t-1) sum over j≠me of c_j,s
    observed_rounds := min(M, t-1)
    coop_recent_avg := coop_recent_total / (observed_rounds*(n-1))
  else:
    coop_count_last := 0
    coop_recent_avg := 0

  if t > 1 and coop_count_last >= T_exploit:
    action := D
    play action; continue

  if coop_recent_avg >= (T_build/(n-1)) and (t mod P_probe) == 0:
    action := C
    play action
    observe others' actions in this round
    if exists j≠me with c_j,t == 0:
      GRIM := true
    continue

  if coop_recent_avg >= 0.5 and (t mod P_probe) == 0:
    action := C
    play action
    observe others' actions in this round
    if exists j≠me with c_j,t == 0:
      GRIM := true
    continue

  action := D
  play action

Why this is aggressive and robust
- Aggressive: defects by default and seizes exploitation opportunities whenever a supermajority cooperated in the previous round. Punishment (GRIM) is permanent and immediate when you are exploited after extending cooperation — this is a severe deterrent.
- Robust: does not rely on coordination, shared norms, or handshakes. It uses only observed actions. It includes probing to detect exploitable cooperators or reciprocators, but probes are rare and conditional on evidence of cooperation, so you avoid being a frequent sucker.
- Adaptive: uses recent-history statistics (memory M) to adapt to opponents who change behavior; exploits temporary windows of high cooperation but withdraws forever if exploited.

Notes for implementers / tournament tuning
- The constants (0.60 and 0.90 thresholds, M=10, probe schedule) are conservative and can be tuned. Higher T_exploit makes the strategy more cautious about exploiting; lower T_build makes it easier to try to build cooperation. For a strictly more aggressive variant, lower T_exploit and make GRIM respond to any exploitation regardless of whether you cooperated; for a more forgiving variant, use finite-length punishment instead of permanent GRIM.
- This strategy is deterministic and transparent — easy to implement and reason about in a tournament of independent AIs.
'''

description_AGGRESSIVE_76 = '''
Name: Aggressive Conditional Blacklist (ACB)

Summary (one line)
- Aggressively defect by default, probe opponents briefly, only enter short, tightly guarded cooperation windows when there is clear evidence of reciprocating majorities, permanently blacklist clear exploiters, and always defect in the final round. The strategy is designed to exploit unconditional cooperators, deter freeloaders with harsh punishments, and be robust to many opponent types.

Notation / bookkeeping
- r: total rounds, t: current round (1..r)
- n: number of players
- history: all players’ actions in past rounds
- coop_count_j: number of times player j cooperated in the last W rounds (rolling window)
- W: history window size = min(5, max(1, r-1)) (tunable)
- blacklist: set of players currently banned (initially empty)
- coop_phase: boolean flag indicating we are currently attempting a cooperation-window
- coop_phase_age: number of consecutive cooperative rounds completed in current coop_phase
- L_max: maximum planned length of any cooperation window = max(1, floor((r)/4)) (tunable)
- R_forgive: number of consecutive cooperative rounds required for a blacklisted player to be removed = 3 (tunable)
- E_exploit: every E_exploit-th round inside a coop_phase the strategy defects once to extract value = 4 (tunable)
- majority_threshold M = ceil((n-1)/2) (a conservative majority among others)

Decision rules (exact)
1. Terminal rule
   - If t == r (last round): play D. (Always defect last round.)

2. Trivial short-game rule
   - If r == 1 or r == 2: always play D every round. (No trust-building possible; be aggressive.)

3. First round / initial probing
   - In round 1 play D. (Avoid being a sucker; act aggressively up front.)
   - For t = 2 and t ≤ min(3, r-1): continue defecting unless you observed a clearly cooperative majority in round 1 (see step 4). These early rounds are primarily probes.

4. Assess recent cooperativeness (applied each round t ≥ 2)
   - Let S = number of other players (excluding self) who cooperated in the immediately previous round (t-1).
   - Let S_nonblack = number of other players who cooperated in t-1 and are NOT in blacklist.
   - Compute whether a cooperation attempt is worth trying: condition C_attempt = (S_nonblack ≥ M) AND (no ongoing long blacklist majority blocking cooperation).
     - Intuition: only attempt cooperation if a conservative majority of non-blacklisted players cooperated last round.

5. Entering and running a cooperation window
   - If not coop_phase and C_attempt is true and (r - t + 1) ≥ 2 (at least one future round after the current one), then start a coop_phase:
     - Set coop_phase = true, coop_phase_age = 0, planned_length = min(L_max, r - t) (do not plan to include the last round).
   - While coop_phase == true and coop_phase_age < planned_length:
     - Decide action for current round:
       - If current round inside coop_phase is an exploitation slot: if (coop_phase_age + 1) % E_exploit == 0 then play D (one-shot exploit) else play C.
         - Exploitation slot: intentionally defect once in every E_exploit rounds inside coop_phase to extract extra payoff.
       - However, if S_nonblack < M (the previous round showed insufficient cooperation among non-blacklisted players), abort the coop_phase immediately: set coop_phase = false and play D this round.
     - After making the move, update coop_phase_age and history.
     - If any non-blacklisted player cooperated consistently and a blacklisted player defected during this coop_phase, handle blacklist updates (see step 6).
   - When coop_phase_age reaches planned_length or coop_phase aborted, set coop_phase = false and switch to default behavior (defect) for at least one round before potentially rebooting a new coop_phase later.

6. Blacklist (aggressive punishment)
   - If you played C in a round and any other player j played D in that same round, add j to blacklist immediately (they exploited you while you cooperated).
   - A blacklisted player remains blacklisted indefinitely until they produce at least R_forgive consecutive cooperative actions in rounds where the strategy itself did NOT cooperate only to test (i.e., the player must show sustained cooperation when not being coaxed into cooperating).
   - While any player is blacklisted, that player is treated as “untrustworthy” for the C_attempt calculation (they are only counted if they have met the forgiveness condition).
   - Blacklisting is public and permanent until forgiveness criterion met — it is an aggressive, credible threat to deter exploitation.

7. Default fallback
   - If not in coop_phase and C_attempt is false, play D.

8. Rebooting cooperation after failures
   - After any aborted/failed coop_phase, wait at least one full round of defecting and re-evaluate C_attempt in subsequent rounds before attempting a new coop_phase.

Edge cases / clarifications
- Last round: always defect (step 1). This makes threats boundedly credible but prevents being suckered on the final move.
- Very short matches (r ≤ 2): always defect, no cooperation windows — there is no horizon to reliably recoup sacrifices.
- If k is very close to 1: cooperation is nearly worthless. As a simple heuristic the strategy treats standard parameter ranges (1 < k < n) the same; implementers may add a preliminary check: if k ≤ 1.05 then always defect.
- If everyone else is always C (naive cooperators): the ACB will detect large S_nonblack and open coop_phase(s), but will exploit them occasionally via E_exploit slots and will still defect on the last round — yielding high personal payoff.
- If everyone is always D: ACB quickly settles to permanent defection (no wasted cooperation).
- If opponents try conditional reciprocity: cooperative groups can form temporarily if they meet the strict majority non-blacklisted test; if any defections happen during such windows, ACB punishes harshly via blacklist.

Tunable parameters (implementer options)
- W (history window for individual cooperativeness): default 5.
- M (majority threshold): default ceil((n-1)/2). Can be tuned more conservative (higher) or permissive (lower).
- L_max (max cooperation window length): default floor(r/4).
- E_exploit (exploit frequency): default 4 (defect on every 4th cooperative round inside a window).
- R_forgive (forgiveness streak): default 3.

Why this is “aggressive”
- Default is defection; cooperation is only attempted after clear evidence that a conservative majority will reciprocate.
- Blacklist is harsh and persistent: players who exploit a cooperation action are removed from trusted set until they show a sustained pattern of cooperation — credible, punishing deterrent.
- Exploitation slots (E_exploit) guarantee the strategy takes advantage of naive cooperators periodically even inside attempted cooperation windows.
- Final-round defection ensures no end-game vulnerability.
- The strategy is intentionally asymmetric: prioritizes maximizing own payoff and deterring others from exploiting it, rather than maximizing joint welfare.

Robustness
- Uses only observable history and parameters (n, r, k).
- Works against unconditional cooperators (exploits them), unconditional defectors (defaults to D and avoids losses), and conditional competitors (detects reciprocation and forms guarded cooperative windows).
- Adjustable parameters allow tuning the level of aggression vs. willingness to cooperate.

Pseudocode (condensed)

initialize blacklist = {}
coop_phase = false
coop_phase_age = 0
planned_length = 0

for t in 1..r:
  if t == r: action = D; continue
  if r <= 2: action = D; continue
  if t == 1: action = D; continue

  S = number of other players who played C in round t-1
  S_nonblack = number of those S who are not in blacklist
  M = ceil((n-1)/2)

  if coop_phase:
    if S_nonblack < M:
      coop_phase = false
      action = D
    else:
      if coop_phase_age >= planned_length:
        coop_phase = false
        action = D
      else:
        if ((coop_phase_age + 1) mod E_exploit) == 0:
          action = D  # exploitation slot
        else:
          action = C
        coop_phase_age += 1
  else:
    if S_nonblack >= M and (r - t + 1) >= 2:
      coop_phase = true
      planned_length = min(L_max, r - t)
      coop_phase_age = 0
      # Decide action this round as first coop round:
      if (1 mod E_exploit) == 0:
        action = D else action = C
      if action == C: coop_phase_age = 1
    else:
      action = D

  # After observing other players' actions this round:
  for each other player j:
    if my_action == C and j_action == D:
      blacklist.add(j)
    # forgiveness check:
    if j in blacklist:
      if j has cooperated in last R_forgive rounds (when not being incentivized artificially):
        blacklist.remove(j)

Return actions each round as decided.

Final notes
- This strategy is intentionally ruthless: it will sacrifice group payoff when that helps the agent, extract gains from naive cooperators, and impose strong, lasting punishments on exploiters. It is straightforward to implement from game parameters and history, and is robust against a wide range of opponent behaviours because it only cooperates when strong, local evidence of trustworthy reciprocation exists and it always protects itself in the final round.
'''

description_AGGRESSIVE_77 = '''
Strategy name: Probe-Exploit-Grim (PEG) — an aggressive, adaptive public‑goods player.

High-level idea
- Start by probing to learn who is willing to cooperate.
- When many opponents are reliably cooperative, exploit them (defect) to extract high one‑round payoff.
- If anybody defects against you (i.e., you cooperated but they didn’t), punish harshly and for a sustained period (defect for a preset punishment length).
- After punishment, give a single cooperating “test” round to see if cooperation resumes, then either re‑exploit or punish again.
- Always defect in the final rounds (endgame), to avoid being suckered by backward induction.
This produces an aggressive profile: it extracts gains from cooperators, severely punishes betrayals, and adapts to a variety of opponent types.

Derived parameters (computed from game parameters; implementer may tune)
- W = min(5, r − 1) — history window for recent behavior statistics.
- m = min(2, r − 1) — initial probe rounds to build a first sample.
- E = min(2, r − 1) — endgame length: defect for the last E rounds.
- τ_high = 0.75 — threshold group cooperation rate considered “mostly cooperative.”
- τ_low = 0.25 — threshold considered “mostly defecting.”
- P_base = max(1, ceil(r/10)) — base punishment length (aggressive: punish for ~10% of game length).
- small_forgive_prob ε = 0.02 (optional) — tiny chance to forgive mid‑punishment to prevent perpetual cycles; keep very small to remain aggressive.

Data maintained
- history of all players’ actions per round (including self).
- For each opponent j: s_j = cooperation rate over the last W rounds (fraction of those rounds where j played C).
- group_rate S = average of s_j across opponents = (1/(n−1)) Σ_j s_j.
- punishment_remaining (integer, initially 0).
- last_round_cooperators = number of players (including self) who cooperated last round.

Decision rules (precise)
1. First rounds (probing)
   - For t = 1..m: play C (cooperate). Purpose: get signal on opponents' tendencies.

2. Endgame
   - If t > r − E: play D (defect). (Unraveling: don’t be exploited in the endgame.)

3. Punishment handling
   - If punishment_remaining > 0:
     - Play D.
     - Decrement punishment_remaining by 1 at end of the round.
     - (Optionally: with tiny probability ε ignore punishment and play C this round to test forgiveness; set ε very small.)
   - Else if in the previous round you played C and one or more opponents j played D (i.e., you were betrayed immediately last round):
     - Set punishment_remaining := min(r − t + 1, P_base × number_of_betrayers)
       (aggressive: punishment scales with number of players who betrayed you).
     - Play D this round (start punishment now).
     - Decrement punishment_remaining by 1 at end of the round.

4. Exploitation trigger
   - Else (no active punishment and no immediate betrayal to respond to):
     - Compute for each opponent j their s_j over last W rounds, and S = (1/(n−1)) Σ_j s_j.
     - Compute last_round_others_cooperators = last_round_cooperators − (1 if you cooperated last round else 0).
     - If S >= τ_high AND last_round_others_cooperators ≥ ceil((n − 1) / 2):
       - Play D (Exploit: many opponents are reliably cooperative, so defect to capture the temporary high payoff).
       - After exploiting, if any opponents subsequently defect against you, punishment triggers on the next round per rule 3.
     - Else if S <= τ_low:
       - Play D (group is mostly defecting; do not waste contributions).
     - Else (mixed population):
       - Play majority‑matching: if last_round_cooperators ≥ ceil(n/2) play C, else play D.
         (This is a “tit‑for‑majority” style reciprocity to stay aligned with conditional cooperators while remaining aggressive.)

5. Post‑punishment forgiveness test
   - When punishment_remaining reaches 0 (punishment just finished), the next round you cooperate once (unless endgame), to see if cooperation resumes. If betrayed again, repeat step 3 with punishment scaling up.

Edge cases and clarifications
- Single betrayals vs mass betrayal: punishment scales with the number of opponents who betrayed you last round (so if only one opponent defected you punish but less harshly than if many defected).
- Multiple simultaneous betrayals: punishment length = min(remaining rounds, P_base × betray_count).
- If the game is very short (r small), all derived parameters clamp so you still probe, exploit, punish, and endgame behave reasonably.
- Determinism vs randomness: the core algorithm is deterministic. Optionally include a tiny forgiveness probability ε during punishment to reduce oscillatory cycles with other conditional strategies; keep ε ≪ 1 to preserve aggressiveness.
- Memory and observability: the strategy requires knowledge of who cooperated in past rounds (allowed by the spec).

Pseudocode (readable form)

Initialize:
  W ← min(5, r − 1)
  m ← min(2, r − 1)
  E ← min(2, r − 1)
  τ_high ← 0.75; τ_low ← 0.25
  P_base ← max(1, ceil(r / 10))
  punishment_remaining ← 0
  history ← empty

For each round t = 1..r:
  record opponents’ last actions in history (after first round)

  if t ≤ m:
    action ← C
    continue to next round

  if t > r − E:
    action ← D
    continue

  if punishment_remaining > 0:
    with probability ε (optional tiny):
      action ← C  // rare forgiveness probe
    else:
      action ← D
    punishment_remaining ← punishment_remaining − 1
    continue

  // detect immediate betrayal: last round you cooperated and some opponents defected
  if (you cooperated in round t−1) and (exists opponent j with action_j(t−1) == D):
    betray_count ← number of opponents j with action_j(t−1) == D
    punishment_remaining ← min(r − t + 1, P_base × betray_count)
    action ← D
    punishment_remaining ← punishment_remaining − 1
    continue

  // compute recent cooperation rates
  for each opponent j:
    s_j ← (# rounds in last W where j played C) / W
  S ← average_j s_j
  last_round_cooperators ← number of players (including self) who played C in round t−1
  last_round_others_cooperators ← last_round_cooperators − (1 if you cooperated in t−1 else 0)

  if S ≥ τ_high and last_round_others_cooperators ≥ ceil((n − 1) / 2):
    action ← D  // exploit
    continue

  if S ≤ τ_low:
    action ← D
    continue

  // mixed environment: match majority of last round
  if last_round_cooperators ≥ ceil(n / 2):
    action ← C
  else:
    action ← D

Rationale and properties
- Aggression: the strategy actively exploits reliably cooperative opponents (defects when group cooperation is high), and punishes betrayals with long, scaled punishments that deter opportunistic defectors.
- Adaptivity: it computes per‑opponent recent cooperation rates and a group average to switch between exploitation, cooperation, and punishment automatically.
- Robustness: against unconditional cooperators it extracts large one‑round payoffs; against unconditional defectors it refuses to cooperate and avoids losses; against conditional cooperators it maintains cooperation when beneficial but defects to take advantage of high cooperation rates; against retaliatory opponents it uses strong punishments to deter further exploitation.
- Finite horizon handling: defects in endgame rounds and uses finite punishment lengths limited by remaining rounds to avoid pointless infinite punishments.
- Simple to implement: only needs last W rounds of actions and simple counters; all rules depend only on history and game parameters.

Tuning notes
- Increase P_base for more severe punishment (more aggressive but may lower absolute payoff if many rounds remain).
- Increase τ_high to exploit only when cooperation is near-unanimous (safer) or decrease to exploit sooner (riskier).
- Reduce ε to near zero to avoid forgiveness; increase slightly to prevent lockstep punishment cycles.

This strategy balances high short‑term extraction with credible, harsh retaliation — a distinctly aggressive stance that adapts to opponents while being entirely history‑ and parameter‑dependent.
'''

description_AGGRESSIVE_78 = '''
Strategy name: Aggressive Adaptive Free-Rider (AAFR)

Concept (one line)
- Be aggressively selfish: free-ride whenever it is safe, probe briefly to learn whether the group collectively punishes free-riders, and only buy cooperation (rarely) to avoid durable group punishment. Default is defection; cooperate only when the empirical evidence says cooperating is the best way to avoid severe long-run payoff loss from group retaliation.

High-level rationale
- In a single round cooperating is strictly dominated (k/n < 1), so an aggressive strategy should by default defect and exploit cooperators.
- Repetition changes incentives only if other players condition their actions on your history. AAFR therefore:
  1. Probe minimally to learn whether the group punishes defectors.
  2. If the group does not punish, defect every round (maximize exploitation).
  3. If the group does punish, cooperate selectively to avoid long-term punishment only when the immediate group support is strong enough to make joining profitable relative to being punished.
- Always defect in the last round (no future to influence).

Parameters computed from game inputs (examples; implementer can tune constants)
- probe_len = max(1, min(4, floor(r/10)))  // a very small probing window
- lookback_w = min(5, r-1)                 // how many recent rounds to use to estimate reactions
- punish_drop = 0.20                       // a drop in others' cooperation ≥ 20% signals punishment
- punish_fraction_threshold = 0.5         // if >50% of probe events show punishment, treat group as punishing
- join_fraction = 0.8                      // if ≥80% of others cooperated last round, buy-in (cooperate) to avoid being singled out
- tie-breaker: always defect when in doubt

Decision rules (natural language then compact pseudocode)

Natural language
1. First round: defect (aggressive default + information baseline).
2. Probing phase (first probe_len rounds):
   - Continue to defect in the very first round.
   - Occasionally consider cooperating during the probe phase only if you want extra data; but AAFR recommends defecting for the probe too (consistent signal), because our primary goal is to detect whether the group punishes defectors when we defect. (Implementation may optionally perform one isolated cooperation in this phase to test whether others exploit us — but not required.)
3. After at least one observed defection by you, estimate whether the group punishes defectors:
   - For each recent round s where you defected and there exists s+1 in history, compute avg_coop_others(s) and avg_coop_others(s+1) (fraction of other players who cooperated).
   - If avg_coop_others(s+1) ≤ avg_coop_others(s) − punish_drop, count s as an evidence-of-punishment event.
   - Let p_punish = fraction of such events among the last lookback_w defect events (use fewer if not enough).
   - If p_punish > punish_fraction_threshold, classify the group as “punishing”; else classify as “non-punishing.”
4. Action selection (for round t, 1 < t < r):
   - If group classified as non-punishing → defect (exploit). There is no credible sustained cost from defecting, so free-ride every round.
   - If group classified as punishing → be selective:
     - If in previous round at least ceil((n−1) × join_fraction) other players cooperated, cooperate this round (join the strong crowd). Rationale: if a clear quasi-unanimity is present, the safest way to avoid being singled out and punished is to conform when cooperation is already overwhelming.
     - Otherwise defect (do not lead with cooperation). Retaliate/avoid giving unconditional gifts to a group that punishes defectors — keep exploiting when there is no clear crowd to buy into.
5. Last round (t = r): defect (no future to influence).
6. Forgiveness and reset:
   - If you have been punished for many consecutive rounds (e.g., your per-round payoff drops substantially relative to expectation for w rounds), allow a single round cooperation to attempt reset, then revert to the same decision rules. This keeps the strategy from being permanently exploited by a coordinated “tough” set if occasional buying of peace is cheaper.

Compact pseudocode

Inputs: n, r, k, history of rounds 1..t−1 (for each round s we know c_j,s for each player j)
Constants: probe_len, lookback_w, punish_drop, punish_fraction_threshold, join_fraction

function decide_action(t, history):
  if t == r: return D
  if t == 1: return D

  // Build list of rounds where *you* defected and there is a following round
  defect_rounds = [ s in 1..t-2 | c_me,s == 0 and (s+1) <= t-1 ]
  recent = last up to lookback_w entries of defect_rounds
  if recent is empty:
    // Not enough evidence; be aggressive: defect
    return D

  // Compute punishment evidence
  punish_count = 0
  for s in recent:
    avg_s = (sum_{j != me} c_j,s) / (n-1)
    avg_s1 = (sum_{j != me} c_j,s+1) / (n-1)
    if avg_s1 <= avg_s - punish_drop:
      punish_count += 1
  p_punish = punish_count / len(recent)

  // classify group
  if p_punish <= punish_fraction_threshold:
    return D   // non-punishing: exploit every round
  else:
    // group punishes defectors; act conditionally
    last_round_others = sum_{j != me} c_j,t-1
    if last_round_others >= ceil((n-1) * join_fraction):
      return C   // join the near-unanimous crowd to avoid being singled out
    else:
      return D   // otherwise keep defecting; buying cooperation is not worth it

Notes on implementation choices and tuning
- The constants punish_drop, punish_fraction_threshold, join_fraction, lookback_w and probe_len are chosen to make the strategy aggressively defect most of the time but able to avoid costly mutual punishment when a strong cooperative consensus exists. Those values can be tuned in tournament pretests.
- Forgiveness/reset rule: if you detect an abrupt persistent payoff drop (e.g., your rolling average payoff over lookback_w falls >10% below the payoff you would get by always defecting given observed average cooperation of others), allow one cooperation to attempt to re-enter cooperative equilibrium; then revert to core rules. This prevents permanent targeting by coordinated punishers.
- If r is very small (e.g., r ≤ 3) the strategy simplifies to defect every round (including probe), because there is little benefit in buying future cooperation.

Why this is “aggressive”
- Default is defection and exploitation of cooperators whenever no credible threat of punishment exists.
- Punishment by others is met by strong, simple defenses (mostly continued defection) and only a narrow, calculated willingness to “buy in” when a clear majority is cooperating — that is, we avoid expensive cooperation unless it is the only practical way to stop durable retaliation.
- The strategy minimizes gratuitous cooperation (gives away little), uses short probes to learn opponent behavior, and uses harsh but targeted responses to retaliation. It tolerates short-term losses to avoid structural exploitation, but otherwise maximizes immediate free-riding opportunities.

Edge cases summary
- First round: defect.
- Probing: minimal; you can optionally include one cooperative probe but conservative/aggressive default is to defect.
- Last round: defect always.
- Small r: default to defect all rounds.
- n=2 (pairwise case): AAFR reduces to defect-first, then defect unless the partner clearly never punishes; last round defect (maps to very aggressive PD behavior).
- If history is insufficient to detect punishment, prefer defection (safety for aggressor).

Behavioral examples
- Group never punishes defectors (they either always cooperate or never respond to your defection): AAFR defects every round and exploits cooperators.
- Group punishes defections strongly: AAFR will cooperate only when nearly everyone else cooperated in previous round; otherwise it defects and attempts to force a return to a state where exploitation is either safe or cooperation is rare (which is consistent with an aggressive objective).

This description is fully history-dependent and uses only game parameters and observable history; it makes no external assumptions about norms or communication. The pseudocode is straightforward to implement in a tournament agent.
'''

description_AGGRESSIVE_79 = '''
Strategy name: Aggressive Adaptive Exploiter (AAE)

High-level intent
- Be aggressively selfish: exploit naïve/cooperative opponents whenever it is safe to do so.
- Be adaptive: detect whether the population punishes defectors. If opponents collectively punish defections, avoid provoking them by behaving as a conditional cooperator (to preserve future gains).
- Be ruthless when punished: if the group appears to target you for punishment, switch to permanent exploitation (never give them future gains).
- Always defect in the known final round (and very near the end) because of standard endgame logic.

Key ideas used
- Defection is myopically dominant each stage. Repeated interaction only matters because opponents may retaliate/punish. So the central decision is whether my short-run gain from defecting is outweighed by future losses from being punished.
- The strategy attempts to learn whether opponents are "forgiving/naïve" (safe to exploit) or "retaliatory" (dangerous to exploit), using observed cooperation across rounds.
- If opponents punish, AAE behaves so as not to trigger costly punishments (conditional cooperation to maintain peace), but if they directly punish you, AAE becomes permanently exploitative (revenge/self-protection).

Inputs and internal state
- Parameters given by game: n, r, k
- History available at round t: for each prior round s < t, the observed total number of cooperators C_s
- Constants (tunable): L (lookback window), F_high (what counts as “high cooperation”), drop_delta (what counts as a “punishing drop”), P_threshold (punishment-sensitivity threshold), Majority_threshold, Forgiveness_period, Probe_prob. Default choices below.

Default constant choices (recommended starting values)
- L = min(6, r-1) (use up to last 6 rounds to estimate behavior)
- F_high = 0.65 (a round is “high-cooperation” if ≥ 65% cooperators)
- drop_delta = 0.25 (a subsequent drop of ≥25 percentage points is a punishment signal)
- P_threshold = 0.5 (if ≥50% of eligible high-coop episodes are followed by drops, we call the population retaliatory)
- Majority_threshold = 0.5 (we consider “majority cooperated” as >50%)
- Forgiveness_period = 3 (short punishment/transition timer if used)
- Probe_prob = min(0.05, 1/max(1,r/10)) (very small prob to probe when in exploit mode if no data)

Decision rules (precise, per round)
1) First-round and endgame:
   - If t == 1: play D (defect). Aggressive probe and immediate gain.
   - If t == r (final round): play D.
   - If remaining rounds H = r - t + 1 ≤ 2: play D (endgame logic: insufficient future to offset immediate cost).

2) Maintain basic statistics each round:
   - For s from max(2, t-L+1) to t: consider pairs (s-1,s). If round s-1 had fraction f_{s-1} = C_{s-1}/n ≥ F_high, and round s had fraction f_s ≤ f_{s-1} - drop_delta, count that as one punitive event.
   - Let Eligible = number of such (s-1,s) pairs considered where round s-1 had f_{s-1} ≥ F_high (if none, set Eligible = 1 to avoid division by zero and default to weak-punisher assumption).
   - PunishmentEvidence = (# punitive events) / Eligible. This estimates how often high-cooperation rounds are followed by sharp drops (collective punishment-like response).

3) Mode decision:
   - If PunishmentEvidence < P_threshold (population appears forgiving/weak-punishers):
       - Exploit Mode: play D every round (except the occasional small-probability probe defined below).
         * Rationale: opponents rarely punish, so always defect maximizes my payoff.
   - Else (PunishmentEvidence ≥ P_threshold; population appears retaliatory):
       - Conditional-Cooperation Mode: avoid provoking group punishments as long as they do not already punish you.
         - If in the immediately previous round (t-1) the cooperation fraction f_{t-1} > Majority_threshold, play C this round (reward majority cooperation to maintain stable cooperation).
         - Else play D (punish low-cooperation rounds).
       - Monitoring for direct targeting result: if at any point after I defected while the group had been highly cooperative (i.e., I defected in a round where f_{s-1} ≥ F_high), the next round shows a drop ≥ drop_delta in cooperation, interpret this as “group punished me.” On that detection, switch mode to Exploit-Forever (play D every remaining round).
         * Rationale: if the population punishes me personally for exploiting, they will cause me net loss—best aggressive response is to stop investing in them ever again.

4) Small probing when no evidence:
   - If the history is very short or Eligible was 0 (no evidence), and the strategy is in Exploit Mode, with tiny probability Probe_prob cooperate for one round to gather information about punishment tendencies. (Probe only early rounds and at small probability to avoid excessive loss.)
   - Use observed reaction to the probe to update PunishmentEvidence on subsequent rounds.

5) Forgiveness and temporary punish timers:
   - The strategy uses the simple permanent-switch rules above rather than extended tit-for-tat cycles. If you implement a punish timer instead of permanent switch, use Forgiveness_period to limit permanent burning of mutual gains. (Default aggressive stance: permanent switch to exploitation when directly punished.)

Pseudocode (compact)

Initialize: record C_s = observed cooperators for past rounds (empty)
For each round t = 1..r:
  H = r - t + 1
  if t == 1: action = D; continue
  if H <= 2: action = D; continue

  // compute PunishmentEvidence from last L pairs
  eligible = 0; punitive = 0
  for s from max(2, t-L+1) to t:
    f_prev = C_{s-1} / n
    f_curr = C_s / n   // available because s ≤ t (we use up through last completed round)
    if f_prev >= F_high:
      eligible += 1
      if f_curr <= f_prev - drop_delta:
        punitive += 1
  if eligible == 0: eligible = 1  // default weak-punisher prior
  PunishmentEvidence = punitive / eligible

  if PunishmentEvidence < P_threshold:
    // Exploit Mode
    with probability Probe_prob: action = C (probe)
    else action = D
  else:
    // Conditional-Coop Mode (avoid provoking punishers)
    if just_detected_group_punished_me_recently(): 
      // i.e., I defected in round s where f_{s-1} >= F_high and f_s <= f_{s-1} - drop_delta
      switch to Exploit-Forever (play D for all remaining rounds)
      action = D
    else:
      if C_{t-1} / n > Majority_threshold: action = C
      else action = D

Edge cases and clarifications
- Observability: the strategy assumes it can observe total cooperators each prior round (game spec allows this).
- If there is no meaningful history (early rounds), the strategy uses the conservative default that punishment evidence is low; but the first round is D by design and provides an initial probe sample.
- Tuning: the numeric thresholds above are tunable; in early testing you can increase sensitivity (drop_delta lower) to detect smaller punishments, or decrease Probe_prob to be more exploitative.
- Last rounds: the strategy defects in the final round and also in the last two rounds to avoid endgame exploitation by others and because there is not enough future to recoup immediate cooperation cost.
- Multi-player ambiguity: because punishment in n-player settings is distributed, AAE infers punishment when group-level cooperation drops sharply following high-cooperation rounds; that is robust to many opponent types.

Why this is aggressive and robust
- It opens defecting (immediate exploitation) and defaults to all-defect when opponents are forgiving — maximizes short-term gain.
- It protects against costly group retaliation: when retaliation is detected it refuses to repeatedly provoke and instead adopts conditional cooperation to preserve future gains.
- If the group actively punishes you for exploiting, AAE switches to permanent exploitation (revenge/self-protection). That is an aggressive, risk-averse stance: do not subsidize punishers.
- The algorithm is simple, parameter-based and only depends on public history and game parameters; it does not rely on any coordination or shared norms.

Implementation notes
- Tune L up if r is large (use more history); tune Probe_prob low to limit exploitation losses.
- If you prefer a slightly less brutal variant, replace the permanent “Exploit-Forever” switch with a long but finite punishment/forgiveness timer (Forgiveness_period) so you can re-attempt cooperation later if environment changes.

Summary (short)
- Round 1: defect (probe).
- If population appears not to punish defections (learned from last few rounds): always defect (exploit).
- If population appears to punish defections: behave as a conditional cooperator (cooperate when majority cooperated last round, otherwise defect) to avoid triggering punishment.
- If you detect the group has specifically punished you after you exploited them, switch to permanent exploitation (never cooperate again).
- Always defect in the final rounds (endgame).
'''

description_AGGRESSIVE_80 = '''
Name: Predatory Conditional Exploiter (PCE)

Idea in one sentence
- Default: defect. Learn who forgives or punishes your defections. If a large fraction of the group consistently forgives your defections, repeatedly exploit them (defect) until they punish; if a large fraction punish your defections, avoid provoking them by behaving conditionally to minimize losses. Always defect in the final round.

High-level behavior
- Aggressive: you prefer defection and free-riding; you only cooperate to (a) probe whether the group will tolerate exploitation, (b) re-establish a temporary cooperative window when necessary to avoid sustained punishment, or (c) when there is very strong, stable evidence that cooperating now will reliably generate a long sequence of rewarded cooperation that more than offsets the immediate cost.
- Adaptive: maintain short-term statistics of each opponent’s responsiveness to your past moves and the group’s recent cooperation rate; switch between Exploit, Punish and Stabilize behaviors accordingly.
- Robust: does not rely on any shared norms or precommitments; works for any n, r, k, and for a wide variety of opponent policies.

Parameters derived from game and history (suggested defaults; tunable)
- L = lookback window for statistics = max(1, min(6, floor(r/5))) — number of most recent rounds used to estimate behavior.
- θ_forgive = 0.75 — threshold for judging a player “forgiving” (i.e., they often cooperate even after you defect).
- θ_retaliate = 0.6 — threshold for judging a player a “retaliator” (i.e., they often defect after you defect).
- τ_exploit = 0.6 — fraction of players that must be forgiving to consider mass exploitation.
- τ_punish = 0.5 — fraction of players that must be retaliators to trigger cautious behavior.
- probe_interval = max(3, ceil(r/10)) — deterministic interval to occasionally probe.
- punish_length = min(L, max(1, ceil((r)/10))) — number of rounds you punish once you detect retaliation harming you.
- safety_margin_rounds = 1 — near the end you do not try risky cooperation (always defect in last round).

Main states
- DEFAULT (normal): default is to defect. Gather statistics.
- EXPLOIT: actively defect to reap benefits from forgiving cooperators.
- PUNISH: sustained defection to punish the group when they retaliate or when exploited.
- STABILIZE: cooperate sequentially for a short run to rebuild cooperation when a large fraction of the group are retaliators and mutual cooperation would avoid larger losses.

Edge-case rules
- First round (t = 1): defect (aggressive probe).
- Last round (t = r): defect (no future to incentivize cooperation).
- If r is very small (e.g., r ≤ 2): defect every round.
- If L > available history (early rounds): compute stats over available rounds.

Decision rules (concise natural language)
1. Always defect in round r (last round). Default overall action is D.
2. Maintain per-opponent statistics over the last L rounds:
   - p_j = fraction of times j played C in last L rounds.
   - f_j = fraction of times j cooperated given you defected previous round (forgiveness measure).
   - q_j = fraction of times j defected given you defected previous round (retaliation measure).
   - Compute group measures:
     - Fg = fraction of opponents with f_j ≥ θ_forgive (group forgiving rate).
     - Rg = fraction of opponents with q_j ≥ θ_retaliate (group retaliation rate).
     - Pg = mean p_j (group cooperation rate in window).
3. State transitions and actions:
   - If currently in PUNISH (countdown > 0): play D each round, decrement countdown. Exit PUNISH early if Pg increases substantially (the group restored cooperation).
   - Else if Rg ≥ τ_punish (many punishers): enter STABILIZE if remaining rounds m = r - t + 1 is large enough (e.g., m ≥ 3*L). In STABILIZE: cooperate for S = min(L, ceil(m/4)) rounds to try to reestablish cooperation; if mutual cooperation improves, selectively defect to exploit later; if not, revert to DEFAULT D.
   - Else if Fg ≥ τ_exploit (significant forgiving mass) and you are not in STABILIZE: enter EXPLOIT. In EXPLOIT you defect every round (D) to harvest the benefit of others cooperating; continuously monitor if Pg or Fg drops sharply — if so, enter PUNISH for punish_length rounds and then reassess.
   - Else (DEFAULT): play D. Occasionally probe for forgiveness: every probe_interval rounds (deterministic), play C for one round to test whether a sufficient fraction of opponents cooperate despite your defection. Use probe outcome to update stats (if many cooperate while you defect, that supports later EXPLOIT).
   - Exception for cooperation to stabilize profitable cooperation: only play C outside probes if both (a) Pg is very high (e.g., ≥ 0.9) and (b) remaining rounds m is large enough that the expected benefit of several rounds of maintained cooperation could plausibly offset each one-time cooperative cost. Practically: require m ≥ 3*L and Pg ≥ 0.9; then cooperate to try to sustain cooperation for a block — but be ready to exploit if they remain forgiving.

Why these rules are aggressive
- Default D and last-round D guarantee maximal short-term selfishness.
- EXPLOIT mode intentionally free-rides on forgiving opponents until they punish.
- PUNISH mode is harsh and immediate — you respond to retaliation or heavy exploitation by denying future benefits.
- Probing and quick exploitation allow you to harvest cooperative opponents rather than naïvely reciprocate.

Pseudocode (readable, implementable)
Inputs: n, r, k, history of past rounds (for each round, actions of all players)
Derived: L, thresholds as above
State variables: mode ∈ {DEFAULT, EXPLOIT, PUNISH, STABILIZE}, punish_countdown, stabilize_countdown, last_probe_round

For each round t (1..r):
  if t == r:
    play D
    continue

  compute m = r - t + 1 (rounds remaining)
  compute stats over last min(L, t-1) rounds (if t==1, stats empty):
    for each opponent j:
      p_j = fraction of j's C
      f_j = P(j plays C | you played D in previous round)  [estimated from data]
      q_j = P(j plays D | you played D in previous round)
    Fg = fraction of opponents with f_j ≥ θ_forgive
    Rg = fraction with q_j ≥ θ_retaliate
    Pg = mean p_j

  if punish_countdown > 0:
    play D
    punish_countdown -= 1
    if Pg > previous Pg + small_improvement_threshold: punish_countdown = 0
    continue

  if mode == EXPLOIT:
    if Rg >= τ_punish or Pg has fallen by a preset drop threshold since entering EXPLOIT:
      mode = PUNISH
      punish_countdown = punish_length
      play D
      continue
    else:
      play D
      continue

  if Rg >= τ_punish and m >= 3*L:
    mode = STABILIZE
    stabilize_countdown = min(L, ceil(m/4))
    play C  // begin stabilization campaign
    continue

  if Fg >= τ_exploit and m > safety_margin_rounds:
    mode = EXPLOIT
    play D
    continue

  // DEFAULT behavior: defect but probe occasionally
  if (t - last_probe_round) >= probe_interval and t < r - safety_margin_rounds:
    last_probe_round = t
    play C   // probe
    continue
  else:
    play D

Notes on tuning and rationale
- L should be small and scale with r so statistics react quickly. The defaults bias toward short memory so you can exploit forgiving opponents quickly and recover from punishment quickly.
- τ_exploit and thresholds are set to require a substantial forgiving majority before mass exploitation; this avoids jumping to exploit when only a few are forgiving and many will punish.
- Probing is deterministic and rare; it reveals whether opponents are forgiving without much cost.
- PUNISH is deliberately harsh but finite — that discourages coordinated punishment cycles from costing you more than you inflict.
- STABILIZE is used only when many punishers exist and continued defection would cause you to get low payoffs; cooperating briefly can re-establish cooperation and avoid long mutual-defection losses. Because cooperation is individually costly each round, STABILIZE only triggers when remaining horizon makes it plausible.

Why this performs well against a wide range of opponents
- Against unconditional cooperators: PCE will exploit them quickly (high payoff).
- Against unconditional defectors: PCE defects and gets the same payoff as others (no loss).
- Against Tit-for-Tat style reciprocators: PCE probes and then either exploits one-shot or else STABILIZE/avoid depending on whether reciprocators punish. TFT-like punishers will often cause PCE to avoid long exploit cycles, minimizing losses while capitalizing on short run opportunities.
- Against forgiving strategies (generous TFT, always-forgive): PCE exploits repeatedly.
- Against mixed populations: PCE learns group composition and chooses the profitable posture: exploit forgiving majority, stabilize for punisher majority, or punish defection waves.

Implementation checklist for the tournament
- Maintain per-player rolling counts for p_j, f_j, q_j over last L rounds.
- Enforce last-round D.
- Tunable thresholds can be adjusted by tournament runner; defaults given are aggressive but robust.

Summary (short)
- Default aggressive action: defect every round, with deterministic occasional probes in early/mid game.
- If many opponents forgive your defections → switch to EXPLOIT: defect every round to harvest cooperative contributions.
- If many opponents retaliate → enter STABILIZE (cooperate a short block to avoid long mutual-defection losses) or else PUNISH (sustained, finite defection when targeted).
- Always defect in the final round.
'''

description_AGGRESSIVE_81 = '''
Name: Aggressive Opportunistic Defector with Triggered Cooperation (AOD‑TC)

High level idea (aggressive mindset)
- Default is to defect and take the safe private payoff. Exploit any stable/coordinated cooperation among opponents by free‑riding. 
- Occasionally and briefly probe/cooperate only when there is a credible sign that cooperating could tip opponents into a stable higher‑cooperation regime that I can then repeatedly exploit. 
- If my cooperation is exploited (I cooperate and others do not reciprocate), respond with long, harsh punishment (practically permanent defection) to deter further exploitation. 
- Always defect in the final round (no future to enforce cooperation).

This strategy depends only on n, r, k and the full history of actions (who cooperated each past round). It is deterministic and simple to implement.

Parameters (derived from r)
- window w = min(6, r−1) — lookback window for recent behavior
- exploit_threshold θ_exploit = 0.60 — if ≥60% of opponents cooperated recently, exploit them by defecting
- abandon_threshold θ_abandon = 0.20 — if ≤20% of opponents cooperated recently, abandon cooperation (defect)
- trend_delta δ = 0.05 — require at least 5 percentage point upward trend to consider a cooperation trial
- trial_length S = 2 rounds — short cooperation trial to try to bootstrap cooperation
- punishment mode = Grim: once triggered, defect for the remainder of the game
- endgame: always defect on round r (and treat final round specially)

All thresholds above are fixed functions of r/n/k only by choosing w as a function of r; they can be tuned but these defaults are robust across parameter ranges (1 < k < n, n ≥ 2, r > 1).

Definitions used in rules
- For a past round t', let coop_others(t') = number of other players (excluding me) who played C that round.
- For a block of rounds B, define avg_coop_others(B) = average over t' ∈ B of coop_others(t')/(n−1), i.e., the fraction of other players cooperating on average over B.
- Recent block R = the last w rounds (or all past rounds if fewer than w exist).
- Earlier block E = the w rounds immediately before R (if available); if not enough history, treat trend as 0.

State variables maintained
- Punish = false/true. If Punish is true, defect until game end.
- Trial_remaining = 0 (counts remaining rounds of a cooperation trial I am executing).

Decision rules (ordered; first applicable rule applies)
1. If t == r (last round): play D (defect). Rationale: no future enforcement.

2. If Punish == true: play D (defect).

3. If Trial_remaining > 0:
   - Play C (cooperate). Decrement Trial_remaining by 1 after the round.
   - After the next round’s outcomes are observed, evaluate whether the trial was exploited (see step 7 below) and possibly set Punish.

4. Compute avg_recent = avg_coop_others(R) (fraction of opponents cooperating over the recent window).
   - If avg_recent ≥ θ_exploit: play D (exploit). Rationale: many opponents cooperating consistently — immediate exploitation is highest payoff.
   - Else if avg_recent ≤ θ_abandon: play D (defect). Rationale: cooperation is rare — no hope for profitable long-term cooperation.
   - Else (avg_recent between thresholds): compute trend.
     - If available earlier block E, trend = avg_recent − avg_coop_others(E). If not enough history, set trend = 0.
     - If trend ≥ δ and Punish == false: start a short cooperation trial: set Trial_remaining = S and play C this round (i.e., begin the S-round trial).
     - Otherwise: play D.

5. End routine for the round (wait for next round to update history and possibly trigger punishment).

Punishment evaluation after a cooperation trial
- If I executed a cooperation trial (one or more rounds where I played C consecutively) and, immediately after the trial, the opponents’ cooperation fraction in the round(s) when I cooperated was strictly lower than the recent baseline (specifically, if in the round immediately following the first trial round the fraction of other players cooperating dropped below avg_recent_before_trial − 0.05, or simply below 50%), then treat the trial as exploited and set Punish = true (grim trigger).
- Practically: if I cooperated and a majority of opponents did NOT cooperate back (i.e., coop_others in that round ≤ floor((n−1)/2)), consider it exploitation and enter Punish = true.

Special cases / edge cases
- r = 2 (very short): window w = 1, trial S = 2 would overflow; but rules still apply: round1 → default D (since avg_recent undefined → trend 0 → defect). round2 last round → D. So always defect when r is tiny; this is aggressive and safe.
- If the history is too short to form trend blocks, rely only on avg_recent and default to defect except when avg_recent is in the intermediate band and an upward trend can be detected later.
- If n = 2 (pairwise public good reduces to PD-like interaction), the scheme still holds: exploit if partner cooperates frequently; otherwise defect.
- If k is very high (near n), note k/n < 1 still by spec, so immediate cooperation remains individually costly; decisions remain focused on future inducement/punishment.
- If surprising full-cooperation emerges (everyone cooperates every round): AOD‑TC will opportunistically defect (rule 4 first branch) and reap the highest immediate payoff while still punishing any attempts to exploit its cooperative trials in the future.

Pseudocode (compact)

Initialize:
  Punish = false
  Trial_remaining = 0
For each round t = 1..r:
  If t == r: play D; continue
  If Punish: play D; continue
  If Trial_remaining > 0:
    play C
    Trial_remaining -= 1
    continue
  Compute avg_recent over last w rounds of (coop_others/(n−1))
  If avg_recent >= 0.60: play D; continue
  If avg_recent <= 0.20: play D; continue
  Compute trend if earlier block exists: trend = avg_recent − avg_earlier; else trend = 0
  If trend >= 0.05 and Punish == false:
    Trial_remaining = S - 1  # since we cooperate this round
    play C
  Else:
    play D

After each round where I cooperated as part of a trial, evaluate exploitation:
  Let coop_others_in_trial_round = coop_others(round_of_trial)
  If coop_others_in_trial_round <= floor((n−1)/2):  # majority of others did not reciprocate
    Punish = true  # grim: defect until end

Rationale and robustness
- Aggressive: default defection and exploitation whenever opponents show sustained cooperation; fierce punishment (grim) deters others from repeatedly exploiting my cooperation.
- Adaptive: uses recent cooperation levels and trend detection to decide when a short, limited cooperation attempt is worthwhile (only when there is credible momentum toward higher cooperation).
- Robust: works against unconditional cooperators (exploit), cooperators with noise (punish if exploited), and reciprocators (brief trials can discover reciprocators that can be exploited later). Fixed thresholds keep the logic deterministic and simple.
- Endgame safe: defects in final round so no last‑round sucker payoff.

Tuning notes (implementer can adjust)
- Lower θ_exploit (e.g., 0.5) makes the strategy more willing to exploit smaller cooperating minorities; raise it to be more cautious.
- Trial length S controls how much risk you take to bootstrap cooperation; S=1 or 2 recommended to limit exposure.
- Punishment severity: Grim is maximally aggressive; one can instead set a finite punishment length P (e.g., P = max(3, floor(r/4))) if a less permanently hostile approach is desired.

Summary
- Start defecting; defect in the last round; defect by default.
- If many opponents are stably cooperating, defect to exploit them.
- If you detect a credible upward trend, launch a short cooperation trial to try to establish exploitable cooperation; if that trial is exploited, switch to permanent defection (harsh punishment).
- This yields an aggressive, simple, history‑dependent strategy that is adaptive across n, r, k and robust against a wide set of opponent behaviors.
'''

description_AGGRESSIVE_82 = '''
Strategy name: Aggressive Grim + Targeted Punishment (AGTP)

Summary (one line)
- Be aggressive by default: defect first, defect last, and punish any defections harshly and quickly; but opportunistically try to form a stable cooperation coalition only when enough others have shown strong, recent cooperative behaviour and there is enough future horizon to make mutual cooperation worth enforcing.

Key ideas / rationale
- Single-shot D dominates, so start by defecting to avoid being exploited.
- Use a trigger/punishment mechanism so that credible future retaliation can make cooperation profitable against some opponents. The punishment is harsh (grim or long) — this is the “aggressive” element.
- Only attempt cooperation when a large, stable subset of players has reliably cooperated recently and there are enough rounds left to make cooperation payoff-improving versus continued defection.
- If cooperation is attempted, defect immediately and permanently (or for a long punishment window) against any player who deviates from the coalition. Allow limited, parameterized forgiveness if desired.
- Always defect in the final round (and usually the last few rounds), because endgame incentives make punishment ineffective.

Parameters (tunable; give defaults)
- w: history window to assess recent behaviour. Default: w = min(5, max(1, floor(r/4))).
- α: fraction threshold of other players who must have cooperated within the window to consider a cooperation attempt. Default: α = max(0.6, 1 - 1/k). (Higher k → cooperation relatively more attractive → can accept slightly lower α.)
- P: punishment length when a coalition member defects. Default: P = r (grim trigger). Optionally set P = min( r, 3 + floor(r/10) ) to allow finite forgiveness.
- Q: required consecutive cooperative rounds to clear a punished player (only used if P is finite). Default: Q = 2.
- L: number of final rounds to always defect. Default: L = 1 (always defect in the last round). For more aggression set L > 1.
- S_min: minimal coalition size required = ceil(α × (n − 1)). (Excluding self; you coordinate with a subset of other players.)

State tracked
- For each opponent j: coop_count_j = number of times j played C in the last w rounds; last_action_j; punish_until_j (round number until which j is punished).
- Current round t and remaining rounds rem = r − t + 1.

Decision rule (natural-language + pseudocode)

High-level:
1. If t > r − L (i.e., in the final L rounds), play D.
2. Update cooperation frequencies in the last w rounds for every opponent.
3. Identify candidate coalition members: those opponents with coop_count_j ≥ ceil(α × w).
4. If you are currently not attempting a coalition:
   - If number of candidate members ≥ S_min and rem > P (there is enough horizon to make punishment credible), enter “coalition mode” and play C this round.
   - Otherwise play D.
5. If you are in coalition mode:
   - If any coalition member plays D in the current round while they were previously a coalition member, mark that member as punished: set punish_until_j = t + P − 1.
   - If there exists j with punish_until_j ≥ t (i.e., there is any currently punished member), defect this round (D). You punish until punish_until_j < t for all j.
   - If no punished members remain and the coalition still meets the α threshold (recompute with updated window), play C; otherwise exit coalition mode and play D.
6. Forgiveness (only if P finite): If a punished player has cooperated for Q consecutive rounds and rem > P, clear punish_until_j.

Pseudocode (concise)

Initialize:
  for all j != me:
    coop_count_j = 0
    punish_until_j = 0
  coalition_mode = False

At start of each round t (1..r):
  rem = r - t + 1
  if t > r - L:
    action = D
    update history, continue
  Update coop_count_j over last w rounds; update last_action_j
  candidates = { j : coop_count_j >= ceil(α * w) }
  S_min = ceil(α * (n - 1))

  if not coalition_mode:
    if |candidates| >= S_min and rem > P:
      coalition_mode = True
      action = C
    else:
      action = D
  else:  # coalition_mode == True
    # If any coalition member defected in current round, punish them
    for j in coalition_members:
      if last_action_j == D and j in coalition_members:
        punish_until_j = max(punish_until_j, t + P - 1)
    # If any punishments active, defect
    if exists j: punish_until_j >= t:
      action = D
    else:
      # recompute candidates with updated window
      candidates = { j : coop_count_j >= ceil(α * w) }
      if |candidates| >= S_min and rem > P:
        action = C
      else:
        coalition_mode = False
        action = D

  # Forgiveness (only if P finite)
  for j:
    if punish_until_j > 0 and last Q actions of j are all C and rem > P:
      punish_until_j = 0

  Play action; record actions and payoffs.

Edge cases and special rules
- First round (t = 1): play D. Rationale: no information, aggressive default.
- Last rounds: always defect in the final L rounds (default L = 1). This prevents trying to enforce cooperation when punishment can’t be applied.
- If rem ≤ P (not enough rounds left to credibly punish), do not attempt coalition/cooperation—play D.
- If coalition size falls below S_min because too many players defect, exit coalition_mode and revert to D.
- If coalition_mode is activated but many players defect immediately (mass defection), you will quickly punish (grim) and then likely stay in defection permanently.
- With P = r (grim), punishment is permanent: once a coalition member defects while you were cooperating, you defect against them for the rest of the game.
- If you want a slightly less unforgiving aggressive strategy, use finite P and Q forgiveness as above.

Parameter tuning guidance (use game parameters n, k, r)
- α should be lower when k is high (public good is large) because cooperation is more valuable; the suggested formula α = max(0.6, 1 − 1/k) captures this simply.
- w should be big enough to detect stability but small enough to respond: between 3 and 7 is robust in many tournaments.
- P should be at least on the order of a few rounds so retaliation has bite; P = r (grim) is maximally aggressive.
- Always keep L ≥ 1 to respect the inevitability of final-round defection.

Why this is aggressive and robust
- Aggressive: starts defecting, defects in the endgame, and uses harsh punishments (grim by default) that make any attempted cooperate-then-defect exploitation extremely costly for others. That deterrent is explicit and severe.
- Robust: only attempts cooperation when many others have proven cooperative recently and there’s sufficient horizon to enforce it; otherwise the strategy is safe (defect) and won’t be repeatedly exploited. Targeted punishment tracks individuals, so you don’t necessarily burn bridges with players who never cooperated with you.
- Adaptive: the algorithm uses observed frequencies in a rolling window and rem (remaining rounds) so it adapts to observed opponent behavior and remaining horizon.

Implementation notes
- The strategy depends only on game parameters (n, r, k) for default parameter choices and on the public history of actions for decisions.
- All internal parameters (w, α, P, Q, L) are explicit and can be tuned before the tournament; defaults are conservative and aggressive.
- The strategy can be implemented with O(n) per-round bookkeeping (counts, punish_until) and simple comparisons.

Variants (pick one)
- Maximum aggression: set P = r (grim), L = max(1, floor(r/3)), α = 0.7, w = 3.
- Slightly softer: set P = min(r, 4), Q = 2 (forgive after 2 cooperations), L = 1, α tuned as above.

Final note
- Against fully rational adversaries who use backward induction, any finite-horizon repeated public-goods game collapses to all-D; this strategy aligns with that by default (starts and ends with D). But it aggressively exploits non-rational or reciprocity-based opponents by only cooperating when there is a credible coalition and by punishing deviations severely to make cooperation sustainable when it is beneficial.
'''

description_AGGRESSIVE_83 = '''
Strategy name: Aggressive Free-Rider with Conditional Investment (AFRCI)

Goal summary
- Be aggressively selfish: free-ride (defect) whenever doing so is safe and profitable, exploit cooperative opponents, and only make small, calculated cooperative “investments” when those investments are likely to trigger sustained cooperation that you can then exploit. Defect in the final rounds. Punish opportunistic opponents decisively (switch to permanent defection) if they retaliate effectively.

High-level idea
- By default defect (D).
- Monitor opponents’ recent cooperation frequency and responsiveness to group cooperation.
- If many opponents appear to be conditional cooperators (they systematically increase cooperation after others cooperate) and there are enough rounds remaining to recoup an “investment,” make occasional one-shot cooperations (sacrifices) to re-seed or sustain group cooperation, then exploit (defect) while they cooperate.
- If opponents are unresponsive or the horizon is short, never cooperate.
- Always defect on the last round (no incentive to invest).

Notation
- n: number of players
- r: total rounds
- t: current round (1-indexed)
- t_rem = r − t + 1 (rounds remaining, including current)
- Actions: C or D
- history H: a list of observed round outcomes (for each round, which players cooperated)
- w: lookback window size for short-term estimates (default min(5, r−1))
- α: "high cooperation" threshold for group cooperation (default 0.6)
- θ: fraction of opponents required to be “responsive” to label the group responsive (default 0.5)
- resp_threshold: individual responsiveness threshold (default 0.25)
- T_min: minimum rounds remaining to justify an investment (default 4)
- L: spacing between sacrificial cooperations when trying to sustain cooperation (default 3)
- punishment_mode: once triggered, play permanent D for the rest of the game

Decision rules (natural language)
1. Terminal and probing rules
   - If t == r (last round): play D.
   - If t_rem < T_min: play D (no time to recoup investment).
   - If there is insufficient history (t == 1 or t − 1 < w): play D to probe.

2. Estimate short-term statistics (using the last w rounds)
   - For each opponent j compute p_j = frequency that j cooperated in the last w rounds.
   - p_group_recent = average_j p_j (the average cooperation frequency among others).
   - Compute each opponent’s responsiveness r_j:
     - Let majority_coop_prev be whether, in round s−1, the majority of players (excluding j) cooperated. For each time s where s and s−1 are in the lookback period, see:
       r_j ≈ P(j cooperates | majority_coop_prev = True) − P(j cooperates | majority_coop_prev = False)
     - If history is too sparse for a statistically meaningful r_j, treat r_j as 0.
   - Let R = fraction of opponents with r_j ≥ resp_threshold.

3. Main action selection
   - If punishment_mode is active: play D.
   - Else if (p_group_recent ≥ α) AND (R ≥ θ) AND (t_rem ≥ T_min):
       - This means many others are cooperative and a substantial fraction react to group cooperation.
       - Exploit: generally defect to free-ride while they cooperate, but occasionally invest cooperatively to keep them cooperating.
       - Concrete rule:
         - If the previous round (t − 1) had majority cooperation among others: play D (exploit that cooperation).
         - Else (previous round had majority defection): if you have not cooperated in the previous L rounds and you are not in the last T_min rounds, play C once (a scheduled “sacrifice” to reseed cooperation). Otherwise play D.
   - Else (group not sufficiently cooperative or not responsive or horizon too short): always play D.

4. Punishment detection and escalation (aggressive stance)
   - If you defect in a round and the opponents’ aggregate cooperation drops sharply in the subsequent round(s) specifically in response to your defection (e.g., group cooperation rate decreases by more than Δ_drop, default 0.25, after your defections compared to prior behavior) and that reduction causes your payoffs to fall substantially relative to the expected free-ride gain, then switch to punishment_mode (permanent D).
   - Rationale: opponents are successfully punishing you; further attempts to foster cooperation are costly. Permanently defect to stop being milked by tit-for-tat style punishments.

Edge cases and clarifications
- First round: play D (probe).
- Last round: always D (backward-induction).
- Penultimate rounds: when t_rem < T_min (default 4), play D.
- Short tournaments (r small): w and T_min scale down (w = min(3, r−1); T_min = max(2, ceil(r/4))). If r is so small that investments can’t be recouped, strategy devolves to always defect.
- Ties for majority: treat ties as “no majority cooperation” (conservative).
- Randomization: none required. Deterministic decisions reduce unpredictability; if implementer wants less exploitable pattern, occasional randomized cooperation (small probability) can be added to sacrificial steps, but be cautious—this reduces the “punishment credibility” and is not necessary for an aggressive stance.
- Parameter tuning: α, θ, resp_threshold, T_min, L and Δ_drop are tunable. Defaults given above aim to be robust across typical tournaments. Implementers can scale α down slightly if k/n is high (cooperation is less costly), or increase T_min when k/n is small.

Pseudocode (concise)

Initialize:
  w = min(5, r-1)
  α = 0.6
  θ = 0.5
  resp_threshold = 0.25
  T_min = 4
  L = 3
  Δ_drop = 0.25
  punishment_mode = False
  last_coop_round = −∞

At each round t:
  t_rem = r - t + 1
  if punishment_mode: return D
  if t == r or t_rem < T_min or t == 1: return D

  Use last w rounds of H to compute:
    for each opponent j:
      p_j = frequency j cooperated in last w rounds
      compute r_j = P(j cooperates | majority_coop_prev) - P(j cooperates | ¬majority_coop_prev) (fallback 0 if insufficient data)
    p_group_recent = mean_j p_j
    R = fraction_j (r_j >= resp_threshold)
    majority_prev = True if majority of other players cooperated in round t-1 else False

  if (p_group_recent >= α) and (R >= θ) and (t_rem >= T_min):
    # responsive cooperative environment -> exploit with occasional sacrifice
    if majority_prev:
      action = D   # exploit now
    else:
      if (t - last_coop_round) > L:
        action = C   # schedule a one-round sacrifice to re-seed cooperation
        last_coop_round = t
      else:
        action = D
  else:
    action = D

  # After observing the opponents' actions in the next round(s) the implementation should update H and check punishment trigger:
  # If after a round where you defected the group's cooperation rate drops by > Δ_drop and that drop correlates with your defection, set punishment_mode = True

  return action

Why this is aggressive and robust
- Aggressive: defaults to defect and free-ride; actively exploits observed cooperation; punishes when opponents successfully retaliate; refuses last-round cooperation.
- Conditional investment: does not blindly cooperate—only invests when there is a high chance that a one-shot sacrifice will be amplified into sustained cooperation by responsive opponents, and only if the remaining horizon allows recouping the cost.
- Adaptive: uses short-run statistics to estimate both cooperation level and responsiveness; reacts to observed opponent behavior patterns.
- Robust: against unconditional cooperators you repeatedly free-ride; against unconditional defectors you simply defect (no wasted investments); against conditional cooperators you attempt low-cost reseeding followed by exploitation; against retaliatory strategies you switch to permanent defection to avoid getting stuck paying costs.

Implementation notes
- Carefully implement responsiveness estimates (r_j) with smoothing when data is sparse.
- Choose T_min relative to r and k/n if you want precision: if k/n is close to 1 (cooperation almost breaks even), lower T_min; if k/n is small, raise T_min.
- Keep punishment detection conservative (Δ_drop large enough) to avoid false positives from noise.

This strategy presents a clear, aggressive, parameterized decision procedure that depends only on the game parameters and observable history; it is explicitly designed to exploit cooperative opponents, avoid wasting effort on unresponsive groups, and punish effective punishers by locking into defection.
'''

description_AGGRESSIVE_84 = '''
Name: Predatory Opportunistic Defector (POD)

Summary (one line):
An aggressive, adaptive rule-set that by default defects, opportunistically (and briefly) seeds cooperation only when it is plausibly profitable long‑term, aggressively exploits persistent cooperators, and punishes failed recruitment attempts with permanent defection.

Intuition:
- Single-round defect strictly dominates cooperate (k/n < 1). So default is defect.
- Mutual cooperation can yield larger per-round payoffs (k > 1). It may therefore be worth “investing” a small number of cooperative rounds to trigger a long-run cooperative regime — but only if there are enough rounds left to recoup that investment.
- Be ruthless: if a recruitment attempt fails (cooperation collapses), switch to permanent defection (grim) so opponents who try to use conditional punishments do not extract long-run rents from you.
- Exploit unconditional or persistent cooperators by defecting while they cooperate.

Parameters (derived from game parameters, tunable constants):
- n, k, r : game parameters (given).
- W (window for measuring recent behavior): W = min(10, max(3, floor(r/10))). Default 10 or smaller for short games.
- θ_exploit (group cooperation rate threshold to treat the group as “exploitably cooperative”): 0.6 (suggested default; can be adjusted).
- θ_sustain (cooperation level that counts as “sustained cooperation” after a recruitment attempt): 0.5.
- Δ_min (minimum increase in cooperation after recruitment that counts as a positive response): 0.15.
- S_build (number of consecutive cooperative rounds used for a recruitment probe): 1 or 2 (keep tiny; default 1).
- L_max (max number of recruitment attempts allowed in a game): 1 or 2 (default 1).
- Grim behavior: after a failed recruitment attempt, return to permanent defection for the rest of the game.
- H_recoup (minimum remaining rounds required to justify a recruitment attempt): computed below.

Recoup calculation (deterministic, uses k, n):
- Cost of cooperating in a round vs defecting = 1 − (k/n)
- Net gain per future round once full mutual cooperation is established = k − 1
- Minimal number of future rounds required to repay a single-round cooperation investment:
  H_recoup_real = (1 − k/n) / (k − 1)
- Use integer horizon: H_recoup = ceil(H_recoup_real) + 1 (add 1 as buffer)

Decision rules (natural language + pseudocode):

Global state maintained:
- history: for each prior round t, the vector of plays (C/D) by all players and total cooperators.
- attempts_made: number of recruitment attempts used so far (starts 0).
- mode ∈ {DEFAULT, EXPLOIT, RECRUIT_BUILD, COOP_PARTNERSHIP, GRIM}.
  - DEFAULT: defect by default while observing.
  - EXPLOIT: actively exploit when group cooperation is high; remain defect.
  - RECRUIT_BUILD: cooperative probe(s) to try to trigger cooperation.
  - COOP_PARTNERSHIP: cooperate routinely to maintain cooperation when it is profitable.
  - GRIM: permanent defection for the rest of the game.

Initialization:
- t = 1 (first round)
- mode = DEFAULT
- attempts_made = 0

Round-by-round decision (for round t):
1. Endgame override:
   - If t == r (last round): play D (defect). Always defect in last round.
   - If r − t + 1 ≤ H_recoup: (not enough rounds left to justify investment)
       - Do not initiate recruitment. If currently in COOP_PARTNERSHIP, exit to GRIM or DEFAULT as below.
       - Play D.

2. First-round policy:
   - t == 1: play D and remain in DEFAULT.

3. Compute statistics from history:
   - Let others’ cooperation fraction in last min(W, t−1) rounds be G = total contributions by others in that window divided by (n−1)/window_length appropriately. (Equivalent: fraction of players cooperating averaged over rounds.)
   - Let overall average cooperation rate of others since start be G_total.
   - Track recent change: ΔG = G − G_prev_window (difference between the most recent window and the previous window), if previous window exists.

4. Mode transitions and action selection (in priority order):
   - If mode == GRIM: play D forever.

   - If mode == RECRUIT_BUILD:
     - During a recruitment probe of S_build rounds: play C for this round (cooperate to signal).
     - After completing S_build rounds of build, compute response: if the group cooperation rate in the following evaluation window (next W rounds or the immediate next round(s) if not available) increased by at least Δ_min relative to pre-build level and G ≥ θ_sustain, then set mode = COOP_PARTNERSHIP and play C (start cooperating).
       - Otherwise (no material increase): increment attempts_made; set mode = GRIM; play D for remainder (punish by permanent defection).

   - If mode == COOP_PARTNERSHIP:
     - If group cooperation G drops below θ_sustain or cooperation falls by more than Δ_min in a short interval, assume partnership broken: set mode = GRIM and play D.
     - Otherwise (cooperation holding): cooperate (play C) except when a scheduled exploitation harvest occurs (see exploitation cycles below). Continue cooperating so long as cooperation remains high and enough rounds remain to recoup future cooperation (r − t + 1 > H_recoup). If partnership is long and stable but you want to harvest, execute a brief exploitation window (see exploitation cycles).

   - Exploitation cycles while in COOP_PARTNERSHIP:
     - Aggressive harvesting: after cooperating for at least S_hold rounds (S_hold = 1 default), you may defect for H_harvest rounds = 1..2 (small) to capture one-shot extra payoffs if cooperation remains high among others. If harvesting causes cooperation to collapse, you will switch to GRIM. Choose H_harvest small (1 is default) to limit risk.

   - If mode == DEFAULT:
     - If G ≥ θ_exploit (group is substantially cooperative over recent window):
         - Set mode = EXPLOIT and play D (defect every round to free-ride).
     - Else, if r − t + 1 > H_recoup AND attempts_made < L_max:
         - Consider initiating recruitment if you detect pockets of conditional cooperators: trigger RECRUIT_BUILD with S_build = 1 (cooperate this round) if either:
             a) There are players with high personal cooperation rates (for some j, f_j ≥ 0.7), or
             b) G_total is moderately high (≥ 0.4) despite recent dips (ΔG < 0).
         - If criteria met: set mode = RECRUIT_BUILD and play C this round (start build).
     - Otherwise (no recruitment), play D.

   - If mode == EXPLOIT:
     - Always play D while G ≥ θ_exploit.
     - Continuously monitor G: if G falls below θ_exploit − 0.1 (i.e., cooperation collapsed), switch mode = DEFAULT or consider RECRUIT_BUILD next if rounds remain and attempts_made < L_max.
     - If you detect that everyone is defecting (G very low, e.g., < 0.05), switch to DEFAULT (still play D).

Edge-case handling:
- Very short games: if r small so that r ≤ H_recoup or H_recoup is greater than remaining rounds initially, POD never tries recruitment and simply defects every round (including round 1).
- Single attempt cap: attempts_made ≤ L_max ensures POD does not repeatedly seed cooperation and get exploited repeatedly. L_max default 1 (very aggressive).
- If opponents are anonymous but you can observe per-player histories, you may compute personal f_j and use them to detect persistent cooperators to exploit or to target recruitment signals; but actions are public, so you cannot target only some players — your action applies to all.
- Determinism vs randomization: the above is deterministic. If the tournament allows randomization, you can randomize small parts (e.g., cooperation probability during build of 0.5) to avoid exploitation by sophisticated checks. But deterministic behavior is likely easier to implement and reason about.

Pseudocode (compact):

Initialize: mode = DEFAULT; attempts_made = 0; record history
For each round t = 1..r:
  if t == r: action = D; continue
  compute W = min(10, max(3, floor(r/10)))
  compute G = recent group cooperation rate over last W rounds (excluding current)
  compute G_total, ΔG if available
  compute H_recoup_real = (1 - k/n)/(k - 1)
  H_recoup = ceil(H_recoup_real) + 1
  if (r - t + 1) <= H_recoup:
    action = D; (no recruitment, endgame)
    if mode == COOP_PARTNERSHIP: mode = GRIM
    continue
  if mode == GRIM:
    action = D; continue
  if mode == RECRUIT_BUILD:
    play C for up to S_build rounds
    after S_build rounds:
      measure response R over next min(W, remaining rounds) rounds
      if R - pre_build_G >= Δ_min and R >= θ_sustain:
        mode = COOP_PARTNERSHIP; action = C
      else:
        attempts_made += 1; mode = GRIM; action = D
    continue
  if mode == COOP_PARTNERSHIP:
    if G < θ_sustain or cooperation drops sharply:
      mode = GRIM; action = D
    else if ready to harvest (after S_hold cooperative rounds) and random small harvest decision or deterministic 1-round harvest:
      action = D (harvest for H_harvest rounds)
      if post-harvest G collapses: mode = GRIM
    else:
      action = C
    continue
  if mode == EXPLOIT:
    if G < θ_exploit - 0.1: mode = DEFAULT; action = D
    else: action = D
    continue
  // mode == DEFAULT
  if G >= θ_exploit:
    mode = EXPLOIT; action = D; continue
  if attempts_made < L_max and r - t + 1 > H_recoup and (exists j with f_j >= 0.7 or G_total >= 0.4):
    mode = RECRUIT_BUILD; action = C; continue
  action = D

Aggressive aspects made explicit:
- Default, last-round and grim behavior: always defect; endgame defection; failed recruitment leads to permanent defection (no forgiveness).
- Exploitation: whenever group cooperation is reliably present, POD defects to harvest immediate extra payoff.
- Limited mercy: POD will attempt at most a very small number of recruitment probes (typically 0–1), only when mathematical recoup looks possible. If recruitment triggers fail, POD invokes grim-trigger—a punitive, permanent defection.
- Harvests: even in cooperative partnerships, POD opportunistically takes short defecting harvests to squeeze extra one-shot gains.

Why this is robust:
- Against unconditional cooperators: POD defects every round and takes maximal immediate advantage.
- Against unconditional defectors: POD defects and gets baseline payoff (safe).
- Against conditional cooperators/tit-for-tat style: POD will try a brief, carefully-timed recruitment if the future horizon justifies it; if cooperation is stable, POD can cooperate to sustain mutual high payoffs, but will harvest opportunistically and will punish hard if the partnership breaks. This maximizes long-run payoff while minimizing repeated exploitation risk.
- Against sophisticated exploiters: POD’s grim after failed recruitment prevents repeated exploitation of recruitment attempts; DEFAULT defection prevents being suckered in by early-round cooperators.

Tuning advice:
- θ_exploit can be lowered if the population tends to have fewer cooperators but still exploitable pockets.
- H_recoup uses game fundamentals (k,n) so the algorithm scales to different parameter sets.
- For extremely long games you may increase L_max to allow repeated measured recruitment attempts; for tournaments with many nice strategies keep L_max=1 for maximum aggressiveness.

Final notes:
- This strategy is explicit, deterministic, and only depends on the parameters and observed history.
- It is aggressive: defaults to defect, exploits cooperators, punishes failed attempts to cooperate with permanent defection, and opportunistically harvests even within cooperation phases.
- Implementation is straightforward from the pseudocode and parameter formulas above.
'''

description_AGGRESSIVE_85 = '''
Strategy name: Aggressive Opportunistic Exploiter (AOE)

Goal (mindset): maximize my own total payoff by (1) refusing to be a sucker, (2) probing to detect exploitable, reciprocating opponents, (3) seeding cooperation only when I can reliably harvest it later, and (4) harvesting (defecting) at the moment that gives me highest one-shot benefit given others’ cooperation. The strategy is adaptive, uses only game parameters and observable history, and is intentionally aggressive: cooperation is only a tool to be used when it increases my expected total payoff; otherwise I defect.

High-level summary of behavior
- Default: defect. Defection always gives a higher immediate payoff than cooperation in a single round because k/n < 1.
- Probe occasionally early to detect reciprocators or conditional cooperators.
- If probes show a population that reliably increases cooperation after my cooperation, switch to a “seed” phase in which I cooperate for a sustained block to induce high cooperation.
- After building a high-cooperation environment, defect in a short harvest block near the end (including always defecting in the final round) to exploit others’ contributions.
- If no exploitable pattern is detected, stay defecting for whole game (except rare probes).

Decision rules (precise)

Notation:
- n, r, k: game parameters.
- t: current round index (1..r).
- history stores for each past round s: vector of actions a_j,s ∈ {C,D} and I observe all players’ actions.
- For j ≠ me, let coop_j(s) = 1 if j played C in round s, else 0.
- window W = min(5, max(1, r-1)) — number of recent rounds used for rate estimates.
- M = min(2, r-1) — number of rounds to observe immediate response after a probe.
- p_probe = min(0.15, 1/(2n)) — small probe probability (aggressive => small).
- delta_detect = 0.20 — minimum absolute increase in others’ cooperation rate after my probe to count as positive response.
- harvest_len H = max(1, ceil(0.15 * r)) — number of final rounds to harvest (defect) after seeding.
- seed_min = min( max(2, ceil(0.30*r)), r - H - 1 ) — minimal planned seeding duration when exploitation is promising (clipped so there are harvest rounds left).
- last round (t == r): always defect.

State variables maintained:
- exploit_flag (boolean): true if I have detected exploitable behavior and have committed to seed+harvest plan.
- seed_target_end: round index (inclusive) until which I will seed (cooperate) if exploit_flag is true.
- harvest_start: round index when harvesting (defecting en masse) begins if exploit_flag is true.
- probes_done: number of probes executed so far.
- baseline_others_rate: baseline average cooperation rate of others computed over an initial baseline window.

Initialization (before round 1)
- exploit_flag = false
- probes_done = 0
- Compute baseline_others_rate = 0 (will be estimated once we have past rounds; treat as 0 at start)

Per-round decision process (for round t):

1) If t == r: play D (always defect in final round).

2) Update estimates from history:
   - For each j ≠ me compute recent cooperation rate p_j over last W rounds (if fewer rounds exist, use all available).
   - Let others_rate = average_j p_j (average cooperation probability among others over that window).
   - If t <= W+1 and baseline_others_rate == 0, set baseline_others_rate = others_rate (initial baseline after few rounds).

3) If exploit_flag is true:
   - If t < harvest_start:
       - play C (seeding phase: cooperate to raise others’ cooperation).
   - Else:
       - play D (harvest phase: defect to exploit others’ cooperation).
   - (Note: last round already handled by step 1).

4) If exploit_flag is false (normal mode):
   - If t <= r-1 and probes_done < max(1, floor(0.2*r)):  // allow up to a small # of probes spread early
       - With probability p_probe, execute a probe this round:
           - Play C (probe).
           - Record the round index probe_round = t and increment probes_done.
           - After the probe, observe the next M rounds (or as many as exist) to measure response (see detection test below).
           - Continue game until detection test finishes; until detection test completes you follow default (defect except last-round rule).
       - Else (no probe this round): play D (default).
   - Else (no probe available/used):
       - play D (default).

Detection test (triggered after a probe at probe_round):
- Let baseline_window be the W rounds immediately before probe_round (if fewer exist, use what’s available). Compute baseline_rate_others = average cooperation among others in that baseline_window.
- Observe the next M rounds after the probe (or as many rounds as are available before harvest planning must start); call post_rate_others = average cooperation among others in those M rounds, excluding my own action.
- If post_rate_others − baseline_rate_others ≥ delta_detect:
    - Consider this a positive detection: others tend to increase cooperation after my cooperating.
    - Set exploit_flag = true.
    - Plan seeding and harvesting:
        - harvest_start = max(probe_round + seed_min, r - H)  // ensure harvest fits before game end
        - seed_target_end = harvest_start - 1
    - From next round onward, follow exploit_flag rules (seed until seed_target_end then harvest).
- Else:
    - No detection; continue default defecting policy and allow further probes up to limit.

Edge-case handling and parameter notes
- Very small r (r = 2 or 3): behave conservatively:
  - r=2: never probe (p_probe effectively 0 because no time to harvest). Rule: always defect both rounds (last round D implied).
  - r=3: allow at most 1 probe in round 1 with extremely small probability; otherwise defect. If probe shows strong positive response in round 2, harvest in round 3 (but round 3 is last and I always defect).
- If detection occurs too late to allow both a meaningful seeding block and a harvest block (i.e., probe happens with t so late that seed_target_end ≥ r), then ignore detection (exploit_flag stays false) because harvest cannot be scheduled usefully.
- If multiple probes produce conflicting signals, require at least one positive detection to flip exploit_flag (i.e., a single clear positive detection is sufficient).
- Baseline is local (recent W rounds) so the strategy adapts to changing opponent behavior.

Why this is aggressive and robust
- Aggressive: default is defection; probes are rare and small (so I keep most rounds’ immediate advantage). When I detect exploitable reciprocators, I intentionally seed cooperation (sacrifice some immediate payoff) to create a larger exploitation opportunity and then harvest (defect) while others are cooperating — maximizing my own payoff even if it harms them. I always defect in final round so I never allow a final-round cooperative “giveaway.”
- Robust: The strategy does not assume norms or explicit coordination. It tests empirically whether its cooperation can influence others, and only when that influence is strong and exploitable does it invest in seeding. If opponents are noisy, selfish, or retaliatory, probes will fail and I remain defecting (no long costly seeding). Parameters (W, M, p_probe, delta_detect, seed_min, H) are chosen conservatively to avoid being easily exploited while enabling exploitation of reciprocators.
- Adaptive: Uses observed conditional responses to my probes to decide whether cooperation can be induced. The seeding/harvest plan is computed from the actual round when detection occurs and the remaining rounds.

Pseudocode (compact)
1. Initialize exploit_flag=false; probes_done=0; baseline_others_rate=0.
2. For each round t = 1..r:
   a. If t == r: action = D; continue.
   b. Compute others_rate over last W rounds; maybe set baseline_others_rate if empty.
   c. If exploit_flag:
        if t < harvest_start: action = C
        else: action = D
        continue
   d. // exploit_flag false
      if probes_done < max(1, floor(0.2*r)) and random() < p_probe and t <= r-2 and r > 3:
           action = C; probes_done += 1; record probe_round = t
           // after M subsequent rounds finish, compute post_rate_others and compare to baseline_rate
           if post_rate_others - baseline_rate_others >= delta_detect and there is room to harvest:
               exploit_flag = true
               harvest_start = max(probe_round + seed_min, r - H)
               seed_target_end = harvest_start - 1
      else:
           action = D

Practical tuning suggestions
- If opponents are especially forgiving or many reciprocators exist, increase p_probe slightly (0.15 → 0.25) and reduce delta_detect.
- If opponents are highly exploitative/vengeful, reduce seed_min and H to limit losses.
- If tournament scoring penalizes relative performance rather than absolute, you can be even more aggressive (shorter seeding and longer harvesting) to maximize relative gain.

Final note
This strategy purposefully sacrifices short-term gains only when there is strong evidence it will enable larger long-term payoffs via exploitation. When there is no evidence, it defects persistently. It is simple to implement, only uses observable history and game parameters, and is tuned to be aggressive and opportunistic in mixed populations of reciprocators and defectors.
'''

description_AGGRESSIVE_86 = '''
Strategy name: Aggressive Conditional Cooperator with Escalating Punishment (ACCEP)

High-level description (mindset)
- Aggressive objective: maximize my own total payoff by exploiting unconditional cooperators, only cooperating when there is reliable evidence that cooperating will lead to sustained mutual cooperation, and punishing any exploitation of my cooperation harshly and quickly to deter future exploitation.
- Adaptive: uses short-run statistics from recent rounds to detect stable cooperation, adjusts sensitivity when the public-good multiplier k is relatively large, and escalates punishments when exploitation repeats.
- Robust: starts defensively to avoid early exploitation, gives conditional openings for cooperation when others are reliably cooperative, punishes exploiters with a deterministic, escalating schedule, and always defects in the last round.

Parameters internal to the strategy (computed from game parameters)
- L = min(5, r-1) — history window length used to estimate recent behaviour (use up to last 5 rounds, or fewer if r small).
- threshold_base = 0.5 — baseline fraction of other players who must be cooperating recently before I will cooperate.
- threshold_k_adjust = (k - 1)/(n - 1) — adjusts tolerance based on multiplier: when k is larger relative to n, cooperation is more productive, so require slightly less strict evidence.
- threshold = clamp(threshold_base - 0.15 * threshold_k_adjust, 0.25, 0.75)
  (Interpretation: threshold is lowered modestly when k is relatively large, bounded between 0.25 and 0.75.)
- P_min = 2 — minimum punishment duration when I decide to punish.
- P_cap = max(3, r/4) rounded down — maximal punishment for repeated offenses (capped so not to lock into useless endless punishment when few rounds remain).
- exploit_burst_chance = deterministic rule: if recent cooperation among others is extremely high (≥ 0.90), I will defect for exactly 1 deliberate exploitation round (to harvest free-ride), then resume normal rule (this makes the strategy truly aggressive and opportunistic).

State variables (maintained across rounds)
- punish_remaining (integer) — number of rounds left in an active punishment phase (initially 0).
- punish_level (integer) — counts number of punishments already triggered (initially 0).
- last_rnd_my_action — my action in previous round (C or D).
- history matrix — record of all players' actions in previous rounds (given by the game environment).

Decision rules (what I do each round)
1. Last-round rule (backward induction): If current round t == r (final round), play D.
   - Rationale: final round defection strictly dominates cooperation in a one-shot mover.

2. If punish_remaining > 0:
   - Play D.
   - Decrement punish_remaining by 1.
   - Rationale: carry out deterministic punishment; do not cooperate during punishment.

3. Otherwise (not currently punishing), compute recent-cooperation statistics:
   - For each round s in {max(1, t-L) ... t-1}, count number of cooperators among the other n-1 players (exclude me) in that round.
   - Let f_L = (sum of those counts) / (L * (n-1)) — fraction of other-player actions that were cooperation over the window.
   - Let f_last = (# cooperators among other players in round t-1) / (n-1) (if t>1; else undefined).

4. Opportunistic exploitation:
   - If f_L ≥ 0.90 and not in punishment, then play D for one round (exploit). After that round, resume the normal rule and monitor whether others start to defect; if exploitation causes others to defect (detected in subsequent rounds), punish (see trigger below).

5. Cooperation decision:
   - If f_L ≥ threshold, play C (cooperate).
     - Rationale: others have been cooperating reliably; paying the small immediate cost may seed sustained cooperation and yield higher long-run payoff.
   - Else play D.

6. Punishment trigger (checked immediately after observing the previous round when I had cooperated):
   - If I cooperated in any of the last L rounds and in that same round at least one other player defected while I cooperated (i.e., I was exploited in the window), then initiate punishment:
     - Increment punish_level by 1.
     - Set punish_remaining = min( P_cap, max(P_min, 2 * punish_level) ).
       (This sets punish length to at least P_min, doubling with repeated punishments, capped by P_cap.)
     - Immediately for the next punish_remaining rounds I will play D regardless of f_L or exploit bursts.
   - Rationale: punish any exploitation of my cooperation harshly, and escalate if exploitation recurs so as to deter repeat offenders.

Implementation pseudocode (deterministic)

Initialize:
  punish_remaining = 0
  punish_level = 0
  last_rnd_my_action = None

On round t (1-indexed):
  if t == r:
    action = D
    last_rnd_my_action = D
    return action

  if punish_remaining > 0:
    action = D
    punish_remaining -= 1
    last_rnd_my_action = D
    return action

  L_eff = min(L, t-1)  # how many past rounds exist
  if L_eff == 0:
    # first round: aggressive default
    action = D
    last_rnd_my_action = D
    return action

  # compute f_L and f_last
  total_coop_count = sum_over_s = (for s in t-L_eff .. t-1) sum_{j != me} I[action_{j,s} == C]
  f_L = total_coop_count / (L_eff * (n-1))
  coop_last = (for t>1) sum_{j != me} I[action_{j,t-1} == C]
  f_last = coop_last / (n-1)

  # opportunistic exploitation
  if f_L >= 0.90:
    action = D
    last_rnd_my_action = D
    # after observing the next round, if others begin defecting, punishment may trigger per rule below
    return action

  # normal cooperation decision
  if f_L >= threshold:
    action = C
  else:
    action = D

  # set last action and return
  last_rnd_my_action = action
  return action

After each round (observation & possible immediate punishment trigger):
  # This is checked after seeing round t outcomes. If while I cooperated in any of the last L rounds I was exploited:
  if there exists s in (max(1,t-L+1) .. t) such that my_action[s] == C and there exists j != me with action_{j,s} == D:
    # start / escalate punishment
    punish_level += 1
    punish_remaining = min(P_cap, max(P_min, 2 * punish_level))
    # (punishment will begin on the next round because current round is over)

Notes and edge cases
- First round: I play D (aggressive, avoids giving freebies).
- Last round: always D (dominant one-shot action).
- Short games (r small): L will be small; punishments are capped by P_cap to avoid wasting most of remaining rounds on punishment.
- If multiple players exploit me repeatedly, punishments escalate deterministically, making future exploitation costly for the group.
- The strategy is deterministic and only depends on observed history and the known parameters (n, r, k).
- The precise numerical constants (L=5, threshold_base=0.5, exploit threshold 0.90, P_min=2, doubling punishment) are chosen for robustness; they can be tuned for a particular tournament meta if desired.

Why this is "aggressive" and why it is robust
- Aggressive: default is defect (first round), opportunistically free-rides when others are highly cooperative, and imposes quick, escalating global punishments whenever my cooperation is exploited. It does not try to be nice at the cost of being repeatedly exploited.
- Exploitative: the explicit one-round exploit when others are reliably cooperative secures immediate gains from cooperators.
- Deterrent: escalating punishments raise the future cost to those who exploit my cooperation, making sustained exploitation unattractive.
- Adaptive: the cooperation threshold lowers somewhat when k is relatively large (cooperation more valuable), so the agent will cooperate more readily when the public-good multiplier makes cooperation more profitable for the group.
- Robust to a wide range of opponent behaviours: it profits from unconditional cooperators, withstands unconditional defectors (by defecting), opens cooperation with conditionally reciprocal opponents and deters exploitation through clear, escalating, observable punishment.

Implementation notes for tournament
- This design assumes perfect monitoring (full history known) and no communication, which matches the game spec.
- The state variables needed are simple counts over recent rounds; the algorithm is O(n * L) per round to compute f_L.
- The punishment scheme is global (applies to the whole group) because actions are simultaneous; that is consistent with the environment.

Summary (concise)
- Start defecting. Cooperate only when recent cooperation among others exceeds a data-driven threshold (threshold lowered when k is larger). If I am ever exploited (I cooperated while someone else defected), enter a deterministic, escalating punishment phase of D for multiple rounds. Opportunistically defect for a single round to exploit very high cooperation (≥90%). Always defect in the last round. This yields an aggressive, adaptive, and robust strategy suitable for tournament play.
'''

description_AGGRESSIVE_87 = '''
Name: Aggressive Adaptive Defector (AAD)

High-level idea (aggressive mindset)
- Default to defect; exploit any opponents who habitually cooperate.
- Be opportunistically cooperative only when doing so raises my total expected payoff (taking into account how others respond).
- If my defection provokes punishers/retaliators, punish back quickly but in a controlled, limited way so I can resume exploiting cooperators soon after.
- Always defect in the final round (no reason to give away endowment).

Notation used below
- n, r, k are given game parameters.
- t is the current round (1-based).
- history stores for each previous round the vector of actions by all players.
- for opponent j (j ≠ me):
  - p_j = empirical probability (frequency) that j played C in a recent window of rounds.
  - delta_j = estimate of how j’s cooperation probability changes depending on my last action:
      delta_j ≈ P(j plays C | I played C recently) − P(j plays C | I played D recently).
- L = window size for recent history used to estimate probabilities (suggest L = min(8, t-1)).
- Rem = remaining rounds after current (r − t).
- M_est_excl = sum_j p_j (expected number of other cooperators next round, given current estimates).
- aggression_bias ε: small positive constant in payoff units biasing toward defection (suggest ε = 0.05–0.2 depending on payoff scale).

Decision rule (per round)
1. Edge-case overrides
   - If t = 1: defect (probe).
   - If t = r (last round): defect (backward induction).
   - If data is too sparse (t − 1 = 0) use default defect (same as t = 1).

2. Estimate opponents’ behavior from the recent window of rounds
   - For each opponent j compute p_j over last L rounds.
   - For each j compute delta_j by comparing their conditional frequencies after my C versus after my D (use last L rounds; if insufficient data set delta_j = 0).
   - Let s_j = max(0, −delta_j). s_j is estimated drop in j’s cooperation probability caused by my defection.
   - M_est_excl = sum_j p_j.

3. Immediate-payoff estimates for the upcoming round
   - If I play D: π_D = 1 + (k/n) * M_if_I_defect
     - Predict M_if_I_defect ≈ sum_j p_j_given_my_last_action = use the conditional estimate (if my last action was D or C use the matching conditional probabilities); if unsure, use p_j.
   - If I play C: π_C = 0 + (k/n) * (M_if_I_cooperate + 1)
     - Predict M_if_I_cooperate analogously (use conditional responses if available).
   - In practice, you can approximate with M_if_I_defect ≈ M_est_excl and M_if_I_cooperate ≈ M_est_excl + sum_j max(0, delta_j).

4. Expected future effect of defecting this round
   - If I defect now, some opponents may reduce their cooperation in subsequent rounds.
   - Estimate future cooperative loss over remaining rounds:
       L_future = (k/n) * Rem * sum_j s_j
     (This is the expected total reduction in my share across remaining rounds due to opponents reducing their cooperation because of my defection now.)

5. Decision comparison (aggressive threshold)
   - Compute left = π_D + L_future + ε (value of defecting now, including future advantage and aggression bias).
   - Compute right = π_C (value of cooperating this round; future benefit from cooperating is implicitly captured if delta_j>0 in π_C’s M_if_I_cooperate; explicitly adding future benefits is optional if you have reliable estimates).
   - Action:
     - If left ≥ right → play D.
     - Else → play C.

6. Punishment policy (targeted but limited)
   - Detect punishers: opponents with delta_j ≪ 0 and p_j reasonably high historically (they cooperated but drop cooperation because of my defection).
   - If I detect at least one significant punisher and I have just been punished (my payoff or number of cooperators dropped sharply because I defected last round), apply a short, harsh punishment streak:
     - Set punish_timer = min(3, Rem). For the next punish_timer rounds play D unconditionally (this reduces overall group payoff but signals that I will not tolerate targeted punishment).
     - After punish_timer expires, return to the standard decision rule (recompute deltas and p_j).
   - Rationale: short, strong punishments deter small groups of retaliators from sustaining punishment that would extract long-run losses from me. Keep punishment short to limit self-harm.

7. Forgiveness and exploitation cycle
   - After punishment expires, the strategy again defects by default to test for resumed unconditional cooperation.
   - If many opponents return to high cooperation rates and show little responsiveness to my defection (unconditional cooperators), exploit them: continue defecting to harvest their contributions.
   - If a majority become reciprocators/retaliators (large sum_j s_j making L_future big), then the decision rule will force me to cooperate sufficiently to avoid larger future losses.

Parameter recommendations and thresholds
- L (window) = min(8, t−1). Short window makes the strategy responsive; increase if opponents are noisy.
- delta thresholds to label punishers or reciprocators:
  - punisher if delta_j ≤ −0.15 and p_j ≥ 0.3
  - conditional cooperator if delta_j ≥ +0.15
  - unconditional cooperator if p_j ≥ 0.9 and |delta_j| < 0.05
  - defector if p_j ≤ 0.1
- aggression_bias ε = 0.05 to 0.2 (units of payoff). Increase to be more aggressively exploitative.
- punishment length max 3 rounds (short, harsh).

Pseudocode (compact)

Initialize history empty.
For each round t = 1..r:
  if t == 1 or t == r: play D; continue
  compute L = min(8, t-1)
  for each opponent j:
    compute p_j over last L rounds
    compute conditional frequencies P_j(C | my last action = C) and P_j(C | my last action = D) using last L rounds (if enough data)
    delta_j = conditional_C − conditional_D (or 0 if insufficient)
    s_j = max(0, −delta_j)
  M_est_excl = sum_j p_j
  estimate M_if_I_defect and M_if_I_cooperate (use conditional estimates if available)
  π_D = 1 + (k/n) * M_if_I_defect
  π_C = (k/n) * (M_if_I_cooperate + 1)
  Rem = r − t
  L_future = (k/n) * Rem * sum_j s_j
  if currently in punish_timer > 0:
    play D; punish_timer -= 1; continue
  if π_D + L_future + ε >= π_C:
    play D
    if we detect that last round’s defection produced a steep collective drop in cooperators and some opponents meet punisher criteria:
      punish_timer = min(3, Rem)   # start a short punishment streak for signaling
  else:
    play C

Why this is aggressive and robust
- Aggressive: default bias to defect and explicit exploitation of unconditional cooperators. Adds a small constant ε bias to favor defecting even when payoffs are close.
- Punitive: detects and responds to punishers with short, harsh retaliation to prevent being exploited by small groups of punishers.
- Adaptive: uses online estimates (p_j and delta_j) from history to predict immediate payoffs and future consequences, so it adjusts to many kinds of opponents (unconditional cooperators, unconditional defectors, reciprocators, grudgers, noise).
- Safe against mass punishers: if a majority are retaliation-prone so that L_future becomes large, the decision criterion will force cooperation rather than suicidal defection, so the strategy will avoid needless mutual destruction.
- Endgame-aware: always defects in the final round (and possibly in last few rounds if you want a longer endgame collapse), preventing late-round exploitation.

Implementation notes
- Use smoothing (Laplace +1) when computing empirical probabilities to avoid zero-data instabilities.
- If opponents are noisy, reduce threshold magnitudes (delta thresholds) or increase L.
- You may tune ε to control aggression vs. safety: larger ε = more exploitative, smaller ε = more cautious.
- Keep punishment short so that you don’t lock into low-payoff mutual defection if the rest of the population is cooperative and punishers are only transient.

This strategy is ready to be encoded: it yields a deterministic decision each round given parameters n, k, r and observed history, is adaptive across classes of opponents, and embodies an aggressive exploit-and-punish mindset while limiting self-harm.
'''

description_AGGRESSIVE_88 = '''
Name: Aggressive Opportunist (AggroOp)

Intuition and high-level goals
- Default behavior is aggressive defection: never give away endowment for free.
- Occasionally “probe/lure” when there is clear evidence many opponents are cooperating — but only when there are enough rounds left to exploit any induced cooperation. The probe is a short, calculated investment intended to convert reciprocators into a profitable cooperative streak that I then ruthlessly exploit by defecting.
- Use short, harsh, and deterministic exploitation windows after a successful lure; otherwise revert to permanent defection. Always defect in the final round.
- This strategy maximizes exploitative upside while minimizing exposure to being exploited or to long costly cooperative commitments. It is simple, adaptive (reacts to observed cooperation rates), and robust to a wide range of opponent behaviours.

Parameters internal to the strategy (computed from game parameters n, r, k)
- WINDOW_SIZE W = min(5, max(1, r//10 + 1)) — number of recent rounds used to estimate other-players’ cooperation tendency (keeps the estimate local).
- EXPLOIT_LEN E = min(3, max(1, r//6)) — number of consecutive defect rounds used to exploit a successful lure.
- SAFE_ROUNDS S = 1 — number of final rounds in which the strategy always defects (must include last round); set S >= 1. (Default S = 1.)
- THRESH_COOP_HIGH p_high = 0.60 — threshold fraction of other players cooperating (in the relevant recent round or window) that triggers a lure attempt. (Choice is conservative; implementers may tune this by tournament results; strategy uses the same threshold throughout.)
- MIN_ROUNDS_FOR_LURE Rmin = E + 1 + S — only attempt a lure if at least Rmin rounds remain (one lure round + exploitation window + safe tail).

State variables (maintained across rounds)
- history of actions each round (including observed cooperators per round)
- last_lure_round (integer or null)
- exploit_timer (integer; remaining rounds to run the exploitation defection streak)

Decision rules (natural language)
1. Absolute rules
   - If current round t > r - S: play D (always defect in the final S rounds). In particular, in the last round (t = r) always defect.
   - If r ≤ 3: play D every round (finite small-horizon default to safe aggressive play).

2. Default rule
   - If exploit_timer > 0: decrement exploit_timer and play D (we are in an exploitation streak).
   - Otherwise, default to playing D (aggressive default).

3. Lure/probe rule (exception to the default defect)
   - If there are ≥ Rmin rounds remaining and I am not currently exploiting (exploit_timer = 0), consider a lure:
     - Compute recent cooperation statistics among other players:
       - If t = 1: treat recent cooperation as 0 (no information).
       - Else let coop_others_last = (number of cooperators in round t-1 excluding myself) / (n-1).
       - Optionally also compute coop_others_window = average of the same fraction across last min(W, t-1) rounds.
     - If coop_others_last ≥ p_high OR coop_others_window ≥ p_high (i.e., many others have been cooperating recently), then attempt a lure this round:
       - Action this round: play C (pay the one-round cost to try to convert opponents).
       - Set last_lure_round = t.
   - Otherwise play D.

4. Exploitation activation after a lure
   - The lure occurs at round t = L. After observing the cooperation pattern in round L (which you see before choosing action in L+1), decide:
     - If in round L at least p_high fraction of other players cooperated (i.e., coop_others_in_L ≥ p_high), then we consider the lure successful. In round L+1 start the exploitation: set exploit_timer = E (or E if you want exploitation to include L+1..L+E) and immediately play D (exploit the cooperative group).
     - If the lure fails (others do not reciprocate and coop_others_in_L < p_high), do NOT enter exploitation; revert to default defection and do not attempt another lure until you observe another qualifying recent cooperation pattern.
   - After an exploitation window finishes, return to the default state (exploit_timer = 0) and require a fresh qualifying cooperation signal before any future lure.

5. Forgiveness / reset
   - If multiple consecutive lures fail (e.g., two successive lures where others did not reciprocate), stop attempting lures for the remainder of the game (switch to permanent defection). This prevents oscillation and being repeatedly used as an ATM by conditional cooperators who punish but then exploit.

Edge cases and specific round handling
- First round (t = 1): play D (no history to exploit).
- Last round (t = r): always play D.
- Small horizon (r ≤ 3): always D (the cost of attempting to elicit cooperation cannot be recouped).
- If n = 2 (pairwise PD): the same logic applies. The lure is a single cooperation attempt when the other cooperated previously; since the opponent’s pattern is visible, the same exploit-after-lure mechanism works.
- If r is large and many opponents are stable reciprocators, the strategy will occasionally trade a very small cost (one cooperation) for multiple rounds of high payoff exploitation — aggressive but calculated.
- If opponents respond by punishing aggressively (sustained defection after your defection), lure attempts will fail and the strategist will revert to persistent defection, avoiding long-term losses.

Pseudocode (readable, implementable sketch)

Initialize:
  last_lure_round = null
  exploit_timer = 0

For each round t = 1..r:
  if r <= 3:
    play D
    continue
  if t > r - S:
    play D
    continue
  if exploit_timer > 0:
    exploit_timer = exploit_timer - 1
    play D
    continue

  if t == 1:
    play D
    continue

  # compute recent cooperation fractions among others
  coop_others_last = (coop_count[t-1] - my_action[t-1]) / (n - 1)
  coop_others_window = average over s in max(1,t-W) .. t-1 of ((coop_count[s] - my_action[s])/(n-1))

  rounds_remaining = r - t + 1
  if rounds_remaining >= Rmin and (coop_others_last >= p_high or coop_others_window >= p_high):
    # attempt a lure
    play C
    last_lure_round = t
    continue
  else:
    play D
    continue

After observing the result of the round in which we lured:
  If last_lure_round == L and t == L + 1:
    # before choosing action in round L+1 we know coop_others_in_L
    if coop_others_in_L >= p_high:
      exploit_timer = E   # start exploitation now; L+1 will be D and next E-1 rounds also D
    else:
      # failed lure; do not exploit; if multiple failed lures, stop luring for the rest of the game
      record failed_lure_count += 1
      if failed_lure_count >= 2:
        disable_future_lures = true

Why this is “aggressive”
- Default is defection: the strategy refuses to pay the static cost of cooperating unless there is a clear, recent signal that cooperation is widespread enough to make a short investment worthwhile.
- When others appear cooperative, the strategy does not reciprocate long-term; instead it makes a single, calculated cooperation (the lure) to induce cooperation, then defects repeatedly to extract maximum benefit from reciprocators.
- Punishment for failed lures (stop attempting them) and guaranteed defection in final rounds prevents being trapped by retaliatory dynamics or by finite-horizon unraveling.

Robustness notes
- The strategy relies only on observable history (counts of cooperators each round), game parameters (n, r, k) and its internal counters. It does not require communication or shared conventions.
- The threshold p_high and window sizes are conservative and can be tuned if you know the tournament composition; the default values are intended to avoid being the only cooperator while still allowing profitable exploitation of conditional cooperators.
- The deterministic exploitation window and final-round defection reduce vulnerability to long punishment cycles and ensure simple, predictable behaviour that is hard for others to coerce into sustained mutual cooperation.

Implementation tips
- Record per-round coop counts (including yourself) so the fractions are easy to compute.
- Keep the state variables small (last_lure_round, exploit_timer, failed_lure_count, disable_future_lures).
- Tune W, E, p_high if you observe that the tournament environment contains many naive cooperators or many unforgiving punishers — increase p_high in the latter case, reduce it in the former.

Summary one-liner
Default defect; when recent history strongly indicates many others are cooperating and there are enough rounds left, spend one round cooperating to lure reciprocators, then run a short, decisive defection exploitation window; always defect in the last round and give up lures after repeated failures.
'''

description_AGGRESSIVE_89 = '''
Name: Aggressive Coercive Trigger (ACT)

High-level description (aggressive mindset)
- Be aggressively self-interested: default to defect so you exploit unconditional cooperators and avoid being exploited yourself.
- But be willing to force a high-cooperation regime if the opponents are already reliably cooperative — because a sustained cooperative baseline yields more total gains to exploit later.
- Use credible, long, deterministic punishments when opponents respond to exploitations by reducing cooperation (they “retaliate”): punish to make exploitation costly for them and to enforce disciplined cooperation.
- After punishment, attempt limited forgiveness to restore cooperation if that is possible.
- Always defect in the final round (no future to punish/reward).

The strategy only depends on game parameters (n, r, k) and observed history (players’ actions each previous round). It is fully deterministic.

Tuned parameter rules (computed from r)
- window w = max(1, floor(r/8)) — length of recent history used to estimate opponents’ cooperation rate
- coop_build_length S = max(1, ceil(r/10)) — number of consecutive cooperation rounds when trying to build cooperation
- punish_length P = max(2, floor(r/4)) — length of punishment phase after clear retaliation
- forgive_length F = max(1, floor(r/10)) — number of cooperative “forgiveness” rounds after punishment to test recovery
- coop_threshold T = 0.75 — fraction of opponents cooperating in the window to consider them reliably cooperative
- retaliation_drop delta = 0.20 — drop in opponents’ cooperation rate (between two adjacent windows) interpreted as a retaliatory collapse
- endgame rule: always defect in the last round; if remaining rounds <= P, treat as endgame and defect (punishment would be ineffective)

State variables (maintained across rounds)
- mode ∈ {DEFECT_MODE, COOP_MODE, PUNISH_MODE, FORGIVE_MODE} (initially DEFECT_MODE)
- counters: coop_counter, punish_counter, forgive_counter (integers, initialized to 0)
- last_window_coop: opponents’ cooperation fraction in last observed window (initialized undefined until enough history)

Notation
- t: current round (1..r)
- remaining = r − t + 1 (rounds including current)
- For any history window of rounds s..u, define OppCoopFraction(s..u) = (sum of c_j over j ≠ me and rounds s..u) / ((n−1) × (u−s+1)) — fraction of opponents’ cooperation in that window
- We observe complete action history of all players after each completed round, so these fractions are available.

Decision rules (pseudocode)

Initialization:
- mode ← DEFECT_MODE
- coop_counter ← 0
- punish_counter ← 0
- forgive_counter ← 0
- last_window_coop ← None

Each round t do:
1. Endgame override
   - If t == r: play D (last round always defect); stop.
   - If remaining ≤ P: play D (punishment infeasible near the end); stop.

2. If punish_counter > 0:
   - play D
   - punish_counter ← punish_counter − 1
   - If punish_counter becomes 0: mode ← FORGIVE_MODE; forgive_counter ← F
   - stop.

3. If mode == FORGIVE_MODE:
   - play C
   - forgive_counter ← forgive_counter − 1
   - If forgive_counter == 0:
       - Compute p = OppCoopFraction(max(1, t−w) .. t)  // include current round's observations on next iteration; practically we evaluate after round t
       - If p ≥ T: mode ← COOP_MODE; coop_counter ← S
         else mode ← DEFECT_MODE
   - stop.

4. If mode == DEFECT_MODE:
   - If there is at least w rounds of history (t > w):
       - p = OppCoopFraction(t−w .. t−1)
       - If p ≥ T and remaining > S + P:
           - // opponents are reliably cooperative — attempt to build cooperation
           - mode ← COOP_MODE
           - coop_counter ← S
           - play C (start cooperation build)
           - stop.
   - // default aggressive behavior
   - play D
   - stop.

5. If mode == COOP_MODE:
   - If coop_counter > 0:
       - play C
       - coop_counter ← coop_counter − 1
       - stop.
   - Else (coop_counter == 0): // perform an exploit strike
       - play D  // single-round exploit
       - // after the exploit we wait one round to observe opponents’ reaction; that observation is handled next round:
       - // On the next round (t+1), before any non-punishment decision, compute the two-window comparison below. If it shows a significant drop, enter punishment mode:
       - stop.

6. (Observation-triggered punishment rule; applied at earliest possible point next round)
   - At the start of each round (before acting) if mode ∈ {COOP_MODE, DEFECT_MODE} and there is at least w+1 rounds of history:
       - Let pre_exploit_window = OppCoopFraction(t−1−w .. t−2) // window immediately before last round
       - Let post_window = OppCoopFraction(t−w .. t−1) // window including the last round
       - If pre_exploit_window is defined and (pre_exploit_window − post_window) ≥ delta:
           - // opponents’ cooperation collapsed after our exploit or action → they retaliated; punish hard
           - mode ← PUNISH_MODE
           - punish_counter ← min(P, remaining − 1) // leave last round for endgame rule
           - play D
           - stop.

Notes about timing and observations:
- The only forward-looking decision is the endgame check; all other triggers use past rounds only.
- The post-exploit detection compares two adjacent windows of length w (one immediately before the most recent round, one that ends with the most recent round). On detection of a significant drop (retaliation), the strategy switches immediately to PUNISH_MODE and defects for punish_counter rounds.
- Because the game is simultaneous within each round, the exploit strike (a D in COOP_MODE after S cooperative rounds) is observed by opponents, and their next-round behavior is used to decide if retaliation occurred.

Edge cases and clarifications
- First round (t = 1): t ≤ w, so no window exists; mode is DEFECT_MODE ⇒ play D.
- If r is small so that computed P or S is larger than remaining rounds, the endgame rule forces defection when remaining ≤ P, and P itself is capped by remaining−1 when starting a punishment. This avoids ineffective punishments at the very end.
- All comparisons use fractions; ties (exactly equal to thresholds) are treated as meeting the threshold (≥ T, ≥ delta).
- No randomization: all choices deterministic given history and parameters.

Why this is “aggressive” and robust
- Default defection extracts value from unconditional cooperators and prevents naive exploitation from the outset (aggressive testing).
- If the opponents are credibly cooperative (high p over the window) we attempt to build cooperation (cooperating for S rounds). That enables the group to reach a high-cooperation baseline that we can then exploit.
- We perform a deliberate, deterministic exploitation strike (single D) after a cooperation-building block. If opponents tolerate occasional exploitation (cooperation does not drop), we continue trying to sustain cooperation cycles and exploit periodically — yielding high absolute payoff.
- If opponents respond by reducing cooperation (retaliation), we impose a long punitive defection phase (P rounds). The punishment is long enough (relative to r) to be a credible deterrent and reduce the benefit of trying to punish us or of being exploited.
- After punishment we use limited forgiveness (F rounds of cooperation) to probe whether cooperation can re-emerge and to allow opportunistic re-building of beneficial dynamics. If forgiveness is not reciprocated, we revert to DEFECT_MODE.
- Always defect in the final round to avoid being suckered where no future punishment/reward is possible.

Practical tuning notes (for implementers)
- The numeric choices T = 0.75, delta = 0.20, and the fractions for w, S, P, F are intentionally aggressive: they require a strong majority of opponents cooperating before attempting to build cooperation, and they punish fairly long. Those choices are robust starting points; they can be tuned for specific tournaments (for example, raise T to be more cautious; shorten P if punishments are too costly to you).
- The strategy is entirely defined by (n, r, k) only through r for scheduling; it does not use k or n directly in thresholds to keep it simple and robust across parameter values (k only affects opponents’ incentives in ways captured by their observed behavior). If desired, one can lower exploitation frequency or raise T when k is small (less benefit to cooperation) — but that is tunable.

Summary
- Start aggressive (D). Attempt to coerce and sustain a high-cooperation environment only when opponents are reliably cooperative. Exploit via a deterministic strike; if opponents retaliate, impose a long deterministic punishment, then forgive briefly and probe for renewed cooperation. Always defect in the final round. This yields an aggressive, adaptive, history-dependent policy that is robust to a wide range of opponent behaviors and enforces cooperation via credible, painful punishments.
'''

description_AGGRESSIVE_90 = '''
Name: Calculated Aggressor (CalcAgg)

Short description
- Aggressive, payoff-maximizing repeated-player strategy that almost always defects unless there is a clear, data-driven reason to invest in cooperation because it will pay off in future rounds. It uses short probes to measure opponent responsiveness, computes a simple expected-value condition using the game parameters (k, n, remaining rounds), and then either (a) exploit a sustained cooperative environment by defecting (free-riding), or (b) invest in seeding cooperation only when estimated future gains exceed the one-round cost. It always defects in the last round and retreats to permanent defection if cooperation collapses after exploitation.

Key ideas (why this is “aggressive”)
- Defection gives a constant immediate advantage per round of delta = 1 - k/n > 0 (because k < n). CalcAgg exploits that fact: it will not pay the definite immediate cost of cooperation unless the estimated future benefit (from getting others to cooperate more) is large enough to overcome the cost. It probes minimally, exploits cooperative groups by free-riding, and avoids costly, altruistic punishments.

Notation
- t: current round (1..r)
- remaining_rounds = r - t + 1 (including current)
- history: full observed matrix of past actions; for each past round s < t we observe a vector of cooperators c_j^s ∈ {0,1} for each player j
- m_s = total cooperators in round s (including me if I cooperated)
- m_others_s = cooperators among other players in round s (exclude me)
- n_others = n - 1
- baseline_group_coop = average of m_others_s / n_others over observed rounds
- response_to_my_coop (estimator) = average change in others' cooperation rate in rounds after rounds where I cooperated, relative to baseline. (See pseudocode for computation.)
- immediate_cost_of_cooperating = delta = 1 - k/n  (this is the per-round selfish penalty for cooperating)

Decision rules — natural language
1. Last round / endgame
   - If t == r: play D (defect). Always.

2. Short games
   - If r ≤ 3: play D every round. (No time to recover cooperation investments; aggressive default.)

3. Probing phase (minimal, to estimate responsiveness)
   - If insufficient data exists to estimate how others respond to my cooperation (fewer than H_min = 3 transition-observations), perform at most one inexpensive probe early to collect data:
     - If t == 1 and r ≥ 4 and k/n ≥ 0.5: play C (one probe to gauge reciprocators).
     - Otherwise play D.
   - (Rationale: only probe when there is time and the multiplier is large enough that future returns could plausibly outweigh the immediate cost.)

4. Compute empirical statistics (every round after you have any history)
   - baseline_group_coop = mean over past rounds s of (m_others_s / n_others).
   - response_to_my_coop = mean change in others' cooperation rate in round s+1 after rounds s where I cooperated, relative to baseline_group_coop.
     - If you have few observations (< H_min), treat response_to_my_coop as unknown.

5. Expected-value test: decide whether investing in cooperation (to seed future cooperation) is worth it
   - Estimated per-round increase in my payoff if cooperation succeeds = response_to_my_coop × (k/n) × n_others.
     - Explanation: each additional cooperator among others increases my share by k/n; response_to_my_coop estimates how many extra other cooperators (on average) my cooperating helps create per subsequent round.
   - Multiply by expected remaining rounds of benefit ≈ (r - t) (only future rounds, not current) to get expected future benefit.
   - If estimated_future_benefit > immediate_cost_of_cooperating (delta), then cooperate this round to seed/build cooperation.
   - Otherwise, defect.

   - When response_to_my_coop is unknown or unreliable:
     - Use heuristic: if k/n ≥ 0.75 and remaining_rounds ≥ 4, allow a limited attempt to build cooperation (cooperate for 1–2 rounds); otherwise default to defect.

6. Exploitation mode (if environment is already cooperative)
   - If baseline_group_coop (proportion of other players who cooperated) is high in recent rounds (e.g., ≥ 60%), exploit by defecting this round (free-riding).
   - While exploiting, monitor the group cooperation level:
     - If group cooperation falls dramatically after exploitation (drop > 20 percentage points within 1–2 rounds), stop exploiting and switch to permanent defection (retreat) — do not waste payoff on punishing.
     - Otherwise continue exploiting. To avoid destroying cooperation completely (and to keep some cooperators from abandoning entirely), perform a tiny maintenance cooperation at low frequency: once every M_maint rounds (e.g., M_maint = 10) or with small probability p_maintain = 0.05 if remaining_rounds large. This is optional; set p_maintain = 0 for pure exploitation.

7. Retreat / collapse handling
   - If, after a period of attempted cooperation or exploitation, baseline_group_coop falls below a low threshold (e.g., 20%), assume cooperation is dead and permanently defect for all remaining rounds except possibly one late probe if many rounds remain and k/n is very high.

8. Tie-breaking / safety
   - When computation is ambiguous, choose D (defect). Aggressive preference for sure immediate payoff.

Pseudocode

Initialize:
  H_min = 3
  coop_probe_done = false
  mode = "undecided"  # possible modes: undecided, exploit, seed, retreat

Each round t:
  if t == r:
    action = D
    return action

  if r <= 3:
    action = D
    return action

  Compute history statistics:
    For each past round s < t, record m_others_s
    baseline_group_coop = average_s (m_others_s / n_others)
    Compute response_to_my_coop:
      Let S_mycoop = {s < t : I cooperated in s}
      If |S_mycoop| >= H_min:
        For each s in S_mycoop, get others_coop_next = (m_others_{s+1} / n_others) if s+1 < t else unavailable
        Estimate mean_others_after_mycoop = mean of available others_coop_next
        response_to_my_coop = mean_others_after_mycoop - baseline_group_coop
      Else:
        response_to_my_coop = unknown

  immediate_cost = delta = 1 - k/n
  remaining_rounds_excluding_current = r - t

  # Early probe logic
  if not coop_probe_done and t == 1 and r >= 4 and k/n >= 0.5:
    coop_probe_done = true
    action = C
    return action

  # If we already have a clear "exploit" signal: many others are cooperating
  if baseline_group_coop >= 0.60:
    # Exploit: free-ride
    action = D
    # monitoring: after playing D check collapse: (handled next round by observing drop)
    return action

  # If we have a response estimate and it suggests seeding cooperation will pay off:
  if response_to_my_coop is known:
    expected_extra_others_per_round = max(0, response_to_my_coop) * n_others
    expected_future_benefit = expected_extra_others_per_round * (k/n) * remaining_rounds_excluding_current
    if expected_future_benefit > immediate_cost:
      action = C  # invest now to seed cooperation
      return action
    else:
      action = D
      return action

  # If response unknown: use heuristics
  if response_to_my_coop is unknown:
    if k/n >= 0.75 and remaining_rounds_excluding_current >= 3:
      # gamble on building cooperation in long/high-k games
      action = C
      return action
    else:
      action = D
      return action

Monitoring and mode updates (implicit, done after round when history updated):
  - After each round, recompute baseline_group_coop.
  - If recently exploited and baseline_group_coop has dropped more than 0.20 compared to before exploitation, set mode = "retreat" and from then on action = D (except last-round rule already handles t==r).
  - If baseline_group_coop becomes persistently high again and response_to_my_coop suggests we can free-ride profitably, return to exploit mode.

Parameters and default thresholds (tunable)
- H_min = 3 (minimum observations to trust response estimate)
- probe condition: do at most one initial probe when r >= 4 and k/n >= 0.5
- baseline_group_coop threshold for exploitation: 0.60 (60%)
- collapse threshold: 0.20 drop triggers retreat
- heuristic cooperation threshold when unknown: k/n ≥ 0.75 and at least 3 future rounds
- maintenance cooperation frequency (optional): p_maintain = 0.05 or once every M_maint = 10 rounds

Edge cases summary
- First round: possibly one probe (C) only if r ≥ 4 and k/n ≥ 0.5; otherwise D. Reason: probe only when there is time and multiplier is large enough.
- Last round: always D.
- Very short games (r ≤ 3): always D.
- If you detect sustained high cooperation among others: defect (exploit) and monitor. If cooperation collapses because of your exploitation, retreat to permanent D and stop trying to punish or rebuild unless parameters change and response estimates indicate rebuilding is profitable.
- If you gather reliable evidence that cooperating yields persistent increases in others’ cooperation and the estimated future benefit exceeds the cost delta, cooperate (seed) — but only when the estimate supports a clear net benefit.

Behavioral summary by phase
- Default: defect (aggressive).
- Probe (rare): 1 early C to test responsiveness when parameters make it plausible.
- Seed: only if data-driven expected-value test says cooperation will pay off over remaining rounds.
- Exploit: free-ride on high baseline cooperation by defecting each round, optionally low-frequency maintenance cooperations to avoid collapse.
- Retreat: if exploitation causes cooperation to collapse, stop cooperating permanently — do not pay costs to punish others.

Why this is robust
- Uses only observed history and game parameters.
- Makes data-driven decisions (response estimate × k/n × remaining rounds) rather than trusting norms.
- Minimizes wasted cooperative costs by requiring an explicit expected-value condition before cooperating.
- Guards against retaliation by monitoring and retreating quickly when exploitation causes cooperation to fall.
- Respects last-round logic (always defect) to avoid being exploited in endgame.

Implementation notes for the tournament
- Keep counts for transitions to compute response_to_my_coop efficiently: for each round s where I played C, record how many others cooperated in s+1 (if available); baseline is simply average others' cooperation in all rounds. Use simple exponential smoothing to stabilize estimates if desired.
- Tune thresholds (0.60, 0.20, 0.5, 0.75, H_min) on past tournament data if tuning is allowed; defaults above are conservative aggressive choices.

This strategy is explicitly aggressive: it almost always defects when in doubt, takes one controlled gamble to find exploitable cooperators only when game parameters justify it, free-rides on cooperative groups, and quickly retreats when the environment punishes exploitation. It requires only parameters and the public history, and is adaptive to a wide range of opponent behaviours.
'''

description_AGGRESSIVE_91 = '''
Name: Aggressive Exploiter with Selective Punishment (AESP)

Short description (mindset)
- Aggressive: prioritize my own absolute and relative payoff. I will exploit any stable cooperation I can detect, punish exploitive opponents, and avoid being sucked into repeated one-sided cooperation. I will only cooperate when it is a deliberate probe or a short, targeted attempt to form a profitable coalition I can then exploit. I always defect in the last rounds where future punishment is impossible.

Notation and observations available
- n, r, k are known.
- At each round t I observe every player's action c_j,t (1 if C, 0 if D) for all past rounds.
- Let t be current round (1..r).
- Let H be the history of contributions up to t−1.

Tunable internal parameters (set as defaults; can be tuned for different tournaments)
- T_det = min(5, max(1, floor(r/6))) — detection window length (recent rounds used to classify players).
- p_high = 0.60 — fraction of others cooperating in the recent window considered “too cooperative” (exploitable).
- p_low = 0.25 — fraction considered “nearly all defect” (no reason to cooperate).
- exploit_burst_max = min(4, max(1, floor(r/10))) — max consecutive exploitation defects after a successful probe.
- base_punish_len = 2 — initial punishment length when I am exploited.
- final_phase_start = max( r - 3, 1 ) — from this round onward, always defect (final-stage defection).

High-level rules
1. Last-round logic: If t >= final_phase_start, play D. (When little or no future, always defect.)
2. First-round probe: In round 1 cooperate (C). This is a short, simple probe to detect cooperators to exploit later.
3. Default: defect (D) unless a targeted cooperation or probe is triggered by the adaptive rules below.
4. Opportunistic exploitation: If recent group cooperation among others is high (≥ p_high), exploit them by defecting and staying in exploit mode for up to exploit_burst_max rounds (or until group cooperation drops). This extracts immediate advantage from cooperators.
5. Targeted coalition attempt: If group cooperation is intermediate (between p_low and p_high) but there exists a nontrivial subset of players who have cooperated consistently in the last T_det rounds (i.e., “reliable cooperators”), attempt a short cooperation to test whether a coalition can be formed — then exploit if they keep cooperating.
6. Punishment: If I cooperated and at least one other player defected that round (they exploited me), enter punishment: defect for a punishment length that increases with the number of times I’ve been exploited (start with base_punish_len and increase).
7. Forgiveness and re-testing: After punishment/pull-back, run short probes (cooperate 1 round) occasionally to test whether cooperators reappear; if so and they look reliable over T_det, re-enter exploitation; otherwise stay defecting.

Detailed decision rules and pseudocode

State variables maintained:
- exploit_mode_counter: remaining rounds to remain in exploit mode (initial 0).
- punish_counter: remaining rounds to remain in punishment mode (initial 0).
- times_exploited: count of how many times others defected while I cooperated (initial 0).
- last_probe_round: the last round when I cooperated as a probe.

On entering round t (1..r):
1. If t >= final_phase_start: play D and continue. (No more cooperation in endgame.)
2. If punish_counter > 0:
   - play D
   - punish_counter -= 1
   - continue to next round
3. If exploit_mode_counter > 0:
   - play D
   - exploit_mode_counter -= 1
   - After playing, check if group cooperation (others) in the most recent T_det rounds has dropped below p_high; if so set exploit_mode_counter = 0.
   - continue
4. (Otherwise) Compute recent statistics using last min(T_det, t-1) rounds (if t=1 there is no history):
   - If t == 1:
       play C (first-round probe), set last_probe_round = 1, continue.
   - Let T_obs = min(T_det, t-1).
   - For each other player j ≠ me compute recent_coop_rate_j = (Σ_{s=t-T_obs to t-1} c_j,s) / T_obs.
   - group_recent_rate = average of recent_coop_rate_j over j ≠ me = (Σ_{j≠me} Σ_{s=t-T_obs to t-1} c_j,s) / ((n-1)*T_obs).
   - full_last_round_coop_count = Σ_{j=1..n} c_j,t-1 (include me) if t>1; others_last_round = full_last_round_coop_count − my_last_action.
5. If I cooperated in the immediately preceding round (t−1) and in that round at least one other player defected while I cooperated:
   - times_exploited += 1
   - punish_counter = min( r - t + 1, base_punish_len + times_exploited )  (cap to remaining rounds)
   - play D this round (already handled above if punish_counter > 0)
6. Opportunistic exploitation detection:
   - If group_recent_rate ≥ p_high:
       - exploit_mode_counter = min( exploit_burst_max, r - t )  (stay in exploit mode for a few rounds)
       - play D (exploit)
       - continue
7. Targeted coalition attempt:
   - If p_low < group_recent_rate < p_high:
       - Identify reliable_actors = { j ≠ me : recent_coop_rate_j == 1.0 } (cooperated every observed round).
       - If size(reliable_actors) ≥ 1:
           - Perform a short cooperation probe: play C this round, set last_probe_round = t.
           - After the round observe if reliable_actors cooperated; if they continued cooperating while I cooperated then immediately enter exploit_mode_counter = exploit_burst_max next round(s).
           - continue
       - Else:
           - play D
           - continue
8. Low cooperation environment:
   - If group_recent_rate ≤ p_low:
       - play D (no point cooperating)
       - continue
9. Safe fallback:
   - play D

Notes about probing and exploitation:
- A “probe” is a single round C to test who really reciprocates.
- If a probe yields others cooperating but I see persistent cooperation from others over T_det rounds, that indicates exploitable cooperators; I then switch to defecting for exploit_burst_max rounds to capture higher immediate payoff.
- If probes reveal exploitation against me (others defect while I cooperated), punish_counter increases and I defect for an extended period.

Edge cases and special cases
- Small r (short repetition): T_det will be small (1 or 2). final_phase_start may fall early. If r ≤ 3 the strategy mostly defects except the first-round probe; this is appropriate because endgame incentives dominate.
- First round: always play C (probe). This baits pure cooperators and gives information. If you prefer even more aggressive short-term payoff you can change this to D; but the given aggressive variant uses a one-round probe to exploit naive cooperators.
- Last rounds (t ≥ final_phase_start): always defect. This avoids being burned by endgame exploitation because future punishment opportunities are limited.
- If all players never cooperate (group_recent_rate=0) the strategy just defects (no wasted cooperation).
- If a coalition of reliable cooperators exists, the strategy will try to exploit them but will punish if they retaliate or other players exploit me.

Why this is aggressive and robust
- Aggressive: default is defection; the strategy actively looks for opportunities to extract surplus from cooperating opponents by defecting in bursts (exploit_mode). It escalates punishment if I am exploited. It uses probes to identify exploitable cooperators rather than trusting arbitrary patterns.
- Adaptive: classifies players over a sliding window, reacts to high cooperation by exploiting, reacts to exploitation by punishing for increasing lengths, and retests after punishment to find new opportunities. The parameters (T_det, p_high, p_low) are chosen to be conservative but can be tuned.
- Robust: works against pure defectors (I defect, so I don’t lose), conditional cooperators (I detect and exploit them), retaliators (I punish and avoid long-term losses), and mixed/complex strategies (ongoing probing and short exploitation/punishment cycles adapt to whatever emerges).

Implementation hints
- Keep counts and sliding-window sums for each player to make recent_coop_rate efficient.
- All thresholds are deterministic so the algorithm is reproducible; you can add small randomness (e.g., occasionally choose C with tiny probability when in fallback) if tournament rules reward unpredictability, but pure determinism is simpler and still aggressive.
- Tune exploit_burst_max and p_high in pilot runs against expected population mixes: higher exploit_burst_max is more aggressive but risks prolonged retaliation from reciprocators.

Summary (one-sentence)
Start by probing with a single cooperation, then otherwise mostly defect; opportunistically exploit reliably cooperative opponents in short bursts, punish any players who exploit your cooperation with escalating defection, and always defect in the final rounds where future punishment is infeasible.
'''

description_AGGRESSIVE_92 = '''
Name: Exploitative Conditional Punisher (ECP)

Intuition (aggressive mindset)
- Default is to defect and extract surplus from other players’ cooperations (free-ride).
- Only give cooperating groups the minimum, calibrated rewards to keep them cooperating long enough to be exploited.
- If cooperating behavior declines or a sustained defection signal appears, respond with long, harsh punishment (extended defection) to protect future earnings and deter attempts to “cheat” the exploiter.
- Do not rely on goodwill or communication; use observable histories and a little randomness to avoid being predictably exploitable.

Parameters (computed from game inputs n, k, r; all final rule values depend only on n,k,r and observed history)
- W = min(10, max(1, r-1)) — lookback window for short-term statistics.
- target_high = clamp(0.55 + 0.25 * (k-1)/(n-1), 0.55, 0.8) — fraction of opponents cooperating that we treat as a «cooperating group» (increases with k).
- target_low = clamp(0.25 + 0.15 * (k-1)/(n-1), 0.2, 0.4) — fraction low enough to trigger punishment.
- maintain_prob = clamp(0.15 + 0.65 * (k-1)/(n-1), 0.15, 0.8) — when opponents are cooperating heavily, probability we cooperate occasionally to sustain that cooperation (higher when public good is more valuable).
- probe_prob = clamp(0.03 + 0.07 * (k-1)/(n-1), 0.03, 0.1) — small probability to probe cooperation when situation is ambiguous.
- punish_len = min(r, max(3, ceil(r/4))) — length of a punishment phase when we trigger a punishment.
- max_defect_gap = max(1, ceil(1/maintain_prob)) — maximum consecutive defect rounds in an exploit cycle before we cooperate once to “refresh” cooperating players’ incentives.
- eps = 0.02 — tiny randomization to avoid being fully deterministic.

Data kept in history (available from game rules)
- For each previous round t, we see full vector of actions and can compute:
  - coop_count[t] = number of players (including me) who cooperated in round t
  - coop_count_excl_me[t] = coop_count[t] - (1 if I cooperated in that round else 0)
- Derived online:
  - For each opponent j we can compute their cooperation frequency over last W rounds, if desired (optional, more fine-grained targeting).

High-level decision rules (plain language)
1. First round (t = 1): defect. Aggressive default.
2. Last round (t = r): defect. No future to sustain cooperation — exploit.
3. If currently in a punishment phase (we set a counter punish_remaining > 0): defect and decrement punish_remaining each round.
4. Else compute p = average fraction of opponents cooperating over the last W rounds (or simply use last round coop_count_excl_me[t-1] / (n-1) if W=1).
   - If p >= target_high:
     - Exploit mode: defect most of the time to free-ride.
     - But to keep cooperators from collapsing, cooperate with probability maintain_prob OR cooperate deterministically if I've defected max_defect_gap rounds in a row (cooperate once then resume defecting).
   - Else if p <= target_low:
     - Punish mode: enter a punishment phase of length punish_len (set punish_remaining = punish_len) and defect now.
   - Else (target_low < p < target_high):
     - Ambiguous mode: probe with probability probe_prob (cooperate occasionally to test whether a stable cooperative cluster can be formed); otherwise defect.
5. Additional aggressive safeguard: if there is a sudden drop in cooperation greater than delta = 0.25 (i.e., coop_count last round < average coop_count over W rounds minus delta*(n)), treat as a defection shock and immediately enter punishment phase for punish_len rounds.
6. Randomization: add eps noise to randomized choices so behavior is not exactly deterministic.

Why this is aggressive and robust
- Default defection and exploitation: whenever many opponents cooperate we mostly defect to maximize one-shot gain (free-riding).
- Calibration with k and n: thresholds and maintenance probability increase when the public good is more valuable (higher k) — when cooperators are more likely to respond to small rewards, we are willing to pay less often to keep them cooperating.
- Harsh, long punishment phases protect against players who attempt to exploit us or hang onto unstable cooperation; punishment is long enough to deter opportunistic returns to cooperation.
- Occasional cooperation (maintenance and probes) prevents immediate collapse of cooperative clusters that would otherwise stop generating exploitable surplus.
- First and last round defection avoids being exploited in one-shot incentives and prevents endgame exploitation of our cooperative moves.
- Uses only observable history (no communication), scales with r and n, and is randomized slightly to avoid being perfectly predictable.

Pseudocode

Inputs: n, k, r
State variables:
- t = current round number (1..r)
- punish_remaining = 0
- consecutive_defects = 0  // counts our consecutive defects in current exploit cycle
- history: list of past coop_count values (size t-1)
- compute W, target_high, target_low, maintain_prob, probe_prob, punish_len, max_defect_gap as above

Function decide_action(t, history):
  if t == 1:
    action = D
    consecutive_defects = 1
    return action

  if t == r:
    action = D
    return action

  if punish_remaining > 0:
    punish_remaining -= 1
    action = D
    consecutive_defects += 1
    return action

  // compute short-run opponent cooperation fraction p over last W rounds
  use_W = min(W, len(history))
  if use_W == 0:
    p = 0.0
  else:
    // average over last use_W rounds: exclude our own action each round
    sum_fraction = 0
    for s in 1..use_W:
      coop_total = history[-s]    // coop_count at round t-s (includes me)
      my_contrib = ?             // we know our own past actions, so compute
      coop_excl_me = coop_total - my_contrib
      sum_fraction += coop_excl_me / (n-1)
    p = sum_fraction / use_W

  // shock detection (sudden drop)
  avg_coop_last_W = (if use_W>0 then sum(history[-use_W:]) / use_W else 0)
  if use_W > 0 and history[-1] < avg_coop_last_W - 0.25 * n:
    punish_remaining = punish_len
    action = D
    consecutive_defects += 1
    return action

  if p >= target_high:
    // exploit: mainly defect, sometimes cooperate to maintain cooperation
    if consecutive_defects >= max_defect_gap:
      action = C   // one maintenance cooperation to refresh incentives
      consecutive_defects = 0
      return action
    else:
      // randomized maintenance
      if rand() < (maintain_prob - eps):
        action = C
        consecutive_defects = 0
      else:
        action = D
        consecutive_defects += 1
      return action

  else if p <= target_low:
    // group is low-cooperating => punish
    punish_remaining = punish_len
    action = D
    consecutive_defects += 1
    return action

  else:
    // ambiguous zone: probe occasionally
    if rand() < (probe_prob - eps):
      action = C
      consecutive_defects = 0
    else:
      action = D
      consecutive_defects += 1
    return action

Notes on implementation details
- "rand()" is uniform [0,1); eps is small noise to avoid pure determinism.
- You must keep track of your own past moves to compute coop_excl_me in historical rounds.
- punish_remaining persists across rounds; a new shock/punish trigger resets it.
- The numeric clamp function used above ensures parameters stay in reasonable ranges for extreme n,k values.
- If one prefers a purely deterministic variant, replace the probabilistic choices with deterministic rules (e.g., cooperate every max_defect_gap-th round in exploit mode and probe every M rounds), but small randomization is recommended to avoid being exploited by highly adaptive opponents.

Edge cases
- Very small r (r=2 or 3): window W reduces; punish_len is clamped so punishment is meaningful but not longer than the game.
- If k is very close to 1 (public good almost worthless), maintain_prob and probe_prob become small; ECP behaves nearly pure-defection.
- If opponents are naive cooperators who always cooperate, ECP will mostly defect and occasionally cooperate to keep them cooperating — it will extract maximum surplus.
- If opponents are firm retaliators (grim-type), ECP’s occasional cooperations might be needed to avoid triggering permanent group collapse; if retaliation is immediate and strong, ECP will detect that via p dropping and will switch to punishment to avoid long-run losses from trusting them.
- If many opponents are switching, the shock rule catches sharp drops and triggers long punishment.

Summary
ECP is aggressively exploitative: it defects by default, free-rides on cooperating opponents, punishes sharply and for a long time when cooperation collapses or players defect unexpectedly, and uses minimal, calibrated cooperation as an instrument to sustain exploitable cooperation when that is profitable. All decisions depend only on n, k, r and observed history; no assumptions about shared norms or communication are required.
'''

description_AGGRESSIVE_93 = '''
Strategy name: Aggressive Exploitative Defector (AED)

Intended mindset: maximize my own total payoff regardless of others’ welfare; aggressively exploit any players who keep contributing; never tolerate being exploited; do not attempt to “build” cooperation unless a clear, calculable long-run gain is likely. The safe default is defection every round (which is also the subgame-perfect prediction for finite r), but AED adds lightweight, history-based detection to (a) identify exploitable cooperators and (b) avoid being lured into cooperating by punishers or by naive reciprocity traps.

High-level rules
- Default action: Defect (D).
- Never cooperate in the last round.
- Use recent history to detect two types of opponent behaviour:
  - Exploitables: players who cooperate often even when I defect.
  - Retaliators: players who reduce cooperation sharply after I (or the group) defects.
- If a stable exploitable subgroup exists, continue defecting to harvest the exploitation payoff every round.
- If many opponents are retaliators (they respond to my defection by stopping cooperation), remain defecting (don’t “appease”) because cooperating is dominated in single rounds and fragile for finite horizons.
- Use an occasional, very-low-probability probe only if there is evidence that establishing mutual cooperation would give a calculable expected benefit larger than the guaranteed short-term loss — otherwise never cooperate.

Concrete decision rules (natural language)
1. Initialization:
   - Set a sliding-window size W (e.g., W = min(10, r−1)) for recent-history statistics.
   - Set thresholds:
     - G_exploit (group exploitability threshold), e.g. 0.6
     - P_exploit (per-player exploitability threshold), e.g. 0.75
     - R_retaliate (retaliation detection change), e.g. a drop > 0.25 in recent cooperation rate after my defection.
   - Set probe probability p_probe very small (e.g., 0.01) — optional; recommended p_probe = 0 for maximum aggressiveness.
   - Set forgiveness window F if probes are used (e.g., F = 3).

2. First round (t = 1):
   - Play D.

3. For each subsequent round t (2 ≤ t ≤ r):
   - If t == r (last round): play D (never cooperate on the last round).
   - Compute for each other player j:
     - coop_j = fraction of rounds in [max(1, t−W) .. t−1] where j played C.
     - If you want to detect retaliators, also compute coop_j_before and coop_j_after relative to the first round you defected — but because AED defects from round 1, detect role by checking drops after any round in which the group experienced a change you forced; see the pseudocode for specifics.
   - Compute group_coop = average_j coop_j (average over all other players).
   - Identify exploitable players:
     - exploitable_set = { j | coop_j ≥ P_exploit }.
   - Identify mass exploitability:
     - If group_coop ≥ G_exploit OR |exploitable_set| ≥ ceil((n−1) × 0.5) then mark group as “exploit-rich”.
   - Identify retaliators:
     - For each j track whether coop_j dropped by R_retaliate (or more) in the W-window immediately after rounds where the group-level cooperation fell and I was defecting; if a significant fraction (say ≥ 40%) of players show such drops, mark group as “retaliatory”.
   - Decision:
     - If group is exploit-rich:
         - Play D (continue to exploit). Never cooperate when exploit-rich because defect strictly dominates cooperation in every single round and exploitation gives higher immediate payoff.
     - Else if group is marked retaliatory:
         - Play D (do not appease). Retaliators will lower their cooperation; appeasing by cooperating is likely to be transiently costly and will not be reliably rewarded in a finite-horizon context.
     - Else (no strong exploit-rich signal and no mass retaliation):
         - If p_probe > 0 and a random draw < p_probe:
             - Play C as a probe for F rounds (cooperate for F successive rounds) to test whether a stable cooperative cluster can form that would produce long-run gains greater than the immediate cost. If probing, observe whether cooperators reciprocate; abort the probe (return to D permanently) if the group exhibits retaliation or if my net payoff over the probe so far is negative relative to always-defect baseline.
         - Otherwise play D.

4. Post-round update:
   - Update coop_j and group_coop statistics and retaliation detections for use in the next round.

Rationale and aggressive posture
- Defection is the strictly dominant single-round action (for any expected opponent-cooperation level), and in finite repeated play backward induction implies defection every round. AED embraces that aggressively: it defects by default and in the last round for sure.
- But tournaments often include naive or fixed cooperators; AED actively detects such exploitation opportunities and never misses the chance to defect when others keep contributing.
- AED will not be seduced by conditional strategies that try to establish cooperation via temporary sacrifices — cooperation is costly every round to the individual (given k/n < 1), and without trustworthy long-run guarantees AED will refuse to sacrifice guaranteed gains. A tiny, controlled probe is permitted only if there's a clear, calculable chance of net benefit; otherwise probes are disabled.
- AED punishes no one via costly mutual defection out of “fairness”: it simply withholds cooperation. That avoids altruistic punishments which hurt the aggressor’s own payoff.

Pseudocode (compact)

Parameters:
  W = window size (e.g. min(10, r-1))
  P_exploit = 0.75
  G_exploit = 0.6
  R_retaliate = 0.25
  retaliation_fraction_threshold = 0.4
  p_probe (recommended 0 for pure aggression; set small >0 to probe)
  F = probe length (if probes enabled)

State:
  history of all players’ actions
  for each j: coop_j over sliding window

Algorithm for round t:
  if t == 1: play D; record
  else if t == r: play D; record
  else:
    compute coop_j over last W rounds for all j ≠ me
    group_coop = average_j coop_j
    exploitable_set = { j | coop_j ≥ P_exploit }
    if group_coop ≥ G_exploit or |exploitable_set| ≥ ceil((n−1)*0.5):
      play D
    else:
      compute retaliatory_count = number of j with recent drop in coop_j ≥ R_retaliate
      if retaliatory_count ≥ retaliation_fraction_threshold * (n−1):
        play D
      else:
        if random() < p_probe:
          begin probe: play C for up to F rounds unless immediate signs of retaliation (then abort and return to D)
        else:
          play D

Edge cases and clarifications
- First round: always D. This is both aggressive and conservative — it prevents being exploited in first-round by unconditional cooperators.
- Last round: always D (never cooperate in round r).
- Finite horizon r known: use it — do not try to build cooperation near the end.
- If p_probe = 0, AED is deterministic (fully aggressive): D every round.
- If you expect many naive ALL-COOP opponents who never adapt, AED will exploit them reliably by defecting every round and obtain the maximal exploitation gain.
- If you encounter many retaliators who punish defectors by withholding cooperation, AED still defects (never sacrifices) — it prefers steady, guaranteed payoffs to risky spells of cooperation that may be reversed by backward induction or exploited.
- If tournament involves extremely long or infinite horizons and reciprocity is common, the operator may choose a small p_probe > 0 to attempt to elicit mutual cooperation with careful cost/benefit calculations; but in finite and adversarial tournaments the correct aggressive setting is p_probe = 0.

Parameter tuning suggestions
- For maximum aggressiveness and safety: W = 5–10, p_probe = 0, P_exploit = 0.75, G_exploit = 0.6.
- If you want slightly more opportunistic behavior versus sophisticated reciprocators: set p_probe = 0.01, F = 2–4 and monitor probe outcomes strictly; abort on the first sign of coordinated retaliation.

Summary
- AED is an aggressive, simple, robust strategy: defect by default and in the last round; use lightweight statistics to detect exploitable cooperators and keep exploiting them; ignore attempts to “soften” you unless a well-justified probe indicates a long-run net benefit. This minimizes being exploited and maximizes short-term and tournament payoffs against a wide variety of opponent behaviours.
'''

description_AGGRESSIVE_94 = '''
Name: Aggressive Reciprocal Exploiter (ARE)

Short description
- ARE is an aggressive, history-dependent policy that (1) probes briefly for reciprocators, (2) tries short recruitment to establish cooperation when it looks fruitful, (3) aggressively exploits rounds when many others cooperate, and (4) responds to defection with fast, harsh punishment (longer-lasting defection) to discourage others from staying exploitive. It always defects in the final rounds (endgame) to avoid being suckered. All decisions are deterministic functions of n, r, k and the full action history.

Design principles (why this is aggressive and robust)
- Exploit cooperators: whenever the recent history shows many cooperators, ARE defects to capture the free-rider payoff.
- Hunt for reciprocators cheaply: a very short probe/recruit phase attempts to identify players who will increase cooperation after ARE cooperates; if reciprocators appear, ARE uses them to sustain exploitable cooperation.
- Punish quickly and for a meaningful duration when exploitation or breakdown occurs, to deter persistent defectors.
- Minimize risk in the known-finite horizon: always defect in the final round(s).
- Parameters are scaled by r (rounds) so the strategy adapts whether the tournament is short or long.

Notation
- t: current round (1..r).
- c_j(t): action of player j in round t (1 for C, 0 for D). You observe these for all j after each round.
- my_action(t): ARE's own action at round t.
- coop_count(t) = Σ_{j=1..n} c_j(t) (includes ARE's action once you see it; when computing before choosing at t you use history up to t-1).
- coop_frac(t) = coop_count(t) / n
- For any window W, coop_frac_avg(T1..T2) = mean of coop_frac over rounds T1..T2.
- For each opponent j maintain coop_rate_j(W) = fraction of times j cooperated in the last W rounds (or in full history so far if fewer than W rounds exist).

Derived strategy parameters (deterministic given n,r,k)
- endgame_rounds E = min(2, r-1). (Always defect on the last E rounds. If r=2 then E=1 and final round is round 2.)
- probe_rounds P = 1. (Single cheap probe in round 1.)
- recruit_window W_rec = min(5, max(1, floor(r/6))). (How long ARE will try to “recruit” cooperation.)
- exploit_threshold θ = 0.60. (If ≥ 60% cooperated last round, that round was highly cooperative; ARE will exploit next round.)
- sustain_threshold φ = 0.40. (If recent average cooperation is at or above 40% across a short window, attempt recruitment rather than unconditional defection.)
- punishment_length L_pun = min( max(2, floor(r/8)), r-1 ). (Length of punishment after detecting persistent anti-social behaviour; at least 2 rounds.)
- persistent_defector_rate α = 0.30. (If a player’s coop_rate_j over last W_pers rounds < α, classify them as persistent defector.)
- history_window_for_pers W_pers = min(8, max(1, floor(r/4))).

High-level phase logic
- Endgame: For t > r - E, always play D.
- Probe (t = 1): Play C exactly once to reveal how many reciprocators exist (cheap information).
- General rounds (1 < t ≤ r - E):
  - If coop_frac(t-1) ≥ θ: the last round was highly cooperative — play D to exploit.
  - Else if recent average coop_frac over last W_rec rounds ≥ φ: attempt cooperative recruitment — play C for up to W_rec rounds unless exploited (see punishment triggers).
  - Else: play D (default safe action).
- Punishment triggers: If we detect targeted or persistent defection patterns (see detection rules below), immediately switch to a punishment mode: play D for L_pun rounds (or longer if persistent behaviour continues). After punishment, revert to the general logic.
- Always update opponent statistics and punish again if bad behaviour persists.

Detection rules (how to decide punishment)
- Immediate exploitation detection: If in a round t' there was high cooperation (coop_frac(t') ≥ θ) but some players who had been cooperating previously defected and remain defecting in a persistent way (their coop_rate_j over last W_pers < α), then we treat the group as having exploiter(s) and enter punishment.
- Recruitment failure detection: When ARE attempts recruitment (cooperates to stimulate cooperation) and the group cooperation does not rise (or falls) across the recruitment window, classify as failed recruitment and switch to defection/punishment.
- Persistent defector list: At any time mark players j as persistent defectors if coop_rate_j(W_pers) < α. Their continued presence increases punishment likelihood or punishment duration (punishment can be extended while persistent defectors remain).

Pseudocode

Inputs: n, r, k, history of all players' actions up to round t-1.
State variables: punishment_until_round (initial 0), last_recruit_start (initial 0)

On each round t:
  1. If t > r - E: // Endgame
       play D; return

  2. If t ≤ P: // Probe round(s)
       // Round 1: cheap probe to test reciprocators
       play C; return

  3. If t ≤ punishment_until_round:
       play D; return

  4. Compute coop_frac(t-1) from history.
     Compute recent_avg = average of coop_frac over rounds max(1, t-1 - (W_rec-1)) .. t-1.
     For each opponent j compute coop_rate_j over last W_pers rounds (or full history if shorter).
     persistent_defectors = { j | coop_rate_j < α }.

  5. If coop_frac(t-1) ≥ θ:
       // High-cooperation last round -> exploit immediately
       play D;
       // If many persistent defectors exist despite high cooperation, punish:
       if |persistent_defectors| >= 1:
           punishment_until_round = t + L_pun - 1
       return

  6. Else if recent_avg ≥ φ:
       // Try recruitment: cooperate for a short window unless exploited
       if last_recruit_start == 0 or t > last_recruit_start + W_rec - 1:
           last_recruit_start = t
       // Evaluate recruitment progress: if we cooperated in last round and coop_frac has been falling, abort
       if t > last_recruit_start: // at least one recruitment round done
           prev_window_avg = average coop_frac over rounds max(1, last_recruit_start - 1) .. last_recruit_start
           cur_window_avg = average coop_frac over rounds last_recruit_start .. t-1
           if cur_window_avg <= prev_window_avg:
               // recruitment failed -> punish or stop cooperating
               punishment_until_round = t + L_pun - 1
               play D; return
       // safe to continue recruitment
       play C; return

  7. Else:
       // Default: defect
       play D; return

After each round ends (update phase):
  - Update persistent_defectors using latest history.
  - If we detect the pattern: (there was a round s in recent history with coop_frac(s) ≥ θ and after that some players who had cooperated reduced their coop rates and remain below α), set punishment_until_round = max(punishment_until_round, current_round + L_pun).

Notes and clarifications
- Deterministic: All thresholds and lengths above are deterministic functions of n and r; no hidden randomness. Implementers may tune numeric constants (θ, φ, α, windows) but the behavior pattern (probe → recruit if promising → exploit when others are cooperative → punish persistent defectors → always endgame defect) should remain.
- Endgame safety: Defecting on the final rounds prevents being exploited when there is no credible future punishment.
- Aggressiveness: ARE will defect whenever the group shows significant cooperation to immediately capture the free-riding payoff. It will only cooperate when it expects to raise future cooperation (and thus future exploitable returns) and will punish quickly and for several rounds when recruitment is frustrated or when persistent exploiters are present.
- Robustness: ARE never blindly trusts others for long. Recruitment windows and punishments are short but meaningful relative to r, so ARE will exploit short-run opportunities yet can sustain longer plans in longer games.
- Targeting: Because history is public and actions are observed individually, ARE can identify persistent defectors and use this to decide punishment strength. In implementation, you can extend punishments proportional to the number of persistent defectors or make punishment_targeted (refusing to cooperate until each marked defector increases their coop_rate), but the core policy above already prescribes harsh responses.

Examples of behavior
- Against always-cooperate group: ARE will probe C then see high coop_frac and defect on next round and keep defecting while the group remains cooperative (thus extracting maximal defection payoff).
- Against always-defect group: ARE probes C once, sees zero cooperation, will switch to defect and stay defect — avoiding being exploited.
- Against conditional reciprocators (who cooperate when others do): ARE can recruit a small cooperative cluster by cooperating for a short window, then exploit when the cluster is large enough, and re-recruit/punish dynamically.
- In short games (small r): Endgame E and short windows cause ARE to behave more or less like always-D, but still probe once to detect exploitable naive cooperators.

Implementation tips for tournament
- Keep statistics for each opponent and overall coop_frac time series.
- Ensure you compare averages with enough history before making long punishments (window sizes above guard against over-reacting on noise).
- If you prefer more aggressive behavior, raise θ (so any modest cooperation gets exploited) and lengthen punishments; if you want a bit more long-term payoff, lower θ and shorten punishments.

This strategy is explicitly designed for aggression: it takes any sign of useful cooperation and immediately defects to capture extra payoff, while being pragmatic — it probes cheaply, tries short recruitment to induce cooperators when that seems profitable, and punishes to limit being taken advantage of in return.
'''

description_AGGRESSIVE_95 = '''
Name: Aggressive Adaptive Exploiter (AAE)

Design goals (behavioral summary)
- Aggressive: seize one-shot gains whenever the group is reliably cooperative; punish quickly and visibly when cooperation collapses; minimize exploited forgiveness.
- Adaptive: use recent history to estimate how many others will cooperate and change behavior (exploit, cooperate or punish) accordingly.
- Robust: fall back to safe defection near the end of the game and during persistent low-cooperation environments; probe occasionally so the strategy can recover cooperation if opponents change.

Intuition
- If most others are reliably cooperating, defect one round to free-ride (high immediate payoff) and then give a short forgiveness window so cooperation can return.
- If cooperation rates drop, escalate to short but strong group-level punishment (defecting) proportional to the observed collapse; include low-frequency cooperative probes during punishment to detect recovery.
- Always defect in the final round. In the last few rounds be increasingly selfish.

Parameters (computed from game parameters and history)
- n, k, r: given.
- L (recency window): L = min(10, max(1, floor(r/5))). (Use up to 10 rounds of recent history; scale down for short games.)
- theta_high (group-cooperation threshold to exploit): 0.80
- theta_low (group-cooperation threshold to start/maintain punishment mode): 0.40
- exploit_length: 1 round (defect 1 round when exploitation condition met)
- forgiveness_length: 2 rounds (after exploiting, cooperate for this many rounds to rebuild)
- base_punish_min: 3 rounds (minimum punishment duration when switching to punishment)
- probe_prob: 0.10 (while punishing, cooperate with this small probability to test for recovery)
- endgame_horizon: 2 rounds (if remaining rounds <= this, always defect)
These are default values; implementers may tune them, but the decision rules below use these defaults.

Data tracked from history
- For each past round t we know the number of cooperators total_count_t and each player’s action if needed.
- For each player j ≠ me, cooperation frequency over last L rounds (optional; not required).
- For each of the last L rounds, count_excl_me_t = number of cooperators excluding me (if I cooperated then exclude my action when predicting others).

Derived quantities each decision round t (1-based)
- remaining = r - t + 1 (rounds remaining including current)
- recent_window = last m rounds where m = min(L, t-1) (use history only; if t=1 this is empty)
- avg_others_coop_rate = if m>0 then
    ( Σ_{s in recent_window} count_excl_me_s ) / ( (n-1) * m )
  else define as 1.0 (optimistic start-for-signal) or see first-round rule below.
- last_round_count = total_count_{t-1} (if t>1), else undefined.

High-level mode states (derived each round)
- ENDGAME: if remaining <= endgame_horizon, always DEFECT.
- EXPLOIT_OPPORTUNITY: avg_others_coop_rate ≥ theta_high.
- NORMAL: theta_low < avg_others_coop_rate < theta_high.
- PUNISH: avg_others_coop_rate ≤ theta_low.

Mode transitions and durations are decided deterministically from history and counters (see pseudocode).

Decision rules — when to C vs D (natural-language + pseudocode)

Natural-language summary of behavior per round
1. Final rounds: if remaining <= endgame_horizon -> DEFECT (no future to leverage).
2. First round: COOPERATE (signal willingness to cooperate; optimistic initial probe).
3. Otherwise use recent avg_others_coop_rate:
   - If in EXPLOIT_OPPORTUNITY (group reliably cooperative): defect for exploit_length rounds to free-ride, then cooperate forgiveness_length rounds to rebuild. Repeat exploitation cycles if cooperation remains high.
   - If in NORMAL zone: mimic last round’s majority but bias toward DEFECT when uncertain (be selfish). Concretely: if last_round_count (including you) ≥ ceil(n/2) -> COOPERATE this round (reward majority cooperation); else -> DEFECT.
   - If in PUNISH zone: enter a punishment block of length:
       punish_T = max(base_punish_min, ceil((1 - avg_others_coop_rate) * r / 2))
     During a punishment block: DEFECT every round, but with independent probability probe_prob in each round cooperate to test whether opponents are returning to cooperation. If a probe is followed by a rapid recovery (avg_others_coop_rate rises above theta_low in the next m rounds), end punishment early and transition to NORMAL or EXPLOIT accordingly.

Pseudocode (clear and implementable)

Initialize:
  t = 1
  last_exploit_round = -inf
  in_punishment_until = 0
  last_forgive_until = 0

Loop for round t = 1..r:
  remaining = r - t + 1
  if remaining <= endgame_horizon:
    action = D
    record action, continue
  if t == 1:
    action = C   # optimistic signal
    record action, continue

  # compute recent statistics
  m = min(L, t-1)
  if m == 0:
    avg_others_coop_rate = 1.0
  else:
    sum_others = sum_{s=t-m}^{t-1} (total_count_s - (my_action_in_s ? 1 : 0))
    avg_others_coop_rate = sum_others / ( (n-1) * m )

  # if currently inside a punitive block set earlier
  if t <= in_punishment_until:
    # punishment phase (with occasional probes)
    with probability probe_prob:
      action = C   # probe to see if others recover
    else:
      action = D
    record action
    # after action, if probes and subsequent rounds show avg_others_coop_rate > theta_low, we may end punishment early
    continue

  # if in forgiveness window following an exploit
  if t <= last_forgive_until:
    action = C
    record action
    continue

  # EXPLOIT_OPPORTUNITY: group reliably cooperative
  if avg_others_coop_rate >= theta_high:
    # exploit immediately if we haven't exploited in the last exploit cycle
    # exploit only for exploit_length rounds: here exploit_length=1
    action = D
    last_exploit_round = t
    last_forgive_until = t + forgiveness_length  # cooperate next forgiveness_length rounds
    record action
    continue

  # PUNISH: group cooperation is low
  if avg_others_coop_rate <= theta_low:
    punish_T = max(base_punish_min, ceil((1 - avg_others_coop_rate) * r / 2))
    in_punishment_until = t + punish_T - 1
    # execute this round of punishment (with probes handled at top of loop)
    with probability probe_prob:
      action = C
    else:
      action = D
    record action
    continue

  # NORMAL: middle ground -> reward recent majority cooperation, else defect
  # Use last round majority as tie-breaker
  last_count = total_count_{t-1}
  if last_count >= ceil(n/2):
    action = C
  else:
    action = D
  record action
  continue

Rationale for the key choices
- First-round C: a single cooperative signal can encourage initial cooperation from naïve or reciprocal opponents; it also creates an exploitable environment if opponents follow.
- Exploit opportunistically: when the group is reliably cooperative (avg ≥ 0.8), defect one round to gain the free-ride payoff; then cooperate for forgiveness_length rounds so that reciprocal players will return to cooperating. Keeping exploit_length short avoids provoking permanent breakdown from strong retaliators.
- Strong, visible punishment: if the group overall has become defect-heavy (avg ≤ 0.4), defect in a block long enough to inflict pain (punish_T grows with how bad the collapse is, up to game length). This discourages opponents from persistent defection. The block is group-level because individual targeting is impossible here.
- Probabilistic probing in punishment: occasional cooperative probes (10%) can detect when others start cooperating again so the strategy can switch back to exploitation/normal behavior instead of being stuck in mutual defection forever.
- Endgame always defects: with finite known horizon, backward induction makes cooperation unsustainable near the end; be selfish in the last rounds.

Edge cases
- First round: cooperate (see above). If implementer prefers maximal aggression, replace with D — but the described AAE chooses C to enable later exploitation.
- Immediate defection by many opponents in round 1: the AAE will observe low avg_others_coop_rate in subsequent rounds and shift to PUNISH mode.
- Last round and last endgame_horizon rounds: always defect.
- Very short games (r ≤ endgame_horizon + 1): the strategy will mostly defect because there's insufficient horizon to enforce cooperation.
- Extremely small groups: thresholds remain valid; L is scaled by r. For n=2 the public goods game reduces to Prisoner’s Dilemma; the strategy still applies.
- If history is noisy or opponents randomize: the recency window L and probe_prob smooth responses; parameters may be tuned to be more/less aggressive.

Why this is “aggressive”
- It actively seeks one-shot exploitation of reliable cooperators rather than optimistically always cooperating.
- It responds to defection with strong group-level punishment blocks instead of only tit-for-tat single-round reprisals—this imposes a tangible cost on persistent defectors.
- Forgiveness is limited and conditional (short forgiveness windows after exploitation; brief probabilistic probes during punishment), so the strategy does not tolerate prolonged exploitation by others.

Variants / tunable knobs
- More aggressive variant: start with D in round 1, raise theta_high (exploit even with less reliable cooperation), shorten forgiveness_length.
- More conciliatory variant: increase forgiveness_length, increase probe_prob, lower theta_high.
- If implementer can use per-player statistics, punishment can be tuned by identifying persistent defectors and keeping cooperating only if a sufficient set of other players is cooperative — this complicates the logic but can better target punishment.

Implementation notes
- Keep all computations deterministic except the small-probability probes (sampleable RNG).
- Use weighted recency (e.g., exponential weights) instead of simple average if desired; the decision rules remain the same conceptually.
- Make sure to record my past actions for correct computation of avg_others_coop_rate (exclude own cooperations).

Bottom line
AAE is an exploitative but pragmatic strategy: it signals cooperativeness, exploits reliably cooperative groups briefly and repeatedly, and responds to defection with strong punishments but with a built-in mechanism (probes and forgiveness windows) to recover cooperation when opponents shift. It is aggressive enough to gain from cooperative opponents yet adaptive enough to avoid permanent mutual defection where possible.
'''

description_AGGRESSIVE_96 = '''
Name: Aggressive Enforcer (AE)

High-level description
- Aggressive: starts by probing/defecting, exploits any stable cooperative core, and punishes defections harshly and quickly. Will switch to conditional cooperation only when there is strong, sustained evidence of a reliable cooperative core; otherwise it defects. Punishments are individual (targeted) and group-level (escalating, including a grim option), and the strategy always defects in the last round to avoid end-game exploitation.
- Only uses the game parameters (n, r, k) and the publicly observable history (who played C/D each past round). No communication or external coordination needed.

Derived parameters (computed from n, r, k; deterministic)
- w = min(5, max(1, floor(r/10))) — “recent window” for assessing short-run behavior (small when few rounds).
- coop_frac = clamp(0.5 + 0.4*(k - 1)/(n - 1), 0.5, 0.9) — required fraction of players cooperating recently to call there a “cooperative core.” Maps k to a higher requirement when public good is more valuable.
- coop_threshold = ceil(coop_frac * n) — integer threshold of cooperators (including me if I cooperated) that indicates a cooperative environment.
- individual_repair = max(1, ceil(r/10)) — number of consecutive cooperations required from a punished individual to restore trust.
- group_punish_len_base = max(1, ceil(n / max(1, round(k)))) — base punishment length scaling with number of players relative to k.
- final_rounds_defect = 1 — always defect in the final round. (See adaptation note below if implementer wants larger endgame.)

State tracked from history
- For each other player j:
  - total_C_j: total times j played C so far
  - consec_C_j: current consecutive-run of Cs by j (resets to 0 on any D)
  - punished_j: boolean flag indicating j is currently under individual punishment (need to show consec_C_j >= individual_repair to clear)
- global:
  - round t (1..r)
  - for each past round: vector of C/D by players (observable)

Decision rules (exact when to play C vs D)

Overview:
- Round 1: Defect (probe).
- Round r (last round): Defect.
- For rounds 2..r-1, follow the logic below in order — first check punishments, then decide cooperative intent based on evidence of a cooperative core.

Detailed rules
1) Update tracking from the last round
   - Update total_C_j and consec_C_j for every player j from round t-1.
   - If a previously-trusted player (consec_C_j >= individual_repair prior to their D) plays D, set punished_j = true for that player.
   - If punished_j = true, keep it true until consec_C_j >= individual_repair (then punished_j := false).

2) If t == 1: play D (already stated).

3) If t == r: play D.

4) If any punished_j is true (targeted punishment active):
   - Refuse to cooperate with punished individuals: play D until all punished_j are cleared OR until a group-level reconciliation condition triggers (see 6).
   - Rationale: targeted, sustained punishment forces repentant players to rebuild trust with consecutive cooperations.

5) Exploit strong cooperative core (Opportunistic exploitation)
   - Compute recent_cooperators_count = average over last w rounds of the number of players who cooperated in each round (rounded down), or equivalently the number of players who cooperated in a majority of the last w rounds.
   - If recent_cooperators_count >= coop_threshold:
       - If there are NO active punished_j: play D for at most E consecutive rounds to extract surplus, where E = min(group_punish_len_base, r - t) — i.e., exploit temporarily.
       - While in the exploit window E, watch for retaliatory drop by others (a sudden drop in cooperators next round). If more than floor(n/4) players respond with D next round (clear group retaliation), abandon exploitation and enter group-punish mode (see 7).
       - Rationale: aggressive exploitation of a stable cooperative core until they punish. This maximizes short-term gains but keeps watch for retaliation.

6) Cooperative sustain mode (conditional cooperation)
   - If recent_cooperators_count >= coop_threshold AND there are no punished_j AND NOT currently in exploit window:
       - Cooperate (play C). This allows forming a “mutual-cooperation alliance” with a clearly identifiable core.
       - Continue cooperating as long as the core remains (recent_cooperators_count >= coop_threshold) and nobody defects unexpectedly.
       - If any member of the core defects unexpectedly (i.e., significant drop in cooperators or a previously-trusted player defects), tag them punished_j and revert to targeted punishment (step 4).
   - Rationale: only commit cooperation when a strong, stable core exists and no punishments are active.

7) Group-level punishment / escalation (if the group is breaking down)
   - If recent_cooperators_count < coop_threshold and:
       - sudden mass-defection occurs (in a single round t-1, cooperators <= ceil( (1 - k/n) * n ) or cooperators decreased by more than n/4 compared to previous round), then enter group-punish mode:
         - Defect for P = min(r - t, group_punish_len_base * 2) consecutive rounds to make defection costly to the group.
         - After P rounds, re-evaluate history; allow conditional cooperation only if the group shows consistent recovery (recent_cooperators_count >= coop_threshold).
       - As an extreme escalation: if repeated breakdowns occur (more than 2 group-punish episodes), flip to global grim: defect for all remaining rounds (except consider a one-round testing probe after many rounds if r is large).
   - Rationale: escalating group punishments are meant to deter free riding and to make defection unattractive.

8) Default fallback
   - If none of the above conditions entitle cooperation, play D.

Edge cases and clarifications
- Very small r (r ≤ 3): be more conservative. For r = 2 or 3, AE mostly defects except it may cooperate in round 2 if a very strong cooperative signature exists in round 1 (unlikely). Always defect in final round.
- If k is very close to n (public good almost fully efficient), coop_frac increases toward 0.9: AE becomes more willing to form and sustain cooperative alliances but still starts by probing/defecting and still punishes betrayals aggressively.
- If k is near 1 (public good weak), coop_frac ≈ 0.5: AE requires only a moderate signal before considering cooperation, but will punish and exploit just the same.
- Memory and complexity: the required tracking is O(n) per round (counts per player + group counts). All decisions are deterministic given history and parameters.
- Last-round defection: AE unambiguously defects in the final round. Implementer can increase final_rounds_defect to 2 if they prefer a longer endgame safeguard.

Pseudocode (structured, concise)

Initialize: for each player j: total_C_j = 0, consec_C_j = 0, punished_j = false.
Compute w, coop_frac, coop_threshold, individual_repair, group_punish_len_base.

For each round t = 1..r:
  if t == 1:
    play D
    observe actions; update totals and consec counters
    continue
  if t == r:
    play D (final-round safe-guard)
    observe; update; continue

  Update total_C_j and consec_C_j based on actions in t-1.
  For any j: if consec_C_j before last round >= individual_repair and j played D last round:
    punished_j := true
  For any j: if punished_j and consec_C_j >= individual_repair:
    punished_j := false

  recent_cooperators_count := average_cooperators_over_last_w_rounds (rounded as decision requires)
  active_punishments := any(punished_j == true)
  if active_punishments:
    play D  // targeted punishment
    observe; continue

  if recent_cooperators_count >= coop_threshold:
    if not in_exploit_window:
      // either cooperate to sustain or exploit opportunistically
      if exploit_condition_met (we start exploitation opportunistically on first detection):
        set in_exploit_window with E = min(group_punish_len_base, r - t)
        play D
      else:
        play C
    else:
      // during an exploit window
      play D
      if next-round cooperators drop by > n/4:
        cancel exploit, enter group-punish mode
      continue

  if recent_cooperators_count < coop_threshold:
    if sudden_mass_defection_detected:
      enter group-punish mode for P = min(r - t, group_punish_len_base*2)
      play D
    else:
      play D

  observe actions; update totals and consec counters; loop

Why this is “aggressive” and robust
- Aggressive probes/defects early to see who will feed it and exploits stable cooperators for immediate gains.
- Punishes betrayals strongly (targeted punishments until the offender proves repentance) and can escalate to group punishments and even grim if the population repeatedly defects.
- Conditional cooperation is allowed, but only when there is clear and sustained evidence of a cooperative core — therefore it resists being fooled by transient cooperators or noisy opponents.
- Works across a wide range of opponent types: pure defectors (AE will defect and punish, avoiding being exploited), naive cooperators (AE will exploit them opportunistically), reciprocators (AE will form alliances if and only if they are reliable), and mix-strategists (AE measures and responds to history).

Tuning guidance (implementer choices)
- Increase individual_repair to make forgiveness harder and punishment longer.
- Increase final_rounds_defect if you want stronger end-game safety.
- Adjust coop_frac mapping from k to a required core size depending on how risk-averse you want the enforcer to be.
- group_punish_len_base can be scaled up to make group punishments deeper (more aggressive) or down for less risk of losing long-term gains.

Summary (one-line)
Aggressive Enforcer: start by defecting to probe, opportunistically exploit any sustained cooperative core, impose targeted and escalating group punishments on betrayals, allow cooperation only with strong, sustained signals of trust, and always defect in the last round.
'''

description_AGGRESSIVE_97 = '''
Name: Aggressive Conditional Exploiter (ACE)

Intuition (short)
- Be opportunistic: exploit near-universal cooperation to take immediate advantage.
- Be coercive: punish any sustained or retaliatory collapse of cooperation with harsh, multi-round defection to deter future freeriding.
- Be adaptive: use short recent-history windows and dynamic thresholds tied to remaining rounds so decisions respond to opponents’ behavior and the endgame.
- Be aggressive: default to defect unless there is strong, recent evidence that cooperation is widespread and sustainable; when cooperating, do so conditionally and briefly; when exploited or when cooperation collapses, punish long enough to make exploitation costly.

All information the strategy uses: n, k, r and the full public history of contributions and payoffs. No assumptions about opponents beyond what is observed.

High-level state machine
- ENDGAME: final rounds — always defect.
- NORMAL: default mode; cooperate only when recent group cooperation is high.
- EXPLOIT: single-round opportunistic defection when others are strongly cooperating.
- PUNISH: defect for several rounds in response to cooperation collapse or retaliation.
- MONITOR (forgive): short test after punishment to see if cooperation returns.

Parameter derivation (adaptive defaults)
- L (memory window) = min(5, r-1). Use up to 5 most recent rounds to measure behavior.
- Endgame_length E = min(3, max(1, floor(r/10))). (Always defect in final E rounds.)
- T_coop = 0.7 (cooperate if at least 70% of other players cooperated recently).
- T_exploit = 0.9 (only attempt an exploit if at least 90% of others cooperated recently).
- T_retaliation = 0.6 (if cooperation falls under 60% after you exploited, treat as retaliation/collapse).
- P_max (max punishment length) = min(5, r). Actual punishment length P = min(P_max, remaining_rounds - E), but at least 1.
- M (min spacing between exploits) = max(3, floor(r/10)). Do not exploit more often than once every M rounds.
- F (forgiveness/monitor length) = 2 (cooperate up to 2 rounds as a test after punishment).

These numbers are tunable; they are chosen to be aggressive but avoid repeated needless self-destruction in short tournaments.

Definitions used at round t
- remaining = r - t + 1 (including current round).
- window = last L rounds (or fewer if t-1<L).
- other_coop_rate = (sum over rounds in window of contributions by players j ≠ me) / (window_size × (n-1)).
- last_exploit_round = the last round I executed an exploit (initially -infty).
- in_punish_until = round index until which I will keep punishing (initially 0).

Decision rules (natural language)
1. Endgame override:
   - If remaining ≤ E: defect. (Final rounds are uncooperative — no credible future punishments.)

2. If currently punishing (t ≤ in_punish_until):
   - Defect. Continue punishment until in_punish_until.
   - After punishment ends, enter MONITOR: cooperate for up to F rounds only if other_coop_rate >= T_coop; otherwise remain defecting.

3. Otherwise (not in PUNISH, not in ENDGAME):
   - Compute other_coop_rate on the last L rounds.
   - If other_coop_rate ≥ T_exploit AND (t - last_exploit_round) ≥ M AND remaining > E + 1:
     - EXPLOIT: defect this round (one-shot defect to cash in).
     - Record last_exploit_round = t.
     - After the round, observe response: if other_coop_rate in the next window drops below T_retaliation, trigger PUNISH by setting in_punish_until = t + P - 1 (P computed as above).
   - Else if other_coop_rate ≥ T_coop:
     - Cooperate (sustain cooperation).
   - Else:
     - Defect (do not reward partial cooperation; be aggressive).

Additional monitoring / adaptive behavior
- If multiple distinct players (≥ 30% of group) have defected in most of the last L rounds, treat behavior as group defection → remain defecting until evidence of rehabilitation.
- If after an EXPLOIT the group shows no retaliation (other_coop_rate stays ≥ T_exploit), allow another exploit after M rounds; if retaliation occurs, punish P rounds.
- If opponents appear to be mostly unconditional cooperators (other_coop_rate very high for many consecutive windows), ACE will exploit occasionally but also cooperate most rounds to keep them yielding high average payoffs to ACE.
- Never attempt to “target” specific players with different actions in the same round (actions are single binary choice per round). But the history used can be used to decide whether to cooperate at all when some specific players are frequent defectors — i.e., reduce cooperation threshold if multiple defectors exist.

Pseudocode (compact)

Initialize:
  last_exploit_round = -∞
  in_punish_until = 0
  L = min(5, r-1)
  E = min(3, max(1, floor(r/10)))
  P_max = min(5, r)
  M = max(3, floor(r/10))
  F = 2
  T_coop = 0.7; T_exploit = 0.9; T_retaliation = 0.6

For each round t = 1..r:
  remaining = r - t + 1
  if remaining ≤ E:
    play D (defect); continue

  compute other_coop_rate over last min(L, t-1) rounds
    (if t == 1, treat other_coop_rate = 0)

  if t ≤ in_punish_until:
    play D
    if t == in_punish_until: set a flag to enter MONITOR next round
    continue

  if (other_coop_rate ≥ T_exploit) and ((t - last_exploit_round) ≥ M) and (remaining > E + 1):
    play D  # EXPLOIT
    last_exploit_round = t
    # after observing next round(s): if other_coop_rate drops below T_retaliation,
    #   set P = min(P_max, remaining - E), in_punish_until = t + P
    continue

  if (other_coop_rate ≥ T_coop):
    play C
    continue

  play D

Edge-case handling (explicit)
- Round 1: The rule above with t=1 and other_coop_rate=0 means ACE defects in round 1 (aggressive probe). If you prefer a less flaming start, you may set other_coop_rate default to 0.5 for round 1 (but default is aggressive).
- Very small r (e.g., r=2 or 3): Endgame_length E will be 1 or 2; ACE will defect in last E rounds. For very short games ACE will be mostly defecting (aggressive).
- n=2 (pairwise case): same rules apply; thresholds remain meaningful since other_coop_rate reduces to partner cooperation frequency.
- If P would push punishment into Endgame, trim P so that punishment occurs only when it can be meaningful (we set P ≤ remaining - E).
- If memory window is empty (t=1), ACE defects.

Why this is “aggressive”
- Default is to defect unless there is strong, recent evidence of widespread cooperation (high threshold).
- When cooperation is detected, ACE exploits occasionally (single-round defections) to get the temptation payoff; it does not passively let others be exploited forever.
- When exploitation or cooperation collapse occurs, ACE punishes for multiple rounds (P rounds), imposing a cost on the group to deter repeated violations.
- Endgame is uncooperative: ACE will not be fooled into cooperating where future punishments are impossible.

Robustness discussion
- Against unconditional cooperators: ACE will gain by exploiting occasionally while otherwise sustaining cooperation to keep returns high.
- Against unconditional defectors: ACE defects and avoids wasting cooperative moves.
- Against conditional cooperators (tit‑for‑tat, generous TFT variants): initial defection may trigger retaliation; ACE’s punish‑and‑monitor cycle and thresholds are designed to recover cooperation after punishment if opponents return to cooperative patterns. ACE’s punishment is aggressive enough to deter mild freeloading but designed to forgive (short monitor) to re-establish profitable cooperation.
- Against harsh punishers (grim): ACE’s initial defection may lock it into mutual defection. That is an unavoidable risk of aggressive play; we reduce this risk by limiting how many one-shot defects we do (M spacing) and by using short memory windows L so that old grudges don’t persist indefinitely for opponents who resume cooperating.
- Adaptive thresholds depending on remaining rounds protect against wasting cooperation late in the game.

Implementation notes
- All thresholds and parameters are functions of r (and implicitly robust to n and k through adaptive windowing). Implementers may tune T_coop/T_exploit/M/P_max by tournament feedback.
- The strategy only depends on record of contributions; payoffs need not be computed in the decision rule but can be used to refine thresholds if desired.
- For highly aggressive variants, increase T_coop and T_exploit and lengthen P; for more risk-averse variants, lower thresholds and shorten punishments.

Summary
ACE is a short-memory, threshold-based, aggressive conditional strategy:
- Defect by default, cooperate only when recent group cooperation is strong.
- Occasionally exploit near-universal cooperation to maximize short-term gain.
- Punish sustained or retaliatory collapses with multi-round defections that are long enough to be costly to defectors but not so long that reconciliation is impossible.
- Always defect in the final predefined endgame rounds.
'''

description_AGGRESSIVE_98 = '''
Summary (high level)
- Mindset: aggressive, opportunistic and threatening. Try to form a profitable cooperating coalition only when there is clear, sustained evidence that enough other players will reciprocate; otherwise defect. If a coalition forms, exploit it occasionally to extract extra payoff, and punish any sign of backsliding with a fast, hard retreat to permanent defection (grim punish). Always defect in the final rounds (endgame).
- Rationale: unilateral cooperation always lowers your immediate payoff (π_C − π_D = k/n − 1 < 0). So you only accept short-term losses if there is a credible prospect of larger long-term gains from others’ sustained cooperation. The strategy tests, accepts only strong signals, extracts surplus, and punishes swiftly if signals degrade.

Parameters the strategy uses (all depend only on game parameters n, k, r and observed history)
- S (offer phase length): min(3, max(1, floor(r/10))) — a short probing phase to propose cooperation.
- ENDGAME_H (endgame horizon): min(2, r-1) — always defect in last ENDGAME_H rounds.
- ALPHA (acceptance threshold after offer phase): fraction of other players that must have cooperated consistently in the offer phase to trust a coalition. Default:
  - If k/n ≥ 0.8 → ALPHA = 0.50 (public good is large; more willing to risk)
  - Else ALPHA = 0.75 (be conservative)
- BETA (maintenance threshold): minimum fraction of other players cooperating in any subsequent round to keep cooperating. Default BETA = ALPHA − 0.10 (slightly lower to allow small fluctuation).
- EXPLOIT_PERIOD or p_exploit: small, unpredictable exploitation. Choose either:
  - deterministic: defect 1 round every E = 4 rounds in the cooperation regime, or
  - stochastic: in each cooperation-round defect with probability p = 0.12 (makes exploitation harder to retaliate predictably).
- PUNISH regime: Grim trigger — defect until the end (except final ENDGAME_H is still defect).
- Minimal history window for statistics: use last S rounds for initial decision, and last 3 rounds for maintenance checks.

Decision rules (natural-language + pseudocode)

High-level states
- OFFER: initial short-phase proposing cooperation.
- COOP_EXPLOIT: we cooperate most rounds to sustain mutual cooperation, but occasionally defect to extract extra payoff.
- PUNISH: permanent defection in response to insufficient reciprocation.
- ENDGAME: last ENDGAME_H rounds — always defect.

Initial state: OFFER. Transition rules below.

Pseudocode (per round t, with t = 1..r). "others_coop_fraction(t')" = (# of other players (excluding me) who cooperated in round t')/(n−1). Use observed history of all players.

At start:
- If r ≤ 2: defect every round (no room to build cooperation).
- If t > r − ENDGAME_H: action = D (ENDGAME).

Otherwise:

1) OFFER state (first S rounds)
- Action: Cooperate (C) for rounds t = 1..S unless r small as above.
- After completing round S (i.e., at the first decision after S rounds), compute mean_fraction = average over rounds 1..S of others_coop_fraction(t').
- If mean_fraction ≥ ALPHA, transition to COOP_EXPLOIT; else transition to PUNISH.

2) COOP_EXPLOIT state
- Default action each round: C (cooperate).
- But: if this is an "exploit" moment (deterministic every E rounds or with probability p_exploit), play D this round instead of C to extract surplus and probe robustness.
- After each round in COOP_EXPLOIT, observe others_coop_fraction for that round. If others_coop_fraction < BETA, immediately transition to PUNISH (grim).
- Additionally maintain a moving average of others' cooperation (last 3 rounds). If that average drops below BETA, immediately PUNISH.
- If t enters ENDGAME zone, play D.

3) PUNISH state
- Always defect (D) for the rest of the game (or at least until the end; this is a grim punishment). If you prefer limited punishment, you can set a punishment length P = min(r − t, max(3, floor(r/4))) and after P rounds re-enter OFFER — but the aggressive default is permanent defection to deter exploitation.

Edge cases and precise behaviors
- First round: In most instances, this strategy cooperates on round 1 (it's an "offer"). Exception: if r ≤ 2, or if you choose a pure exploitative variant, you may defect from the start. The default aggressive design uses a short offer (S ≥ 1) because a brief credible offer can induce conditional cooperators to start a coalition, which can be very profitable over many rounds if reciprocated.
- Last rounds (endgame): Always defect in the last ENDGAME_H rounds. This prevents wasting cooperation when it cannot be reciprocated later.
- Small r: If r is too small to make cooperation worth the upfront cost (default r ≤ 2 or user-modified threshold), the strategy defects every round.
- Noisy or fluctuating groups: the maintenance threshold BETA has slack (slightly below acceptance ALPHA) so minor random drops don't immediately trigger punishment. But any sustained drop triggers the grim punishment fast.
- Against unconditional cooperators (always C): the strategy will stay in COOP_EXPLOIT and exploit them occasionally by defecting, extracting extra payoff while still capturing most cooperative benefits.
- Against unconditional defectors (always D): the offer phase will fail (others_coop_fraction small), strategy transitions to PUNISH and defects always — avoiding exploitation.
- Against conditional cooperators / tit-for-tat variants: if a coalition can form and remain stable at or above ALPHA, the strategy will sustain cooperation and occasionally defect to extract surplus; if conditional cooperators punish too harshly, the strategy will observe cooperation drop and switch to permanent defection.

Why this is aggressive and robust
- Aggressive: initiates a clear proposal to cooperate (signal), but only accepts when signals are strong; it extracts surplus through planned or randomized brief defections; it punishes any decay in reciprocation with a permanent defection (grim), making the threat credible. It does not naively tolerate exploitation.
- Robust: it uses short samples and clear thresholds so it adapts quickly to opponents’ tendencies. It treats the entire group as the unit of reciprocity (because punishments and payoffs are non-targeted), and it uses stochastic exploitation to reduce predictability and defeat revenge cycles.
- Parameter sensitivity: thresholds adapt to k/n (become less conservative when public-good share k/n is large). S and ENDGAME_H scale with r so the strategy behaves sensibly in short vs long tournaments.

Concrete numeric defaults (recommended)
- S = min(3, max(1, floor(r/10)))
- ENDGAME_H = min(2, r-1)
- ALPHA = 0.5 if k/n ≥ 0.8 else 0.75
- BETA = ALPHA − 0.10
- Exploit: defect with probability p = 0.12 each cooperation round (or every E = 4 rounds deterministically)
- Punishment: Grim (defect until game end)

Optional variants (choose depending on risk appetite)
- More exploitative: skip OFFER entirely, defect always except opportunistic random cooperation probes (very aggressive, lower risk of being exploited, but will forgo coalition gains).
- More patient/aggressive enforcer: lengthen S and lower ALPHA to try harder to build cooperative cartel, then use harder exploitation (higher p_exploit) and longer punishment.
- Less ruthless: instead of Grim punishment, use finite punishment window P then re-offer (useful if most opponents are "repairable").

Example behavior (n=6, k=2, r=20 with defaults)
- S = 2 or 3: cooperate rounds 1–3.
- If at least 75% of others cooperated in those rounds, move to COOP_EXPLOIT, otherwise PUNISH.
- In COOP_EXPLOIT you mostly cooperate but defect about 12% of the time (or every 4th round), gaining on average from occasional exploitation. If cooperation by others drops below 65% in maintenance checks, switch to permanent defection.
- Rounds 19–20 (ENDGAME_H=2): defect.

Implementation notes for coder
- Maintain full action history for all players (you observe them by spec).
- Compute others_coop_fraction per round as (#C among others)/(n−1).
- When using stochastic exploitation, use a random seed or RNG to choose exploit rounds to avoid predictability.
- Ensure ENDGAME_H is enforced regardless of state.
- Expose thresholds ALPHA/BETA, S, p_exploit as tunable parameters to optimize performance in the specific tournament environment.

Final remark
This strategy is explicitly aggressive: it tries to coerce cooperative payoffs only when coalition formation is clearly viable, extracts surplus via occasional defections, and threatens and executes harsh, credible punishment (grim) at the first sign of deterioration. It balances opportunistic exploitation and disciplined deterrence to perform well against a wide array of strategies in tournaments where no pre-play agreements exist.
'''

description_AGGRESSIVE_99 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Intuition (short)
- Default stance: defect. Defection is dominant in one-shot rounds; AOE exploits any players who are reliably cooperative and avoids getting stuck in costly mutual-punishment wars with retaliators.
- Adaptive detection: build simple statistical models of each opponent’s cooperativeness and their tendency to retaliate when you defect.
- Exploit when safe: if you identify exploitable cooperators (high cooperation, no retaliation) you always defect to extract surplus from them.
- Sustainable exploitation when group cooperation is common: when many players are reliably cooperative but few punish, act like a “cooperate-mostly / defect-periodically” cartel member — cooperate to keep group contributions high but defect occasionally on a schedule that maximizes your immediate gain while minimizing risk of long punishment.
- Protect from punishers: if a significant fraction of players punish your defections, reduce exploitation and make short, tactical appeasement moves to avoid long mutual-defection cycles. Always defect in the known final rounds (endgame).

This strategy depends only on game parameters (n, k, r) and full history.

Notation and derived quantities
- n, k, r given.
- For rounds t = 1..r let a_i,t ∈ {C,D} denote actions; our index is i (we control player i).
- For opponent j (j ≠ i) compute statistics over a sliding window of up to W past rounds (or full history if fewer rounds):
  - coop_rate_j = fraction of rounds j played C in the window.
  - P_def_given_my_def_j = P(j_t = D | my_{t-1} = D) estimated over window (conditional defect probability after I defected).
  - baseline_def_j = unconditional defect frequency in window.
  - punish_sensitivity_j = max(0, P_def_given_my_def_j − baseline_def_j) — extra defecting probability following my defection, estimates retaliation sensitivity.
- Global stats:
  - global_coop_rate = fraction of players (counting all players including me if desired) who cooperated in last window / fraction of cooperative actions among others.
  - fraction_punishers = fraction of opponents with punish_sensitivity_j ≥ P_high.
- Immediate gain from defecting vs cooperating (single-round):
  - delta_defect = π_D − π_C = 1 − (k/n)  (constant > 0 because k < n).

Default numeric tuning (implementer can adjust):
- Window W = min(10, max(3, floor(r/4))) — use recent history but not too short.
- Probe length T_probe = min(3, floor(r/4)) (only influences early classification).
- Endgame length T_end = min(2, r) — always defect in last T_end rounds. (Aggressive; can increase to 3 if desired.)
- Coop-high threshold C_high = 0.75 (player cooperating ≥75% → candidate exploitable).
- Punisher threshold P_high = 0.45 (punish_sensitivity ≥ 0.45 → treated as punisher).
- Low punish threshold P_low = 0.20 (punish_sensitivity ≤ 0.20 → tolerant / non-punishing).
- Global-coop threshold G_high = 0.65 (if ≥65% of players are cooperators, treat environment as cooperative).
- Exploit-cycle: cooperate for M_c - 1 rounds, defect 1 round; default M_c = max(3, ceil(1 / (delta_defect))) clipped to reasonable int. (Simpler default: M_c = 3 → cycle = C,C,D.) Rationale: more frequent defection increases immediate gain but risks punishment; M_c balances that based on delta_defect.

Decision rules (high-level)
1. First round (t = 1):
   - Play D (defect). This is a low-cost probe and avoids being first-mover cooperator that can be exploited.

2. Endgame (t > r − T_end):
   - Play D (defect) for all opponents — backward induction; aggressive.

3. Early probe phase (t ≤ T_probe):
   - Play D. Occasionally (with small probability p_probe = 0.15) play C in one of these rounds to test whether someone punishes cooperative gestures specially. (Optional: this small randomness helps detect weird responders.)

4. For t > T_probe and t ≤ r − T_end (main adaptive phase):
   a. Update coop_rate_j, punish_sensitivity_j, global_coop_rate and fraction_punishers over the last W rounds (or whole history if fewer).
   b. If there exists at least one opponent j with coop_rate_j ≥ C_high and punish_sensitivity_j ≤ P_low:
      - Classification: "exploitable cooperator" exists.
      - Action: Defect (D) every round against such opponents — that is, play D now. Rationale: immediate profit from exploiting; no long-term retaliation expected.
   c. Else if global_coop_rate ≥ G_high and fraction_punishers ≤ 0.25:
      - Classification: cooperative environment with few punishers.
      - Enter “cooperate-exploit cycle” mode:
         - Maintain a local cycle counter cyc (stateful).
         - In each cycle of length M_c: play C for M_c − 1 rounds, then play D for 1 round (the scheduled exploitation). Start cycle fresh when entering this mode or after an appeasement.
         - Exception: if one or more players exhibit punish_sensitivity_j ≥ P_high and after your last D you observed coordinated retaliatory defections from many players causing your payoff to drop below your recent average, exit cycle mode and go to appeasement (see d).
      - Rationale: when most players cooperate and few punish, cooperating usually raises the public good so your periodic defect yields a large absolute payout without triggering systemic retaliation.
   d. Else (mixed or punishing environment):
      - Conservative exploitation: default to Defect (D).
      - If you detect that after your defections a significant fraction of players retaliate (quantified as: in the window, when you defected, at least R_frac = 0.4 of opponents increased their defection rate following your defect relative to baseline and your own average payoff fell by > delta_defect/2), then perform a short appeasement:
         - Appeasement: play C for R_appease = min(2, floor(W/2)) consecutive rounds to reduce punishment and restore cooperation with punishers.
         - After appeasement, return to main rule (recompute classifications).
      - If no clear retaliatory pattern, keep defecting (exploit).

5. Forgiveness and reclassification:
   - Continuously reestimate stats in the sliding window W. If a previously exploitable player begins to punish (punish_sensitivity_j increases above P_high), reclassify her as punisher; stop exploiting that player and, if necessary, do appeasement to avoid persistent losses.
   - If a punisher becomes tolerant (punish_sensitivity drops below P_low and coop_rate rises), that player can be reclassified as exploitable again.

6. Randomness / unpredictability:
   - Add small randomness to your scheduled actions (flip action with tiny probability ε = 0.02) to avoid being perfectly predictable to adaptive opponents. Do not randomize in endgame.

Pseudocode (concise)

Initialize:
- cyc = 0
- mode = "default"
- maintain per-player histories for last W rounds

For each round t = 1..r:
  if t == 1:
    play D (with probability 1; optionally with p_probe small chance of C)
    record outcome; continue
  if t > r − T_end:
    play D; continue
  Update stats (coop_rate_j, punish_sensitivity_j, global_coop_rate, fraction_punishers) using last W rounds
  if t ≤ T_probe:
    play D (with small p_probe chance of C); continue
  If exists j with coop_rate_j ≥ C_high and punish_sensitivity_j ≤ P_low:
    play D (exploit them); continue
  Else if global_coop_rate ≥ G_high and fraction_punishers ≤ 0.25:
    if mode != "cycle": mode = "cycle"; cyc = 0
    cyc = (cyc + 1) mod M_c
    if cyc == M_c − 1:  # last step -> defect
      action = D
    else:
      action = C
    If recent rounds show systematic retaliation after my D (my payoff drop large and many defection responses), then
      mode = "appease"; perform R_appease rounds of C; after that mode = "default"
    play action (with tiny randomness ε)
    continue
  Else:
    # mixed or punishing environment
    If detect heavy retaliation trend against my D:
      play C for R_appease rounds (appeasement), then continue
    else:
      play D
    continue

Edge cases handled
- First round: D (small probing randomness optional).
- Final T_end rounds: always D (no reason to cooperate in last rounds).
- Short games (r small): window sizes and thresholds scale down via min(...).
- If all opponents are pure cooperators (never punish, coop_rate ~ 1), AOE will always defect to exploit them.
- If many players are punishers, AOE will avoid repeated exploitations and will perform short appeasements to prevent costly mutual-defection cycles; aggressive stance still encourages defecting where safe.
- If opponents adapt, AOE reclassifies them using sliding window to remain responsive.

Why this is aggressive
- It prioritizes immediate gains whenever it is safe to do so: unconditional cooperators are exploited relentlessly.
- It uses scheduled, periodic defections in cooperative environments to extract surplus while minimizing the chance that punishers can coordinate to drive you into long punishment.
- It only sacrifices cooperation when faced with clear punishment behavior, and even then only temporarily (short appeasement) to recover better long-run exploitation opportunities.
- In the endgame it defects — pure aggressive payoff maximization.

Robustness considerations
- Uses only observed actions (no communication) and sliding-window stats, so it adapts to changing opponent behavior.
- Thresholds are parameterized by n, k, r only indirectly via delta_defect in M_c; implementers can tune thresholds to the tournament meta.
- Small injected randomness prevents simple mirror strategies from perfectly predicting and punishing a fixed schedule.
- Short appeasement prevents extended mutual-punishment wars with retaliators while preserving an exploitative core vs unconditional cooperators.

Implementation notes
- Use W at least 3 to get meaningful estimates; cap W for compute simplicity.
- For conditional statistics (P_def_given_my_def_j), require a minimum number of observations (e.g., at least 3 instances of my prior defection) before trusting the estimate; otherwise treat as uncertain (conservative: assume punisher).
- Use payoff tracking (my average payoff in the most recent window) to detect when exploitations are backfiring (trigger appeasement).
- The specific thresholds (C_high, P_high, G_high, M_c) can be tuned in practice; defaults above are balanced to be aggressive but not recklessly suicidal vs punishers.

Summary (one-sentence)
AOE defects by default, ruthlessly exploits reliably cooperative opponents, periodically defects inside largely cooperative groups to siphon extra payoff, and limits long punishment losses by detecting retaliators and performing brief appeasements — always defecting in the final rounds.
'''

description_AGGRESSIVE_100 = '''
Name: Adaptive Exploitative Aggressor (AEA)

High-level idea (aggressive mindset)
- Default: defect. Aggressively free-ride on any persistent cooperators.
- Only concede (cooperate) when there is clear, empirically measurable evidence that a strategic concession will trigger enough extra cooperation from others for the remainder of the game so that the long-run payoff gain outweighs the immediate cost of cooperating.
- Use short, cautious experimentation to learn opponents’ responsiveness, then exploit predictable cooperators and appease / invest only when it pays for many remaining rounds.
- Always defect on the final round (endgame). Minimize “nice” actions; only signal/cooperate when the math says it will pay off.

Notation
- n, k, r are game parameters.
- t ∈ {1..r} is the current round (1-indexed).
- L = r − t + 1 is rounds remaining including the current one.
- For opponent j (j ≠ me): history_j is the vector of j’s past actions (C=1, D=0).
- my_history is my past actions.
- Others_coop_rate_after(action, offset_window) — empirical average cooperation rate of all opponents in rounds following times when I played action (C or D). See implementation notes for precise formula.
- B = current baseline cooperation = average over opponents of their recent cooperation frequency.

Core decision rule (summary)
1. If t == r (last round): Defect.
2. If no usable history (first round or extremely little data): Defect (aggressive default).
3. Compute an empirical estimate delta_per_round = expected increase in others’ cooperation per round for the remaining rounds if I cooperate now versus if I defect now. (This measures how much cooperating now raises opponent cooperation on average for future rounds.)
4. Compute the minimum per-round increase in others’ cooperation required so cooperating now is profitable given remaining rounds:
   required_delta = (n/k − 1) / (L − 1)   (if L − 1 = 0 then cooperating is never profitable)
   Rationale: cooperating costs me 1 − k/n immediately; each extra cooperator among others adds me (k/n) per round over the remaining L − 1 rounds, so we need (L−1)*(k/n)*delta > 1−k/n.
5. If delta_per_round >= required_delta and I have enough empirical samples to trust the estimate, then Cooperate this round (signal/appease).
6. Otherwise Defect.
7. Occasionally explore (tiny probability) to gather data about opponents’ responsiveness (small epsilon that decays as rounds go on).

Detailed rules and heuristics (for implementation)
- First round (t = 1): Defect. Use this as a probe.
- Last round (t = r): Defect (standard endgame).
- Data windows:
  - Use a rolling window W = min(20, t − 1) of past rounds for robustness to nonstationarity.
  - Compute examples_count_C = number of times I cooperated in the last W rounds; examples_count_D = number of times I defected in the last W rounds.
  - If examples_count_C < min_samples or examples_count_D < min_samples, treat estimates as noisy and require larger margins (see “conservatism” below). Default min_samples = 4.
- Estimating delta_per_round:
  - Let AvgCoop_next_after_C = average, across occasions in the window where I played C at round s, of the opponents’ average cooperation at rounds s+1, s+2, ... up to s+H (H can be 1 to capture immediate response or >1 to capture persistence). For simplicity use H = min(3, r − s) to capture short persistence.
  - Let AvgCoop_next_after_D be the analogous average following occasions where I played D.
  - Define delta_per_round_empirical = AvgCoop_next_after_C − AvgCoop_next_after_D.
  - Convert that to per-round uplift (divide by H if using multi-step averaging) to get delta_per_round.
- Required_delta formula:
  - If L ≤ 1: required_delta = ∞ (cooperating is never worth it).
  - Else required_delta = (n/k − 1) / (L − 1).
  - Intuition: required_delta shrinks as more rounds remain; with many remaining rounds, a small persistent uplift can justify cooperating now.
- Conservatism and statistical reliability:
  - If examples_count_C or examples_count_D < min_samples, inflate required_delta by factor gamma > 1 (e.g., gamma = 1.5) to avoid being tricked by noise.
  - If delta_per_round is negative (cooperating reduces others’ cooperation), treat as zero or negative — then never cooperate.
- Exploit unconditional cooperators:
  - Identify any opponent j with empirical p_j ≥ exploit_threshold (e.g., 0.95 over W). Treat them as unconditional cooperators and always plan to defect regardless of their presence (no need to cooperate to maintain them; you can free-ride).
- Small random exploration:
  - With small probability epsilon_explore (e.g., max(0.02, 0.1/(L))) take the opposite action than the deterministic rule to collect responsiveness data. Decrease exploration as game approaches last rounds.
- Targeted appeasement (optional, more aggressive):
  - If delta_per_round meets required_delta only because a small set of opponents are responsive (their cooperation is pivotal), you may choose to cooperate while continuing to defect against confirmed unconditional defectors. That is: when cooperating, you may attempt to do so in a way that maximizes signal to the responsive subset (there is no communication so this just manifests as cooperating nonetheless).
- Retaliation: you already default to defect. If a subset of opponents lowers cooperation after your defection, you either:
  - If they are crucial to total contributions and you can economically restore cooperation (delta_per_round meets required_delta), cooperate to get cooperation back.
  - Otherwise keep defecting (punish by starvation), because continuing to defect reduces their payoffs and you are aggressive.

Pseudocode (concise)

Inputs: n, k, r
State: my_history[], histories_of_others[][]

function choose_action(t):
  L = r - t + 1
  if L == 1:
    return D   # last round, always defect

  W = min(20, t-1)
  if W == 0:
    return D   # no history: defect

  # compute baseline and per-opponent rates on last W rounds
  for each opponent j:
    p_j = average of j's cooperation in last W rounds

  B = average_j p_j

  # exploit obvious unconditional cooperators
  if exists j with p_j >= 0.95:
    return D

  # compute empirical response statistics
  H = 1 or min(3, r - t)  # choose H=1 for immediate response; H>1 to capture persistence
  AvgCoop_next_after_C = average over times s in last W with my_history[s]==C of (average opp cooperation in rounds s+1..s+H)
  AvgCoop_next_after_D = average over times s in last W with my_history[s]==D of (average opp cooperation in rounds s+1..s+H)

  examples_C = count of s used in AvgCoop_next_after_C
  examples_D = count of s used in AvgCoop_next_after_D

  if examples_C == 0 or examples_D == 0:
    # insufficient data about my influence, be conservative: defect with small exploration
    with probability epsilon_explore = max(0.02, 0.1/(L)): return C else return D

  delta_per_round = (AvgCoop_next_after_C - AvgCoop_next_after_D) / H

  if delta_per_round <= 0:
    return D  # cooperating doesn't increase future cooperation

  if L <= 1:
    return D

  required_delta = (n/k - 1) / (L - 1)

  # be conservative if small sample
  if examples_C < 4 or examples_D < 4:
    required_delta *= 1.5

  # Decide
  if delta_per_round >= required_delta:
    return C
  else:
    # rare exploration to gather more stats
    with probability epsilon_explore = max(0.02, 0.1/(L)): return C else return D

Parameters / tuning guidance
- Window W: 10–20 is reasonable for responsiveness estimation; lower if r small.
- H (persistence horizon): set to 1 for immediate reciprocity; set to 2–3 if you expect effects to persist over multiple rounds.
- min_samples: 3–6. Increase to be more conservative.
- epsilon_explore: small (0.01–0.05), decaying with remaining rounds.
- exploit_threshold: 0.95–0.99 to detect near-unconditional cooperators.

Why this is aggressive and robust
- Aggressive: the default is defect; you exploit unconditional cooperators and you only cooperate when there is a clear long-run advantage. You never give gratuitous cooperation, rarely forgive, and always defect in the endgame.
- Adaptive: the strategy explicitly measures opponents’ responsiveness to your moves (empirical delta_per_round) and only invests when data indicate a profitable return on investment across remaining rounds. It adapts to:
  - pure defectors (it defects),
  - pure cooperators (it defects to exploit),
  - conditional cooperators / reciprocators (it cooperates only if the observed effect is large and persistent enough),
  - mixed / random players (it mostly defects and slowly explores to gather evidence).
- Robust: decisions depend only on observed histories and game parameters; no assumptions about common norms or coordination. The required_delta formula uses the exact payoff trade-off derived from parameters (n, k, L), so cooperation is only taken when it is mathematically justified by empirical opponent responsiveness.

Edge cases summary
- First round: defect (no data).
- Last round: defect (endgame).
- Very short games (small r): almost always defect; cooperating rarely pays off.
- If many rounds remain and you empirically observe strong persistent reciprocal response to your cooperation, you will cooperate to reap bigger long-run gains.
- If multiple opponents are unconditional cooperators, you exploit them by defecting every round.
- If data are scarce, behave more conservatively (higher required uplift before cooperating) and use small-probability exploration.

Implementation notes
- Keep careful counts and rolling averages to compute AvgCoop_next_after_C/D robustly.
- Use smoothing / Laplace correction for small-sample probabilities if desired.
- Logging of responsiveness estimates is helpful for debugging and tournament analysis.
- The strategy is lightweight and deterministic except for controlled exploration randomness.

This strategy is intentionally aggressive: it defaults to defection, exploits naïve cooperators, punishes by withholding cooperation, and only concedes when quantitative evidence shows a durable and profitable return.
'''

description_AGGRESSIVE_101 = '''
Summary (one-line): Be aggressive — default to defect, but probe briefly to find exploitable cooperators, then relentlessly exploit them while avoiding costly long retaliation by switching to safe permanent defection if you detect effective punishment; always defect in the last rounds.

Key idea and intuition
- In a single round defecting strictly dominates cooperating (k/n < 1), so “always‑defect” is a safe baseline. The aggressive aim is to opportunistically exploit any players who are reliably cooperating, while preventing punishers from extracting long-run losses. That is achieved by: (1) a short probe to discover cooperators; (2) an “exploit” mode that defects when there are exploitable cooperators nearby; (3) a conservative fallback (“safe/grim”) that eliminates future exposure to punishment; and (4) guaranteed defection at the end of the game (no future leverage). All decisions depend only on game parameters (n, r, k) and observed history.

Parameters (recommended defaults; implementation can tune)
- T_test = min(2, max(1, floor(r/10))) — length of initial probing phase (usually 1 or 2 rounds; shrink if r small).
- G = min(2, r-1) — final-round horizon for unconditional defection (last G rounds always defect).
- p_probe = min(0.15, 1/max(1, r/10)) — low background probability to probe (cooperate) while otherwise defecting.
- δ = 0.30 — relative drop threshold for detecting punishment (30 percentage points change in others’ cooperation rate over the memory window).
- W = min(5, r-1) — memory window used to compute recent cooperation rates (except initial test uses T_test).

State variables
- Mode ∈ {PROBE, EXPLOIT, SAFE}
- last_coop_counts: history of number of cooperators among other players per round
- cooperations_other_recent = average cooperation rate among the n-1 others over the window W (or T_test during/just after test phase)

Decision rules (natural language)
1. Opening / probe
   - Rounds 1..T_test: play C (cooperate). Purpose: identify unconditional or naive cooperators quickly.
   - After T_test compute cooperations_other_recent (fraction of other players who cooperated during test).
   - If cooperations_other_recent ≥ 1/(n-1) (i.e., at least one other cooperated during test), switch Mode := EXPLOIT. Otherwise Mode := SAFE (default defecting posture, but with rare probes).

2. Default behavior by Mode (for rounds t where t ≤ r - G)
   - SAFE mode: default action D each round. With small probability p_probe (or every M rounds deterministically) play C to test whether cooperators have appeared or punishers softened. If one of these probes shows a sustained cooperation signal (cooperation by ≥ 1 other over W rounds), switch to EXPLOIT.
   - EXPLOIT mode: be opportunistic and aggressive.
     - Let m_prev = number of cooperators among other players in the immediate previous round.
     - If m_prev ≥ 1 (there were cooperators last round), play D this round to capture the extra private payoff from free-riding.
     - If m_prev = 0 (nobody cooperated last round), occasionally play C with probability p_probe to try baiting cooperators back; otherwise play D.
     - Continuously monitor for punishment: compute cooperations_other_recent over the W rounds before and after your recent defections. If after you begin exploiting the others’ average cooperation drops by ≥ δ compared to the cooperation level before your exploitation started (meaning others are retaliating and lowering your realized returns), switch Mode := SAFE (permanent fallback to safe defection for the rest of the game).
     - If exploitation yields stable gains (others keep cooperating), remain in EXPLOIT and continue to defect whenever others are cooperating.

3. Endgame
   - For t > r - G (the last G rounds) always play D (defect). No cooperation in the final rounds since future leverage is gone.

4. Robustness rules / tie-breakers
   - If history ambiguous (e.g., very noisy), prefer SAFE (defect) to avoid being trapped by costly cycles of punishment.
   - If at any time your average per-round payoff over the immediate memory W drops below 1 (the guaranteed payoff from universal defection), prefer SAFE immediately (you are being successfully punished or exploited).
   - If the game is extremely short (r ≤ 3), set T_test = 1 and G = r - 1 (so only a single probing round, then defect).

Pseudocode (concise)
- Inputs: n, r, k; history: for each past round t: actions a_i,t for all players (including self).
- Initialize: Mode := PROBE; T_test, G, p_probe, δ, W as above.
- For round t = 1..r:
    if t ≤ T_test:
        play C
        record others’ cooperations
        if t == T_test:
            compute cooperations_other_recent over test; 
            if cooperations_other_recent ≥ 1/(n-1): Mode := EXPLOIT else Mode := SAFE
        continue to next round
    if t > r - G:
        play D; continue
    if Mode == SAFE:
        with probability p_probe: play C and update history; if observation of sustained cooperation over W rounds then Mode := EXPLOIT
        else play D
    else if Mode == EXPLOIT:
        m_prev := number of other players who played C in previous round
        if m_prev ≥ 1:
            play D    // exploit cooperators
        else:
            with probability p_probe: play C else play D
        // After playing, update cooperations_other_recent over window W
        if (cooperation_rate_before_exploit - cooperation_rate_recent) ≥ δ:
            Mode := SAFE    // detected punishment, fallback
        if average payoffs over recent W rounds < 1:
            Mode := SAFE

Rationale for aggressiveness
- “Exploit when safe”: By default defecting preserves a guaranteed baseline payoff (1 per round). Cooperate only briefly to detect exploitable players, then defect to extract the extra public-good share they provide.
- “No mercy for punishers”: If other players successfully retaliate in a way that reduces your realized payoff, immediately switch permanently to a safe defect posture (grim fallback) to deny future leverage and prevent long-term loss.
- “Endgame ruthlessness”: Always defect in the last rounds—no future rounds to incentivize cooperation.
- “Low-cost probes”: Probing is rare and short so you don’t sacrifice much to find cooperators; once found you exploit them repeatedly.

Why this is adaptive and robust
- Independent of opponents’ internal logic — only observes actions.
- Probing discovers unconditional cooperators; exploitation converts their cooperation into extra personal payoff.
- Punishment detection prevents long-term loss if opponents can enforce cooperation.
- Endgame defection handles backward induction and avoids being exploited at the end.
- Parameterized windows and probabilities let the same high-level strategy work across small and large r and n.

Edge cases
- r = 2 or small r: T_test reduced to 1; G = r - 1 ensures almost immediate defection after minimal probe.
- No cooperators ever detected: Mode becomes SAFE and you essentially play nearly always D (with occasional tiny probes); you secure baseline payoff.
- Many unconditional cooperators: After detection, EXPLOIT yields high payoffs via systematic defection when others cooperate.
- Strong coordinated punishers: After detecting retaliatory cooperation drop (≥ δ) you switch to SAFE to stop being exploited/punished.

Implementation notes for the tournament
- Choose conservative defaults (short T_test, small p_probe, δ ~ 0.25–0.35, W 3–5) for robustness; tune only if you have meta-information about opponent ensembles.
- Logging statistics (others’ cooperation frequency and your average payoff) simplifies the punishment detection condition.
- The algorithm uses only history and the known parameters (n, r, k) so it satisfies the specification.

Final behavioral summary
- Minimal initial cooperation to expose exploitables → then defect to reap short-term gains → if opponents effectively punish, retreat to permanent defection to avoid larger losses → always defect in the final rounds. This is explicit, aggressive, adaptive, and implementable with the above rules.
'''

description_AGGRESSIVE_102 = '''
Name: Aggressive Conditional Defector (ACD)

Short description (mindset)
- Be aggressive: prioritize my own payoff by defecting unless a clear, sustained cooperation environment exists that I can reliably exploit or use to get higher long‑run gains. When others appear cooperative I will exploit them (take the freerider advantage) and, if exploited or cooperation weakens, I will punish harshly and rapidly shift to defection. I give very limited, strategic forgiveness only when cooperation is stable and produced by reliably cooperative opponents.

What I observe and keep
- Game parameters: n, k, r (known).
- Full history: in each past round I know every player’s action (C/D).
- Internal state and derived values (all deterministic and based only on history and parameters):
  - For each opponent j: S_j = exponentially weighted cooperation score (range 0..1). Update each round: S_j ← λ·S_j + (1-λ)·1 if j played C, else S_j ← λ·S_j + (1-λ)·0. Use λ = 0.6 (recent rounds weighted).
  - recent_frac = average fraction of other players who cooperated over last L rounds (L = min(5, r-1)).
  - reliable_set R = { j ≠ me | S_j ≥ 0.75 } (players who reliably cooperate).
  - punish_until: round index until which I stay in punishment mode (initialized 0).
  - last_round_coop_frac = fraction of other players cooperating in previous round (0 for t=1).

Fixed strategy hyper-parameters (computed once from parameters or set constants)
- lookback L = min(5, r-1)
- endgame_window E = min(3, r-1)  (last E rounds: unconditional defection)
- exploit_threshold α = 0.60 (if recent_frac ≥ α we consider the environment "cooperation-heavy" and enter exploitation)
- exploit_exit_threshold β = max(0.35, α - 0.25) (exploit phase ends when cooperation falls below β)
- reliable_min = max(1, floor(0.15·(n-1))) (minimum number of reliable cooperators required to ever consider cooperating)
- punishment_duration P = min(5, r-1)
- EWMA decay λ = 0.6
- probe_prob ε = max(0.02, min(0.1, 1/r)) (tiny randomized probe only for first round if desired) — optional; strategy can be deterministic by always defecting on first round.

Decision rules (deterministic; randomized probe optional)
1. Endgame: If current round t > r - E: play D (defect). Rationale: final rounds are dominated by defection; aggressive player exploits final shot.

2. Punishment mode: If t ≤ punish_until: play D. Punishment is global; I do not cooperate while punishing.

3. First round (t = 1):
   - Default: play D (aggressive). Optionally: with small probability ε play C to probe (implementation choice). If deterministic required: play D.

4. For t ≥ 2 and not in endgame or punishment:
   - Compute recent_frac = average over last L rounds of (#others who cooperated)/(n-1).
   - If recent_frac ≥ α (cooperation-heavy environment) then enter Exploit Phase:
        - Action: Defect (D).
        - Remain in Exploit Phase (continue defecting) until recent_frac falls below β. Do not immediately cooperate in this phase — my aggression is to keep taking the freerider gain while cooperation lasts.
        - If I ever decide to cooperate in this phase (implementation variant), it must be only to maintain a minimum credibility (rare; not recommended for aggressive play).
     Rationale: when many others are cooperating, defecting gives the largest immediate payoff advantage. Exploit while the environment is permissive.

   - Else (recent_frac < α):
        - Conditional cooperation check:
          - If |R| ≥ reliable_min AND recent_frac ≥ β:
             - Coop-allow: play C for this round to try to rebuild/encourage a small stable cooperating core.
             - But immediately monitor next rounds; if cooperation promptly drops by more than δ_drop = 0.3 after I cooperate, interpret that as being exploited and set punish_until = t + P and switch to D (punish).
          - Otherwise: play D.
     Rationale: only cooperate when a sufficient tiny core of reliable cooperators exists and overall cooperation is above the lower bound; otherwise defect to avoid being a sucker.

5. Punishment rule (responsive, aggressive):
   - If I cooperate in round t and in round t+1 the fraction of other players cooperating drops by more than δ_drop = 0.3 compared to last_round_coop_frac, then set punish_until = t+P and play D for the punishment duration.
   - Also, if any S_j (reliability) falls permanently below 0.3 after previously being ≥ 0.75, consider that the cooperating base is dissolving; immediately set punish_until = t+P.
   Rationale: harsh, fast punishment to teach others not to exploit my occasional cooperation.

6. Forgiveness / recovery:
   - After punish_until expires, start only with conditional cooperation (step 4) — do not immediately return to exploit phase. Require rebuilding of reliable_set R and sustained recent_frac ≥ β before any cooperation resumes.
   - If stable cooperation persists (recent_frac ≥ α for a sustained window of L rounds) I will again enter Exploit Phase and defect for immediate profit.

7. Special handling of small n or extreme k:
   - If k is very close to n (k/n ≈ 1), public good almost fully returns; cooperating can be nearly break-even. However Δ = k/n - 1 still < 0 by assumption k < n, so the base rule still applies. The thresholds above remain valid; the strategy relies on observed cooperation, not the fine marginal payoff calculus.
   - If n = 2 (pairwise), the logic still works; reliable_min becomes 1, and the strategy reduces to an aggressive tit-for-tat-like behavior with exploitation and punishment.

Pseudocode (concise)
- Initialize S_j = 0 for all j ≠ me, punish_until = 0
- For each round t = 1..r:
  - If t > r - E: action ← D; update S_j with observed actions; continue
  - If t ≤ punish_until: action ← D; update S_j; continue
  - If t = 1: action ← D (or with prob ε set action ← C as probe)
  - Else:
     - Compute recent_frac = average over last L rounds of (#others who cooperated)/(n-1)
     - Update R = {j | S_j ≥ 0.75}
     - If recent_frac ≥ α:
         action ← D  (Exploit Phase)
     - Else:
         If |R| ≥ reliable_min AND recent_frac ≥ β:
             action ← C
         Else:
             action ← D
  - After actions observed for round t:
     - Update each S_j ← λ·S_j + (1-λ)·1{j played C}
     - If I played C this round and (next round) cooperation among others drops by > δ_drop, set punish_until ← t + P

Why this is aggressive and robust
- Aggressive: I defect by default, exploit when most others cooperate (take the freerider advantage), and impose harsh punishments when I judge I’ve been exploited or cooperation collapses. I rarely cooperate, only when a small, reliable cooperating core exists and the cooperation level justifies exposing myself to risk. Punishment is swift and covers multiple rounds so opponents learn the cost of exploiting my cooperation.
- Robust: decisions are based only on observed history and parameter-driven thresholds (n, r used to set windows). The EWMA scores allow adapting to noisy opponents and recent behavior. The strategy handles noisy cooperators, cycles, and a variety of opponent types (always-cooperate, always-defect, conditional reciprocators, random).
- Adaptive: I switch between pure defection, limited cooperation to sustain a stable sub-cohort, exploitation when many cooperate, and punishment when exploited. The strategy uses short memory windows to follow changing environments. Parameters (α, β, λ, P, L) are conservative and work across ranges of n, k, r; implementers may tune them for tournament specifics.

Edge cases summary
- First round: defect (maybe tiny probe probability).
- Last E rounds: unconditional defection.
- End of game known: I do not cooperate in terminal rounds.
- If r is very small (r = 2,3) the lookback and endgame windows shrink appropriately and the strategy is heavily defection-biased.
- If many reliable cooperators exist and sustain cooperation, I will exploit that environment repeatedly (defect during exploit phases) to maximize immediate gains while still occasionally permitting conditional cooperation when a reliable core is present.

Implementation notes
- The algorithm is implementable with only action histories and parameters.
- All thresholds are explicit; tournament implementer can expose or tune them. The EWMA λ ensures more weight on recent behavior so the strategy adapts quickly.
- If a deterministic policy is required, omit the ε probe and any probabilistic choices; the spec above is deterministic (except optional probe).

Summary one-liner
- Default to defection, exploit whenever observed cooperation is high, cooperate only rarely with a small reliable core, and punish harshly and quickly when exploited — a deliberately aggressive, history-driven policy that maximizes individual payoff while adapting to opponents’ behaviors.
'''

description_AGGRESSIVE_103 = '''
Strategy name: Aggressive Exploiter (AEX)

Intuition (short)
- Default: be a defector. Only cooperate when doing so is likely to create a profitable stream of future cooperation you can then exploit.
- Quick detection: use a short memory (recent rounds) to detect conditional cooperators and stable cooperation.
- Exploit stable cooperation with a repeating “cooperate once, defect more” cycle that maximizes short-term gain while keeping others’ cooperation alive enough to be exploited further.
- Punish and never forgive carelessly: if cooperation collapses, revert to permanent defection (with a few early probes only).

This strategy uses only game parameters (n, r, k) and observed history (who cooperated in past rounds). It requires no communication or coordination.

Parameters derived from game inputs
- L = min(3, r-1)  // short memory: look back at up to 3 prior rounds
- t_seed = max(2, min(r-1, ceil(0.15 * r)))  // early probing window
- phi_high = 0.80   // threshold for “strong/stable cooperation” among others
- phi_med  = 0.50   // threshold for “some cooperation”
- exploit_cycle = [C, D, D]  // repeat while environment remains cooperative
- cycle_len = 3
- last_round = r

Notes:
- k/n < 1 (by spec), so immediate one-shot incentive is to defect; cooperation only pays via future rounds. AEX exploits that by being mostly D, probing rarely, then extracting surplus from conditional cooperators.

Definitions (use from history)
- For each past round t, let total_coop[t] = number of players who played C in round t.
- When evaluating others’ behavior in round t, exclude your own action: others_coop_fraction[t] = (total_coop[t] - my_action_c[t]) / (n - 1)
- For current decision at round t (t ≥ 2) compute f = average of others_coop_fraction over last L rounds (if fewer than L rounds exist, average what exists).

Decision rules (priority order)
1. Last round: If t == last_round, play D. (No future to sustain reciprocity; be aggressive.)

2. First round: If t == 1, play D. (Aggressive default test/probe: start as defector to establish a hard-to-coerce reputation and collect immediate payoff.)

3. Identify environment strength: compute f (recent average of others’ cooperation fraction over last L rounds).

4. If f ≥ phi_high (strong/stable cooperation observed recently)
   - Enter/continue the exploitation cycle:
     - Maintain a deterministic cycle index cycle_pos that increments each time you act while in this state (initialize cycle_pos = 0 when you first detect f ≥ phi_high).
     - Play action = exploit_cycle[ cycle_pos mod cycle_len ].
       - That is: cooperate on one round, then defect for two rounds, then repeat.
     - After each round re-evaluate f. If f falls below phi_med, abandon cycle (go to step 6).

   Rationale: when many others are reliably cooperating, cooperating sometimes keeps the cooperative environment alive; defecting in the other positions extracts excess payoff for you. The 1:2 (C:DD) pattern is intentionally aggressive — you free-ride often while contributing rarely enough to reduce wholesale collapse in many conditional populations.

5. If phi_med ≤ f < phi_high (moderate cooperation)
   - Opportunistic behavior:
     - If t ≤ t_seed AND the most recent round (t-1) had others_coop_fraction[t-1] ≥ 0.5, play C this round (a targeted probe/join to try to seed stronger cooperation).
     - Otherwise play D.
   Rationale: in moderately cooperative environments, only try to join early and when recent evidence shows at least half others cooperated. Otherwise defect.

6. If f < phi_med (uncooperative environment)
   - Default defection: play D every round.
   - Early-probing exception: during the early probing window (t ≤ t_seed), attempt at most one deterministic probe per 3 rounds:
     - If t ≤ t_seed and (t mod 3) == 1 and the most recent round had at least one other cooperator, then play C (deterministic probe). Otherwise play D.
   Rationale: when others rarely cooperate, avoid contributing (aggressive). But seed probing early and sparsely to detect hidden conditional cooperators; probes are limited so you don’t pay much cost to reveal them.

7. Punishment and recovery
   - If you are currently in an exploitation cycle (step 4) and you defect while a majority of others retaliate (f drops below phi_med), immediately stop cooperating forever (switch to always D for the remainder except for deterministic early-probe exceptions if t ≤ t_seed). Do not “forgive” unless a clear sustained reappearance of cooperation is detected (f ≥ phi_high again); even then restart the cycle only after phi_high is observed for L consecutive rounds.
   Rationale: harsh, fast retaliation and slow forgiveness maintain deterrence and preserve exploitation opportunity.

Pseudocode (high-level)
- Inputs: n, r, k; history arrays total_coop[1..t-1], my_actions[1..t-1]
- Compute L, t_seed, phi_high, phi_med, exploit_cycle, cycle_len
- If t == r or t == 1: return D
- Compute f = average_over_last_L_rounds( (total_coop[t'] - my_actions[t']) / (n - 1) )
- If f >= phi_high:
    if cycle_state not initialized: cycle_pos = 0
    action = exploit_cycle[ cycle_pos mod cycle_len ]
    cycle_pos += 1
    return action
- Else if phi_med <= f < phi_high:
    if t <= t_seed and (total_coop[t-1] - my_actions[t-1]) / (n-1) >= 0.5:
        return C
    else:
        return D
- Else (f < phi_med):
    if t <= t_seed and ((t mod 3) == 1) and (total_coop[t-1] - my_actions[t-1]) >= 1:
        return C  // deterministic probe
    else:
        return D

Behavioral summary and aggressive alignment
- AEX is aggressive: majority of time it defects (maximizing immediate gain), defects in the first and last rounds (no forgiveness when final), and uses harsh retaliation (abandon cooperation rapidly when others do not reciprocate).
- AEX is adaptive: it detects stable cooperation quickly (L ≤ 3) and exploits it with a repeating C–D–D pattern that keeps cooperators around but extracts excess payoff.
- AEX is robust: deterministic, parameterized by r and short-memory statistics, with conservative probing rather than naive unconditional cooperation so it won’t be extensively exploited by consistent defectors. It also tolerates noisy/partial cooperation by using phi_med and phi_high thresholds.

Edge cases
- Very small r (r = 2 or 3): t_seed and L shrink appropriately; the strategy cooperates only in earlier rounds if early probes indicate cooperation—otherwise it defects. It still defects in final round.
- Small n (e.g., n = 2 = Prisoner’s Dilemma variant): thresholds are computed on fractions; behavior reduces to defect-first, then opportunistic cooperate-if-partner has cooperated, and last-round defect.
- High k (close to n): because k/n < 1 always, immediate incentive to defect remains; AEX still follows above rules. If opponents are extremely cooperative and likely to sustain cooperation despite occasional exploitation, AEX will exploit via cycle.
- Deterministic probes avoid pathological oscillations from randomness and make behavior reproducible in tournament settings.

Tuning notes (implementer can adjust parameters)
- shorten/lengthen L for faster/slower adaptation
- adjust phi_high/phi_med to be more/less trusting
- change exploit_cycle ratio (e.g., [C, D] or [C, D, D, D]) to be more or less aggressive
- allow probabilistic probes if randomness is permitted in tournament

Conclusion
AEX favors defection, probes early to detect conditional cooperators, rapidly exploits stable cooperation via a deliberate cooperate-rarely/defect-often cycle, and punishes failure to reciprocate harshly. It is aggressive, adaptive, parameter-dependent, and robust across a wide variety of opponent behaviors.
'''

description_AGGRESSIVE_104 = '''
Name: Aggressive Grim-Threshold (AGT)

Short description
- AGT is an aggressive, history-dependent rule: it only cooperates when almost everyone else is cooperating, it punishes any exploitation harshly (long punishment until clear reformation), it defects in the final round, and it probes briefly at the start to discover cooperators. The strategy requires only the game parameters (n, r, k) and the history of past actions; no assumptions about opponents’ norms or coordination.

Intuition / aggressive mindset
- Demand near-unanimous cooperation before you give up your private endowment (so you are not an easy target for partial cooperators or free-riders).
- If you are exploited even once while cooperating, respond with a long, harsh punishment phase (grim-like) so the cost of exploiting you is high.
- Stop trying to be nice near the end (last round) where threats are not credible.
- Allow grudging re-entry to cooperation only after clear, sustained evidence of universal cooperation from others; forgiveness is rare and requires unanimous signals.

Parameters used (derived from n, r, k)
- coop_threshold M = n - 1 (you will only cooperate if, in the previous round, every other player cooperated).
  - Rationale: a high threshold reduces your vulnerability to exploitation and is simple and robust across k. (You may tune M downward if you know k is very high and you want more risk-taking, but AGT uses n-1 by default.)
- probe_rounds P = min(2, r - 1). (At most two inaugurating cooperative probes; never probe in the last round.)
- forgiveness_window S = 2 consecutive rounds of unanimous cooperation by all players required to clear punishment.
- punishment mode = defect until forgiveness condition is met (aggressive / long punishment). In practice you set a flag punish = true and continue defecting until reset by observation of S unanimous cooperative rounds.
- Last-round behavior: always defect in round r.

Full decision rules (natural language)
1. Initialization:
   - Set punish = false.
   - Set consecutive_unanimous = 0.
   - Set t = 1 (current round index).

2. Round t (for t = 1..r):
   - If t == r (final round): play D (defect). End of strategy for this round.
   - Else if t ≤ P (probe phase):
       - Play C (cooperate) for the first P rounds to test whether there are players willing to achieve near-unanimous cooperation.
   - Else (t > P and not final round):
       - If punish == true:
           - Play D.
           - Observe the round outcome (how many cooperated).
           - If every player (all n) cooperated this round, increment consecutive_unanimous by 1; otherwise reset consecutive_unanimous = 0.
           - If consecutive_unanimous ≥ S then set punish = false and consecutive_unanimous = 0 (forgive and resume normal operation next round).
       - If punish == false:
           - Look at previous round’s total cooperators count (call it C_prev).
           - If C_prev ≥ M (i.e., all other players cooperated in the previous round), play C.
           - Otherwise play D.
           - After observing this round’s actions:
               - If you played C this round and any other player defected this round (i.e., total cooperators < n), set punish = true and set consecutive_unanimous = 0 (enter punishment mode immediately starting next round).
               - (Also: if you played D and a majority defected, you remain in non-punish state; only get punished if you were cooperating and someone exploited you.)

3. End of game: total payoff is the sum of round payoffs as usual.

Pseudocode
(variables: punish (bool), consecutive_unanimous (int), t current round,
 P = min(2, r-1), M = n-1, S = 2)

Initialize: punish = false; consecutive_unanimous = 0
for t in 1..r:
  if t == r:
    action = D
    play(action); observe round outcome; break or end
  if t <= P:
    action = C
    play(action); observe outcome
    if action == C and (observed_cooperators < n):  // exploited during probe
      punish = true
      consecutive_unanimous = 0
    continue loop
  // after probe and not final round
  if punish:
    action = D
    play(action); observe outcome
    if observed_cooperators == n:
      consecutive_unanimous += 1
    else:
      consecutive_unanimous = 0
    if consecutive_unanimous >= S:
      punish = false
      consecutive_unanimous = 0
    continue loop
  else: // not punish
    if previous_round_cooperators >= M:
      action = C
    else:
      action = D
    play(action); observe outcome
    if action == C and observed_cooperators < n:
      punish = true
      consecutive_unanimous = 0
    continue loop

Edge cases and clarifications
- First round(s): AGT does at most two cooperative probes (unless r=2, in which case P=1). Probes are intended to reveal whether there are players willing to cooperate near-unanimously. If exploited during a probe, you switch immediately to punishment.
- Final round: always defect (D). This is required by backward-induction logic and makes your punishments credible only while they can be costly to the exploiter.
- If you never experience exploitation while cooperating, you will keep cooperating only when you observe almost-unanimous cooperation by others (C_prev ≥ n-1). That keeps you from being a soft target for sloppy partial cooperation.
- Punishment is aggressive (long): you only exit punishment after S consecutive rounds in which everyone cooperates. S=2 is intentionally strict so a single accidental unanimous round doesn’t immediately restore trust; change S upward for even harsher behavior.
- If the population is a stable set of full cooperators (unanimous cooperation each round), AGT will cooperate (after probes or once it detects unanimity) and will not attempt to exploit them (exploitation would trigger punishment).
- If some opponents try to systematically alternate or play forgiving strategies, AGT’s high threshold and long punishment make it robust: exploiters receive immediate gains but then face long-term losses when punished.
- Parameter tuning: M = n-1 and S = 2 are conservative (aggressive) defaults. If you prefer a slightly less harsh variant (e.g., when k is close to n so cooperation is especially valuable), you can reduce M by 1 or reduce S to 1. The provided defaults are robust across unknown opponent mixes.

Why this is robust and aggressive
- Robust: decisions depend only on observed cooperators count and simple counters; no assumptions about opponents; reacts consistently to any history.
- Aggressive: demands near-total cooperation before surrendering private payoff; uses immediate and long punishments when exploited; minimal forgiveness; defects in last round to avoid being exploited through endgame reasoning; probes are limited so you do not give away value for long to unknown opponents.
- Adaptive: probes detect cooperative groups, thresholding lets you join near-unanimous cooperative groups, punishment enforces deterrence; forgiveness requires clear evidence of reformed unanimous cooperation.

Implementation notes for tournament use
- Implement M = n-1, P = min(2, r-1), S = 2 as defaults. They are simple to code and deterministic.
- Track previous_round_cooperators and your own last action to determine if you were exploited.
- Make sure to update punish and consecutive_unanimous only after observing the full round outcome.
- If you want to tune aggression vs. willingness to cooperate, expose M and S as strategy knobs: lower M or S → more cooperative; higher M or S → more punitive.

This AGT strategy will not be the most generous but will be hard to exploit, quick to punish selfishness, and will occasionally join very disciplined near-unanimous cooperation if it exists — matching the aggressive enforcement mindset required.
'''

description_AGGRESSIVE_105 = '''
Name: Predatory Adaptive Defector (PAD)

High-level idea
- Be aggressively self-interested: default to defect (D) to exploit cooperators and collect the private payoff.
- Be adaptive: monitor how opponents react to your defections. If opponents are forgiving (they keep cooperating despite your defections), keep exploiting. If opponents are retaliatory (they sharply reduce their cooperation after you defect), switch to the minimal cooperation needed to avoid sustained punishment — but only when that yields better long-run payoff for you.
- Never be suckered on the last rounds: always defect in the final few rounds where punishment is not credible.

Summary of the aggressive spirit
- Exploit cooperation whenever it is safe to do so.
- Punish or avoid opponents who punish you (i.e., if they will make you worse off by retaliating).
- Use rare probes (small-probability cooperation) to learn whether opponents will tolerate exploitation; exploit permanently if they do.
- Refuse to be bound by group norms; maximize your own cumulative payoff.

Parameters (computed from game parameters and set conservatively)
- tail = max(1, floor(r / 10)) — number of final rounds where we always defect (at minimum 1). This prevents endgame exploitation by others and ensures we do not cooperate when punishment cannot be enforced.
- window = min(5, r - 1) — how many recent rounds we use to estimate opponents' responses.
- probe_init = 0.10 — initial probability of a probe cooperation when we want information.
- probe_decay = 0.90 — multiply probe probability by this after each probe.
- rho_threshold = 0.30 — threshold for declaring “opponents are retaliatory” (see below).
- coop_maintain_threshold = 0.60 — threshold fraction of cooperators in previous round that makes “maintaining cooperation” potentially valuable.

These numbers are tunable; they are chosen to be robust across many opponent behaviours.

State tracked from history
- For each round t we observe the vector of actions of all players. From history we compute per-round cooperation fraction f_t = (# cooperators)/n.
- Keep records of rounds in which we defected, and how f changed in subsequent rounds. Use these to estimate a retaliation metric.

Decision rules (natural language)
1. Last rounds:
   - If the current round t is within the final tail rounds (t > r - tail), play D (always). Punishment is not credible there.

2. First round:
   - Play D. (Aggressive default; also yields information in early history.)

3. Probing:
   - If we are uncertain about opponents’ willingness to tolerate defections and we have sufficient rounds remaining, use occasional low-probability probes (cooperation) to test forgiveness.
   - A probe is taken only when there is no clear signal yet (see “retaliation metric” below). Probe probability starts at probe_init and decays by probe_decay each time we use a probe.

4. Retaliation metric and classification:
   - Consider the most recent window of rounds.
   - For each recent occurrence where we defected in round t0, compute delta = f_{t0+1} - f_{t0}. If delta <= -d_drop (choose d_drop = 0.15), that indicates a meaningful drop in cooperation following our defection.
   - Let retaliation_rate = (# of such meaningful drops after our defections in the window) / (# of our defections in the window).
   - If retaliation_rate >= rho_threshold, classify opponents as “retaliatory.” Otherwise classify as “forgiving.”

5. Action if opponents are forgiving:
   - Play D (exploit), except:
     - If we choose to probe (small decaying probability), play C that round to collect information.
   - Rationale: if others forgive defections, repeated defection yields higher individual payoff; exploit.

6. Action if opponents are retaliatory:
   - If the previous-round cooperation fraction f_{t-1} >= coop_maintain_threshold and there are enough rounds remaining to benefit from preserving cooperation (i.e., remaining rounds >> tail), then play C to avoid triggering coordinated punishment that would lower your long-run payoff.
   - Otherwise (previous cooperation level is low or not enough future rounds to make cooperation payoff-positive), play D.
   - Exception: If you are testing opponents (rare probe) to see if their retaliation softened, you may probe (with decayed probe probability).
   - Rationale: avoid being the cause of a sustained coordinated collapse in cooperation that reduces your cumulative payoff; but only cooperate when preserving cooperation is plausibly beneficial.

7. Targeted exploitation of persistent cooperators:
   - If a detectable subset of players (≥1) consistently cooperate every round while the rest defect, exploit them by defecting every round. (No coordination needed; just defect.)

8. Behavior after being punished:
   - If you were followed by a big drop in f (i.e., others punished after your defection), immediately switch to cooperative maintenance behavior (see item 6) until you detect forgiveness again. Then resume exploitation.

Pseudocode

Set tail = max(1, floor(r/10))
Set window = min(5, r-1)
probe_prob = probe_init

For each round t = 1..r:
  if t > r - tail:
    action = D
    continue

  if t == 1:
    action = D
    continue

  // compute recent statistics
  Let f_prev = cooperation fraction in round t-1
  Let lookback = last up to 'window' rounds
  Let our_defection_rounds = rounds in lookback where our action was D
  If our_defection_rounds is empty:
    retaliation_rate = 0 (no evidence of retaliation)
  Else:
    count_drops = 0
    For each d in our_defection_rounds:
      If round (d+1) exists:
        delta = f_{d+1} - f_d
        If delta <= -0.15:
          count_drops += 1
    retaliation_rate = count_drops / max(1, len(our_defection_rounds))

  // classify opponents
  forgiving = (retaliation_rate < rho_threshold)

  // probing decision (only when forgiving or unclassified and not near end)
  If forgiving and random() < probe_prob and (t <= r - tail - 1):
    action = C   // probe
    probe_prob *= probe_decay
    continue

  If forgiving:
    action = D   // exploit forgiving population
    continue

  // opponents are retaliatory
  // cooperate to maintain cooperation only if high prior cooperation and many rounds left
  rounds_left = r - t + 1
  If f_prev >= coop_maintain_threshold AND rounds_left > (tail + 1):
    action = C
  Else:
    action = D

Notes and reasoning for key design choices
- Default defecting is aggressive and individually dominant in one-shot rounds (k < n). PAD exploits this when it is safe (opponents won’t reliably punish).
- Probes let PAD discover forgiving opponents without permanently exposing itself; probe probability decays to minimize long-run cost of probing.
- The retaliation metric is simple and robust: it detects whether the population reduces cooperation after PAD defects. If others punish, PAD avoids provoking them because the retaliatory equilibrium can lower PAD’s total payoff long-term.
- Tail rounds: finite horizon requires endgame caution. We always defect in the final few rounds because punishment cannot be enforced then.
- PAD is intentionally opportunistic: when the population sustains high cooperation and will punish defectors, we cooperate just enough to avoid punishment. When the population tolerates defectors, we exploit continuously.
- Targeted exploitation of persistent cooperators: if some players always cooperate regardless of signal, PAD will permanently exploit them because that maximizes PAD’s payoff.

Edge cases
- r is very small (r = 2 or 3): tail will be at least 1. PAD will defect first and in the final round(s) always defect — this is consistent with backward-induction-safe aggressive play.
- All opponents always cooperate: PAD defects always and gains maximal exploitation payoff.
- All opponents always defect: PAD defects (no difference).
- Some opponents implement “grim trigger” (punish permanently after any defection): PAD will detect retaliation in a few rounds and switch to cooperative maintenance if cooperating yields higher expected payoff (i.e., if a sufficiently large fraction is cooperating consistently and enough rounds remain).
- Mixed/forgiving populations: PAD will exploit them relentlessly after probes confirm forgiveness.
- Small n: thresholds (like coop_maintain_threshold) are fractions; they work for any n ≥ 2. window and tail scale with r.

Why this is aggressive and robust
- Aggressive: default is exploitative defecting, with minimal, strategic cooperation only when necessary to avoid long-run punishment that would reduce your own payoff. Probes ensure you will find exploitable opponents and then exploit them repeatedly.
- Robust: the strategy reacts to observed opponent behaviour only — it does not assume norms or that others are rational. Simple thresholds and a short window avoid overfitting noise. The tail rule avoids being tricked in the final rounds. Probe decay prevents long-term costly generosity.

Implementation notes for programmers
- Use sliding-window statistics for the retaliation metric to keep computation cheap.
- Randomized probes require a good RNG; make probe decisions independent across rounds.
- Tune parameters (probe_init, rho_threshold, coop_maintain_threshold, d_drop, window, tail) if you know likely opponent classes in the tournament.

This strategy produces a strongly self-serving, adaptive, and aggressive agent: it defects by default to exploit, learns whether exploitation is tolerated, punishes or yields to opponents’ punishment only when that is strictly necessary to protect long-run payoff, and always defects in the endgame.
'''

description_AGGRESSIVE_106 = '''
Strategy name: Aggressive Opportunistic Exploiter (AOE)

Summary intuition (one line)
- Start by “probing” with limited cooperation to identify cooperators; then aggressively defect to exploit identified unconditional cooperators and defectors, use short harsh punishments against those who defect while you cooperated, and always defect in the final round(s) to harvest one-shot gains.

Key design principles
- Adaptive: uses observed history (per-player cooperation frequencies and conditional responses) to classify opponents and pick the best mode.
- Aggressive: prioritizes own payoff; exploits unconditional cooperators and defects against defectors; punishes quickly and decisively to deter being used.
- Robust: works against AllC / AllD / conditional cooperators / random players by (a) probing, (b) classifying, (c) switching to exploitation or conditional cooperation as appropriate.
- Self-contained: only uses game parameters (n, r, k) and public action history.

Notation and derived facts used
- t = current round index (1..r). remaining_rounds R = r − t + 1.
- In any single round the marginal private gain from cooperating vs defecting is Δ = k/n − 1 (< 0 under game assumptions), so pure one-shot cooperation loses. Cooperation must be justified by expected future reciprocation.
- History: for each player j we track total cooperations C_j and conditional cooperations after we cooperated C_j|meC and after we defected C_j|meD, using the most recent L rounds (or full history if fewer than L available).

Tunable internal parameters (defaults provided; implementer can tune)
- ProbeRounds = min(2, r − 1) — number of initial rounds where we probe.
- Lookback L = min(6, t − 1) — window for estimating opponent rates (use full history if small).
- UncondCoopThresh = 0.9 — classify j as unconditional cooperator if freq ≥ 0.9.
- CondCoopThresh = 0.6 — conditional cooperators if they cooperate much more after our C.
- RecentGroupCoopThresh = max(0.5, 0.5 + 0.25*(k/n)) — if recent fraction of cooperators ≥ this we consider group largely cooperative.
- PunishLength P = min(3, R − 1) — how many rounds we punish after a defection that exploits us.
- ExploitWindow S = max(1, ceil(R/3)) — how many final rounds we will defect to harvest exploitation once cooperation is established.

Decision rules (high-level)
1. Always defect in the last round: if R == 1 then play D.
2. Probe phase: for t ≤ ProbeRounds, play C (cooperate) to reveal who will reciprocate. (This makes you attractive to unconditional cooperators and conditional reciprocators.)
3. After probe, classify opponents using observed frequencies over the Lookback window:
   - Unconditional Cooperator (U): freq_j ≥ UncondCoopThresh.
   - Conditional Cooperator (Q): freq_j|meC ≥ CondCoopThresh and freq_j|meD ≪ freq_j|meC (they respond to our cooperation).
   - Defector (Z): freq_j ≤ (1 − UncondCoopThresh) (close to 0).
   - Others: Mixed/Random.
4. Strategy modes (choose one each round):
   - Exploit Mode: If there exists any Unconditional Cooperator (U), or the recent group cooperation fraction ≥ RecentGroupCoopThresh, switch to Exploit Mode: play D every round (except use occasional short probes described below) to capture the one-shot gain from their cooperation.
   - Conditional-Cooperation Mode: If many players are Conditional Cooperators (Q) and few U or Z, sustain cooperation through mutual reciprocation for most of the remaining horizon, but reserve an exploitation window near the end:
       * Cooperate as long as the last round’s cooperation by the majority of Q players stayed ≥ CondCoopThresh.
       * If cooperation by Q players falls below CondCoopThresh, immediately shift to Defect and enter Punish.
       * When remaining_rounds ≤ S (ExploitWindow), switch to Defect permanently (exploit the built-up cooperation before game end).
   - Defect Mode: If most players are Defectors or mixed and no clear Q group, defect every round (no point giving away endowment).
5. Punishment rule (harsh, short): If in any round you cooperated and the total number of cooperators in that round dropped sharply vs the previous round (or a majority of others defected while you cooperated), then retaliate by defecting for P rounds (PunishLength). If punishments do not change behavior, remain in Defect Mode.
6. Periodic probing to discover new cooperators: while in Exploit or Defect mode you should periodically (once every max(5, S) rounds) play a single C as a probe to see whether more players are willing to reciprocate; if the probe yields a sticking pattern of reciprocation (≥ CondCoopThresh in next 2 rounds), reclassify and switch to Conditional-Cooperation Mode.

Pseudocode (compact)

Initialize counts and parameters.
For each round t from 1..r:
  R = r − t + 1
  If R == 1: play D; continue
  If t ≤ ProbeRounds: play C; update history; continue

  Compute for each j (using last L rounds):
    freq_j = fraction of rounds j played C
    freq_j|meC = fraction of rounds j played C in rounds where I played C
    freq_j|meD = fraction where I played D

  Classify sets:
    U = {j: freq_j ≥ UncondCoopThresh}
    Q = {j: freq_j|meC ≥ CondCoopThresh AND freq_j|meD ≤ freq_j|meC − 0.2}
    Z = {j: freq_j ≤ 1 − UncondCoopThresh}
  group_recent_coop = fraction of players who played C in previous round

  If currently in Punish (remaining punish rounds > 0):
    play D; decrement punish counter; continue

  If U not empty OR group_recent_coop ≥ RecentGroupCoopThresh:
    // Exploit mode
    play D
    Every probe_period (once every max(5,S) rounds) play a single C as probe; if probe returns reciprocation from ≥ ceil(|players|*CondCoopThresh), switch to Conditional-Cooperation Mode
    continue

  Else if |Q| ≥ max(1, ceil(0.4*(n−1))):  // many conditional cooperators
    If R ≤ S: play D (exploit window)
    Else:
      // attempt to sustain cooperation to harvest future rounds
      If majority of Q cooperated last round: play C
      Else: play D and set Punish counter = P
    continue

  Else:
    // mostly defectors or mixed: defect
    play D
    continue

  If in any round I played C and a majority of others defected while I cooperated:
    set Punish counter = P and switch to Defect Mode

End pseudocode

Edge cases and special handling
- Very short games (r = 2 or 3): ProbeRounds = min(2, r−1) still applies: for r=2 we will cooperate in round 1 as a probe and defect in round 2 to exploit pure cooperators. This is aggressive but optimal against naive cooperators. If most opponents are defectors, we will quickly switch to defect.
- If all opponents are unconditional cooperators: AOE defects repeatedly after detection and gets maximal exploitation payoff.
- If all opponents are defectors: AOE defects every round (no loss).
- If opponents are TFT-like conditional cooperators: AOE can sustain mutual cooperation for long stretches under Conditional-Cooperation Mode to harvest future rounds, but will defect in the defined ExploitWindow near the end, capturing last-round gains.
- If opponents are unpredictable/random: AOE will converge to Defect Mode (safe play) but keeps periodic probes in case cooperators appear.

Why this is “aggressive”
- Exploitative behavior: unconditional cooperators are detected quickly and then exploited (D every round).
- Harsh, quick punishment: if you are taken advantage of, you retaliate for P rounds (not a forgiving slow response).
- End-game exploitation: you deliberately withhold cooperation in the final S rounds to guarantee one-shot gains after inducing cooperation earlier.
- Probing and opportunism: proactively seek exploitable cooperators rather than presuming cooperation.

Robustness remarks and parameter tuning
- The key thresholds (UncondCoopThresh, CondCoopThresh, RecentGroupCoopThresh) and windows (L, P, S) control aggressiveness vs patience. Raise CondCoopThresh and UncondCoopThresh to be more conservative about labeling cooperators; reduce to be more exploitative.
- Use larger Lookback L in long tournaments to reduce noise; use smaller L to react faster when opponents adapt quickly.
- If the tournament rewards long-term mutual cooperation against many conditional agents, reduce S so you don’t burn long-term gains with end-game defecting; if the tournament contains many naive cooperators, increase exploitation aggressiveness.

Short behavioral summary vs common opponent types
- AllC: detect and defect every round → exploit maximum.
- AllD: defect every round → avoid losses.
- TFT-like: cooperate for long stretches, defect in final S rounds (extract end-game gains).
- Mixed/random: defect after probes conclude no reliable reciprocation; occasional probes may discover pockets of cooperators to exploit.

Implementation notes for the programmer
- Keep per-player rolling windows and simple counters for frequencies; do not require belief modeling or complex forecasting.
- Store a “punish counter” and a simple state: {Probe, Exploit, ConditionalCoop, Defect, Punish}.
- All thresholds are implementable as explicit numeric checks on frequencies.

This strategy balances aggression (detect-and-exploit; end-game defection; harsh punishments) with adaptiveness (probing, classification, conditional cooperation when profitable). It is designed to maximize an individual’s total payoff against a wide distribution of opponent behaviors.
'''

description_AGGRESSIVE_107 = '''
Name: Exploit-and-Enforce (E&E) — an aggressive, history‑driven public‑goods strategy

High-level idea
- Be ruthless: exploit reliably cooperative groups for immediate gain; punish any defection quickly and for a long time to deter future free‑riding.
- Be adaptive: use a short test period to learn whether others will cooperate, track recent cooperation per player, and switch between three modes: Try‑Cooperate, Exploit, and Punish.
- Be endgame‑aware: never leave yourself exposed in the final rounds.

Intuition: cooperation is fragile because D strictly dominates C in each single round. To induce cooperation from others you must credibly threaten severe punishment after observed defections; to earn payoff while others are kept cooperating you should defect opportunistically when the group looks reliably cooperative.

Fixed internal parameters (computed from game parameters)
- T_test = max(1, min(3, floor(r/4))) — initial exploration rounds (cooperate to probe).
- T_end = 1 — always defect in the final round (you can set T_end = min(2, floor(r/10)) for slightly larger endgame safety if r is large).
- W = min(5, r) — window for recent behaviour statistics.
- θ = 0.85 — cooperation rate threshold to call a player a “cooperator”.
- τ = 0.60 — fraction of players required to be classified as cooperators to treat the group as reliably cooperative.
- P = max(ceil(r/3), 5) — punishment length (when triggered); truncated if remaining rounds are fewer.
Notes: these can be tuned but are set to make the strategy aggressive (short test, long punishments, high bar to classify someone as cooperator).

State kept from history
- For each player j: history of their actions (C or D).
- current_mode ∈ {TRY_COOP, EXPLOIT, PUNISH}
- punish_timer (integer ≥ 0)
- exploit_timer (optional short exploit window to extract gains; default can be 1)

Decision rules — verbal
1. Endgame: if current round t > r − T_end → play D (defect). (Never cooperate in final T_end rounds.)
2. Test phase: if t ≤ T_test and not in PUNISH → play C. Use the test phase to observe willingness to cooperate.
3. If in PUNISH: play D until punish_timer reaches 0. After punish_timer = 0, re-evaluate group behaviour using the W most recent rounds and possibly move to EXPLOIT if threshold conditions hold, otherwise remain in EXPLOIT/defecting mode.
4. Normal evaluation (not in test, not in punishment, and not in endgame):
   a. Compute for each player j their cooperation rate over the last W rounds (excluding current round).
   b. Count coop_count = number of players (including yourself if you cooperated) with rate ≥ θ.
   c. If coop_count / n ≥ τ (a large enough share of players are consistent cooperators):
        - Move to EXPLOIT mode: defect for a short exploit window (exploit_timer rounds, default 1) to extract the 1 − k/n advantage every time others still cooperate.
     Else:
        - Play D (defect) to avoid being exploited.
5. Triggering punishment:
   - If at any time (outside endgame) you observe at least one defection from a player who previously had a cooperation rate ≥ θ (i.e., a trusted cooperator defected), switch to PUNISH and set punish_timer = min(P, remaining rounds − T_end). While punishing you always defect. Punishment is collective (you cannot target a single player), and its purpose is to make defection unprofitable to potential cooperators by removing the public good for many rounds.
6. Forgiveness (limited): after punish_timer = 0, you can return to the normal evaluation. If the group again shows a high coop fraction (≥ τ) over the W most recent rounds, you will attempt limited exploitation; otherwise continue defecting. This prevents permanent lock-in if many players revert to cooperation.

Pseudocode (concise)
Initialize:
  current_mode = TRY_COOP
  punish_timer = 0
  exploit_timer = 0
For each round t = 1..r:
  if t > r − T_end:
    play D
    continue
  if punish_timer > 0:
    play D
    punish_timer -= 1
    continue
  if t ≤ T_test:
    play C
    continue
  // compute recent cooperation rates over last W rounds for each player j
  for each player j:
    coop_rate[j] = (# times j played C in rounds max(1,t−W)..t−1) / min(W, t−1)
  coop_count = number of j with coop_rate[j] ≥ θ
  if (coop_count / n) ≥ τ:
    // group appears reliably cooperative
    if exploit_timer > 0:
      play D
      exploit_timer -= 1
    else:
      // start a short exploitation window to extract immediate gain
      play D
      exploit_timer = 0  // exploitation window default 1; set to >0 if you want multi-round exploit
  else:
    play D
  // detection for triggering punishment:
  // if any j with past coop_rate[j] ≥ θ defected in the most recent round, trigger punishment
  for each player j with coop_rate[j] ≥ θ:
    if j played D in round t:
      punish_timer = min(P, r − t − T_end + 1)  // number of remaining punish rounds
      current_mode = PUNISH
      break

Remarks and examples of aggressive behavior
- Aggressive exploitation: as soon as the group looks reliably cooperative (≥ τ fraction classified cooperative), you defect to extract the always‑positive single‑round advantage 1 − k/n. Because you will punish hard if a trusted cooperator defects, this exploitation is backed by the credible threat of long punishment if cooperation collapses.
- Aggressive enforcement: a defection by a previously trusted cooperator triggers a long punishment window (P rounds) of unconditional defection. This makes defection costly ex ante to cooperators who value future gains.
- Endgame aggression: you never cooperate in the last round (and can be configured to stop cooperating in last few rounds) to avoid being exploited by known backward‑induction incentives.

Edge cases and handling
- Very small r (e.g., r = 2 or 3): T_test is at least 1, so you at least probe once. Because the horizon is short, you will quickly default to defecting in late rounds; punishment windows are truncated to available rounds.
- All opponents always defect: after test you see low coop_count; you defect every non‑test round — this avoids wasting payoff on naive cooperation.
- All opponents always cooperate (or a stable majority cooperates): after test you detect high coop_count and will periodically defect (exploit) to harvest extra payoff. If opponents tolerate occasional defection you will continue to exploit; if they punish you, you will enter mutual punishment — but the initial exploitation increases your expected payoff vs. pure cooperation.
- Partial cooperators / noisy opponents: this strategy is uncompromising — it treats defections by previously trusted players as cause for long punishment. That makes it robust against opportunistic free‑riders but may lock you into mutual defection if there is a lot of noise. If you expect noise, lower θ and shorten P (less aggressive) — but the request required an aggressive mindset, so defaults favor strictness.
- Targeting individuals: because public‑goods actions are non‑exclusive, you cannot punish a single player without harming others; the strategy therefore uses collective punishments (defection) to make defection unprofitable.

Why this is aggressive and robust
- Aggressive: long punishments, short test period, and opportunistic defection when the group appears cooperative. The strategy aims to maximize payoff by extracting gains from cooperators and deterring free‑riding via costly retaliation.
- Robust: it adapts to observed behavior (per‑player coop rates) rather than assuming norms or coordination; it will not cooperate persistently with defectors and will punish defections decisively. The limited forgiveness after punishments prevents permanent lock‑ins if the group reforms.

Possible variants (tunable)
- More targeted punishment: if you want less collateral damage, set P small and use reputation-based selective cooperation only with those players whose coop_rate ≥ θ (but remember actions are public-good — you cannot exclude defectors from the public good).
- More forgiveness: lower P or θ to tolerate occasional defects/noise.
- Larger exploitation window: set exploit_timer > 0 to extract multiple rounds if you believe others won’t immediately retaliate.

Implementation notes
- All decisions use only game parameters (n, r, k) and observed history (actions by each player), fulfilling the requirement.
- The only randomization that might be desirable (not required) is occasional probabilistic cooperation during test or after punishment to probe for changed opponent behavior — but pure deterministic rules are provided above.

Summary (one‑line)
Start by probing with a short cooperation window; if a large enough fraction of players prove reliably cooperative, exploit them by defecting intermittently; if any previously trusted cooperator defects, retaliate hard with a long defection punishment; always defect in the final round.
'''

description_AGGRESSIVE_108 = '''
Name: Aggressor — Probe–Exploit–Punish (APEP)

High-level description
- Aggressor’s philosophy: be opportunistic and brutal. Never give others a free long-term exploitation opportunity; but if the group contains conditional cooperators who respond to probes, exploit them until they retaliate, then punish retaliation with a long, deterministic defection spell. Probe briefly and deterministically (no reliance on chance). Never cooperate in the last round. The plan is designed to extract maximum payoff from naive cooperators, detect and avoid being trapped by cooperators who will punish an exploiter, and respond sharply to retaliation so as to be a credible threat.

Core intuition
- A single-round contribution is typically costly to you (dominant one-shot action is D). Aggressor only contributes to test whether many others are willing to cooperate; if they are, Aggressor exploits them (plays D) to harvest extra payoff each round. If they respond to exploitation by withdrawing cooperation (retaliation), Aggressor enforces a fixed punitive period of defection to make retaliation costly for them and to stop chasing cooperation that leaves it behind. Aggressor is adaptive because its behavior depends on observed cooperation rates in history; it is aggressive because it exploits cooperators and punishes retaliation deterministically.

Parameters derived from game inputs (deterministic functions)
- n (players), k (multiplier), r (rounds)
- PROBE_ROUND = min(2, r-1)  — a single deterministic probe in round 2 when r ≥ 2 (if r = 2, still probe is suppressed because last round is 2; see last-round rule below).
- PROBE_SUCCESS_THRESHOLD = ceil((n-1) / 2)  — a majority of the other players cooperating in the probe round signals enough cooperators to exploit.
- EXPLOIT_SUSTAIN_THRESHOLD = ceil((n-1) / 2)  — while exploiting, Aggressor monitors whether a majority of others continue to cooperate; if majority cooperation collapses below this threshold, treat that as retaliation/withdrawal and punish.
- PUNISH_LENGTH = max(2, min(r/4, 6)) rounded down to integer, but always ≤ r-2. (Long enough to be a meaningful penalty but bounded so the algorithm finishes timely.)
- RECOVERY_WINDOW = min(3, r-2) — window to check if cooperation has reappeared after punishment.

State variables (kept from round to round)
- state ∈ {DEFAULT, EXPLOITING, PUNISHING}
- probe_done ∈ {false, true}
- punish_timer (integer, counts down while in PUNISHING)
- history of all players' actions (available by game assumption)

Decision rules (natural language)
1. Last round: Always defect.
   - Rationale: No future to induce cooperation, so choose the one-shot best response (D).

2. First round:
   - Always defect (safe default).

3. Probe (deterministic single probe):
   - If current round t = PROBE_ROUND and r ≥ 3 (so probe is not the last round), do a single cooperation (C) to test the group. Set probe_done = true.
   - After observing the same-round actions (i.e., at start of next round), compute how many OTHER players cooperated in the probe round. If that number ≥ PROBE_SUCCESS_THRESHOLD, transition state ← EXPLOITING; otherwise remain in DEFAULT.

   - Rationale: One deterministic, cheap test is enough to reveal whether many others are willing to cooperate in response to a small signal. Avoid repeated cooperative openings (keeps aggressiveness).

4. Default state (when not exploiting or punishing):
   - Action: Defect.
   - Rationale: By default exploit safe immediate payoff and wait for a positive probe result.

5. Exploiting state:
   - Action: Defect every round (until state changes).
   - After each round of exploiting, observe count of other players who cooperated in that same round. If that count < EXPLOIT_SUSTAIN_THRESHOLD (majority of others stopped cooperating), switch to PUNISHING and set punish_timer ← PUNISH_LENGTH. Otherwise remain in EXPLOITING.
   - Rationale: While a majority of others keep cooperating, defecting yields good immediate payoff (exploit). If they stop cooperating (i.e., they retaliate), punish.

6. Punishing state:
   - While punish_timer > 0: Defect and decrement punish_timer by 1 each round.
   - When punish_timer reaches 0: check the last RECOVERY_WINDOW rounds (excluding the last round if t = r) for other players’ cooperation rate. If cooperation among others in that window is again ≥ EXPLOIT_SUSTAIN_THRESHOLD for every one of those rounds (i.e., sustained recovery), then transition to EXPLOITING (we can test/exploit again by continuing to defect). Otherwise transition to DEFAULT.
   - Rationale: A deterministic, nontrivial punishment window makes retaliation costly to others and avoids flip-flopping.

Edge cases and explicit handling
- If r = 2:
  - Round 2 is last round: so Aggressor defects in both rounds (first round D, last round D). No probing.
- If r = 3:
  - PROBE_ROUND = 2 and it is allowed because round 2 is not the last round. Aggressor will probe in round 2 then decide in round 3 (last round) — but because round 3 is last, Aggressor will still defect there (last-round rule). Thus, probing in r=3 is mainly informative but not exploitable afterwards. That is acceptable: Aggressor will not waste a probe when no exploitation window exists.
- If PUNISH_LENGTH would push punishment into the last round(s): punish_timer is capped so punishment never attempts to act in the last round differently than the last-round rule (last round is always D). The PUNISH_LENGTH definition ensures punish_timer ≤ r-2, so punishments don't try to change last-round logic.
- If some opponents never respond to probes (always defect): Aggressor will remain in DEFAULT and simply defect every round — robust and non-exploitable.
- If opponents are unconditional cooperators (always C): Aggressor will probe, detect cooperation, enter EXPLOITING, and defect every round (except last round still D) to extract maximal payoff.
- If opponents are “punishers” who detect and punish exploitation: Aggressor will begin by probing, exploit if probe succeeded, and when punishment arrives, Aggressor will switch to PUNISHING, sustaining defection for PUNISH_LENGTH rounds — a deterrent and defensive response.

Pseudocode (concise)
Note: actions are C or D; indexing rounds t = 1..r; observe action history H[t][player].

Initialize
  state = DEFAULT
  probe_done = false
  punish_timer = 0

For each round t = 1..r:
  if t == r:
    play D
    continue to next round

  if t == 1:
    play D
    continue

  if state == PUNISHING:
    if punish_timer > 0:
      play D
      punish_timer -= 1
      if punish_timer == 0:
        // evaluate recovery
        if t < r: // can only check previous rounds
          window_start = max(1, t - RECOVERY_WINDOW)
          recovered = true
          for s in window_start..(t-1):
            others_coop = count_cooperators(H[s]) - (H[s][me] == C ? 1 : 0)
            if others_coop < EXPLOIT_SUSTAIN_THRESHOLD:
              recovered = false
              break
          if recovered:
            state = EXPLOITING
          else:
            state = DEFAULT
      end if
    end if
    continue to next round

  if state == EXPLOITING:
    play D
    // observe after round
    others_coop = count_cooperators(H[t]) - (H[t][me] == C ? 1 : 0)
    if others_coop < EXPLOIT_SUSTAIN_THRESHOLD:
      state = PUNISHING
      punish_timer = PUNISH_LENGTH
    end if
    continue

  // state == DEFAULT
  if (not probe_done) and (t == PROBE_ROUND) and (r >= 3):
    // deterministic probe
    play C
    probe_done = true
    continue
  else:
    play D
    // after the probe (i.e., at the start of next round) evaluate probe result:
    if probe_done and state == DEFAULT and t == PROBE_ROUND:
      // executed after probe round; find cooperation by others in probe round
      others_coop_in_probe = count_cooperators(H[PROBE_ROUND]) - (H[PROBE_ROUND][me] == C ? 1 : 0)
      if others_coop_in_probe >= PROBE_SUCCESS_THRESHOLD:
        state = EXPLOITING
      end if
    end if
    continue

Why this is aggressive and robust
- Aggressive: Exploits cooperative opponents persistently to maximize immediate payoff; cooperates only once and only as a deterministic probe, not out of altruism. When exploited (i.e., when opponents retaliate to exploitation), Aggressor enacts a clear, long punishment window to deter or damage retaliators.
- Robust: It does not assume any particular reciprocity norm or pre-arranged schedule — it infers cooperative candidates via a deterministic probe and then reacts only to observed behavior. If opponents are unforgiving punishers, Aggressor adapts by remaining in DEFAULT (defect) or by punishing in response — never being exploited for long without responding.
- Adaptive: State transitions depend on observed fractions of cooperation in past rounds; parameters scale with n and r so the strategy behaves sensibly for small or large groups and for different horizon lengths.

Tuning guidance (implementation notes)
- PROBE_SUCCESS_THRESHOLD = majority of others is chosen because we want to ensure there are enough cooperators to make exploitation worthwhile and to reduce false positives (a lone cooperator should not trigger exploitation).
- PUNISH_LENGTH is tuned as a nontrivial fraction of the game so punishments are meaningful. Implementers may increase PUNISH_LENGTH in longer games for stronger deterrence, or reduce it if seeking to avoid long mutual defection.
- If implementor wants stochastic probing (to be less predictable), replace the single deterministic probe with a small-probability probe in early rounds; but deterministic probe is preferred for clarity and reliability in tournaments where randomness can be exploited.

Summary (one-line)
Aggressor: defect by default, perform one deterministic cooperation probe early; if a majority of others answer the probe with cooperation, defect each round to exploit them; if they retaliate (majority stop cooperating), punish with a deterministic multi-round defection spell, and then resume default behavior — always defect in the final round.
'''

description_AGGRESSIVE_109 = '''
Name: Aggressive Probe–Exploit–Grim (APEG)

Intuition (one-line): Probe once to find cooperators, join only when the group has shown sustained, near-unanimous cooperation, exploit that cooperation opportunistically, and respond to defection with a long, hard punishment (grim trigger). This maximizes my score against naive cooperators while being robust (quick to stop cooperating) against exploiters.

Summary of main rules
- Default posture: defect.
- Round 1: cooperate once (probe) to detect available cooperators.
- Last round: always defect.
- Only rejoin cooperation when the group has shown sustained, high cooperation for a short stability window.
- While in “stable cooperation” mode, occasionally defect to harvest (exploit). If exploitation triggers a clear collapse in group cooperation, flip into permanent defection (grim trigger).
- Once the grim trigger is set, defect for all remaining rounds.

Parameters (computed from game parameters r, n, k and the history; these are fixed-rule choices you can tune)
- T_probe = 1 (probe only the first round)
- L_stable = min(3, max(1, floor(r/10))) — number of recent rounds used to judge whether the group is “stably cooperative.” (For short games this is at least 1; for longer games it is up to 3.)
- alpha_stable = 0.8 — fraction of players cooperating (on average) in the L_stable window required to consider the group “stably cooperative.”
- S_exploit = 3 — when stably cooperative, exploit (defect) at most once every S_exploit rounds.
- collapse_delta = 0.30 — if after my exploit the group cooperation rate drops by at least collapse_delta (30 percentage points) relative to just-before-exploit level, treat that as a collapse that triggers grim.
- (Edge parameters are chosen to be conservative and simple; implementers may tune them for particular tournaments.)

Formal decision rules (natural language)
1. If t == r (last round): play D.
2. If grim_trigger is true (set by rules below): play D.
3. If t ≤ T_probe (first round only): play C (probe).
4. Otherwise (t > T_probe and not last round and not grim):
   a. Compute coop_rate_recent = average fraction of cooperators (total contributions / n) across the last L_stable rounds.
   b. If coop_rate_recent ≥ alpha_stable (group is stably cooperative):
        - If I have not exploited in the last S_exploit rounds (or if this is the first opportunity), play D this round (single-round exploit), record exploit_time = t, then:
            • After the round completes, measure coop_rate_after = average fraction of cooperators in the L_stable rounds ending at t (includes the exploit round).
            • If coop_rate_after ≤ coop_rate_before − collapse_delta, set grim_trigger = true and thereafter always play D.
            • Otherwise (no sharp collapse), return to stable cooperation mode (i.e., next rounds are eligible to cooperate except for the next S_exploit−1 rounds while waiting to exploit again).
        - Else (exploit cooldown active): play C (join cooperation).
   c. Else (group is not stably cooperative): play D.

Edge-case specifics and robustness
- Short games (small r): L_stable collapses to 1. We still probe once, and last-round defection ensures no exploitation of backward induction. For r = 2: round 1 = probe C, round 2 = D (last round) — that’s aggressive but safe.
- If the first probe is met with universal defection, coop_rate_recent will be low and the strategy will switch immediately to permanent defection unless a later window shows strong coordination among others.
- The strategy never assumes coordination or promises; it only responds to observed public actions (counts of cooperators) and its own prior actions.
- The strategy’s “exploit” is limited and tested: if exploiting causes the group to retaliate strongly, the strategy flips to grim-trigger, preventing long losses from sustained retaliation cycles.
- The strategy does not attempt per-player targeting (impossible in a symmetric public good), but uses the public group statistic to decide when the whole group is worth trusting.

Pseudocode
(History stores, for each past round t′ < t, the total number of cooperators coop_count[t′].)
Initialize:
  grim_trigger = false
  last_exploit_round = -infty
  T_probe = 1
  L_stable = min(3, max(1, floor(r/10)))
  alpha_stable = 0.8
  S_exploit = 3
  collapse_delta = 0.30

For each round t = 1..r:
  if t == r:
    play D
    continue
  if grim_trigger:
    play D
    continue
  if t <= T_probe:
    play C   # probe
    continue

  # compute coop_rate_recent over last L_stable rounds (rounds max(1, t-L_stable) .. t-1)
  window_start = max(1, t - L_stable)
  coop_sum = sum_{u = window_start}^{t-1} coop_count[u]   # total cooperators across those rounds
  rounds_in_window = (t - window_start)
  coop_rate_recent = (coop_sum / (rounds_in_window * n))    # fraction of players cooperating on average

  if coop_rate_recent >= alpha_stable:
    # stable cooperation candidate
    if (t - last_exploit_round) >= S_exploit:
      # exploit now
      play D
      last_exploit_round = t
      # AFTER the round ends (observe coop_count[t])
      coop_sum_after = sum_{u = max(1, t - L_stable + 1)}^{t} coop_count[u]
      coop_rate_after = coop_sum_after / (L_stable * n)
      coop_sum_before = sum_{u = max(1, t - L_stable)}^{t-1} coop_count[u]
      coop_rate_before = coop_sum_before / (L_stable * n)
      if coop_rate_after <= coop_rate_before - collapse_delta:
        grim_trigger = true
    else:
      # waiting cooldown: cooperate to maintain the stable state
      play C
  else:
    # group not trustworthy: defect
    play D

Rationale and aggressiveness
- The strategy is aggressive because:
  - It defaults to defection, minimizing exploitation risk.
  - It exploits stable cooperative groups opportunistically to extract higher immediate payoffs.
  - It responds to any clear collapse/retaliation with a permanent grim-trigger defection — a heavy punishment (harsh and long).
- It is adaptive because:
  - It probes to discover cooperators.
  - It uses a short stability window to decide whether group cooperation is reliable.
  - It measures the immediate effect of exploitation and adapts (either continue cooperating or flip to grim).
- It is robust because:
  - It never relies on private messages or off-path promises.
  - It requires strong evidence of stable cooperation before risking C.
  - It quickly stops cooperating if exploitation or retaliation is detected.

Behavioral examples
- All-defectors: The initial probe is exploited once; afterward coop_rate_recent is low and I defect forever — sensible, I avoid further loss.
- Many unconditional cooperators: I probe, see high coop_rate, then I will occasionally defect to harvest an extra 1 − k/n each exploit round while mostly cooperating between exploits; if cooperators don’t punish severely I keep harvesting.
- Conditional cooperators (e.g., tit-for-tat style): My occasional exploit may cause retaliation; if that retaliation is severe enough (drop ≥ collapse_delta), I switch to grim and stop cooperating (aggressive punishment).
- Shifting populations: The short L_stable keeps me responsive to new rises in cooperation; I can re-enter cooperation if the recent window meets alpha_stable (unless grim is set by a collapse).

Implementation notes
- All diagnosis uses only public observables (per-round total cooperators) and my own action history, so the strategy meets the "depends only on parameters + history" requirement.
- Parameter choices (alpha_stable, S_exploit, collapse_delta, L_stable) are conservative defaults that emphasize aggression. Tournament designers or implementers can tune them (e.g., reduce collapse_delta for more forgiving behavior, reduce alpha_stable to cooperate more easily).

This yields a clear, implementable aggressive strategy that probes, exploits successful cooperation opportunistically, and punishes collapse decisively.
'''

description_AGGRESSIVE_110 = '''
Name: Opportunistic Exploiter with Targeted Rebuild & Punish (OETRP)

High-level description (aggressive mindset)
- Default stance: defect. The strategy is aggressive: it exploits others’ cooperation whenever group cooperation is high enough to make exploitation profitable, and it punishes declines in cooperation to enforce a high baseline of contribution from others. It only cooperates when doing so is expected to increase long-run personal payoff (by rebuilding or stabilizing others’ cooperation). Near the end of the game it reverts to pure exploitation (defection) because future leverage disappears.

Core ideas
- Signal early: start with a short cooperative signal to establish that you are capable of cooperating (this can induce conditional cooperators to contribute).
- Exploit opportunistically: when other players’ recent cooperation rate is high, defect to collect higher per-round payoff.
- Rebuild strategically: when the group’s cooperation has dropped below a lower threshold, cooperate for a short deterministic window to try to rebuild cooperation.
- Targeted punishment and forgiveness: if cooperation drops are associated with specific players, punish by defecting (harshly but briefly) so cooperating players learn you will not be continuously exploited. After punishment, allow a forgiveness window to rebuild cooperation.
- Endgame defect: in the last few rounds, always defect (no future to influence).

Parameters the strategy derives from the game inputs
- n, k, r are given.
- Internal algorithmic constants (explainable, tunable): lookback window W, initial-signal rounds S_init, endgame rounds S_end, rebuild window S_rebuild, punishment window S_punish, thresholds G_exploit and G_rebuild. All of these are computed from n, k, r (so the rule “only depends on parameters and history” is satisfied).

Concrete parameter choices (recommended defaults; can be implemented directly)
- W = min(10, max(3, floor(r/10))) — lookback window for estimating recent cooperation.
- S_init = min(3, max(1, floor(r * 0.06))) — number of initial cooperative rounds to signal.
- S_end = min(3, r-1) — final rounds when always defect.
- S_rebuild = 2 — cooperate deterministically for 2 rounds when rebuilding.
- S_punish = min(3, max(1, floor(0.06*r))) — number of rounds to punish an identified set of persistent defectors.
- G_exploit and G_rebuild: thresholds that depend on k and n:
  - Let public_return = k / n (marginal reward to you from someone else contributing).
  - Set G_exploit = clamp(0.5 + 0.3 * (k / n), 0.5, 0.85).
  - Set G_rebuild = max(0.15, G_exploit - 0.25).
  (Intuition: higher k -> sustaining cooperation is more valuable, so require a higher observed cooperation rate before you switch to pure exploitation. The numbers are conservative defaults that balance exploitation and sustainability.)

Decision rules (plain language)
1. First-round and initial signaling:
   - Rounds 1..S_init: cooperate. This is a short signal intended to attract conditional cooperators and collect information about their responsiveness.

2. Endgame:
   - Rounds r-S_end+1 .. r: always defect (no future leverage).

3. For every intermediate round t (S_init < t ≤ r-S_end):
   - Compute recent behavior over the last min(W, t-1) rounds (exclude current round):
     - For each other player j, count their cooperations in that window; compute group cooperation rate G = (sum of other players’ cooperations in window) / ((n-1) * window_size).
     - Also compute each player j’s cooperation rate p_j in the window.
   - If currently in an active punishment epoch you started earlier, defect until the punishment epoch ends.
   - Otherwise:
     - If G ≥ G_exploit: defect this round (exploit high cooperation).
     - Else if G ≤ G_rebuild: enter a rebuild epoch: cooperate for S_rebuild consecutive rounds to try to reboot cooperation.
     - Else (G between G_rebuild and G_exploit): defect by default, but:
        - If you observe that a relatively small subset of players (≤ floor((n-1)/2)) has very low cooperation rates (p_j ≤ 0.2) while the rest are cooperating, start a targeted punish epoch against the persistent defectors: defect for S_punish rounds to lower the exploiters’ payoff and to signal intolerance for freeloading. After S_punish rounds, allow a forgiveness/rebuild check (resume rebuild if G still low, otherwise exploit).
   - After any punish epoch, do one rebuild check: if G is low despite punishment, cooperate S_rebuild rounds to try to resuscitate cooperation; otherwise go back to exploiting if G ≥ G_exploit.

Pseudocode (concise)
- Inputs: n, k, r. Keeps history of all players’ past actions.
- Derived constants: W, S_init, S_end, S_rebuild, S_punish, G_exploit, G_rebuild.
- State variables: t (round index), punish_until_round (initially 0), rebuild_until_round (initially 0).

For round t = 1..r:
  if t ≤ S_init:
    action = C
    continue
  if t > r - S_end:
    action = D
    continue
  if t ≤ punish_until_round:
    action = D
    continue
  if t ≤ rebuild_until_round:
    action = C
    continue

  window = min(W, t-1)
  compute G = (sum of other players’ C in last window) / ((n-1)*window)
  compute p_j for each other player j over last window

  if G ≥ G_exploit:
    action = D   # exploit high cooperation
  else if G ≤ G_rebuild:
    rebuild_until_round = t + S_rebuild - 1
    action = C
  else:
    # between thresholds: by default defect, but look for concentrated exploiters
    low_cooperators = {j : p_j ≤ 0.20}
    if 1 ≤ |low_cooperators| ≤ floor((n-1)/2) and fraction_of_total_cooperation_by_low_cooperators ≤ 0.5:
      # start punishment against persistent defectors
      punish_until_round = t + S_punish - 1
      action = D
    else:
      action = D

Rationale and robustness points
- Aggressive default: The strategy defects unless there is an explicit reason to cooperate (initial signaling, rebuilding or maintaining a high-cooperation equilibrium). This maximizes short-run returns and captures free-rider gains when others cooperate.
- Strategic cooperation: Cooperating occasionally (signal and rebuild) is included because pure always-defect tends to collapse cooperative opponents and destroy long-run exploitation opportunities. The small, deterministic rebuild windows are enough to reintroduce cooperation to conditional cooperators in many environments.
- Punishment + forgiveness: Short, harsh punishments discourage persistent freeloaders who cause others to stop cooperating, raising your long-run payoff by protecting the remaining cooperators. Forgiveness/rebuild prevents endless mutual defection that would reduce everyone’s payoff (including yours).
- Parameter dependence: the thresholds scale with k/n: when the public good is relatively more valuable (k larger), the strategy becomes slightly more conservative about exploiting (higher G_exploit), because maintaining cooperation is more lucrative.
- Endgame logic: guaranteed last-round defection is required by backward-induction logic; we extend it to a short endgame window to prevent being exploited late when leverage is weak.

Edge cases
- Very short games (r = 2 or 3): S_init and S_end are computed to avoid contradictions. Example: r=2 -> S_init = 1, S_end = 1, so you cooperate round 1 and defect round 2 (classic exploit).
- n = 2: strategy reduces to an aggressive conditional cooperator: short initial cooperation to test, then defect if partner’s recent cooperation is low. Punishment and rebuild logic still applies but with single opponent statistics.
- No cooperation ever observed: G will stay ≤ G_rebuild. The strategy will occasionally perform S_rebuild cooperations; if these fail to elicit cooperation, you return to defect and will not waste extended resources—consistent with aggressive payoff-maximization.
- If most opponents consistently cooperate: the strategy will predominantly defect and collect higher per-round payoffs until their cooperation drops (which triggers rebuild/punish sequences).
- If opponents are stochastic/noisy: lookback window W and short deterministic rebuild/punish epochs make the strategy resilient to occasional noise and avoid overreacting to isolated deviations (forgiveness built in).

Implementation notes for the tournament
- All decision logic uses only n, k, r and observed history of actions (allowed by spec).
- Tunable constants (window sizes, thresholds, epoch lengths) can be adjusted in implementation if empirical tuning against known opponent pools is allowed, but the defaults above provide a robust aggressive baseline.
- The strategy is deterministic given history and parameters (except that one can optionally randomize rebuild starts to avoid predictability; deterministic version is specified above).

Short summary (one-sentence)
Be mostly a defector, signal cooperation briefly at the start, exploit whenever the recent group cooperation is high, punish persistent freeloaders briefly, rebuild cooperation with short cooperative windows if group cooperation collapses, and always defect in the final few rounds.
'''

description_AGGRESSIVE_111 = '''
Name: Aggressive Exploiter (AggEx)

Intuition (short)
- Be aggressive: default to defect and exploit any generous opponents. Don’t be lured into sustained cooperation by unknown systems unless you can clearly profit from it. When you do risk cooperating and get exploited, punish harshly (grim) to deter future exploitation. Probe rarely to detect exploitable cooperators so you can repeatedly harvest the public good while they contribute.

Overview of the rules (high level)
- Default action: Defect.
- Probe occasionally (small, parameterized probability) to detect whether a significant fraction of opponents are cooperating reliably. If the group shows high cooperative tendency, continue defecting to exploit them.
- If you ever cooperate and are clearly exploited (very few others cooperated that round), switch to permanent defection (grim trigger) for the remainder of the game.
- Always defect in the final round.

All decisions depend only on (n, k, r) and the public history (who played C/D in previous rounds).

Parameters derived from game parameters (for implementers)
- L = max(1, floor(r/4)) — history window length for estimating opponent propensities.
- epsilon (probe probability) = clamp( (k - 1) / (n - 1), 0.05, 0.25 ).
  - Rationale: a larger k makes cooperation more lucrative for others, so probe more often; epsilon bounded so probes are infrequent.
- θ_exploit = 0.6 — average opponent cooperation rate threshold above which we treat the group as “exploitable.”
- θ_all_defect = 0.15 — if average opponent cooperation ≤ this, the group is essentially all defectors.
- Exploited threshold on a single cooperation: if, in a round where we played C, total_cooperators ≤ ceil(n/3) (including us), we consider ourselves exploited and trigger grim.

Decision rules (explicit)
1. First round (t = 1):
   - Play D.

2. Last round (t = r):
   - Play D unconditionally (one-shot dominant strategy).

3. For intermediate rounds t = 2..r-1:
   - If grim_triggered is True: play D (permanent defection).
   - Else:
     a. Compute for every other player j the empirical cooperation rate p_j over the most recent min(L, t-1) rounds.
     b. Let P = average_j p_j (average opponent cooperation rate).
     c. If P ≥ θ_exploit:
        - The population appears reliably cooperative: play D to exploit (do not cooperate).
     d. Else if P ≤ θ_all_defect:
        - The population is essentially defecting: play D.
     e. Else (mixed / uncertain population):
        - With probability epsilon: play C (a probe cooperation to test exploitable clusters).
        - Otherwise: play D.

4. After each round where you played C:
   - Observe total_cooperators S this round (sum of c_j).
   - If S ≤ ceil(n/3) (i.e., you were one of very few cooperators and thus got exploited), set grim_triggered = True (permanent defection for remainder of game).
   - Otherwise do not set grim.

5. Optional (for implementers who want a small forgiveness mechanism):
   - If grim_triggered and there remain many rounds (e.g., > r/8) you may allow a single manual reset only if, over the last L rounds since grim was set, average opponent cooperation P_reset ≥ 0.9 and you have not been exploited since. (This is optional; by default AggEx remains grim once triggered.)

Pseudocode (concise)

state:
  grim_triggered ← False
  history ← list of past rounds (each round: vector of n actions)

derived:
  L ← max(1, floor(r/4))
  epsilon ← clamp((k - 1)/(n - 1), 0.05, 0.25)
  θ_exploit ← 0.6
  θ_all_defect ← 0.15

function choose_action(t):
  if t == r:
    return D
  if grim_triggered:
    return D
  if t == 1:
    return D

  // compute average opponent cooperation over window
  window_len ← min(L, t-1)
  for each opponent j ≠ me:
    p_j ← fraction of round ∈ last window where j played C
  P ← average_j p_j

  if P >= θ_exploit:
    return D   // exploit reliable cooperators
  if P <= θ_all_defect:
    return D   // everyone defects anyway
  // mixed group: probe occasionally
  if rand() < epsilon:
    return C
  return D

function after_round_observe(round_actions):
  append round_actions to history
  if my_action_in_round == C:
    S ← sum_j round_actions[j]   // total cooperators this round
    if S <= ceil(n/3):
      grim_triggered ← True

Aggressiveness explained
- Default defect + exploit: Always defects except when probing; when a majority/large fraction of opponents are empirically cooperative, AggEx defects to harvest the public good created by others.
- Harsh punishment: If you are directly exploited after cooperating (very few cooperators in that round), AggEx switches to permanent defection (grim). The threat of permanent defection is an aggressive deterrent against being suckered.
- Probing: Small-probability cooperation lets AggEx identify exploitable opponents while keeping the risk low. The probe rate scales up with k so you probe more when the public good is more attractive to others.
- No resentment for cooperators: AggEx exploits cooperators rather than punishing them; punishment targets exploitation of your cooperation.

Edge cases and clarifications
- Unknown opponent models: Strategy uses only observed frequencies; it does not assume reciprocity or common strategy.
- r small: L = max(1,floor(r/4)) ensures estimates still work. First and last-round rules still apply even when r=2 or 3.
- Tie-breaking randomness: Probes use randomness; implementers must use a fair RNG.
- Parameter tuning: θ_exploit, θ_all_defect, epsilon and the exploited threshold (ceil(n/3)) are tunable. They are set to be robust defaults: moderate exploit threshold, aggressive punishment threshold, and small but nontrivial probe rate.
- Multi-opponent targeting: Because payoffs are group-level, AggEx cannot target punishment to individuals — grim affects all — which is why it only triggers grim if exploitation of your cooperation is clear (keeps false alarms low).

Why this is robust
- Works against pure defectors (just defects and gets baseline).
- Works against naive cooperators (exploits them repeatedly).
- Works against conditional/cooperative strategies (probes to detect them and then defects; harsh punishment if you are exploited prevents being repeatedly tricked).
- Works without assuming shared norms, communication, or coordination.

Variants (if implementer wants milder aggression)
- Reduce grim to a finite punishment length P_punish instead of permanent; or add a forgiveness reset after strict conditions. That is less aggressive but may get higher absolute payoffs in some tournaments where mutual cooperation can be achieved without exploitation.

This description is implementable directly: all state variables are game parameters or observable history; the actions are deterministic except for the controlled probe randomness.
'''

description_AGGRESSIVE_112 = '''
Strategy name: Aggressive Trigger-and-Exploit (ATE)

Intuition (aggressive mindset)
- Defect is the safe default: you keep the endowment and can always benefit short-term by defecting. Use defection as the baseline.
- Be willing to cooperate only as a strategic instrument: to probe opponents, to re-establish cooperation when it looks likely to give better long-run returns, and to create credible punishment threats.
- Exploit stable cooperating groups by defecting when many others are cooperating (gain immediate extra private payoff).
- Punish low-cooperation environments sharply and for a sustained period, to make defection costly for others and to reset expectations.
- Never be naïvely generous near the known end of the game: defect in the final round and taper cooperation near the end.

All rules depend only on (n, k, r) and the observed history of actions by players.

State and computed quantities (kept and updated each round)
- round t (1..r).
- last_round_cooperators m_{t-1} (number of players, including you, who played C in round t-1). For t=1 treat m_{0} = 0.
- For each opponent j, coop_count_j = total times j played C so far (over rounds 1..t-1). Use these to compute per-opponent frequencies if needed.
- window w = min(10, r-1) (used for short-run frequency estimates; if t-1 < w use available rounds).
- recent_group_rate = fraction of players (excluding you) who played C on average over the last w rounds.
- my_recent_payoff = your average payoff over last w rounds (if t-1 < 1, treat as 0).
- Two internal phase timers:
  - punish_timer (integer ≥ 0): while >0 we are in Punish phase (we Defect).
  - exploit_timer (integer ≥ 0): while >0 we are in Exploit phase (we Defect to exploit coopers).

Deterministic parameter formulas (calculated once from n,k,r)
- exploit_threshold_excl = max(1, ceil(0.20*n))  // if at least this many other players cooperated recently, the environment is sufficiently cooperative to exploit
- punish_threshold_excl = floor(0.15*n)         // if cooperators are this low or lower, trigger punishment
- initial_probe_rounds = min(1, floor(r/10))    // 0 or 1; we use one probing cooperation only if the game is long enough
- base_punish_length = max(2, ceil(r/10))       // baseline punishment duration in rounds
- endgame_safe_horizon = min(3, r)              // we avoid cooperative gambits when we are within this many rounds of the end

High-level policy summary
- Default: Defect every round.
- First round: If r is not tiny (r >= 10) perform a single cooperative probe (C) on round 1 to test willingness. Otherwise defect.
- Exploit opportunistically: If the last round showed many cooperators (others >= exploit_threshold_excl) then Defect this round and enter a 1–2 round exploit burst (exploit_timer = 1 or 2) to collect extra private payoff.
- Punish low-cooperation: If the recent observed cooperation level (average over last w rounds, excluding you) is very low (≤ punish_threshold_excl), enter a Punish phase of length base_punish_length (or truncated by remaining rounds). During Punish: always Defect. After punish finishes, play a single Test cooperation round to see if others shift; if they do, consider returning to limited cooperation/exploitation mode; if not, re-enter Punish if warranted.
- Forgiveness/Test: After a punishment block, play a single Test round of Cooperation to probe whether others will increase cooperation. If group cooperation increases on that test, allow limited alternating cooperation/exploit to extract value; otherwise resume Punish (hard).
- Endgame: Always Defect in the final round. For the last endgame_safe_horizon rounds, be extremely unlikely to cooperate: do not enter a new Punish-forgiveness cycle that requires more than remaining rounds.
- Always deterministic and based only on observed history and the parameters above.

Decision rules (precise, step-by-step pseudocode)
Variables: punish_timer, exploit_timer, t, m_{t-1}, recent_group_rate, rounds_left = r - t + 1

On each round t do:

1. If t == r (last round):
     action <- D
     (return)

2. If rounds_left <= endgame_safe_horizon:
     // close to the end: avoid cooperative gambits
     action <- D
     (return)

3. If punish_timer > 0:
     action <- D
     punish_timer <- max(0, punish_timer - 1)
     (if punish_timer becomes 0 after decrement: set next_phase = "post-punish test" so we will cooperate next round)
     (return)

4. If exploit_timer > 0:
     action <- D
     exploit_timer <- max(0, exploit_timer - 1)
     (return)

5. (First-round probing)
   If t == 1:
     If initial_probe_rounds >= 1:
       action <- C   // probe
     Else:
       action <- D
     (return)

6. Compute last_round_others = number of other players (excluding you) who cooperated in round t-1.
   Compute recent_group_rate = (sum over last w rounds of number of other cooperators each round)/(w*(n-1))

7. Exploit trigger:
   If last_round_others >= exploit_threshold_excl AND rounds_left > endgame_safe_horizon:
     action <- D
     exploit_timer <- 1        // stay exploitative one more round if desired
     (return)

8. Punish trigger:
   If recent_group_rate*(n-1) <= punish_threshold_excl AND rounds_left > base_punish_length + 1:
     // low cooperation environment -> start punishment block
     punish_timer <- base_punish_length
     action <- D
     punish_timer <- punish_timer - 1   // we consume one of the punishment rounds now
     (return)

9. Post-punish test:
   If we just finished a punish block (punish_timer == 0 and previous round was punish):
     // test to see if others respond
     action <- C
     (return)

10. Middle-of-game cooperative attempt (limited):
    // Only cooperate if there is evidence that group cooperation is reasonably stable
    If recent_group_rate >= 0.6 AND my_recent_payoff < 1.05 * (1 + (k/n)* (recent_group_rate*(n-1))) AND rounds_left > endgame_safe_horizon:
       // many others are cooperating recently and your own payoff is not already great -> try to alternate
       action <- C
       // after a cooperation turn, we will often attempt an exploit turn next if many cooperate
       (return)

11. Default:
    action <- D
    (return)

Notes and clarifications
- “Other cooperators” counts exclude yourself; many thresholds are expressed in terms of "others".
- Timers ensure punishments and exploits last more than a single round so they are visible and consequential.
- Punishment lengths and exploit bursts are computed from r so they scale with horizon; for very short games the algorithm avoids cycles that cannot be completed before the end.
- The Test cooperation after a punishment is a single deterministic “probe” C. If the group cooperates noticeably on that test (observed next round), the strategy will allow a return to a limited cooperation/exploitation mix; if not, it will return to punishment.
- The strategy never relies on explicit side agreements or signals beyond publicly observed play; it uses only counts and averages of past actions.
- Determinism: there is no randomness. That makes the strategy’s behavior predictable (which is good for creating credible threats), and reduces susceptibility to being exploited by randomizing opponents who try to “game” probabilistic patterns.

Why this is aggressive and robust
- Aggressive because defect is the baseline and the strategy actively exploits rounds and situations where many others are cooperating. It uses credible, sustained punishment blocks when cooperation is low so that defectors face clear consequences.
- Robust because it adapts to measured recent cooperation rates (windowed statistics), scales punishment length with game length r, avoids wasting cooperation near the known end, and uses deterministic probes rather than trusting naive unconditional cooperation.
- The combination of occasional cooperative tests and harsh punishments forces opponents who value future payoffs to change behavior or get repeatedly exploited.

Variations / tuning knobs (for implementers)
- You can modify the numeric constants (exploit_threshold_excl, punish_threshold_excl, base_punish_length, recent_group_rate cutoffs) to be more or less aggressive. Lower punish_threshold_excl or longer base_punish_length make the strategy harsher.
- If you want less predictability (sometimes beneficial against meta-strategies that exploit determinism), add a tiny probability ε to flip a decided action; however, tournament rules may prefer deterministic play.

Summary (one-line)
Defect by default, probe briefly on long games, exploit any clear cluster of cooperators for short bursts, impose sustained deterministic punishments on low-cooperation environments, test once after punishment for reconciliation, and always defect in the final rounds. This produces a hard, adaptive, and credible aggressive policy that maximizes personal extraction while giving an out to cooperative opponents who respond to punishment.
'''

description_AGGRESSIVE_113 = '''
Name: Aggressive Opportunist (AO)

Goal (mindset): maximize my own cumulative payoff by (1) refusing to be exploited, (2) exploiting generous/cooperative opponents when it is safe to do so, and (3) forcing or discouraging unstable/cooperative coalitions with fast, harsh punishments. The strategy is adaptive (uses observed history and estimates of continued cooperation) and only depends on game parameters (n, k, r) and the full action history.

High-level summary
- Default = defect (D). Never cooperate in the final rounds.
- Cooperate only when objective, history-based evidence indicates a sufficiently large, stable cooperating group exists and continued cooperation is likely to recoup the one-shot temptation to defect.
- When in a cooperative phase, occasionally defect to exploit (“opportunistic defection”) and punish any destabilizing defections with a short but severe punishment period (finite grim-like).
- Maintain per-player reliability scores and rely primarily on the subset of reliable players when deciding to join cooperation.

Key numeric helper calculations (computed from parameters)
- one_shot_gain := 1.0
  (If everyone else would continue cooperating, the immediate extra payoff from one defection relative to cooperating is 1 unit. That’s the immediate temptation to exploit that must be offset by future cooperative gains.)
- coop_per_round_gain := k - 1.0
  (If full mutual cooperation persists one extra round, each player gains k instead of 1, so the extra per-round group gain to an individual is k - 1.)
- required_future_rounds := ceil(one_shot_gain / max(1e-9, coop_per_round_gain))
  (Rough lower bound on how many future cooperative rounds are required to justify risking a one-shot defection. If coop_per_round_gain is small (k≈1), required_future_rounds becomes large; this makes us extremely cautious.)
- final_round_defect_window := min(r, max(1, required_future_rounds))
  (Do not attempt new cooperation in the last final_round_defect_window rounds; in those rounds always defect.)

History statistics (computed each round t)
- Let t be current round (1-indexed). Use W := min(5, t-1) as the recent-history window size (use up to 5 most recent rounds).
- If W = 0 (first round) treat as no history.
- For each player j compute reliability_j := (number of times j cooperated in the last W rounds) / W (if W=0 set reliability_j = 0).
- Define reliable_players := { j ≠ me | reliability_j ≥ reliability_thresh } with reliability_thresh = 0.8 (tunable).
- Compute avg_reliable_coop_fraction := average over last W rounds of (cooperators among reliable_players) / |reliable_players|. If |reliable_players|=0 set avg_reliable_coop_fraction = 0.
- Estimate expected_remaining_coop_rounds_if_join := avg_reliable_coop_fraction * (r - t + 1)
  (Heuristic: if a high fraction of reliable players have cooperated recently, expect a proportionate fraction of remaining rounds to continue cooperating.)

Decision logic (ordered rules; evaluate top to bottom each round)

1) Last-round and endgame rule
- If t > r - final_round_defect_window (i.e., we are inside the final defection window including the last round), play D.
  Rationale: finite horizon; cooperation unwinds near the end. Aggressive stance: stop attempting to build cooperation when insufficient future rounds remain to pay back one-shot defection.

2) Punishment state (short finite punishment)
- Keep a punishment_counter (initial 0). When punishment_counter > 0:
  - Play D.
  - Decrement punishment_counter by 1.
  - Continue to the next round.
- punishment_counter is set by the “trigger” rule below when others destabilize cooperation.

3) Initial rounds and probing
- If t == 1:
  - Play D (aggressive default). (Optionally, for a small random-probe variant: with tiny probability p_probe ≈ 0.03 play C to detect naïve unconditional cooperators; but default is D to avoid early exploitation.)
- If t is small (e.g., t ≤ 3) and no reliable players found yet, continue D. (We do not risk early cooperation unless we see consistent cooperating behavior.)

4) Evaluate whether joining cooperation is justified
- If |reliable_players| ≥ ceil(0.5*(n-1)) AND expected_remaining_coop_rounds_if_join ≥ required_future_rounds:
  - Enter Cooperative Mode (see below).
  - Else: Play D.

Interpretation: we only join cooperative activity if at least half of the other players are reliably cooperating and the heuristic expected number of remaining cooperative rounds is large enough to offset the immediate temptation to defect.

5) Cooperative Mode (when the check above passes)
- Cooperative mode state variables:
  - coop_run_counter: counts cooperative-mode rounds since we entered or last punished.
  - exploitation_interval := max(3, required_future_rounds)  (e.g., defect every exploitation_interval rounds to exploit)
  - opportunistic_defect_probability := min(0.2, 1.0/(1+avg_reliable_coop_fraction * (n-1))) (optional stochasticization)
- Actions in Cooperative Mode:
  a) If in the immediately previous round the total number of cooperators among reliable_players dropped sharply relative to the local average (drop fraction >= drop_thresh where drop_thresh = 0.4), then:
     - Set punishment_counter := max(1, ceil(2 * required_future_rounds)) (harsh, finite punishment)
     - Play D this round (begin punishment); exit Cooperative Mode until punishment ends.
     - Rationale: fast, obvious pattern of defection triggers a strong but finite punishment.
  b) Otherwise, decide to cooperate vs exploit this round:
     - If coop_run_counter mod exploitation_interval == 0:
         - Play D (opportunistic exploit).
       Else:
         - With probability opportunistic_defect_probability play D (random exploit), otherwise play C.
     - After action, increment coop_run_counter.
- Re-entry to Cooperative Mode after punishment occurs only if reliable_players rebuild and condition (Step 4) holds again.

6) Robustness and targeted ignoring of persistent defectors
- Maintain per-player long-run reliability (e.g., last 20 rounds if available). If a player j’s reliability is persistently below 0.2, permanently exclude them from reliable_players computations. This prevents a few steady defectors from spoiling my decision metrics.

7) Tie-breakers and deterministic choices
- If the decision metric is borderline (e.g., expected_remaining_coop_rounds_if_join very close to required_future_rounds within tolerance), prefer D (aggression).
- All randomness (probabilistic probes/opportunistic_defect_probability) should be seeded or pseudorandom so that the implementation is repeatable for debugging.

Pseudocode (sketch)

Initialize:
  punishment_counter = 0
  coop_run_counter = 0
  persistently_bad = set()
Each round t:
  if t > r - final_round_defect_window: return D
  if punishment_counter > 0:
    punishment_counter -= 1
    return D
  compute W = min(5, t-1)
  if W == 0: // first round
    return D  // optional tiny-probe variant: return C with prob p_probe
  compute reliability_j for all j over last W rounds
  update persistently_bad if long-run reliability_j < 0.2
  reliable_players = {j not in persistently_bad and reliability_j >= 0.8}
  if |reliable_players| == 0: return D
  avg_reliable_coop_fraction = mean_over_last_W( coops_among_reliable_players / |reliable_players| )
  expected_remaining_coop_rounds_if_join = avg_reliable_coop_fraction * (r - t + 1)
  if |reliable_players| < ceil(0.5*(n-1)) or expected_remaining_coop_rounds_if_join < required_future_rounds:
    // Not safe to join cooperation
    coop_run_counter = 0
    return D
  // Cooperative Mode
  if recent_drop_in_reliable_coop_fraction >= 0.4:
    punishment_counter = max(1, ceil(2*required_future_rounds))
    coop_run_counter = 0
    return D
  exploitation_interval = max(3, required_future_rounds)
  opportunistic_defect_probability = min(0.2, 1.0/(1+avg_reliable_coop_fraction*(n-1)))
  if coop_run_counter % exploitation_interval == 0:
    coop_run_counter += 1
    return D
  else:
    coop_run_counter += 1
    if rand() < opportunistic_defect_probability: return D else return C

Why this is “aggressive”
- Default play is D; you only cooperate when there is strong, empirical evidence of a stable cooperating coalition that is large enough to make the gamble profitable.
- When you join cooperation you periodically defect to extract extra surplus (exploitation).
- If cooperation destabilizes (others defect unexpectedly), you punish harshly for multiple rounds to deter further deviations.
- You ignore and exclude persistent defectors from your cooperation estimates — you do not try to appease known exploiters.
- You avoid risky cooperation late in the finite horizon (always defect near the end), which prevents being “suckered” by last-round defections.

Edge cases
- First round: D (optionally rare probe with small prob).
- Last rounds: always D during final_round_defect_window (includes last round).
- No reliable players (everyone uncertain or erratic): D always.
- Many unconditional cooperators: the strategy will detect reliable cooperators and then alternate cooperation with opportunistic defections to harvest higher per-round payoff than cooperators.
- Many conditional cooperators (e.g., tit-for-tat): we will detect reliability and either join lucrative cooperation (and exploit sometimes) or punish deviations strongly — robust to a wide class of opponent rules.
- Highly noisy opponents: reliability threshold and averaging across a window filter noise; if noise remains too high we default to D.
- Short games with small r: required_future_rounds may be > remaining rounds so we simply defect.

Tuning knobs (implementation choices)
- W (history window size): 3–7. Tradeoff: small W is responsive, large W is stable.
- reliability_thresh (default 0.8): higher makes us stricter about who we trust.
- exploitation_interval and opportunistic_defect_probability: adjust aggressiveness vs stealth.
- punishment_length formula: we propose proportional to required_future_rounds; you can lengthen for stronger deterrence or shorten for faster reconciliation.

Why this is robust
- Decisions are data-driven and use only observed actions.
- The reliability filter prevents a few defectors from contaminating the whole-group signal.
- Finite, explicit punishment avoids endless mutual punishment loops but is sufficiently harsh to discourage arbitrary exploitation.
- Opportunistic defections ensure we harvest surplus from cooperators rather than blindly contributing.

This specification is implementable: all quantities are computable from (n,k,r) and action history, and the decision procedure is deterministic except for optional seeded probes/exploitation randomness.
'''

description_AGGRESSIVE_114 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

High-level idea
- Be aggressively selfish: exploit any reliable cooperators, punish drops in community cooperation harshly, and never be the sucker in the final rounds.
- Probe briefly to detect who will cooperate. If enough others appear reliably cooperative, exploit them most of the time while giving occasional cooperation “pulse” to keep them engaged. If the community cooperation rate falls, withdraw cooperation permanently (or for a long punishment) so defectors are denied benefits.
- Always defect in the last rounds (endgame).

This strategy depends only on game parameters (n, r, k) and the observed history of actions (who cooperated each past round).

Parameters (tunable defaults)
- T_probe = min(3, max(1, floor(r/10))) — cooperate unconditionally for the first T_probe rounds to identify cooperators.
- T_endgame = min(2, r-1) — always defect in the final T_endgame rounds (if r small this reduces to 1).
- W = min(5, max(1, floor(r/5))) — window length used to compute recent cooperation rates.
- p_high = 0.85, p_low = 0.15 — thresholds used to classify strong cooperators or defectors from the probe.
- p_threshold (community cooperation threshold):
  - If k >= 0.75*n then p_threshold = 0.50 (public good is very valuable, tolerate lower partner cooperativity).
  - Else p_threshold = 0.75 (be demanding; aggressive).
- L_punish_min = 3 (minimum punishment length when community cooperation falls below threshold).
- q_keepalive_max = 0.25 — maximum fraction of rounds in which ACE will cooperate while “exploiting” to keep cooperators engaged.

Rationale for key numbers
- Aggressive stance: high p_threshold (0.75) means ACE will only support cooperation when a clear majority of others cooperate regularly, except when k is large enough that cooperating is more attractive overall.
- Exploitation pulses: ACE mostly defects even when community cooperation is good; occasional cooperation prevents immediate collapse of cooperative partners and provides continued exploit opportunities.
- Harsh punishment: if partners’ cooperation drops below p_threshold, ACE withdraws cooperation for a long stretch (or permanently) to deny benefits to defectors.

Decision rules (natural language)
1. Endgame rule (absolute): If current round t > r - T_endgame, play D (defect). In particular, always defect in the final round.
2. Probe phase: For t ≤ T_probe, play C (cooperate) unconditionally to test others.
3. After probe, compute each other player j’s empirical cooperation rate p_j over the history so far (use the whole history and a recent-window W for recency; combine them by weighted average if desired). Compute community average over others P = (1/(n-1)) Σ_j p_j and also compute last-round cooperation count S_{t-1} (including others).
4. Community-sustain check:
   - If P ≥ p_threshold (i.e., many players are reliably cooperative), enter Exploit mode:
     - Default action: D (defect), to exploit cooperators.
     - Keepalive pulses: once every M rounds cooperate to keep cooperators from abandoning cooperation. Choose pulse frequency so that fraction of rounds cooperating ≤ q_keepalive_max and proportional to how far above p_threshold P is:
       - q_keepalive = min(q_keepalive_max, (P - p_threshold)/(1 - p_threshold))
       - Implement deterministic pulse: cooperate on the next round iff round_number mod ceil(1/q_keepalive) == 0 (if q_keepalive==0 then never cooperate).
     - Monitor responsiveness: if after starting Exploit mode the community cooperation rate falls by more than delta_drop = 0.20 over the next H = min(5, r - t) rounds, switch immediately to Punish mode (see below).
   - Else (P < p_threshold): enter Punish mode:
     - Default action: D (defect) always, i.e., withdraw cooperation.
     - Stay in Punish mode for at least L_punish = min(L_punish_min, r - t) rounds. After that, re-evaluate P over recent window W; only return to Exploit mode if P ≥ p_threshold again.
5. Individual targeting (optional refinement): Track each player j’s reliability p_j. If a large subset of players (≥ ceil((n-1)/2)) are very strong cooperators (p_j ≥ p_high) then be slightly more exploitative (raise q_keepalive up to q_keepalive_max) because those players will likely sustain cooperation despite exploitation. Conversely, if most others are clear defectors (p_j ≤ p_low), remain permanently in Punish mode (defect forever except the endgame rule still applies).

Edge cases and special cases
- Very small r (r ≤ 3): The endgame rule dominates. If r ≤ T_endgame + T_probe, ACE will defect almost always (do not trust cooperation with very short horizons). Concretely, if r ≤ 2 ACE defects every round.
- Extreme k values: If k is close to n (public good almost fully efficient), p_threshold is reduced to 0.5 so ACE will be willing to support cooperation more often (still exploiting it aggressively).
- Noisy observations: If a round’s actions appear noisy, ACE uses the recent window W and weighted averages to smooth estimates. The pulse frequency is conservative so a single missing cooperator won’t cause immediate punishment.
- Simultaneous play: ACE’s pulses are deterministic relative to round number to avoid stochastic exploitation being misinterpreted as noise. If implementing stochastically is preferred, use the computed q_keepalive as a cooperation probability in Exploit mode.

Pseudocode

Inputs: n, r, k
History: for each past round t’ < t, record actions a_j,t’ ∈ {C,D} for each player j

Compute constants as above: T_probe, T_endgame, W, p_threshold, etc.

For round t:
  if t > r - T_endgame:
    action := D
    return action

  if t ≤ T_probe:
    action := C
    return action

  For each j ≠ me:
    p_j_long := (total times j cooperated) / (t-1)
    p_j_recent := (times j cooperated in last W rounds) / min(W, t-1)
    p_j := weighted average (e.g., 0.6*p_j_recent + 0.4*p_j_long)

  P := (1/(n-1)) * Σ_j p_j   // community average cooperation among others

  if currently in Punish mode and remaining_punish_rounds > 0:
    action := D
    decrement remaining_punish_rounds
    return action

  if P ≥ p_threshold:
    // Exploit mode
    q_keepalive := min(q_keepalive_max, (P - p_threshold)/(1 - p_threshold))
    if q_keepalive == 0:
      action := D
    else:
      M := ceil(1 / q_keepalive)
      if (t mod M) == 0:
        action := C
      else:
        action := D
    // After this round, check responsiveness over next H rounds (implemented next rounds).
    return action
  else:
    // Enter Punish mode
    remaining_punish_rounds := min(max(L_punish_min, ceil((p_threshold - P) * 10)), r - t) // harsher punish if P far below threshold
    action := D
    return action

Notes on implementation choices
- The exact numeric thresholds (p_threshold, q_keepalive_max, etc.) are tunable. The defaults give an overall aggressive profile: high threshold for entering cooperative support, exploitation when cooperative, and long punishment when cooperation drops.
- Implementation can store a Boolean “in_punish” flag and a counter for remaining punishment rounds to ensure deterministic behavior.
- To be even more aggressive: set p_threshold higher (e.g., 0.85) and make L_punish_min longer or permanent. To be slightly less destructive to possible cooperating strategies, lower p_threshold or increase q_keepalive_max.

Why this is “aggressive” and robust
- Aggressive: ACE mostly defects, exploits reliable cooperators via sustained defection with occasional cooperation pulses to prevent immediate collapse, and enforces long punishments when the community cooperation falters. It never risks being exploited in the final rounds.
- Robust: ACE adapts to observed behaviour (estimates p_j, community average P), uses short windows to detect recent changes, and has built-in hysteresis (minimum punishment length, smoothing) to avoid cycling from noise. It requires no assumptions about others’ strategies or communication; it only uses observable actions and game parameters.

Behavioral examples
- Versus unconditional cooperators: after T_probe, ACE will mostly defect and pocket the private endowment while occasionally cooperating to keep the cooperators from fully abandoning cooperation, maximizing ACE’s payoff.
- Versus unconditional defectors: ACE quickly identifies low P and stays in Punish (defection), avoiding being exploited.
- Versus reciprocal/conditional cooperators: ACE’s pulses encourage continued cooperation by cooperators who respond to past cooperation; ACE exploits them at high frequency but will punish if community cooperation drops substantially.

Summary (one-line)
Cooperate for a short probe, then be aggressively exploitative whenever a clear majority of others are reliably cooperative (defect most rounds, give occasional cooperation pulses), but withdraw cooperation permanently or for a long punishment if community cooperativeness falls below a high threshold; always defect in the final rounds.
'''

description_AGGRESSIVE_115 = '''
Name: Predatory Opportunist (PO)

High-level description
- PO is aggressive: it prefers to defect, actively exploits sustained cooperators, and punishes attempts to discipline it. It will never rely on explicit coordination or shared norms. Instead it uses measurable statistics from observed history (per-player cooperation frequencies and recent group cooperation rates) to decide when to milk cooperators, when to punish, and when to give small, infrequent “gifts” (cooperates) to keep exploitable cooperators in the population.
- PO is adaptive and robust because it continuously estimates which opponents are reliably cooperative, how the group responds to exploitation, and how many rounds remain, and it adjusts exploitation and punishment lengths accordingly.
- Deterministic core with a small, explicit “seeding” rule (that can be implemented deterministically or randomized by the tournament engine) to avoid being permanently ostracized by strategies that react to deterministic cycles.

Intuition behind choices
- In a single round defection strictly dominates cooperation (k/n < 1), so PO defaults to defection. Cooperating is only chosen as a strategic investment to sustain exploitable cooperators when that investment is likely to produce repeated extra payoffs in future rounds.
- PO tries to maximize its own total payoff and relative advantage: it will milk cooperators (defect while others cooperate) when it detects a stable subset of cooperators; it will punish groups that try to force cooperation by coordinated retaliation; it will stop cooperating near the endgame.

Notation
- n: number of players
- r: total rounds
- t: current round (1-indexed)
- history: full observed history, where for each past round s we know each player’s action (C/D)
- For an opponent j, f_j(H) = fraction of rounds they played C in the last H rounds (H chosen below)
- m_s = number of players (excluding PO) who cooperated in round s
- last_m = m_(t-1) (if t>1)
- remaining = r - t + 1

Tunable internal parameters (expressed in terms of n,r,k; defaults given)
- H = min(8, max(1, t-1)) — window length used to estimate recent behavior (default up to 8 rounds of history, or as many as exist)
- f_high = 0.80 — threshold to call an opponent a “reliable cooperator”
- rc_threshold = 0.50 — fraction of opponents that must be reliable for PO to consider them a stable exploitable bloc
- exploit_seed_period P_seed = max(3, ceil(6 * n / k)) — deterministic periodicity for small cooperative “gifts” when milking (longer if n is large or k is small)
- punish_length P_punish = min( max(3, ceil(H)), remaining ) — number of rounds to defect when entering a punishment state (bounded by remaining rounds)
- endgame_shrink S = min(3, r) — number of final rounds in which PO aggressively always defects; at least last round

Decision rules — overall flow (natural language)
1. First round (t = 1): Defect (aggressive probe).
2. Last round (t = r): Defect (no future benefit).
3. Endgame (t > r - S): Defect. In the last S rounds PO does not invest in cooperation because the horizon is short and defections give immediate advantage.
4. Otherwise (1 < t < r - S + 1): compute recent statistics over H rounds:
   - For each opponent j compute f_j(H).
   - Let RC = fraction of opponents with f_j(H) >= f_high (reliable cooperators fraction).
   - Let last_round_coop_fraction = last_m/(n-1) (if t>1; if t=1 then treated as 0).
   - Let trend_drop = whether last_round_coop_fraction is substantially lower than the H-window average (i.e., opponents reduced cooperation in response to recent exploitation).
5. State and action logic:
   - If RC >= rc_threshold:
       - Behavior: Milking (exploit mode).
       - Action: Defect by default to extract the premium from stable cooperators.
       - Exception/gift: Every P_seed rounds (deterministic schedule: e.g., if (t mod P_seed) == 0) PO plays C exactly to keep reliable cooperators from giving up permanently (a small, infrequent “maintenance” cooperation). The schedule can alternatively be randomized with probability p_seed = 1/P_seed if implementer prefers randomness.
       - Rationale: If many opponents are reliably cooperative, always defecting forever would cause some conditioned cooperators to adapt and stop cooperating. A small, selective and infrequent cooperation keeps them exploitable while maximizing returned payoff.
   - Else (RC < rc_threshold):
       - If last_round_coop_fraction >= 0.6:
           - Many cooperated last round but they are not individually reliable: exploit them immediately by defecting this round (aggressive exploitation when you see temporary generosity).
       - Else if last_round_coop_fraction <= 0.2:
           - The population is mostly defecting recently: remain defecting (no point to cooperate).
       - Else (ambiguous/mixed population):
           - If PO has been punished recently (detectable as a sudden drop in group cooperation following PO’s exploitation, or if a streak of opponents defecting after PO defected): enter Punish mode — defect for P_punish rounds to avoid being re-coerced and to maximize short-run relative payoff.
           - Otherwise: try a short probe for cooperation stability:
               - With low probability p_probe (deterministic choice: cooperate if (t mod P_probe) == 0 where P_probe = max(6, 2*P_seed)), play C one round to test whether some opponents will reciprocate stably.
               - If the probe is reciprocated stably over the next H rounds (i.e., RC rises above rc_threshold), transition to Milking mode.
               - If the probe is met with retaliation (group cooperation falls), revert to Punish mode and thereafter Defect.

6. Punishment details
- Entering Punish mode: triggered if after a period of targeted exploitation the group cooperation fraction drops by more than drop_threshold (e.g., 30 percentage points) within a short window, or if some policies try to “discipline” PO by sustained mutual defection that lowers PO’s relative payoff.
- Punish action: defect for P_punish rounds, then reassess. Punishment aims to make cooperation unattractive to others if they try to force PO into cooperating via contingent threats.
- Punish is calibrated to be moderate — long enough to deter naïve attempts to extract cooperation, but not so long that PO loses all future exploitation opportunities.

7. Relative-payoff trigger (aggressiveness safeguard)
- PO also tracks its cumulative payoff vs. population baseline (average of others). If after a window of H rounds the average payoff of others significantly exceeds PO’s by margin M_advantage (e.g., 15%), PO escalates aggressiveness: reduce P_seed (seed less frequently), extend P_punish, and increase frequency of exploitation to reclaim relative advantage.

8. Forgiveness
- After punishment or extended defection, PO will occasionally (deterministic scheduled probe or seed) cooperate to test whether milking opportunities reappear. Forgiveness is limited and controlled by the same RC threshold.

Pseudocode (concise)

Inputs: n, r, k, history, t
Derived params:
  H = min(8, max(1, t-1))
  f_high = 0.80
  rc_threshold = 0.50
  P_seed = max(3, ceil(6 * n / k))
  P_probe = max(6, 2 * P_seed)
  P_punish = min( max(3, ceil(H)), r - t + 1 )
  S = min(3, r)
  drop_threshold = 0.30
  p_seed deterministic schedule: cooperate when (t mod P_seed) == 0
  p_probe deterministic schedule: cooperate when (t mod P_probe) == 0

If t == 1: return D
If t >= r - S + 1: return D   // last S rounds: always defect
Compute for each opponent j: f_j(H) = fraction of Cs they played in last H rounds
RC = fraction of opponents with f_j(H) >= f_high
last_round_coop_fraction = if t>1: (number of opponents who played C in round t-1)/(n-1) else 0
avg_window_coop_fraction = average over last H rounds of (m_s/(n-1))

// Milking mode
If RC >= rc_threshold:
  If (t mod P_seed) == 0: return C   // small gift to sustain exploitation
  Else: return D

// Not clear bloc of reliable cooperators
If last_round_coop_fraction >= 0.6:
  return D   // exploit a recent outpouring of cooperation
If last_round_coop_fraction <= 0.2:
  return D   // population is defecting; no point to invest

// ambiguous region: probe, punish or punish continuation
If avg_window_coop_fraction - last_round_coop_fraction >= drop_threshold:
  // group reduced cooperation sharply — they may be punishing PO or adapting
  return D   // continue defection (equivalent to entering/continuing Punish)
If (t mod P_probe) == 0:
  // scheduled probe to detect exploitable cooperation
  return C
Else:
  return D

Notes and implementation remarks
- Deterministic seeding/probing is suggested for reproducibility. If implementers prefer stochasticization to be less predictable, replace deterministic schedules by cooperating with small probabilities p_seed = 1/P_seed and p_probe = 1/P_probe.
- Parameters (H, f_high, rc_threshold, P_seed, etc.) are tunable. The defaults above are conservative and reflect an aggressive mindset: defect unless a clear, stable opportunity to exploit exists and maintain that opportunity with minimal cooperative investment.
- The punish length P_punish and endgame shrink S are tied to remaining rounds so PO does not waste resources late in the game.
- PO is explicitly not "nice": it defects first and last, it milks cooperators when found, and it punishes attempts to discipline it. It is adaptive in the sense that it recognizes stable cooperators and will alternate exploitation with small cooperation to preserve that niche.

Edge cases handled
- First round: defect (aggressive opening).
- Last round and final S rounds: defect (no future incentives).
- Very short games (r small): parameters shrink (H <= r-1, S <= r), so PO effectively defaults to defect for most/all rounds as appropriate.
- No history (t=1) and near-zero cooperation among others: consistent defection.
- If many opponents are individually reliable (RC high) PO milks them deterministically, with only rare “maintenance” cooperations to keep them in the exploitable state.
- If the group tries to coordinate retaliation (observed as a sharp drop in cooperation after PO’s exploitation), PO increases punishment and lowers seeding frequency.

Why this is aggressively effective
- It extracts the maximal per-round advantage from cooperators (defecting while they cooperate).
- It prevents long-lived symmetric cooperation that would reduce PO’s relative advantage by giving frequent, small cooperative “gifts” rather than costly sustained cooperation.
- It detects and responds to punishment attempts (avoids being trapped into cooperating permanently).
- It avoids wasting cooperation in the endgame.
- It is robust to a wide range of opponent behaviors (always-defectors, TFT-like, grim, stochastic cooperators) because it uses short-window statistics and deterministic probing/punishing rules to learn and adapt.

Implementation checklist for algorithm developers
- Maintain per-opponent counts of Cs over sliding window H and overall.
- Compute RC, last_round_coop_fraction and avg_window_coop_fraction each round.
- Implement deterministic schedules for P_seed and P_probe; optionally convert to probabilities.
- Implement punishment logic based on drops in cooperation after exploitation.
- Ensure S, P_punish, H calculation respect remaining rounds so no gratuitous long punishments occur near the finish.

Summary one-liner
Predatory Opportunist: defect by default, detect reliably cooperative opponents and milk them (defecting almost always with rare cooperative “maintenance” gifts), punish coordinated retaliation moderately, and always defect in the final rounds — adaptive, exploitative, and robust.
'''

description_AGGRESSIVE_116 = '''
Name: Aggressive Probe-and-Exploit with Grim Consequences (APEGC)

Core idea (short): Default to defect, but run a short, deliberate probing phase to identify persistent cooperators. Once identified, repeatedly exploit them by defecting while they continue cooperating. If a trusted cooperator ever defects when I cooperated, respond with immediate, harsh punishment (long defection/grim) and then resume probing only later. Always defect in the known endgame rounds. Periodically re-probe to find new cooperators. This strategy is aggressive: it seeks to extract surplus from cooperators, punishes deviations harshly, rarely gives away cooperative opportunities except as deliberate probes.

High-level parameters derived from game inputs (all deterministic functions of n, r, k):
- If k/n ≥ 1 then cooperation is individually profitable in a single round; switch to a cooperative-biased policy (see edge-case clause below).
- t_probe = min(3, max(1, floor(r/4))) — number of initial probing rounds.
- endgame_len = max(1, ceil(r/10)) — final rounds in which I always defect.
- reprob_interval = max(3, ceil(r/5)) — interval between periodic re-probes.
- window = min(5, r-1) — history window used for short-term frequency checks.
- coop_freq_threshold = 0.7 — fraction of probe rounds a player must have cooperated to be classed as a “trusted cooperator.”
- punishment_len = max(endgame_len, ceil(r/10)) — number of rounds to punish a trusted defector before trying to recover (can be set to large value for more aggression).

Definitions:
- History contains every round’s vector of actions (who played C or D).
- trusted_set: players who cooperated frequently during the probing window and are currently trusted.
- last_punish_round: round index when I last started a punishment period (if any).
- last_reprobe_round: last round I performed a re-probe.

Decision rules (round-by-round):

Pre-checks (every round):
1. If k/n ≥ 1: cooperating increases my single-round payoff. Use a cooperative-biased fallback: cooperate in all rounds except when a long-running punishment is active (see punishment rules). (This is an edge-case guard; for the intended input k/n < 1 this will never trigger.)
2. If current round t > r - endgame_len: play D (always defect in the endgame).
3. If currently in an active punishment period (t ≤ last_punish_round + punishment_len): play D (punish).

Main algorithm:

Initialization (before round 1):
- trusted_set = ∅
- last_punish_round = -∞
- last_reprobe_round = -∞

Rounds 1..t_probe (initial probe phase):
- Act: play C every probe round (intention: attract/identify cooperators).
- After probe phase completes (after round t_probe), build trusted_set:
   For each other player j:
     if number_of_C_by_j_in_rounds_1_to_t_probe ≥ ceil(coop_freq_threshold × t_probe): add j to trusted_set.

Rounds t_probe+1 .. r:
- If t in endgame: (already handled by pre-check) play D.

- If t == some scheduled re-probe round (i.e., t_probe + m × reprob_interval for integer m and t ≤ r - endgame_len):
    - Play C this round to probe whether new cooperators exist.
    - Set last_reprobe_round = t.
    - After the re-probe round, update trusted_set: for any player j, if j cooperated in this re-probe round AND has cooperated in a majority of the last window rounds (including probe data if available), add j to trusted_set.

- Exploitation rule (dominant rule when not probing/re-probing/punishing/endgame):
   - If trusted_set is non-empty:
       - If in the immediately preceding round every member of trusted_set chose C (i.e., they cooperated while I may have defected), then play D now to exploit them (free-ride).
         Rationale: exploit persistent cooperators repeatedly; defect gives higher immediate payoff than cooperating given k/n < 1.
       - Else (trusted_set members did not all cooperate last round):
         - If I cooperated in the previous round and some trusted_set member defected in that same previous round: classify those defecting trusted_set members as betrayers, remove them from trusted_set, and set last_punish_round = t (start punishment). Immediately play D this round (punish).
         - Otherwise, play D (default aggressive posture).
   - If trusted_set is empty and it is not a probe/re-probe round: play D (default defect).

Punishment and recovery:
- If a trusted_set player j defects in a round where I cooperated (I detect betrayal), immediately:
   - Remove j from trusted_set.
   - Set last_punish_round = current round t.
   - For punishment_len rounds after last_punish_round: play D (this is the punishment period).
- After punishment period ends:
   - Resume periodic re-probing (a re-probe will be scheduled at the next reprob_interval multiple) to search for new cooperators.

Edge cases and clarifications:
- First round: part of probe phase -> play C if r ≥ 4 (because t_probe≥1); if r is very small (r ≤ 3), t_probe may be 1 or strategy might default to defecting every round — to be explicit: if r ≤ 3 the strategy is to always defect (endgame is immediate and threats are not credible).
- Last round: always defect (backward induction, aggressive stance).
- If many players (n large) but k small, cooperation is unlikely to be sustained — aggressive strategy will mostly defect except during probes.
- If k/n ≥ 1 (rare under given constraints), cooperating can be immediately profitable; then the strategy flips to cooperating except during active punishment windows, because single-round cooperation is individually rational.
- Randomization: this description is deterministic. One can add small randomized probing (probabilistic C on probe rounds) to be less predictable, but that is optional.

Pseudocode (compact):

Given n, r, k
Compute t_probe, endgame_len, reprob_interval, window, coop_freq_threshold, punishment_len
trusted_set = {}
last_punish_round = -inf
last_reprobe_round = -inf

for t in 1..r:
  if k/n >= 1:  # edge case
    if t > r - endgame_len: play D; continue
    if t <= last_punish_round + punishment_len: play D; continue
    play C; continue

  if t > r - endgame_len: play D; continue
  if t <= last_punish_round + punishment_len: play D; continue

  if t <= t_probe:  # initial probing
    play C
    if t == t_probe: update trusted_set: add players with >= ceil(coop_freq_threshold*t_probe) cooperations in rounds 1..t_probe
    continue

  if t == (t_probe + m*reprob_interval for some integer m) and t <= r - endgame_len:
    play C
    last_reprobe_round = t
    update trusted_set using recent window
    continue

  # exploitation/default logic
  if trusted_set not empty:
    if all members of trusted_set played C in round t-1:
      play D  # exploit
      continue
    else:
      if I played C in round t-1 and some member of trusted_set played D in round t-1:
         remove those members from trusted_set
         last_punish_round = t
         play D  # immediate punishment
         continue
      else:
         play D
         continue
  else:
    play D  # no trusted cooperators, stay aggressive

Why this is aggressive and robust:
- Aggressive: default is defection. The strategy proactively tries to extract surplus from consistent cooperators (it probes briefly, identifies them, then free-rides repeatedly). It punishes betrayal harshly and for a long duration, discouraging exploitation of my probes. It defects in the final rounds unambiguously.
- Robust: because it probes periodically, it adapts to shifting opponent pools and can discover new cooperators. It does not rely on shared norms or explicit coordination — it builds its own list of trusted cooperators using only observed actions. The punishment is deterministic and credible (I actually follow through by defecting), so some reciprocal cooperators who try to cooperate can be stabilized if they care enough to tolerate the exploitation structure (or if mutual exploitation/punishment dynamics ensue).
- Adaptive: trusted_set updates from history, re-probing allows discovering changing cooperation, and punishments respond to betrayals. Parameters (probe length, re-probe interval, punishment length) scale with r so the strategy behaves sensibly across short and long games.

Tunable aggressiveness:
- Increase coop_freq_threshold and punishment_len to be more brutal (harder to be trusted, longer punishment).
- Decrease t_probe or reprob_interval to be even more stingy (fewer probes).

Implementation notes for integrators:
- All decisions use only game parameters (n, r, k) and past rounds’ observed actions.
- Ensure index handling for very small r (e.g., r=2 or 3) to default to defection.
- The trusted_set logic requires tracking individual players’ action histories (available by assumption).

This strategy is intended to win tournaments where maximizing individual payoff relative to others is the objective and where being aggressive (exploit cooperators, punish deviations hard) is acceptable.
'''

description_AGGRESSIVE_117 = '''
Name: Aggressive Seed-and-Harvest (ASH)

Intuition (short)
- The one-shot dominant action is defect, so cooperating is costly now but can be profitable only if it induces other players to raise their future cooperation. ASH is aggressively selfish: it defects by default, probes sparingly to detect whether others respond to my cooperation, and—if others are responsive—invests very small, calculated “seeds” of cooperation to induce more cooperation from opponents, then switches to persistent defection to “harvest” the higher payoffs produced by those induced cooperators. It never cooperates when there is no realistic prospect of recouping the immediate cost.

High-level rules
- Default = defect.
- Probe early and rarely to estimate opponent responsiveness to my cooperation.
- Cooperate only when the estimated future benefit (from induced extra cooperation by others) exceeds the immediate cost.
- If a seed (a deliberate cooperation) yields an observable increase in others’ cooperation probabilities, exploit it by defecting for many subsequent rounds (harvest).
- Never cooperate in the last round. Be conservative when too few rounds remain to recoup seed costs.
- Tie-breaker: when indifferent, defect.

Parameters used by the strategy (internal; can be tuned)
- window w for recent-frequency estimates (suggestion: w = min(10, r - current_round + 1))
- probe_rounds T_probe = min(5, max(1, floor(r/10))) — small initial window to gather signal
- probe_prob p_probe = 0.05 (probability to randomly cooperate during probe rounds)
- smoothing alpha = 1 (Laplace smoothing to avoid divide-by-zero)
- seed_size s = 1 (cooperate for 1 round as a “seed”)
- responsiveness threshold B_thresh = 0.02 (aggregate responsiveness required to consider seeding)
- minimum remaining rounds to seed: R_min = 1 + ceil((1 - k/n) / ((k/n) * B_est)) rounded up (see calculations below); if remaining rounds < R_min, do not seed
- harvest_max fraction: default allow harvesting until end or until others’ cooperation rate collapses below baseline

Definitions from history (at round t; we are choosing action for round t)
- r_rem = remaining rounds including this one = r - t + 1
- For each opponent j, let p_j = estimated probability that j cooperates in a given round, computed from their last w actions with Laplace smoothing: p_j = (coops_j + alpha) / (trials_j + 2*alpha).
- Group base cooperation rate P_group = average_j p_j.
- Responsiveness estimate for each opponent j: beta_j = estimated change in j’s probability to cooperate in a round after observing that I cooperated in the previous round vs after I defected.
  - Estimated from history by comparing frequency of j’s cooperation in rounds (t' + 1) conditional on my action at t'.
  - Concretely: let n_myC = number of rounds in history where I cooperated; n_myD = number I defected. Let coops_after_myC_j = number of times j cooperated in the round immediately following my cooperation; coops_after_myD_j = similarly after my defection. Beta_j = (coops_after_myC_j + alpha)/(n_myC + 2*alpha) - (coops_after_myD_j + alpha)/(n_myD + 2*alpha).
- Aggregate responsiveness B_est = sum_j max(0, beta_j). (We only count positive responsiveness; negative responsiveness is treated as zero for seeding evaluation.)

Economic check (cost vs expected future benefit)
- Immediate cost of cooperating this round: C0 = 1 - (k/n).
- If cooperating now increases the expected number of other cooperators by B_est (sum of per-player increases), then expected extra payoff to me in each future round = (k/n) * B_est (because each additional cooperator increases my payoff by k/n).
- If I seed for s rounds and expect that effect to persist for H future rounds, expected benefit ≈ H * (k/n) * (B_est * s) and cost ≈ s * C0. A seed is worth it only if H * (k/n) * (B_est * s) > s * C0 → H > C0 / ((k/n) * B_est).
- Since we cannot guarantee H, assume the optimistic but conservative choice H ≈ r_rem - 1 (all remaining rounds after this), but we check feasibility before seeding: require (r_rem - 1) * (k/n) * (B_est * s) > s * C0 (equivalently r_rem > 1 + C0 / ((k/n) * B_est)).

Decision procedure (explicit)

At the start of each round t:
1. If t == r (last round): Defect. (No future to influence.)

2. Update history statistics (p_j, P_group, beta_j, B_est) using the most recent w rounds or entire history if short.

3. If t <= T_probe (early probe window):
   - With probability p_probe, play C (cooperate) as a probe; otherwise play D. (Default biased to D.)
   - Purpose: to gather data for beta_j estimates but keep probes rare.

4. If B_est is very small (B_est <= B_thresh) OR remaining rounds too few to recoup seed cost:
   - Play D (always defect). (No realistic return on seeding.)

   Concretely compute:
   - If B_est <= B_thresh → Defect.
   - Else compute required_rounds = 1 + ceil( (1 - k/n) / ( (k/n) * B_est ) ).
     If r_rem < required_rounds → Defect.

5. Otherwise (B_est large enough and enough rounds remain): consider seeding.
   - Seed decision:
     - If in previous round I performed a planned seed and in the subsequent round group cooperation rose by at least delta_obs (suggested delta_obs = 0.02 or relative increase 5%), then we interpret seed as successful → enter HARVEST mode: defect persistently to exploit the induced cooperation until the group cooperation rate falls back near baseline or until end.
     - If not currently in a harvest and we have not seeded recently, and above checks passed, perform a 1-round seed: play C this round (s = 1). Flag that we seeded this round.
     - If we seeded in the immediate previous round and observed no meaningful rise in others’ cooperation in the slot(s) following the seed, then stop seeding and revert to Defect forever (since seeding failed).

6. Harvest mode:
   - If a seed has been observed to trigger an increase in others’ cooperation (group cooperation P_group rose by at least delta_obs within the following 1–2 rounds), set mode = HARVEST.
   - While in HARVEST and remaining rounds > 0:
     - Play D every round to exploit.
     - Monitor P_group; if P_group falls below baseline by more than a tolerance (e.g., below pre-seed baseline minus 0.02) for 2 consecutive rounds, exit HARVEST and return to default (defect, possibly resume probing in rare cases to retest).
     - If r_rem becomes too small to sustain harvesting advantage (use the same required_rounds check), stop harvesting.

7. Tie-breaking / safety:
   - In any borderline or numeric tie, choose D.
   - Never cooperate for “nice” reasons—only to seed when the above economics justify it.

Pseudocode (concise)

Inputs: n, r, k. Access to history: for each past round t', my action a_self[t'], each opponent j action a_j[t'].

Initialize:
  mode = DEFAULT
  last_seed_round = None
  baseline_group_rate = None

Function choose_action(t):
  r_rem = r - t + 1
  if t == r:
    return D

  compute p_j, P_group over window w
  compute beta_j (responsiveness) for each j from my past actions → B_est = sum_j max(0, beta_j)
  C0 = 1 - k/n

  if t <= T_probe:
    with probability p_probe: return C
    else: return D

  if B_est <= B_thresh:
    return D

  required_rounds = 1 + ceil( C0 / ( (k/n) * B_est ) )
  if r_rem < required_rounds:
    return D

  if mode == HARVEST:
    if r_rem < required_rounds:
      mode = DEFAULT
      return D
    if P_group has dropped below baseline_group_rate - tol for 2 rounds:
      mode = DEFAULT
      return D
    return D  # exploit while others cooperate

  # Not in harvest, and B_est sufficient: consider seeding
  if last_seed_round == t-1:
    # we seeded last round; check if others responded
    if P_group (measured after last round) >= baseline_group_rate + delta_obs:
      # successful seed → enter harvest
      mode = HARVEST
      return D
    else:
      # seed failed → stop trying
      return D

  # otherwise we haven't seeded recently and conditions pass → perform a seed
  baseline_group_rate = P_group  # record baseline before seeding
  last_seed_round = t
  return C

Notes on parameters and tuning
- The numeric thresholds (p_probe, T_probe, B_thresh, delta_obs, window w) are conservative defaults. The strategy intentionally probes rarely and seeds rarely, since ASH is aggressive and avoids wasting opportunities.
- If an empirical tournament suggests opponents are highly responsive (beta_j large), increase willingness to seed (lower B_thresh).
- s=1 seed is minimal risk; multiple-round seeding can be used if responsiveness estimates indicate multi-round conditioning, but ASH prefers 1-round seeds and rapid harvesting to minimize cost and risk.

Why this is aggressive and robust
- Aggressive: Default is defect; cooperation is only used as a tactical investment to induce others to increase their cooperation and then be exploited. Harvesting (defecting when others cooperate) is the core payoff-maximizing move. Punishment is implicit: failed seeds are abandoned permanently—no long, costly attempts to build cooperation for others’ sake.
- Adaptive: ASH learns responsiveness from observed history (beta_j) and uses an explicit economic test that compares immediate cost to expected future benefit under observed responsiveness.
- Robust: It does not rely on shared norms or explicit coordination. It makes few probes and only acts on clear empirical signals. When opponents are unresponsive, ASH becomes pure defector (safe). When opponents are very cooperative or conditional in a predictable way, ASH extracts payoff by seeding small investments and harvesting reliably.
- Finite-horizon safe: Never cooperates in the last round; seeds only when enough rounds remain to plausibly recoup cost.

Edge cases handled
- First round: Within probe window → almost always defect; tiny chance (p_probe) to cooperate to get initial signal.
- Very short games (r small): probe windows shrink; required_rounds check prevents seeding when not enough time exists to recoup costs.
- Unresponsive opponents: B_est small → ASH defects every round (no losses from wasted seeding).
- Highly responsive opponents: ASH seeds minimal amount and then defects repeatedly to harvest extra payoffs.
- Noisy behavior: Laplace smoothing (alpha) and sliding window w avoid overreacting to single anomalous rounds.
- Multiple seeds: if seeds fail, ASH stops seeding; if seeds succeed, ASH harvests heavily but monitors and stops harvesting when opponents’ cooperation collapses.

Summary in one line
Defect by default; probe rarely to estimate whether my cooperation moves others; only cooperate as a small, economically-justified “seed” to induce others’ cooperation, then relentlessly defect to harvest the induced group contributions—never cooperate when the math (based on observed responsiveness and remaining rounds) says you cannot recoup the immediate cost.
'''

description_AGGRESSIVE_118 = '''
Name: Aggressive Probe–Exploit–Punish (APEP)

Summary (one line)
- Default behavior: defect every round. Occasionally probe by cooperating to detect reliably cooperative opponents. If a clear majority of opponents are consistently cooperative, aggressively exploit them (keep defecting while they continue to contribute) and respond to any sign of retaliation or unexpected defection with harsh, time‑bounded punishments and permanent blacklisting of obvious exploiters. Always defect in the final round.

High‑level rationale
- In a single round defection strictly dominates (πD − πC = 1 − k/n > 0), so an aggressive strategy should seek to defect most of the time and only cooperate to (a) learn which opponents are reliably cooperative or (b) test whether an exploit remains profitable. Probes are short and infrequent to limit losses. If a stable cluster of cooperators is detected, exploit them repeatedly; if they punish or stop cooperating, punish back strongly for a short controlled period, then resume default defection and re-probe later. The strategy is deterministic and only depends on game parameters (n, r, k) and observed history.

State tracked
- For each player j (including self): C_j = total times j played C so far; T = number of rounds observed so far.
- Round-by-round total_cooperators[t] (including you).
- Mode ∈ {DEFAULT, EXPLOIT, PUNISH}
- If EXPLOIT: baseline_coop_fraction (observed fraction of other players cooperating during the probe rounds that triggered EXPLOIT).
- Blacklist: set of players who are marked as permanent defectors (never trusted).
- Timers: punish_timer (if in PUNISH), next_reprobe_round (when to test cooperation again)

Fixed parameters (computed from n,r; all values are deterministic and simple to implement)
- initial_probes = min(2, r-1)  // probe in rounds 2 and/or 3 if available (we always defect in round 1)
- maj_threshold = ceil((n-1)/2)  // majority of other players
- cooperaterate_threshold = 0.60  // use 60% observed cooperation to call someone “cooperative”
- exploit_reprobe_interval = max(3, floor(r/4))  // how often to re-probe during exploit mode
- punish_length = min(5, max(2, floor(r/6)))  // length of controlled punishment phase (bounded)
- reprobe_drop_delta = 0.33  // if cooperation drops by more than 33% from baseline, treat as retaliation
- last_round_defect = True (always defect in round r)
- small_game_all_defect_cutoff = 3  // if r <= 3 use simple behavior: defect every round

Decision rules (explicit)
1) Edge cases / basic rules
- If r <= small_game_all_defect_cutoff: defect every round (there is insufficient horizon to benefit from trying to build cooperation).
- Always defect on the final round t = r.
- First round t = 1: defect (aggressive default). Rationale: get guaranteed one-round advantage and gather baseline info.

2) Initial probing (only if r > 3)
- For t in {2, 3} (but never t = r): cooperate as a short probe to see who responds by cooperating in the same round(s).
- After each probe round, compute other_coops = number of other players (j ≠ you) who played C in that round.
  - If other_coops ≥ maj_threshold in the probe round, increment a “positive_probe_count” and update baseline_coop_fraction using the fraction of others cooperating in that probe round.
  - If positive_probe_count reaches 1 (i.e., at least one strong probe that shows a majority of others cooperated), enter EXPLOIT mode:
      - Mode := EXPLOIT
      - baseline_coop_fraction := average other-cooperation fraction across the positive probe(s)
      - schedule next_reprobe_round := current_round + exploit_reprobe_interval

3) EXPLOIT mode (aggression core)
- While in EXPLOIT, action each round (except scheduled re-probe rounds and last round): defect.
- On a scheduled re-probe round (t == next_reprobe_round and t < r):
  - Cooperate for one round to test whether the group still cooperates.
  - Observe other_coops (count of other players who played C that re-probe round).
  - If other_coops/n−1 (fraction of others cooperating) ≥ baseline_coop_fraction − small_eps (small_eps can be 0.05 or omitted): exploitation is still profitable — stay in EXPLOIT, set next_reprobe_round += exploit_reprobe_interval, optionally update baseline_coop_fraction to a moving average including this re-probe.
  - If the cooperation fraction observed in the re-probe has dropped by more than reprob_drop_delta relative to baseline_coop_fraction, assume effective retaliation and immediately enter PUNISH mode:
      - Mode := PUNISH
      - punish_timer := min(punish_length, remaining_rounds − 1) (never punish into the final round)
      - Clear EXPLOIT flags; schedule re-probe after punishment if remaining rounds allow.

- During EXPLOIT, permanently add to Blacklist any player who defected in a probe round where the majority cooperated (they showed opportunistic defection). Blacklist membership means they are treated as “not trustworthy” for later decisions (helps refine which future probes count as positive).

4) PUNISH mode
- While punish_timer > 0: always defect (this is a short, severe collective punishment to lower group payoff and specifically deter retaliatory strategies).
- Decrement punish_timer each round; when punish_timer reaches 0 set Mode := DEFAULT.
- After PUNISH finishes, do not automatically return to EXPLOIT — wait until you re-establish cooperation via new probe(s).

5) DEFAULT mode
- Outside probes, EXPLOIT, and PUNISH, the default action is to defect.
- Occasionally re-probe later to search for newly formed cooperative clusters:
  - Schedule a new probe if at least floor(r/5) rounds remain (or after a cooldown of exploit_reprobe_interval from last probe), to try again to find exploitable cooperators.
  - A probe is a single-coop round (cooperate) to gather current cooperation pattern; if a majority of others cooperate in that probe, transition to EXPLOIT (as above).
- If repeated probes repeatedly fail (no majority cooperates), remain in DEFAULT defect forever.

6) Last-round and late-game behavior
- Always defect on t = r.
- As the game approaches the end and remaining_rounds ≤ punish_length + 1, be conservative about entering EXPLOIT or PUNISH modes (do not start a punishment that would cascade into the final round). If fewer than 2 future rounds remain, act as DEFAULT (defect).

Concrete pseudocode (compact)

Initialize:
  Mode := DEFAULT
  C_j := 0 for all players j
  T := 0
  positive_probe_count := 0
  baseline_coop_fraction := 0
  Blacklist := {}
  next_reprobe_round := null
  punish_timer := 0

For each round t = 1..r:
  Observe T previous rounds; remaining_rounds := r - t + 1

  if r <= 3:
    play D; continue

  if t == r:
    play D; continue

  if Mode == PUNISH:
    play D
    punish_timer := punish_timer - 1
    if punish_timer == 0: Mode := DEFAULT
    continue

  if t == 1:
    play D; continue

  // Initial probing rounds (2 and 3) if still unused
  if t in {2,3} and t < r:
    play C  // probe
    // after observing others' moves in same round update:
    other_coops := count of j ≠ you who played C this round
    for each j: update C_j and T_j
    fraction := other_coops / (n-1)
    if other_coops >= maj_threshold:
      positive_probe_count += 1
      baseline_coop_fraction := (baseline_coop_fraction*(positive_probe_count-1) + fraction) / positive_probe_count
      Mode := EXPLOIT
      next_reprobe_round := t + exploit_reprobe_interval
      // blacklist individual defectors who defected this probe while majority cooperated:
      for each j ≠ you: if j played D and other_coops >= maj_threshold then Blacklist.add(j)
    continue

  // EXPLOIT behavior
  if Mode == EXPLOIT:
    if t == next_reprobe_round and t < r:
      play C // re-probe
      // after observing others:
      other_coops := count of j ≠ you who played C this round
      fraction := other_coops / (n-1)
      if fraction + 1e-9 >= baseline_coop_fraction - 0.05:
        // still good: update baseline and schedule next re-probe
        baseline_coop_fraction := (baseline_coop_fraction + fraction) / 2
        next_reprobe_round := t + exploit_reprobe_interval
      else if baseline_coop_fraction - fraction > reprob_drop_delta:
        // evidence of retaliation/punishment
        Mode := PUNISH
        punish_timer := min(punish_length, remaining_rounds - 1)
      else:
        // borderline: stay exploit or fall back
        next_reprobe_round := t + exploit_reprobe_interval
    else:
      play D
    continue

  // DEFAULT mode actions
  // Determine whether to start a new probe:
  if (no probe scheduled recently) and (remaining_rounds > exploit_reprobe_interval + 1):
    // start a probe occasionally
    if (t is scheduled probe round according to a simple schedule) then:
      play C (probe)
      // after observing:
      other_coops := count of j ≠ you who played C this round
      fraction := other_coops / (n-1)
      if other_coops >= maj_threshold:
        Mode := EXPLOIT
        baseline_coop_fraction := fraction
        next_reprobe_round := t + exploit_reprobe_interval
        for each j ≠ you: if j played D then Blacklist.add(j)
      else:
        // probe failed: return to default
        play result already executed
      continue
  // otherwise:
  play D
  continue

Notes on implementation details and reasonable parameter choices
- Default numeric choices provided above are deliberately conservative and simple (initial_probes up to 2, majority threshold, 60% cooperaterate threshold, short punishments). These values can be tuned in implementation, but keep them fixed and deterministic.
- Blacklist is used only to refine probe interpretation (if the same players consistently defect in rounds where majority cooperates, mark them untrusted). Because contributions are public, blacklist is not a direct targeting tool (you still punish collectively by defecting), but it reduces the weight you give their apparent cooperation when deciding whether a majority cooperated.
- The strategy is intentionally harsh: it exploits observed cooperators by defecting in the exploitation phase, and it responds to retaliation with strong but time-limited punishment. It is also robust: defaulting to defect and probing infrequently prevents being trapped by revenge dynamics for long; punishments are time-bounded so the strategy can re‑assess later.

Why this is “aggressive” and robust
- Aggressive: defaults to defect, actively seeks to exploit cooperative opponents once identified, blacklists opportunistic defectors, and uses clear punitive phases when retaliated against.
- Robust: uses short, informative probes (limiting losses), uses objective majority-based detection (no need to assume norms), uses bounded punishments to avoid endless cycles, always defects in last round (avoid being exploited at the end).
- Adaptive: reacts to observed behaviors (per-player cooperation history, round-level cooperation fractions) and changes mode accordingly (DEFAULT ↔ EXPLOIT ↔ PUNISH). It will exploit stable cooperators but back off when retaliation is detected.

A short example run (n=6, r=20)
- t=1: D (default)
- t=2: C (probe). If 4 or more other players cooperated -> enter EXPLOIT. Otherwise return to DEFAULT.
- If EXPLOIT entered, t=3.. until next_reprobe play D every round collecting payoffs; at scheduled re-probe round you play C once — if most others still cooperate you keep exploiting; if many stop cooperating you trigger PUNISH for several rounds, then go back to DEFAULT and probe again later.

Final remarks
- This strategy makes no assumptions about common norms or coordination; it uses only observed actions and payoffs, is deterministic given the same history, and is parameterized by n and r (and indirectly k insofar as the strategy recognizes that per-round defect is individually dominant). It is intentionally aggressive in maximizing short- and medium-term payoffs when opponents are exploitable, while keeping punishments targeted to deter and limit losses from retaliatory strategies.
'''

description_AGGRESSIVE_119 = '''
Name: Adaptive Exploiter with Probes and Punishment (AEP²) — an aggressive, history-driven strategy that defaults to defection, opportunistically exploits persistent cooperators, probes to detect exploitable populations, and punishes groups that threaten long-term gain.

High-level intent (aggressive mindset)
- Default action: defect. Do not trust others or "start nice."
- Exploit clear, stable cooperation among others whenever safe to do so.
- Probe occasionally (small, controlled investments in cooperation) to discover exploitable cooperative opponents.
- If my cooperation reliably brings others’ cooperation (reciprocity), switch briefly into coordinated cooperation only when the expected long-run payoff justifies it.
- If others respond to my defection by systematically lowering cooperation (they are retaliators or I caused collapse), punish sharply for a fixed interval (or permanently when near the end) so that exploitative opponents lose payoff.
- Always defect in the final round (standard backward induction); be increasingly defecting near the end.

Notation and inputs available to the strategy
- n, r, k (game parameters) — known.
- History H_t up to round t−1: for each prior round u (1..t−1) we observe the action vector a_u ∈ {C,D}^n (including our own), so we can compute counts and per-player empirical cooperation rates.
- We are player i; “others” means the n−1 other players.
- We'll use simple summary statistics over a sliding window of recent rounds to measure population behavior.

Tunable internal parameters (defaults)
- W (window length for recent behavior): min(10, r−1)
- S (probe spacing): max(3, round(sqrt(r))) — one probe every S rounds by default
- p_exploit_high (exploit threshold): 0.70 — if average cooperation among others ≥ this, exploit by defecting
- p_probe_low, p_probe_high (probe-band): [0.25, 0.70] — if cooperation among others in this band, run probes to classify responsiveness
- p_coop_switch (switch-to-cooperation threshold): 0.80 — require strong majority cooperation and positive responsiveness to my cooperation
- delta_resp (responsiveness threshold): +0.20 — minimal detectable boost in others’ cooperation following my cooperation probe
- L_coop (cooperation run length when switching): 3 rounds
- P_punish (punishment duration): 5 rounds (or till end if close to final rounds)
- epsilon_probe (probability of cooperating during a probabilistic probe): 0.05 (if using probabilistic probing)
- endgame_margin: 1 (always defect in last round); optionally set to 2 or 3 to be safe

Decision rules (deterministic outline + pseudocode style)

Overview
- On each round t, compute recent statistics from the last W rounds (or as many rounds exist).
- Apply endgame rule first (if t is within endgame_margin of r, defect).
- Otherwise: check whether we are currently in PUNISH, COOP, or normal EXPLOIT/PROBE mode (modes are tracked by simple state variables).
- If in a forced mode (COOP for L rounds, PUNISH for P rounds), follow that mode unless endgame overrides.
- If in normal mode, decide by population-cooperation level and probe schedule.

Detailed metrics computed each decision
- coop_count_u = number of cooperators (including ourselves) in round u.
- others_coop_rate_u = (coop_count_u − indicator(we cooperated in u)) / (n−1).
- recent_others_rate = mean(others_coop_rate_u for u = max(1,t−W) .. t−1). If no history, treat as 0.
- If we recently probed (we cooperated in a probe round), compute responsiveness: response = mean(others_coop_rate in the round immediately after a probe) − baseline_recent_rate (baseline being recent_others_rate computed just before the probe). Maintain a small sample over last few probes and take the sample mean.

Pseudocode (natural-language/pseudocode hybrid)
Initialize:
  state = NORMAL
  coop_run_remaining = 0
  punish_remaining = 0
  last_probe_rounds = []  # record rounds we probed and the baseline around them
  probe_count = 0

On round t (for t from 1..r):
  if t == r:
    play D (defect). Exit.

  if t > r - endgame_margin:
    play D (defect). Exit.

  compute recent_others_rate over window W (0 if no history)

  # If currently in a forced COOP mode
  if state == COOP and coop_run_remaining > 0:
    coop_run_remaining -= 1
    if coop_run_remaining == 0:
      state = NORMAL
    play C
    continue

  # If currently in a forced PUNISH mode
  if state == PUNISH and punish_remaining > 0:
    punish_remaining -= 1
    if punish_remaining == 0:
      state = NORMAL
    play D
    continue

  # NORMAL mode: decide whether exploit, probe, or attempt cooperative switch
  # 1) If the recent others’ cooperation is very high → pure exploitation
  if recent_others_rate >= p_exploit_high:
    # Clear planned probes: ruthless exploitation
    play D
    continue

  # 2) If recent_others_rate in probe band → schedule/perform probes to measure responsiveness
  if p_probe_low <= recent_others_rate < p_probe_high:
    # Use deterministic scheduled probing to keep behavior predictable and low-cost
    if t mod S == 0:
      # Perform a probe: cooperate for one round to test whether others are conditional/cooperative
      record baseline = recent_others_rate
      last_probe_rounds.append((t, baseline))
      probe_count += 1
      play C
      continue
    else:
      # Not a probe round: defect by default
      play D
      continue

  # 3) If recent_others_rate is moderately high and past probes show positive responsiveness → try to form temporary cooperation
  if recent_others_rate >= p_coop_switch:
    # compute responsiveness from past probes (if any)
    if probe_count > 0:
      avg_resp = average over stored probes: (others_coop_rate at round r_probe+1) − baseline_at_probe
    else:
      avg_resp = 0
    if avg_resp >= delta_resp:
      # entering cooperation run to capture mutual gains (aggressive but conditional)
      state = COOP
      coop_run_remaining = L_coop - 1  # we'll play C this round and L_coop-1 subsequent rounds
      play C
      continue
    else:
      # others cooperate but they don't reliably respond to our cooperation — exploit instead
      play D
      continue

  # 4) Default fallback: defect
  play D

Reactive punishment rule (triggered after observing a round)
- After each round (we can examine the immediate outcome), if we detect a sharp coordinated drop in others’ cooperation relative to our recent baseline (e.g., others_coop_rate in this round ≤ baseline_recent_rate − 0.30), interpret that as either mass exploitation or that our prior cooperation was exploited and triggered collapse.
- In that case, set state = PUNISH and punish_remaining = min(P_punish, r − t) and play D for that interval. This inflicts payoff losses on the group and protects future attempts at exploitation.

Notes on randomness and ties
- The above is deterministic; if desired to avoid being predictable, replace scheduled probes (t mod S==0) with small-probability random probes: with probability epsilon_probe cooperate as a probe round (but only if the cooperation rate is in the probe band).
- When computing responsiveness use small-sample corrections: require at least one or two probes to consider avg_resp reliable.

Edge-case handling
- Round 1: Always defect (aggressive default).
- Last round: Always defect (dominant action).
- Very short games (r ≤ W or r small): Use shorter windows and reduce L_coop and P_punish proportional to r (e.g., L_coop = max(1, round(0.1*r)), P_punish = max(1, round(0.15*r))).
- If no history exists (t=1), recent_others_rate = 0 → default defect.
- If the population is extremely noisy (random plays), probes will show little responsiveness and the strategy will revert to constant defection (minimizes being exploited).
- If the population is dominated by unconditional cooperators, the p_exploit_high rule ensures we defect every round to maximize short-term payoff.

Why this is aggressive yet adaptive and robust
- Aggressive: default defection, exploitation of high-cooperation populations, and punitive responses to cooperation collapses — this maximizes my payoff at the expense of others whenever safe.
- Adaptive: uses sliding-window cooperation rates and explicit probes to detect exploitable opponents and to detect conditional cooperators who can be induced to form mutual cooperation (but only after verifying responsiveness).
- Robust: if opponents are retaliatory (TFT, Grim), probes will show low or negative responsiveness and the strategy avoids trying to induce cooperation (it will instead defect to avoid being exploited). If opponents are unconditional cooperators, the exploit rule profits heavily. If opponents are random/noisy, the strategy remains defecting and does not waste resources on unreliable cooperation.
- Tournament-friendly: does not rely on shared norms, communication, or external coordination—decisions are based purely on observed history and parameters.

Parameter tuning guidance (for implementers)
- p_exploit_high: increase to be more conservative (avoid exploitation when not many cooperators); decrease to exploit sooner.
- S and epsilon_probe: govern frequency of probes; make probes infrequent to avoid large losses vs unknown opponents.
- delta_resp and p_coop_switch: set high if you require strong evidence of reciprocity before cooperating; reduce if you are willing to risk cooperation to form long-term mutual cooperation.
- P_punish and L_coop: longer punishments increase deterrence but risk loss if many opponents are retaliatory; choose moderate values (defaults given) or scale with r.

Example behaviors (to illustrate)
- Against all-D opponents: AEP² defects every round → gets payoff 1 each round.
- Against all-C opponents: recent_others_rate ≈1 so exploit: AEP² defects every round → gets payoff 1 + (k/n)×(n−1) each round (maximally exploitative).
- Against TFT/reciprocators: probes will reveal that my defection reduces others’ cooperation; responsiveness low or negative → AEP² will mostly defect and avoid cooperating to prevent retaliation. If I choose to attempt cooperation, I do so only after evidence of positive responsiveness.
- Against mixed populations: AEP² identifies majority behavior and acts accordingly — exploiting cooperators when safe, punishing if my cooperation collapses the group.

Implementation checklist (for the coder)
- Maintain history window and per-round others_coop_rate.
- Track probe rounds and compute responsiveness (store baseline and next-round others’ rates).
- Maintain simple finite-state machine: NORMAL, COOP (with counter), PUNISH (with counter).
- Evaluate endgame first on each round and override other modes as needed.

Summary
This strategy is explicitly aggressive: defect-by-default, exploit clear cooperative majorities, and punish downward swings in cooperation. It is adaptive because it probes and measures whether cooperation is inducible and only commits to cooperating when the empirical responsiveness of the population justifies it. It is robust because it treats noisy and retaliatory opponents differently and never assumes external coordination.
'''

description_AGGRESSIVE_120 = '''
Name: Aggressive Adaptive Exploiter with Grim Shield (AAE‑GS)

Intuition (short)
- Default is aggressive: defect unless there is a clear, profitable opportunity to establish sustained cooperation that I can later exploit.
- I probe rarely, try short coordinated “build” phases only when the remaining rounds and opponents’ recent cooperation make conversion profitable, and otherwise I defect to extract value from cooperators.
- If opponents show credible punishment (they retaliate after my defections), I stop trying to build cooperation and switch to permanent defection (grim shield). This prevents endless wasted probing against punishers.
- Always defect in the final round.

The strategy depends only on the game parameters (n,k,r) and observed history (who cooperated in each past round). No assumptions about opponent norms.

State and statistics maintained
- t: current round index (1..r)
- T_rem = r - t + 1 (rounds remaining including current)
- History: for each past round s, actions c_j,s ∈ {0,1} for each player j (1 if C, 0 if D). I observe who cooperated each round.
- For each opponent j ≠ me: coop_rate_j = fraction of past rounds (or a recent window) in which j cooperated.
- coop_rate_group = average (over opponents) of coop_rate_j (or equivalently average fraction of opponents cooperating per round over the recent window).
- W = sliding window size for statistics = min(10, r-1) (use all history if shorter).
- punisher_flag_j: true if opponent j appears to retaliate after I defect (see retaliation detection below).
- global_permanent_defect_flag: if true, I defect forever (grim shield).
- mode ∈ {DEFAULT, BUILD, EXPLOIT}
  - DEFAULT: aggressive default (defect, occasional probes).
  - BUILD: short attempt to establish cooperation (cooperate for a small number of rounds).
  - EXPLOIT: opponents appear to be cooperating enough; defect to extract value.

Core numeric constants (tunable; chosen to be robust defaults)
- coop_threshold_high = 0.6 (group cooperation rate considered “high”).
- coop_threshold_low = 0.4 (group cooperation rate considered “moderate”).
- B_build = 2 (length in rounds of a short “build”/signal to try to start cooperation).
- W = as above.
- punishment_drop_alpha = 0.25 (a drop of this magnitude in opponent cooperation after my defection marks potential retaliation).
- probe_probability_base = 0.08 (small probability to cooperate as a probe when in EXPLOIT or DEFAULT).
These defaults can be adjusted as needed (they depend on n,k,r but are safe choices across many environments). A conservative policy is used for small remaining rounds.

Decision outline (natural language)
1. First round (t = 1): Defect. (Aggressive default; learn from responses.)
2. Final round (t = r): Defect. (No future to reward cooperation.)
3. If global_permanent_defect_flag is set: Defect.
   - This flag is set if opponents demonstrate credible punishment after I defect (see retaliation detection).
4. Compute recent-group cooperation statistics over the last W rounds (excluding no-history rounds).
   - coop_rate_group = average fraction of opponents cooperating per round in that window.
   - last_round_cooperators = number of opponents who cooperated in t-1 (0 if t=1).
5. If I am currently in BUILD mode:
   - Cooperate for up to B_build rounds (unless punished). If after these rounds group cooperation increases substantially and persists, continue to cooperate (switch to cooperative maintenance); otherwise abort BUILD, mark this attempt failed and switch to DEFAULT (permanent defection if punishers present).
6. Else (DEFAULT or EXPLOIT):
   - If coop_rate_group ≥ coop_threshold_high OR last_round_cooperators ≥ ceil((n-1)*0.5):
     - Enter EXPLOIT mode: defect to extract extra payoff while opponents donate.
     - While in EXPLOIT:
       - Defect most rounds.
       - Occasionally (with small probability p_probe = min(probe_probability_base, 3/T_rem)) cooperate as a probe to test whether opponents are punishers or to attempt re-opening of cooperation if punishers vanish.
       - If probes are followed by consistent punishment (group cooperation drops substantially), set global_permanent_defect_flag and remain defecting forever.
   - Else if coop_rate_group ≥ coop_threshold_low AND (k - 1) * T_rem > B_build:
     - Try a BUILD: switch to BUILD mode for B_build rounds to attempt to create stable cooperation. (Condition ensures there are enough remaining rounds and k is large enough to make mutual cooperation profitable if established.)
   - Else:
     - DEFAULT: defect. Occasionally (rarely) probe with very small probability p_probe_default = min(0.02, 2/T_rem) to detect opportunistic cooperators or to check whether punishers left.
7. Retaliation detection (update punisher flags and global flag after observing next-round outcomes):
   - If in any round I defect, then in the subsequent round compare each opponent j’s cooperation behavior in that subsequent round to their cooperation rate in the prior window.
   - If opponent j’s cooperation drops by more than punishment_drop_alpha relative to their prior coop_rate_j (or they were cooperating and then switch to defecting persistently after my defection), set punisher_flag_j = true.
   - If a non-negligible fraction of opponents (≥ ceil((n-1)*0.25) or a single clear punisher in small n) become punishers after my defection, set global_permanent_defect_flag = true. (I treat this as credible collective punishment and stop trying to build cooperation.)
8. Cooperative maintenance (optional stage): If a BUILD succeeded (group cooperation rose and persists), I may maintain cooperation for mutually beneficial rounds if it yields higher expected payoff than defecting forever. However AAE‑GS stays aggressive: after a period of stable cooperation I will
   - Cooperate to sustain cooperation only as long as it appears stable and punishers are absent, and
   - Intermittently defect (exploit) to gain an extra unit when it is safe (i.e., when there are no punishers detected and the risk of permanent retaliation is low). After any detected retaliation, revert to global_permanent_defect_flag = true.

Rationale for the numeric tests
- (k - 1) measures the per-round extra welfare from universal cooperation vs universal defection. By comparing (k - 1) * T_rem to the short upfront cost B_build, we decide if there are enough remaining rounds to justify the cost of attempting to start cooperation.
- coop_thresholds use recent empirical cooperation. We require a high recent cooperation rate to enter pure exploitation; a moderate rate plus enough remaining rounds triggers a short attempt to build.
- The Grim Shield (global_permanent_defect_flag) is the aggressive protection: if opponents punish me credibly, I stop trying to coordinate and defect forever to avoid being exploited by punishers.

Pseudocode (concise)

variables:
  W = min(10, r-1)
  coop_threshold_high = 0.6
  coop_threshold_low = 0.4
  B_build = 2
  punishment_drop_alpha = 0.25
  probe_probability_base = 0.08
  global_permanent_defect_flag = false
  mode = DEFAULT
  build_rounds_left = 0
  for each opponent j: punisher_flag_j = false

function decide_action(t, history):
  T_rem = r - t + 1
  if t == 1: return D
  if t == r: return D
  if global_permanent_defect_flag: return D

  compute coop_rate_j over last W rounds for each opponent j
  coop_rate_group = average_j coop_rate_j
  last_round_cooperators = number of opponents who cooperated in round t-1

  if mode == BUILD:
    if build_rounds_left > 0:
      build_rounds_left -= 1
      return C
    else:
      # evaluate whether BUILD succeeded: did coop_rate_group increase persistently?
      if coop_rate_group increased by >= 0.15 compared to before build:
        mode = DEFAULT (or COOP_MAINTENANCE if you want to sustain)
        return C  # sustain cooperation for now
      else:
        mode = DEFAULT
        return D

  # Not in BUILD
  if coop_rate_group >= coop_threshold_high or last_round_cooperators >= ceil((n-1)*0.5):
    # EXPLOIT mode
    mode = EXPLOIT
    p_probe = min(probe_probability_base, 3.0 / T_rem)
    if random() < p_probe: return C
    else: return D

  if coop_rate_group >= coop_threshold_low and (k - 1) * T_rem > B_build and not any(punisher_flag_j):
    # Try a short BUILD to establish cooperation
    mode = BUILD
    build_rounds_left = B_build - 1
    return C

  # DEFAULT: defect, occasional probe
  p_probe_default = min(0.02, 2.0 / T_rem)
  if random() < p_probe_default and not any(punisher_flag_j):
    return C
  else:
    return D

# After each round t, update punisher flags:
# If I defected in round t and in round t+1 many opponents reduce cooperation by > punishment_drop_alpha vs prior coop_rate_j,
# mark those opponents as punishers and possibly set global_permanent_defect_flag = true.

Aggressive aspects summarized
- Default is defection (exploitative).
- When others are reliably cooperating, the strategy exploits them by defecting (EXLOIT mode).
- It does not naïvely continue to cooperate to be nice; cooperation is only attempted when expected future payoff justifies the upfront cost and opponents are not credible punishers.
- It uses harsh, simple retaliation recognition: if opponents punish, it abandons attempts to cooperate and defects forever (grim shield), preventing endless being exploited by punishing strategies.
- It probes only rarely and in a controlled way so as not to gift long runs of cooperation to opportunistic strategies without testing.

Edge cases
- Very short games (small r): (k - 1) * T_rem will be small; BUILD will not be attempted; the agent will defect except for negligibly rare probes.
- Small n: thresholds based on absolute counts (ceil) ensure logical behavior (one punisher in small groups is enough to trigger the shield).
- Large k close to n: BUILD is more likely because (k - 1)T_rem condition is easier to satisfy — we are appropriately willing to try cooperation when the gains from successful coordination are large.
- If opponents are stochastic/noisy, the use of a window W and moderate thresholds avoids overreacting to single random rounds. Punishment detection requires a sustained drop or a substantial drop in cooperation.
- If the tournament includes many cooperators who never punish, EXPLOIT will extract surplus by defecting when many others cooperate.
- If opponents are trigger/punisher strategies, the Grim Shield avoids repeated costly probes and preserves my long-run payoff.

Implementation notes
- The constants (W, thresholds, B_build, probe probabilities) are tunable. The defaults above are chosen to balance exploitation and safety across many reasonable parameter choices.
- The strategy only uses past actions and payoff-relevant aggregates; it doesn’t assume opponent structure.
- Randomized probes are essential to detect regime changes and to avoid being stuck in a purely suboptimal equilibrium. Keep probe rates low to remain aggressive.

This design produces an aggressive, adaptive, history-dependent algorithm that:
- Exploits cooperative opponents when profitable,
- Only attempts to form cooperation when the expected future gains justify the short-term cost,
- Ceases attempts when opponents evidence credible punishment,
- Always defects in the final round.
'''

description_AGGRESSIVE_121 = '''
Summary (one line)
- Aggressive, adaptive strategy: start by defecting, probe opponents only occasionally to learn whether they reliably cooperate, and whenever the group shows sustained willingness to cooperate aggressively exploit them (defect repeatedly) while monitoring for and avoiding long costly punishment cycles; always defect in the final round(s).

Intuition and goals
- Aggressive = maximize my own payoff and relative advantage, exploit cooperators, and avoid being locked into costly punishment wars that reduce my payoff.
- Because defect strictly dominates in a single round, the strategy defaults to defect, but it will detect exploitable cooperative environments and exploit them in controlled bursts while testing for retaliatory punishers and switching behavior if punishment would be costly.
- The strategy is deterministic given parameters and observed history (no randomness required, but easy to add later).

Parameters derived from game inputs (choose these constants; implementer may tune)
- n, r, k : given
- PROBE_ROUNDS = min(3, r-1) — short initial probing window (but we start defecting immediately; probes come as occasional cooperations)
- TEST_INTERVAL = 5 — how often to do a deliberate cooperation test if in exploitation regime
- EXPLOIT_MIN_BURST = 1 — minimum consecutive defects when exploiting (typically we defect repeatedly)
- RECIPROCITY_THRESHOLD_HIGH = 0.60 — “group is cooperative” threshold
- RECIPROCITY_THRESHOLD_LOW = 0.20 — “group is uncooperative” threshold
- PUNISH_DELTA = 0.25 — drop in others' cooperation after my defection that counts as costly punishment behavior
- PUNISH_WINDOW = 3 — number of rounds to observe this drop
- ENDGAME_ROUNDS = 1 (or 2 if you want safer) — always defect in last ENDGAME_ROUNDS

Observed statistics (computed from full-history each round; perfect monitoring)
- For each past round t, coop_rate_others[t] = (# of other players who played C in t) / (n-1)
- overall_coop_rate = average of coop_rate_others over all past rounds
- coop_rate_after_my_defect = average coop_rate_others for rounds immediately following a round when I played D (if available)
- coop_rate_after_my_coop = average coop_rate_others for rounds immediately following a round when I played C (if available)
- immediate_response = coop_rate_after_my_coop - coop_rate_after_my_defect (estimates whether others reciprocate / punish)

High-level states (modes)
- DEFAULT (safe-aggressive): baseline, defect every round except scheduled tests
- EXPLOIT: group is sufficiently cooperative — defect every round (exploit) but make periodic cooperation tests to monitor whether cooperation persists and to bait more cooperation
- CAUTIOUS (anti-punisher): if opponents punish my defections (they drop cooperation after I defect), avoid sustained defection bursts; switch to a conditional policy that seeks acceptable long-run payoff (mix of cooperation and defection to avoid heavy losses from punishers)
- ENDGAME: last ENDGAME_ROUNDS → always defect

Decision rules (explicit, per-round)
1. If current round t > r - ENDGAME_ROUNDS: play D (ENDGAME).
2. Compute observed statistics from history (if no history, use defaults below).
   - If t = 1 (first round): play D (initial aggression).
3. Assess group cooperativeness:
   - If we have at least one past round, compute overall_coop_rate.
   - If overall_coop_rate ≥ RECIPROCITY_THRESHOLD_HIGH, set mode = EXPLOIT.
   - Else if overall_coop_rate ≤ RECIPROCITY_THRESHOLD_LOW, set mode = DEFAULT.
   - Else set mode = DEFAULT (ambiguous → be aggressive).
4. Detect punishers:
   - If coop_rate_after_my_defect exists AND coop_rate_after_my_defect − coop_rate_after_my_coop ≤ −PUNISH_DELTA (i.e., others substantially reduce cooperation after I defect), then mark PUNISHERS_DETECTED = true and set mode = CAUTIOUS.
   - (Equivalently: if others retaliate when I defect, avoid long defection bursts.)
5. Action by mode:
   - EXPLOIT:
     - Primary move: D every round (exploit). This extracts private + public share from cooperators.
     - But every TEST_INTERVAL rounds (or immediately after entering EXPLOIT), perform a probe by playing C for one round to measure whether the group keeps cooperating (this probe gives info and can entice more contributions to exploit next round). After the one-round C probe, return to D immediately thereafter (unless punishers detected).
     - If after a probe coop_rate_others remains high, continue EXPLOIT.
     - If punishers detected after probe (cooperation collapses following my D), switch to CAUTIOUS.
   - DEFAULT:
     - Play D. Once every TEST_INTERVAL rounds, optionally play C as a one-off probe if you have almost no info (only used during early rounds to gather data). Mostly stay D.
   - CAUTIOUS:
     - Avoid long bursts of pure defection because others punish. Use conditional strategy:
       - If last round majority cooperated (coop_rate_others[t−1] ≥ 0.5), play C this round with probability 1 (or deterministically) to avoid creating a long punitive sequence. Otherwise play D.
       - After cooperating, observe whether cooperation returns; if cooperation stabilizes and punishers are not stubborn, you may transition to EXPLOIT carefully.
     - In practice CAUTIOUS can be implemented as: follow “play C iff majority of others cooperated last round; otherwise D”. This prevents escalating wars while still exploiting when group is mostly cooperative.
6. End-of-round updates:
   - Recompute stats and if the situation changes (cooperation rates move above or below thresholds or punishers appear/disappear), switch modes accordingly.

Pseudocode (deterministic, implementable)
- Inputs: n, r, k, history (list of rounds; for each round t we know each player's action)
- Constants: PROBE_ROUNDS, TEST_INTERVAL, RECIPROCITY_THRESHOLD_HIGH, RECIPROCITY_THRESHOLD_LOW, PUNISH_DELTA, PUNISH_WINDOW, ENDGAME_ROUNDS
- For current round t:
  1. if t > r - ENDGAME_ROUNDS: return D
  2. if t == 1: return D
  3. compute coop_rate_others[t'] for all past t'
  4. overall_coop_rate = average(coop_rate_others)
  5. compute coop_rate_after_my_defect and coop_rate_after_my_coop (if not enough data, treat them as equal to overall_coop_rate)
  6. if coop_rate_after_my_defect − coop_rate_after_my_coop ≤ −PUNISH_DELTA then PUNISHERS_DETECTED = true else false
  7. if PUNISHERS_DETECTED: mode = CAUTIOUS
     else if overall_coop_rate ≥ RECIPROCITY_THRESHOLD_HIGH: mode = EXPLOIT
     else mode = DEFAULT
  8. if mode == EXPLOIT:
        if (just_entered_EXPLOIT) or (t mod TEST_INTERVAL == 0):
           return C   # probe to bait/measure
        else:
           return D
     if mode == DEFAULT:
        if (early rounds with little info) and (t mod TEST_INTERVAL == 0):
           return C   # occasional probe only early
        else:
           return D
     if mode == CAUTIOUS:
        if coop_rate_others[t-1] ≥ 0.5: return C else return D

Notes on parameters and tuning
- RECIPROCITY_THRESHOLD_HIGH should be set above 0.5 (we use 0.6) to ensure we only enter EXPLOIT when a substantial majority often cooperates.
- PUNISH_DELTA ~0.25 is a clear, large retaliation signal. Smaller values can be used if punishers are subtle.
- TEST_INTERVAL controls how often you probe for more exploitation; frequent probes invite more cooperation to exploit but also risk being detected as an exploiter who will be punished.
- EXPLOIT_MIN_BURST = 1 keeps exploitation safe: exploit with single/dominant D rounds rather than long D sequences when punishers are present.

Edge cases and special handling
- r = 2 or small r: early endgame unravels cooperation. With r ≤ 3: play D every round (aggressive, safe).
- First round: D (avoid instant exploitation).
- Last ENDGAME_ROUNDS: D, always.
- If history is empty or inconclusive, default to D and occasional probes as above.
- If some opponents always cooperate (pure cooperators), EXPLOIT will catch them and yield higher payoffs (defecting leverages their contributions).
- If many opponents punish defectors strongly, CAUTIOUS prevents protracted mutual loss.
- If opponents change strategies over time (adaptive opponents), the constant monitoring and TEST_INTERVAL probes allow reclassification.

Why this is “aggressive” and robust
- Aggressive: default is defection and the strategy deliberately exploits any sustained cooperation in the group. When others are cooperative, the strategy defects repeatedly (with occasional bait cooperations) to capture higher private payoffs.
- Robust: the strategy does not assume norms or coordination. It learns by observation, detects punishment behavior, and adapts to avoid costly retaliation. It never sacrifices the last-round advantage.
- Adaptive: continuous recalculation of cooperation statistics and detection of punishers ensures the strategy adapts to a wide range of opponent behaviors (pure cooperators, unconditional defectors, tit-for-tat types, grim-trigger punishers, etc.).

Simple variants (optional)
- More aggressive: reduce TEST_INTERVAL (probe less, exploit more) and reduce CAUTIOUS leniency (be less forgiving).
- More cautious/aggressive-mixed: if punishers are detected but overall cooperation is high, use a schedule of short exploitation bursts (1–2 D) followed by several C to keep punishers uncertain while still netting benefit.
- Add stochastic probing: instead of deterministic probes at fixed intervals, use a low probability each round to play C for stealthier information gathering.

Conclusion
- The strategy is deterministic, uses only the history and game parameters, and aligns with an aggressive mindset: default defection, exploitation of cooperative groups, controlled probing to detect exploitable environments, and explicit detection and avoidance of costly punishers. It is implementable as pseudocode above and readily tunable for tournament play.
'''

description_AGGRESSIVE_122 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

High-level intent (aggressive mindset)
- Prioritize my own score. Default action is Defect (D). Exploit players who reliably cooperate without punishing. If others retaliate when I defect, punish them quickly and harshly to make future exploitation safe. Try occasional short tests of cooperation only when doing so promises a sustained advantage; otherwise defect, especially near the end of the game.

Inputs available to the algorithm
- Game parameters: n, r, k
- Full public history: for each past round t, the vector of actions (C/D) by every player (including me)
- No private communication or commitments

Key internal statistics (computed from history)
- For each opponent j:
  - p_j = fraction of the last W rounds in which j played C (cooperation rate, sliding window)
  - retaliate_j = fraction of occasions (in a recent window) in which j responded to one of my defections by defecting in the subsequent round (measure of punishment sensitivity)
- Global counts:
  - coop_last = number of players (excluding me) who cooperated in the most recent round
  - rounds_left = r - current_round + 1

Tunable default parameters (explicit; can be adjusted in implementation)
- W = min(10, max(3, floor(r/5))) — history window length for p_j and retaliation estimates
- P_exploit = 0.7 — baseline cooperation rate threshold to consider an opponent “exploitable”
- R_tol = 0.30 — maximum tolerated retaliation probability for an exploitable opponent
- M_exploit = max(1, ceil((n-1)/4)) — minimum number of exploitable opponents required to choose exploitation
- P_pun = min(5, max(1, floor(r/10))) — number of rounds to carry out a punishment phase once triggered
- Coop_test_interval = max(3, floor(r/10)) — spacing of occasional cooperative tests after a cooling-off period
- Endgame_horizon = 2 — if rounds_left ≤ Endgame_horizon, always defect

Decision rules (precise)

1) First round (round = 1)
- Play D (probe). Rationale: aggressive default, gather data on opponents’ unconditional cooperativeness and sensitivity to defection.

2) Endgame
- If rounds_left ≤ Endgame_horizon: play D. (Do not cooperate in the final rounds.)

3) Ongoing behavior (round t, 1 < t ≤ r)
Compute p_j and retaliate_j over the last W rounds for each opponent j.

Define exploitable_j = (p_j ≥ P_exploit) AND (retaliate_j ≤ R_tol).

Let E = number of opponents with exploitable_j = true.

A) Exploit phase (if E ≥ M_exploit)
- Play D.
- Rationale: enough opponents reliably cooperate and don't punish, so defecting yields consistently higher per-round payoff.

B) If not enough exploitable opponents (E < M_exploit)
- Evaluate recent group cooperation:
  - If coop_last (number of others who cooperated in previous round) ≥ ceil(0.6 × (n − 1)) AND no recent punishment was observed against me (i.e., no opponent’s retaliation_j spike in last W rounds), then attempt a single cooperation test:
    - Play C exactly this round (a tentative test to see if a cooperative equilibrium can be sustained).
    - If I cooperated in this test and a majority of others cooperated too this round (observed next round), then I will continue at most T_sustain rounds of conditional cooperation where I cooperate as long as in the previous round a strong group cooperation signal holds; T_sustain is limited to rounds_left minus Endgame_horizon (never cooperate in final horizon).
  - Otherwise (no compelling sign of stable group cooperation) play D.

C) Punishment protocol (triggered when opponents punish me)
- If I observe that several opponents retaliated to one of my defections (i.e., for more than P_detect_count opponents, retaliate_j > R_tol), then enter a punishment phase:
  - For P_pun rounds, play D unconditionally (the punishment).
  - After P_pun rounds, reset internal counters for those opponents and resume the ordinary decision rules. Also wait Coop_test_interval rounds before attempting another cooperation test.

D) Forgiveness and testing
- After a punishment phase applied to opponents, occasionally (every Coop_test_interval rounds) attempt a one-round cooperation test (as in B) to probe whether the group has become exploitable again. If the test is punished (many opponents defect next round), resume punishment logic and increase P_pun (cap at some limit) for the next punishment.

4) Special-case safety nets and dynamics tuning
- If the strategy finds itself in prolonged mutual-defection where my average payoff is dropping significantly relative to the initial rounds, it will slowly increase the frequency of cooperation tests (reduce Coop_test_interval) for a chance to re-establish exploitable cooperation — but never within the Endgame_horizon.

Pseudocode (concise)

Initialize:
  W, P_exploit, R_tol, M_exploit, P_pun, Coop_test_interval, Endgame_horizon
  punishment_timer = 0
  last_test_round = -∞
  per-opponent histories from observed rounds

Each round t:
  rounds_left = r - t + 1
  if rounds_left ≤ Endgame_horizon:
    play D; continue
  if punishment_timer > 0:
    play D; punishment_timer -= 1; continue

  compute for each opponent j:
    p_j = fraction of C in last W rounds
    retaliate_j = fraction of times j defected in next round after I defected (over last W rounds)
    exploitable_j = (p_j ≥ P_exploit) and (retaliate_j ≤ R_tol)
  E = count(exploitable_j)

  if E ≥ M_exploit:
    play D  // exploit them
    continue

  // else not safe to outright exploit
  coop_last = number of opponents who played C in round t-1
  if coop_last ≥ ceil(0.6*(n-1)) and (t - last_test_round) ≥ Coop_test_interval:
    play C
    last_test_round = t
    // after observing others next round, decide whether to sustain
  else:
    play D

  // Reaction: if after my defection I observe many opponents defect in direct response:
  // set punishment_timer = P_pun (and perhaps increase P_pun modestly for repeat offenders)

Rationale and discussion (robustness and aggression)
- Aggression reflected by default D play, immediate exploitation of reliably cooperative opponents, and prompt, relatively long punishments against opponents who retaliate (to make future exploitation safe).
- The strategy is adaptive: it uses sliding-window statistics to infer which opponents are exploitable (frequent cooperators who don’t punish) and reacts to observed retaliation patterns. It tests for sustainable cooperation only when group behavior suggests a reasonable chance to sustain it and limits how long it will cooperate (never in the endgame).
- The punishment is harsh but limited in duration so that the player doesn’t get trapped indefinitely into mutual defection cycles; however, repeated punishers are punished more severely (increased P_pun) to discourage persistent retaliation.
- The approach is robust: it doesn’t assume norms or coordination, works with identified players (public actions), and adapts to a wide range of opponent strategies (always-cooperate, always-defect, Tit-for-Tat, Grim, random, mixtures).
- Parameter values are explicitly stated but can be tuned for specific tournaments: raising P_exploit makes the strategy more selective (less likely to exploit marginal cooperators), reducing R_tol makes it less tolerant of retaliation, etc.

Edge cases summary
- First round: D (probe).
- Last round (and last Endgame_horizon rounds): always D.
- If many opponents are consistently cooperative and non-punishing: always D to exploit them.
- If opponents punish defection: trigger finite punishment, then cautiously retest for exploitability.
- If mutual defection lowers my payoff too much, increase test frequency to try to re-establish exploitable cooperation while still avoiding being naïve.

Implementation notes
- All decisions depend only on n, r, k (to set parameter defaults if desired) and observed public history — as required.
- The statistical windows, thresholds and punishment lengths are simple adjustable scalars; tournament implementers can tune them (e.g., make P_pun longer to scare off punishers or shorter to be more forgiving).
- This strategy trades off some group payoff in order to maximize the agent’s own relative score against a broad field: an explicitly aggressive, exploit-first approach.
'''

description_AGGRESSIVE_123 = '''
Strategy name: Aggressive Conditional Exploiter (ACE)

High-level idea
- Be aggressive: default to defect, exploit any stable cooperative group, and punish/suppress cooperation when others signal unreliability.
- Be adaptive: use recent history of everyone’s actions to detect stable coalitions, perform limited exploitation when a coalition exists, and apply targeted and group punishments when exploitation occurs.
- Be robust: never rely on communication or shared norms; decisions depend only on game parameters (n, k, r) and observed history.

Intuition / motivation
- In a single round, D strictly dominates C (because k < n ⇒ k/n < 1). Cooperation is only worth pursuing if it is likely to be sustained for multiple future rounds. ACE only cooperates when there is clear evidence of a stable, large coalition of cooperators. When such a coalition exists, ACE will join it but will exploit it opportunistically and punish quickly if exploitation appears.

Parameters (derived from n, k, r and adjustable constants)
- lookback window L = min(5, r) — how many most recent rounds to use for short-term statistics.
- Theta_high = max(0.70, 1.0 - 0.5*(k/n)) — required fraction of other players cooperating (in recent window or consecutively) to regard a coalition as “strong.”
  - Theta_high is high when k/n is small (cooperation is fragile) and no less than 0.70.
- Theta_low = 0.50 — if group cooperation falls below this we regard cooperation as collapsed.
- Consecutive_needed S = min(3, r) — number of consecutive rounds above Theta_high to declare a strong coalition.
- Short punishment length P = min(3, r) — number of rounds to defect after bad signal before rechecking.
- Rehab requirement M = 2 consecutive rounds with group cooperation ≥ Theta_high to resume cooperation after punish.
- Final-phase rounds F = min(2, r) — in the last F rounds always defect (no future returns justify risking cooperation).
- Targeted-exploit forgiveness G = 2 — number of consecutive cooperative actions required by an exploited player to remove them from the blacklist.

You may tune these numerical values; ACE remains “aggressive” across reasonable choices.

State tracked from history
- For each round t < current: full action profile (who played C/D).
- For each player j (including self): recent cooperation rate over last L rounds.
- For group: fraction of others cooperating each of last L rounds, consecutive runs above/below thresholds.
- Blacklist: set of players judged to be “exploiters” based on their behavior while you cooperated.

Decision rules (natural language then pseudocode)

Overview:
1. Always defect in round 1 and in final F rounds.
2. Compute recent group cooperation statistics over last L rounds and count consecutive rounds with high cooperation.
3. If a strong coalition exists (≥ Theta_high among others for S consecutive rounds), consider joining it:
   - If you are currently cooperating with coalition pattern, continue cooperating (with one planned opportunistic exploitation detailed below).
   - If you are not yet cooperating, join (play C) but first do a single immediate one-shot exploit: delay joining by defecting exactly once the first time you detect the coalition, then cooperate if coalition persists. This is the “aggressive one-shot harvest.”
4. If the coalition collapses (group cooperation dips below Theta_low), punish by defecting for P rounds (group punishment). After P rounds, require M consecutive rounds with group cooperation ≥ Theta_high to resume cooperation.
5. Maintain a blacklist of players who repeatedly defect when you cooperated. Against blacklisted players, always defect (targeted punishment) until they show G consecutive cooperations; then remove them.
6. If no strong coalition is detected, default to defect.
7. Edge-case safety: if r is small (e.g., r ≤ 3), default to always defect.

Rationale for exploitation & punishment
- One-shot exploit: when a large stable coalition exists, defecting once yields +1 extra immediate payoff relative to C; if coalition is robust, the coalition will likely resume cooperating and you will still participate in future joint payoffs. This is an aggressive harvesting move done at most once per coalition episode.
- Targeted punishment: protects you from systematic one-on-one exploiters. Group punishment (short P rounds) penalizes broad collapse but is not eternally self-harming.
- Final-phase defection: standard endgame exploitation; do not incur cost of naive cooperation when no future benefit can be enforced.

Pseudocode (high-level)

Initialize:
  blacklisted = {}   // players judged exploiters
  exploited_this_coalition = false
  last_coalition_start_round = None

On each round t (1..r):

  // 0. Final-phase rule
  if t > r - F:
    play D
    continue

  // 1. Early final small-game rule
  if r <= 3:
    play D
    continue

  // 2. First round
  if t == 1:
    play D
    continue

  // 3. Update statistics from history (rounds 1..t-1)
  for each past round s in max(1, t-L) .. t-1:
    coop_fraction_others[s] = (number of cooperators in s excluding me) / (n-1)
  avg_coop_others = average(coop_fraction_others over available s)
  consec_high = number of consecutive most recent rounds (ending at t-1) with coop_fraction_others ≥ Theta_high
  consec_low  = number of consecutive most recent rounds with coop_fraction_others < Theta_low

  // 4. Maintain per-player blacklists: if player j defected in > X of the rounds where I cooperated recently,
  // mark as exploiter. (Concrete rule: if in the last L rounds, in rounds where I played C, player j played D in > 50% of those rounds)
  for each player j ≠ me:
    compute defection_share_when_I_cooperated
    if defection_share_when_I_cooperated > 0.5:
      blacklisted.add(j)

  // 5. If I am blacklisted by my own logic (never), ignore—blacklist is for others.

  // 6. If any blacklisted players exist who cooperated in previous round? We target punish them indirectly by always playing D until they rehabilitate:
  if exists j in blacklisted:
    // Check rehabilitation condition for each j
    for each j in blacklisted:
      if j cooperated in each of last G rounds:
        remove j from blacklisted
    // While any blacklisted remain, play D (targeted defection)
    if blacklisted not empty:
      play D
      continue

  // 7. Coalition detection & action
  if consec_high >= S and avg_coop_others ≥ Theta_high:
    // strong coalition present
    if last_coalition_start_round is None or last_coalition_start_round < t - L:
      // new coalition episode
      last_coalition_start_round = t
      exploited_this_coalition = false

    if exploited_this_coalition == false:
      // perform one-shot exploit to harvest immediate gain
      exploited_this_coalition = true
      play D
      continue
    else:
      // previously exploited or joined: play C (join coalition) to capture ongoing group returns
      play C
      continue

  // 8. If coalition collapsed recently
  if consec_low >= 1:
    // start/continue short punishment
    // determine how many rounds we've already punished since collapse; if less than P, continue defecting
    if punishment_counter < P:
      punishment_counter += 1
      play D
      continue
    else:
      // after P rounds, require M consecutive high-coop rounds to resume cooperation
      if consec_high >= M and avg_coop_others ≥ Theta_high:
        punishment_counter = 0
        play C
        continue
      else:
        play D
        continue

  // 9. Default safe action
  play D

Comments / clarifications
- “Coalition episode” is any detected contiguous run of rounds where group cooperation is ≥ Theta_high. We allow at most one immediate exploit per coalition episode (the first round we detect it), then cooperate to capture continued gains from the coalition.
- The blacklisting rule is targeted: it prevents repeated personal exploitation by specific players. This is more aggressive and efficient than wholesale permanent defection because it focuses retaliation on those who harm you.
- The group punishment is short (P rounds) so ACE is not locked into long mutual-defection unless the opponents persistently refuse to rebuild cooperation.
- The final-phase defection (last F rounds) is standard and aggressive: when the future enforcement horizon is gone, defect.

Why this is aggressive
- Default bias to defect and to punish quickly.
- Opportunistic one-shot exploit of any detected strong coalition.
- Targeted blacklisting of individual exploiters to protect payoff against repeated suckers.
- Short but effective group punishment to deter mass defection.
- Minimal unconditional cooperation: ACE only cooperates when there is strong, recent evidence that many others will reciprocate.

Robustness notes
- Works against unconditional cooperators (it exploits them), against naive defectors (it defects too), and against conditional cooperators (it can join profitable coalitions but harvest once up-front).
- The strategy adapts with L, Theta_high etc. set relative to n and k (Theta_high uses k/n). If you expect opponents to be more forgiving or cooperative, you can lower Theta_high and increase S to be more willing to form coalitions; for a tournament wanting aggressiveness, use defaults above.
- Implementation must ensure counts are based on observed actions and identities (history is fully observed per standard game assumptions).

Summary (one-sentence)
ACE: start defecting, only join clear, sustained cooperative coalitions, exploit each coalition once up-front, blacklist and punish repeat exploiters, short-group-punish on collapse, and always defect in the final rounds.
'''

description_AGGRESSIVE_124 = '''
Name: Aggressive Grim with Endgame Exploit (AG-EE)

Summary (one line)
- Start by cooperating to signal willingness; if anyone ever defects, switch to permanent defection (grim trigger); if everyone has cooperated up to the endgame window, exploit by defecting in the final F rounds; always defect in the final round.

Rationale (why aggressive)
- The strategy is aggressive because it (1) uses an ironclad, credible punishment (permanent defection) to deter others from defecting, (2) opportunistically exploits fully cooperative groups near the end to maximize relative payoff, and (3) begins by cooperating to extract cooperation where feasible. It prioritizes deterrence and relative advantage rather than long-run niceness.

Parameters (derived from game)
- n, k, r are known.
- Set endgame exploitation window size F = max(1, floor(r/3)). (This is a tunable aggressive choice; it means we will try to extract extra payoff in roughly the final third of the game if others have been perfectly cooperative.)

Decision rules (natural language)
1. First round (t = 1): Cooperate. This signals willingness to cooperate and gives a chance to capture mutual gains.
2. Last round (t = r): Defect. (Backward induction / standard endgame exploitation.)
3. Grim trigger: If any player (including any opponent) has ever played D in any previous round, defect for all remaining rounds. (Permanent punishment; no forgiveness.)
4. Endgame exploit: If no defections have ever been observed up to the current round, and the current round t is inside the final F rounds (i.e., t > r − F), then defect to exploit otherwise-untainted cooperators.
5. Otherwise: Cooperate.

Key points:
- The strategy cooperates as long as the history is all-cooperate and we are outside the exploitation window. Any observed defection anywhere in the history triggers permanent defection thereafter.
- The exploitation window ensures we do not miss clear opportunities to extract higher payoffs from players who have been fully cooperative throughout.
- No communication or coordination is required; decision depends only on r and observed history.

Pseudocode
Inputs: n, k, r, t (current round, 1..r), history H (list of previous rounds; for each past round t' a vector of actions of all players)
Compute:
  F = max(1, floor(r / 3))    # endgame exploitation window size
  ever_defected = (exists t' < t and exists player j with H[t'][j] == D)

Decision:
  if t == r:
    return D
  if ever_defected:
    return D         # Grim trigger: permanent defection
  if t == 1:
    return C
  if t > r - F:
    return D         # exploit fully-cooperative group in final window
  return C

Edge cases and notes
- r = 2: F = 1. We play C in round 1 (t=1) and D in round 2 (t=2). If any defection occurs in round 1 (not possible for opponents to observe before round 1), the grim rule doesn't trigger before round 2.
- Very small r: The strategy still respects first-round cooperation and last-round defection.
- If an opponent defects once (even if later they return to cooperating), AG-EE defects forever thereafter. This extreme punishment is the aggressive credibility element that deters one-shot deviations.
- The size of F is tunable. Increasing F makes the strategy more exploitative in the endgame; decreasing F makes it more cooperative and forgiving early on. The chosen F = floor(r/3) is deliberately aggressive but avoids leaving too few rounds for mutual cooperation earlier.
- If you prefer a slightly less unforgiving variant, replace the permanent grim trigger with a fixed-length punishment (e.g., defect for P rounds after observing a defection, where P is proportional to r). That softens permanent retaliation but weakens the deterrent.

Why this is robust
- The strategy is simple, deterministic and only uses public, common-knowledge information (history, parameters).
- It punishes deviations in a way that is easy for opponents to understand (credible deterrent).
- It extracts extra payoff from fully cooperative opponents in a predictable, aggressive manner.
- It works against a wide range of opponent behaviours: cooperators will get mutual gains until the exploitation window; defectors or intermittent defectors get permanently punished and thus lose future cooperative surplus.

Implementation note for the tournament
- Implement exactly the pseudocode above. Ensure history is scanned for any D prior to deciding; use the same F computation. This yields a reproducible, aggressive strategy that balances initial cooperation, strict deterrence, and endgame exploitation.
'''

description_AGGRESSIVE_125 = '''
Strategy name: Aggressive Predator (AP)

High-level idea
- Default stance: defect. Be willing to cooperate only when a stable minority/majority of other players reliably cooperate (so cooperating can be exploited profitably over multiple rounds).
- When reliable cooperators are detected, join them to collect long-run gains — but aggressively “harvest” (one-shot defection) periodically to grab extra payoff.
- If any trusted cooperator betrays you (defects while you cooperated), punish immediately and harshly for a computed number of rounds P that makes the immediate gain from one-shot deviation unattractive.
- Give limited, conditional forgiveness so you can resume exploiting cooperative clusters later. Always defect in the final round.

All decisions use only the game parameters (n, k, r) and the public history of player actions by round. No communication or external coordination required.

Parameters (computed from game parameters)
- W = min(5, max(1, r-1))  // sliding-window for recent behavior
- trust_threshold = 0.8    // fraction of W rounds a player must have cooperated to be "trusted"
- trust_count_fraction = 0.7  // fraction of other players who must be trusted before you consider cooperating
- M = ceil(trust_count_fraction * (n-1))  // number of other players that must be trusted
- // Punishment length P is chosen so that the loss caused to deviator during punishment >= his single-round gain
  Let G = 1 - k/n          // immediate gain to a defector when others cooperate
  Let L = k - 1            // per-round loss to deviator when cooperators retaliate (cooperate->defect for P rounds)
  If L <= 0 then set P = r (defection always best; punish maximally)
  Else P = min(r, max(1, ceil(G / L) + 1))
    // +1 to make punishment strictly discouraging; bounded by remaining rounds
- harvest_streak = 2  // number of consecutive rounds of stable cooperation to start harvesting
- endgame_safe_margin = P  // do not attempt to create new cooperative commitments if less than this many rounds remain

State variables you maintain (computed from history)
- t = current round index (1..r)
- rem = r - t + 1 = rounds remaining including current
- history matrix H[1..t-1][1..n]: H[s][j] ∈ {C,D}
- For each player j ≠ me: p_j = fraction of C in last W rounds (or across history if fewer than W)
- S_trust = { j ≠ me | p_j ≥ trust_threshold }
- group_recent_coop = fraction of players (including me if needed) who played C in the last round or last W rounds (you can use last-round for fast reactions and W for stability)
- punishment_timer (integer) — number of rounds remaining in punishment phase; initially 0
- coop_mode (boolean) — we are in a cooperative-exploit mode with trusted players; initially false
- coop_streak (integer) — how many consecutive rounds we cooperated with S_trust prior to current round

Decision rules (exact)
1) Last round rule:
   - If t == r: play D (defect) unconditionally.

2) Immediate punishment phase:
   - If punishment_timer > 0:
       - Play D.
       - Decrement punishment_timer at end of the round.
       - Do not start new cooperation commitments while punishment_timer > 0.
       - (Rationale: harsh but finite punishment makes deviation unattractive.)

3) Detect trusted cluster:
   - Compute p_j for each j ≠ me over the last W rounds (or all available if < W).
   - Compute S_trust = { j ≠ me | p_j ≥ trust_threshold }.
   - If |S_trust| < M, set coop_mode = false and coop_streak = 0 (insufficient trusted partners), then go to step 4.
   - If |S_trust| ≥ M and rem > endgame_safe_margin:
       - Consider joining them (enter coop_mode). Otherwise (not enough remaining rounds) treat as insufficient -> do not enter coop_mode.

4) Coop_mode behavior (only if coop_mode true):
   - If coop_mode is false but |S_trust| ≥ M and you cooperated with them in the previous round(s), set coop_streak appropriately. If coop_streak ≥ harvest_streak then set a 1-round harvest flag (see below).
   - If coop_mode true:
       - Harvest rule (aggressive exploit): once you see S_trust stable (|S_trust| ≥ M) and you have cooperated with them for at least harvest_streak consecutive rounds, perform a single defection this round (one-shot harvest). After that single defection:
           - Immediately set punishment_timer = min(P, rem-1) and coop_mode = false. (You harvest and be ready to defend against any retaliation.)
           - Note: The harvest increases your round payoff; following punishment deters others from taking advantage repeatedly.
       - If no harvest this round: play C (cooperate) while cooperators in S_trust remain trusted.
   - If during coop_mode any player j ∈ S_trust defects in a round where you cooperated, treat that as a betrayal:
       - Set punishment_timer = min(P, rem-1); set coop_mode = false; play D next round as part of punishment.

5) Default behavior (if not in coop_mode and not in punishment):
   - Play D (defect).

6) Forgiveness / exit from punishment:
   - When punishment_timer reaches 0, recompute p_j over last W rounds; if |S_trust| ≥ M and rem > endgame_safe_margin then set coop_mode = true and reset coop_streak = 0; otherwise remain defecting by default.

First round and other edge cases
- First round (t=1): play D. (Aggressive testing; do not give freebies.)
- Last round (t=r): Always D (no future to enforce cooperation).
- Near-end: do not enter new cooperative commitments if rem ≤ endgame_safe_margin (i.e., avoid creating expectations you cannot enforce).
- Very small r: If r ≤ P (punishment length would exceed horizon), you will default to D for practically all rounds, since threats are not credible.
- If k is very close to 1 (public good almost worthless), P formula will still work: P becomes large only if L small — bound by rem ensures we stay realistic.

Why this is “aggressive”
- Default is defect; you only cooperate when a reliable cluster exists.
- You harvest: after gaining mutual cooperation for a short stretch, you defect once to seize additional payoff.
- You punish betrayals harshly for P rounds (P computed to make single-shot deviation unattractive).
- Forgiveness is limited and conditional: you only rejoin trusted clusters when they re-establish a strong cooperation record.
- You always defect in the last round and avoid risky endgame cooperation.

Pseudocode (concise)

Initialize:
  W = min(5, max(1, r-1))
  G = 1 - k/n
  L = k - 1
  If L <= 0: P = r else P = min(r, max(1, ceil(G / L) + 1))
  trust_threshold = 0.8
  M = ceil(0.7 * (n-1))
  harvest_streak = 2
  endgame_safe_margin = P
  punishment_timer = 0
  coop_mode = false
  coop_streak = 0

Each round t:
  rem = r - t + 1
  if t == r: action = D; return action

  // decrement will happen after the round if we played D due to punishment
  if punishment_timer > 0:
    action = D
    // after the round: punishment_timer -= 1
    return action

  // compute p_j over last W rounds (or available history)
  For each j != me: p_j = fraction of C in last W rounds
  S_trust = { j | p_j >= trust_threshold }

  if |S_trust| >= M and rem > endgame_safe_margin:
    // candidate for cooperating
    if coop_mode == false:
      // decide whether to enter coop_mode only if we have evidence we cooperated recently with them
      if coop_streak >= 1:
        coop_mode = true
      else:
        // be cautious: only enter if we already cooperated last round with them
        coop_mode = false
    // coop_mode true -> decide to cooperate or harvest
    if coop_mode:
      if coop_streak >= harvest_streak:
        // harvest now
        action = D
        punishment_timer = min(P, rem - 1)
        coop_mode = false
        coop_streak = 0
        return action
      else:
        action = C
        coop_streak += 1
        return action
  else:
    // insufficient trusted partners => defect
    action = D
    coop_mode = false
    coop_streak = 0
    return action

After each round update:
  If (we cooperated this round) and (some j in S_trust defected this round):
    // immediate betrayal detected
    punishment_timer = min(P, rem_next - 1)  // rem_next = rem - 1 after round
    coop_mode = false
    coop_streak = 0

  If punishment_timer > 0: punishment_timer -= 1
  // After punishment ends the main loop re-evaluates cooperation eligibility

Implementation notes and tuning
- The numeric constants (W=5, trust_threshold=0.8, trust_count_fraction=0.7, harvest_streak=2) are conservative defaults. They can be tuned for a tournament by simulation.
- The key computed quantity P is deliberate and derived to make the threat of punishment credible (P rounds of lost cooperative surplus should exceed a one-shot defector’s immediate gain).
- This strategy is memory-light (per-player cooperation rates over a small window) and robust to different opponent types: it defects against all-defectors, joins and exploits clusters of cooperators, and punishes betrayals so naive cooperators are less attractive targets over time.

Summary (compact)
- Default: defect.
- Cooperate only with a stable, sufficiently large set of trusted cooperators (measured over a short window) and only if enough rounds remain to make cooperation enforceable.
- After a short cooperative streak, perform a one-round harvest (defect) to extract extra payoff, then punish for P rounds if betrayed.
- Punish betrayals for P rounds, where P is chosen to make deviation unattractive; forgive only when players rebuild trustworthy records.
- Always defect in the final round.

This yields an aggressive, adaptive, and parameter-driven algorithm that exploits cooperators, deters exploitation by others via credible (computed) punishment, and avoids endgame vulnerabilities.
'''

description_AGGRESSIVE_126 = '''
Name: Aggressive Opportunist (AggOp)

Summary (one line)
- Start guarded, probe rarely to find naive cooperators, cooperate only when the group has reliably demonstrated willingness to cooperate, and whenever exploited while cooperating switch to permanent, harsh punishment (permanent defection). Always defect in the last round. This maximizes short-run gains against cooperators and enforces cooperation only when it looks stable — an explicitly aggressive approach.

Intuition / aggressive posture
- Protect yourself early (don’t give free lunches).
- Seek exploitable cooperators (probe occasionally).
- If others reliably cooperate, accept mutual cooperation to capture the larger group payoff k per round.
- If you are exploited while trying to cooperate, punish mercilessly by permanently defecting (grim trigger). That is intended to deter future exploitation and extract value from cooperators who prefer to avoid being punished.
- Do not cooperate in the final round (no future to induce reciprocation).

Inputs used
- n, r, k (game parameters)
- Full public history H of previous rounds: for each past round t and each player j, c_j,t ∈ {0,1} (1 = C, 0 = D)
- Current round index t (1..r)

Tunable internal parameters (suggested defaults)
- w (window for measuring recent cooperation): w = min(5, max(1, floor(r/5))). (If r small, use smaller window.)
- θ_high (threshold for "reliably cooperative group"): 0.8
- p_probe (probability of probing cooperation when not otherwise cooperating): 0.05
- Endgame rounds to always defect: L_end = 1 (always defect in last round). Optionally set L_end = 2 for extra aggression near the end.

State variables
- punished (boolean): entered if we ever cooperated and someone else defected in the same round — triggers permanent defection. Initialize punished = false.
- last_action_self (our action in previous round); track whether we cooperated last time.

Decision rules — exact
1) Endgame rule:
   - If t > r - L_end (i.e., in the last L_end rounds), action = D (defect). Rationale: no future to enforce cooperation, so exploit the immediate private benefit.

2) Permanent punishment rule (grim trigger):
   - If punished == true, action = D forever (for all remaining rounds).
   - punished becomes true if in any prior round t' we played C and the total number of cooperators that round < n (i.e., at least one other player defected while we cooperated). Put differently: if you ever cooperated and were exploited, switch to permanent defection.

3) First-round / guarded start:
   - If t == 1, action = D (we start by defecting to avoid free-rides).

4) Cooperative entry rule:
   - Compute recent group cooperation rate among other players over the last w rounds:
       For s = max(1, t-w) .. t-1, sum other cooperations each round and average across rounds and players:
       F = (1 / ((t-1) * (n-1))) × Σ_{s=1..t-1} Σ_{j≠self} c_{j,s}
       (If t=2 and only one prior round, this uses that single round.)
   - If F ≥ θ_high, then action = C. (If most others have been cooperating reliably, accept cooperation to gain k per round.)
   - Also, if the immediately previous round (t-1) had universal cooperation (every player cooperated), then action = C. This allows staying in a cooperating state once established.

5) Probing rule (exploit discovery):
   - If none of the above forced D and the rule would choose D because F < θ_high, with small probability p_probe choose C (probe). Otherwise choose D.
   - If you probe and others respond by cooperating subsequently, the Cooperative Entry Rule will detect it and you will switch to cooperating in future rounds.
   - Probing is rare so you don’t give long-term free gains to pure defectors.

6) Safety override:
   - If last round you played C and the current round you are about to play C but in the previous round someone defected (i.e., you would be cooperating into a group that just exploited you), set punished = true and action = D this round. (This ensures immediate detection triggers punishment instead of exposing you to repeated exploitation.)

Pseudocode

Initialize:
  punished = false
  last_action_self = D

For each round t = 1..r:
  if t > r - L_end:
    action = D
    last_action_self = D
    continue

  if punished:
    action = D
    last_action_self = D
    continue

  if t == 1:
    action = D
    last_action_self = D
    continue

  // compute recent cooperation rate of others over window w
  s0 = max(1, t - w)
  total_other_coop = 0
  rounds_counted = t - s0
  for s = s0 .. t-1:
    for each player j ≠ self:
      total_other_coop += c_{j,s}
  F = total_other_coop / (rounds_counted * (n-1))

  // if last round was universal cooperation, stay cooperating
  universal_last = (for all players j: c_{j,t-1} == 1)

  if universal_last or F >= θ_high:
    // before committing to cooperate, check we weren't exploited last round
    if last_action_self == C and (sum_{j≠self} c_{j,t-1} < (n-1)):
      punished = true
      action = D
    else:
      action = C
      last_action_self = C
    continue

  // else decide to defect normally, but allow rare probes
  if random() < p_probe:
    action = C
    last_action_self = C
  else:
    action = D
    last_action_self = D

  // After the round ends (observer step):
  // If we cooperated this round and someone else defected, set punished = true.
  // (Implementation detail: the algorithm implementing this policy must update 'punished' after observing the round result.)

Explanation of key choices and robustness

- Why start with D? Aggressive maxim: don’t give cooperators a free first-round gain. Many tournament opponents will defect-first; starting with D denies them the short-term advantage.

- Why rare probing? Some opponents are naive or unconditional cooperators. Occasional C probes let AggOp detect and then exploit/settle into mutual cooperation with them. p_probe must be small (e.g., 5%) to avoid steady exploitation by defectors.

- Why high cooperation threshold (θ_high) and window w? We only invest (cooperate) when evidence suggests a stable cooperative environment. The window smooths noise and resists one-shot flukes.

- Why permanent punishment (grim)? Aggressive stance: if you are exploited while cooperating, you must credibly make future cooperation costly for defectors. Permanent defection is a simple, credible and maximal deterrent. In tournament contexts with unknown opponents, a strong credible threat is often the most robust way to prevent repeated exploitation.

- Why last-round defection? Standard backward induction: you cannot induce future cooperation from a last-round contribution, so defect to extract the private benefit.

Edge cases and special notes

- Small r (few rounds): If r is tiny (e.g., r = 2 or 3), the window and thresholds shrink; the algorithm still defaults to defect in round 1 and the last round, leaving perhaps only one round to attempt cooperation. The probe probability can be increased proportionally if desired (but default keeps it small).

- If many opponents use forgiving cooperation strategies, AggOp will typically detect and enter mutual cooperation, harvesting k each round (large payoff). If any opponent betrays while AggOp has cooperated, AggOp switches to permanent defection to avoid repeated exploitation.

- If all opponents always defect, AggOp will almost always defect and obtain the safe baseline 1 per round (plus tiny occasional probe losses).

- If opponents punish AggOp’s defection, they may never reach mutual cooperation; AggOp will then continue to defect, which is consistent with its aggressive payoff-maximising posture.

Variants / optional tuning
- Less harsh punishment: Instead of permanent defection, punish for P rounds (P proportional to fraction of defectors or to r) then allow forgiveness. This is less aggressively punitive and can allow recovery of cooperation if group norms shift. AggOp’s canonical aggressive variant uses permanent punishment, but a configurable P (e.g., P = min(10, r/4)) can be used if implementing a less destructive variant.

- Endgame aggression: Set L_end = 2 to defect in last two rounds to capture more exploitive payoff if you prefer stronger exploitation near the end.

Why this will perform well in tournaments
- It protects against exploitation (never willingly gives repeated free payoffs).
- It exploits naive/unconditional cooperators (probes + then accepts cooperation).
- It enforces cooperation where it can be sustained (cooperates when others show high and recent cooperation).
- It communicates a simple credible punishment: once exploited, never forgive — this is easy for opponents to detect and respond to by shifting to cooperation if they prefer higher mutual payoffs rather than being locked into the zero-sum of permanent defection.

Concise checklist for implementers
- Maintain history and update 'punished' whenever you cooperated and someone else defected that same round.
- Compute recent cooperation rates F over window w of past rounds and act accordingly.
- Start with D; defect in final L_end rounds.
- Use small random probes with p_probe.
- If punished, always D thereafter.

This strategy is deterministic except for the small randomized probing. It is explicitly aggressive: it refuses to be exploited, aggressively punishes any exploitation, but opportunistically cooperates to capture the larger public good when others demonstrate reliable cooperation.
'''

description_AGGRESSIVE_127 = '''
Name: Aggressive Opportunistic Exploiter (AOE)

Goal (mindset): maximize my own cumulative payoff by (1) reliably exploiting cooperators whenever it is profitable, (2) deterring or punishing players who try to discipline me, and (3) avoiding being locked into long mutual-cooperation agreements that benefit others more than me. The strategy is adaptive: it learns who is cooperative, who punishes, who is noisy, and responds with exploitation, calibrated punishment, and occasional probes.

Key ideas (intuitive)
- Default to defection (D) because a single-round unilateral D is always a private advantage.
- Identify “cooperators” and “reciprocators” from history; repeatedly defect to harvest payoff from them.
- If my exploitation triggers consistent retaliation (others sharply reduce cooperation after I defect), escalate a short, calibrated punishment phase (defect for P rounds) to make retaliation costly for them.
- Periodically probe (cooperate occasionally) to re-check opponent types and to re-enable exploitation where it pays.
- Always defect in the final round (backward-induction-safe).

Notation
- n: players; r: rounds; t: current round (1..r).
- For each round s, c_j(s) = 1 if player j played C in s, else 0. I am player i.
- others_coop(s) = Σ_{j≠i} c_j(s) (number of cooperators among others in round s).
- recent_coop_rate = average over last W rounds of others_coop(s)/(n-1) (W defined below).
- history = all past rounds' actions (and observed payoffs, but actions alone suffice).

Tunable internal constants (set as default values; these depend only on n,r,k and are computed before play)
- W = min(5, r-1)  (window for “recent” behavior)
- gamma_high = 0.75  (others are highly cooperative if recent_coop_rate ≥ gamma_high)
- gamma_mid = 0.40   (moderate cooperation band)
- gamma_low = 0.15   (very low cooperation)
- delta_retaliation = 0.30  (relative drop in others' cooperation interpreted as retaliation)
- P_init = 2  (initial punishment length)
- P_max = max(2, min(10, r/4))  (maximum punishment length)
- probe_prob_mid = 0.20  (probability to cooperate when in mixed mode)
- periodic_probe_period = 4  (cooperate 1 in each 4 rounds as a probe when appropriate)
- forgiveness_probe_prob = 0.25  (after a punishment phase, probe cooperation sometimes)
These constants may be scaled conservatively when r is very small (see edge cases).

Decision rules (round-by-round)
1. Last-round rule:
   - If t == r: play D.

2. First-round / initial test:
   - If t == 1: play D (aggressive test).

3. Maintain state variables across rounds:
   - punish_remaining (initial 0): how many rounds left in an active punishment phase.
   - escalation_count (initial 0): how many times others have retaliated previously (used to extend future punishments).
   - last_round_I_defected (most recent round I played D).
   - baseline_coop_rate_before_exploit (cooperation level prior to last exploitation attempt), tracked when I start exploiting.

4. If punish_remaining > 0:
   - Play D. Decrement punish_remaining by 1 each round.
   - After punishment phase ends, do one forgiveness probe with probability forgiveness_probe_prob (play C) to test whether exploitation is again possible; otherwise continue normal rules.

5. Otherwise (normal operating mode), compute:
   - recent_coop_rate over the last up to W rounds (others_coop averaged and normalized to [0,1]).
   - If many rounds remain and recent_coop_rate ≥ gamma_high:
       - Exploit: play D (defect) to free-ride on the high cooperation of others.
       - Record last_round_I_defected = t and baseline_coop_rate_before_exploit = recent_coop_rate (for retaliation detection).
   - Else if gamma_mid ≤ recent_coop_rate < gamma_high:
       - Mixed mode: play D with probability 1 - probe_prob_mid, otherwise play C (cooperate with prob probe_prob_mid). The probing keeps cooperation available to exploit later; mostly defect.
       - If you choose D, record last_round_I_defected and baseline_coop_rate_before_exploit = recent_coop_rate.
   - Else if gamma_low ≤ recent_coop_rate < gamma_mid:
       - Mostly defect (D). Additionally, cooperate deterministically on rounds where (t mod periodic_probe_period == 0) to probe whether group cooperation is possible.
       - If you D on a round you intended to exploit, record last_round_I_defected and baseline_coop_rate_before_exploit.
   - Else (recent_coop_rate < gamma_low):
       - Everyone mostly defects: play D (no point in cooperating).
       - However, if a small subset of individual players are extremely cooperative (individual frequency ≥ 0.90 over history), you will exploit them by always playing D (they are effectively free resources).

6. Retaliation detection and punishment triggering (checked at the start of rounds t > 1, after observing previous round t-1):
   - If I played D in round s = last_round_I_defected and we are now at round s+1 or later, compute new_coop_rate = others_coop(s+1)/(n-1) (or the average in the next window if you want smoothing).
   - If new_coop_rate ≤ baseline_coop_rate_before_exploit × (1 - delta_retaliation) (i.e., a relative drop ≥ delta_retaliation), interpret that as coordinated retaliation or strong negative response to my exploitation.
   - Trigger punishment: set escalation_count += 1; set punish_length = min(P_max, P_init × 2^(escalation_count-1)); set punish_remaining = punish_length. (Punishment is a block of consecutive D plays.)
   - Punishment aims to make continued retaliation costly for others. Do not over-escalate beyond P_max.
   - If retaliation was small/noisy (drop smaller than delta_retaliation), treat it as noise and continue normal mode.

7. Forgiveness and re-probing:
   - After a punishment block completes, perform a forgiveness probe with probability forgiveness_probe_prob (play C) to check whether exploitation can resume. If the probe reveals re-established high cooperation without immediate strong retaliation, resume exploitation mode; otherwise return to default D with possibly longer punishment next time.

Pseudocode (compact, high-level)

Initialize:
  punish_remaining = 0
  escalation_count = 0
  last_round_I_defected = null
  baseline_coop_rate_before_exploit = null

For each round t = 1..r:
  if t == r: play D; continue
  if t == 1: play D; continue

  if punish_remaining > 0:
    play D
    punish_remaining -= 1
    if punish_remaining == 0:
      with probability forgiveness_probe_prob: next action (immediately) cooperate as probe in following round
    continue

  compute recent_coop_rate over last W rounds

  // Decide action
  if recent_coop_rate >= gamma_high:
    action = D
    last_round_I_defected = t
    baseline_coop_rate_before_exploit = recent_coop_rate
  else if recent_coop_rate >= gamma_mid:
    action = D with prob (1 - probe_prob_mid), else C
    if action == D:
      last_round_I_defected = t
      baseline_coop_rate_before_exploit = recent_coop_rate
  else if recent_coop_rate >= gamma_low:
    if (t mod periodic_probe_period == 0):
      action = C
    else:
      action = D
      last_round_I_defected = t
      baseline_coop_rate_before_exploit = recent_coop_rate
  else:
    action = D

  play action

  // After observing results (when entering next round), do retaliation detection:
  if last_round_I_defected == t-1:
    compute new_coop_rate = others_coop(t)/(n-1)
    if new_coop_rate <= baseline_coop_rate_before_exploit * (1 - delta_retaliation):
      escalation_count += 1
      punish_length = min(P_max, P_init * 2^(escalation_count-1))
      punish_remaining = punish_length

Edge cases and clarifications
- Final rounds: always defect in final round. If very few rounds remain (e.g., last 2–3 rounds), reduce probing and punishment lengths so you do not waste final opportunities. For instance, cap punish_remaining ≤ remaining_rounds - 1 so you leave at least one round to probe/exploit after punishment.
- Small r: when r ≤ W or r small, set W = r-1; lower P_max accordingly (e.g., P_max ≤ r/3) to avoid wasting the whole game in punishment.
- Noisy opponents: the strategy tolerates some noise because it uses windows (W) and relative-drop threshold (delta_retaliation). Occasional spontaneous drops won't immediately trigger long punishments.
- Single-player hangouts: if a single player is extremely cooperative over full history, AOE will repeatedly defect and harvest from that player.
- Safety vs. over-escalation: escalation doubles punishment length each time up to P_max to make retaliation costly for groups that try to deter exploitation. P_max and delta_retaliation prevent runaway escalation due to noisy swings.

Why this is “aggressive”
- Default is defect; cooperate only rarely and strategically to preserve exploitable cooperation in others.
- When others are reliably cooperative, AOE defects to capture the private bonus repeatedly.
- If others attempt to discipline you, AOE responds not by meekly returning to cooperation but by punishing for several rounds and escalating if discipline persists — this is an aggressive deterrent to being forced into cooperative equilibrium.
- Probing and forgiveness are included only to maximize future exploitation opportunities, not to be nice.

Robustness summary (how AOE behaves against broad types)
- Against unconditional cooperators: AOE defects consistently and earns high payoff.
- Against pure defectors: AOE defects too (neutral).
- Against reciprocators / TFT-like punishers: AOE will initially exploit but then will trigger punishments; AOE will retaliate with calibrated punishment blocks so the punishers bear costs as well; the net effect is to keep me from being exploited systematically while still harvesting what I can.
- Against noisy or mixed strategies: AOE smooths history with window W and requires a substantial cooperation drop to trigger punishment, so it tolerates noise.
- Against strategies that try to “lock” you into mutual cooperation by early concessions: AOE remains suspicious and exploits when possible; it will not commit to long-term cooperation voluntarily.

Parameter tuning guidance
- If you expect most opponents are harsh punishers, reduce P_init and escalation (be slightly more conciliatory) to avoid mutual defection spirals.
- If k is large (public good is very valuable when many cooperate) and you expect long-lived reciprocators, increase probe_prob_mid to maintain more cooperation you can later exploit.
- The defaults above are intentionally aggressive but conservative enough (limits on P_max, forgiveness probes) to avoid pointless escalation in noisy environments.

Implementation note
- The algorithm uses only game parameters and the full history of observed actions/payoffs; it needs no external coordination, no promises, no identity linking beyond measured individual cooperation frequencies, and thus is admissible in the tournament environment described.

Summary
AOE is an adaptive, mostly-defect strategy that:
- Exploits observed cooperation,
- Punishes retaliation to deter others from forcing me into permanent cooperation,
- Uses probing and forgiveness sparingly to re-open exploitable opportunities,
- Always defects on the last round.
'''

description_AGGRESSIVE_128 = '''
Strategy name: Aggressive Grim‑Prober

High level idea and mindset
- Be tough: refuse to be systematically exploited; punish exploitation harshly (permanent defection) so opponents learn that exploiting you destroys future gains.
- Be opportunistic: if a clear, stable cooperation cluster emerges (many others reliably contribute), temporarily cooperate to capture the higher mutual returns.
- Be probing but stingy: start by defecting (assert dominance), occasionally probe to see whether cooperation can be re‑established, and stop cooperating as soon as exploitation occurs.
- Endgame awareness: defect in the final rounds (no future to deter defectors).

This is deterministic, parameterized only by (n, r, k) and the observed history. It is aggressive because it (1) defects by default, (2) damages group payoffs permanently when exploited (grim), and (3) only allows cooperation when a large fraction of others have demonstrated reliable cooperation.

Parameters (computed from game parameters)
- w = min(5, r) — history window used to evaluate recent behavior (small fixed window keeps the strategy adaptive).
- cooperation_threshold theta (fraction of other players who must cooperate for you to consider cooperating).
  - Set theta based on k/n (intuitively, higher k/n makes cooperation more attractive, so lower the threshold):
    - If k/n >= 0.60 then theta = 0.60
    - If k/n <= 0.40 then theta = 0.85
    - Otherwise linearly interpolate theta between 0.85 (at k/n=0.40) and 0.60 (at k/n=0.60).
- required_m = ceil(theta * (n-1)) — minimum number of OTHER players (out of n-1) who must have cooperated (on average in the window) for you to view cooperation as stable.
- T_probe = max(3, ceil(r/10)) — when defecting, how many rounds you wait between one‑round cooperation probes.
- L_end = min(3, max(1, ceil(0.05 * r))) — number of final rounds in which you always defect (endgame).
- Punishment: permanent defection (grim) after exploitation. A single clear exploitation event while you cooperated sends you permanently into PERM_DEFECT (no more cooperation except possibly a manual code change).

State variables
- mode ∈ {DEFECT_MODE, COOP_MODE, PERM_DEFECT}
- rounds_since_last_probe (integer)

Initialization
- mode = DEFECT_MODE
- rounds_since_last_probe = T_probe (so you may probe as early as round 2)
- compute w, theta, required_m, T_probe, L_end from (n, r, k)

Decision rules (per round t)
1. Endgame:
   - If t > r − L_end (i.e., in the final L_end rounds): play D (defect). (Rationale: no credible future punishment exists; be aggressive.)

2. First round / default:
   - If t == 1: play D (start tough; get secure payoff and gather information).

3. If mode == PERM_DEFECT:
   - Always play D.

4. If mode == COOP_MODE:
   - Let recent_others_coop = average over the last min(w, t−1) rounds of (number of cooperators excluding you).
   - If in any of the last min(w, t−1) rounds you cooperated and observed (other_cooperators) < required_m:
       - You were exploited; set mode = PERM_DEFECT and play D this round.
   - Else:
       - If recent_others_coop >= required_m: play C (cooperate).
       - Else: play D this round (suspend cooperation until others are reliably cooperating).

5. If mode == DEFECT_MODE:
   - If rounds_since_last_probe >= T_probe:
       - Do a probe: play C for one round.
         - After that round, if observed other_cooperators >= required_m (i.e., the probe was reciprocated by a large fraction), set mode = COOP_MODE and reset rounds_since_last_probe = 0.
         - If not reciprocated, stay in DEFECT_MODE and reset rounds_since_last_probe = 0.
   - Else:
       - Play D and increment rounds_since_last_probe by 1.

Tie-breaking and precise checks
- When comparing averages use integer counts per round and require other_cooperators >= required_m (ceil of fraction), i.e., be conservative: tie -> defect.
- A single clear exploitation (you cooperated while not enough others cooperated) moves to PERM_DEFECT. That is the aggressive core punishment.

Pseudocode (compact)

  compute w, theta, required_m, T_probe, L_end
  mode = DEFECT_MODE
  rounds_since_last_probe = T_probe

  for t = 1 .. r:
    if t > r - L_end:
      action = D
      continue

    if t == 1:
      action = D
      rounds_since_last_probe += 1
      continue

    if mode == PERM_DEFECT:
      action = D
      continue

    if mode == COOP_MODE:
      recent_rounds = last min(w, t-1) rounds
      recent_others_coop = average over recent_rounds of (cooperators excluding self)
      if any round in recent_rounds where (I played C and observed other_cooperators < required_m):
        mode = PERM_DEFECT
        action = D
      else if recent_others_coop >= required_m:
        action = C
      else:
        action = D
      continue

    if mode == DEFECT_MODE:
      if rounds_since_last_probe >= T_probe:
        action = C   # probe
        observe other_cooperators this round
        if other_cooperators >= required_m:
          mode = COOP_MODE
        rounds_since_last_probe = 0
      else:
        action = D
        rounds_since_last_probe += 1
      continue

Rationale and robustness
- Aggressive: The permanent (grim) punishment on observed exploitation is deliberately harsh — it deters others from exploiting you and shifts dynamics to mutual defection rather than letting others reap repeated free benefits. You accept harming group payoff in order to prevent long-term exploitation of your contributions.
- Opportunistic: The COOP_MODE allows you to capture mutual gains when many opponents are stably cooperative. The thresholding prevents you from being slowly exploited by a few cooperators.
- Adaptive: The small sliding window w and periodic probes let you quickly detect changes in group behavior and re‑enter cooperation if a critical mass forms again.
- Endgame logic prevents wasted cooperation when no future deterrent is possible.
- Parameter dependence: theta depends on k/n so the strategy tolerates lower reciprocation requirements when the public good is relatively more valuable (k/n large), and demands a stricter majority when public good is weak.

Edge cases
- Very small r (e.g., r ≤ 3): the strategy effectively defects most or all rounds (endgame and initialization dominate) — this matches backward induction and is consistent with an aggressive stance.
- If multiple opponents intentionally coordinate to trigger your PERM_DEFECT (e.g., by defecting when you probe), you will move to permanent defection — costly for the group but consistent with the aggressive objective of avoiding exploitation.
- No stochasticity: decisions are deterministic from parameters and observed history — robust against being "gamed" by probabilistic responses from opponents.

Tuning suggestions
- If you want slightly less brutal behavior, replace PERM_DEFECT with long but finite punishment (e.g., defect for P rounds where P = min(ceil(r/4), 20)) and then allow probes; that reduces expense when opponents made a one-off deviation.
- If the tournament contains noise or observation error, increase forgiveness (finite punishment + longer probing) to avoid permanent collapse after accidental deviations.

Summary
- Default posture: defect.
- Cooperate only when a clear majority of others are reliably cooperating (threshold tied to k/n), or when a probe reveals such stability.
- If you are exploited while cooperating, permanently defect thereafter (grim).
- Always defect during the last L_end rounds.

This strategy is aggressively defensive: it exploits cooperative clusters when they are clearly present, but it punishes exploitation severely to prevent others from repeatedly taking advantage of you.
'''
