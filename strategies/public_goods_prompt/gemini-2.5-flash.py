"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash
- Game: public_goods_prompt
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState




class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round. In intermediate rounds,
    it adapts by mirroring the collective cooperation rate of other players from
    the previous round, contributing if 50% or more cooperated, and defecting otherwise.
    In the final round, it rationally defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_idx - 1, :]
        n_others_contributed = np.sum(previous_round_opponent_actions)
        num_other_players = self.n_players - 1
        cooperation_rate_others = n_others_contributed / num_other_players
        cooperation_threshold = 0.5
        if cooperation_rate_others >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy balances an initial commitment
    to collective welfare with adaptive responses to observed group behavior in the
    N-Player Public Goods Game. It initiates cooperation, rewards sustained group
    contributions based on a calculated threshold, and punishes insufficient contributions.
    It defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.min_cooperators = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_count = int(history.my_actions[-1])
        opponents_prev_cooperators_count = sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_count + opponents_prev_cooperators_count
        if C_prev >= self.min_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Adaptive Collective Reciprocation (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain cooperation by adapting to observed collective behavior.
    It initiates with cooperation in the first round and then conditionally reciprocates
    based on whether a simple majority of players contributed in the preceding round.
    This strategy prioritizes collective welfare and maintains its conditional logic
    even in the final rounds, not succumbing to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.cooperation_threshold_T = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action_val = 1 if history.my_actions[-1] else 0
        opponent_prev_actions_sum = np.sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_val + opponent_prev_actions_sum
        if C_prev >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    This strategy implements an Adaptive Conditional Cooperator, starting with cooperation
    (unless it's a one-shot game), and then adapting based on the previous round's
    collective cooperation level. It defects in the final round, consistent with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.floor(self.n_players * 0.7)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            if self.n_rounds == 1:
                return Action.D
            else:
                return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_is_C = history.my_actions[-1]
            opponent_prev_cooperators_count = sum(history.opponent_actions[-1, :])
            C_prev = int(my_prev_action_is_C) + int(opponent_prev_cooperators_count)
            if C_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    Adaptive Reciprocator with Forgiveness (ARF) strategy for the N-Player Public Goods Game.
    This strategy conditionally cooperates, adapting its behavior based on the observed actions
    of other players to encourage higher overall contributions while protecting itself from exploitation.
    It initiates cooperation, punishes low cooperation, and offers forgiveness when cooperation
    reaches a higher threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_stance = 'Cooperating'
        self.cst = 0.5
        self.crt = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_players = self.game_description.n_players
        n_total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_total_rounds - 1:
            return Action.D
        n_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        proportion_cooperators_prev = n_cooperators_prev / n_players
        if self.current_stance == 'Cooperating':
            if proportion_cooperators_prev >= self.cst:
                return Action.C
            else:
                self.current_stance = 'Defecting'
                return Action.D
        elif self.current_stance == 'Defecting':
            if proportion_cooperators_prev >= self.crt:
                self.current_stance = 'Cooperating'
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain high levels of collective cooperation by 
    adaptively adjusting its expectations of group contributions and responding reciprocally.
    It prioritizes maximizing overall group welfare over the long term, while protecting 
    itself from persistent exploitation.

    Key features:
    - Optimistic start in repeated games (Contribute in Round 1).
    - Dynamic adjustment of a 'cooperation level target' based on observed collective behavior.
    - Rewards meeting/exceeding the target by pushing for higher cooperation.
    - Punishes falling short of the target by lowering expectations and potentially defecting.
    - Defection in single-round games and the final round of repeated games due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_cooperation_level_target = None
        self.min_cooperation_target_floor = self.game_description.k / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            if n_rounds == 1:
                return Action.D
            else:
                self.current_cooperation_level_target = 0.5
                return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        n_c_prev = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        actual_cooperation_proportion_prev = n_c_prev / n_players
        if actual_cooperation_proportion_prev >= self.current_cooperation_level_target:
            self.current_cooperation_level_target = min(1.0, self.current_cooperation_level_target + (1.0 - self.current_cooperation_level_target) * 0.1)
        else:
            self.current_cooperation_level_target = max(self.min_cooperation_target_floor, self.current_cooperation_level_target - (self.current_cooperation_level_target - self.min_cooperation_target_floor) * 0.2)
        if actual_cooperation_proportion_prev >= self.current_cooperation_level_target:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Adaptive Reciprocal Group Contributor strategy for the N-Player Public Goods Game.
    This strategy aims to foster collective cooperation by adaptively responding
    to the group's observed behavior while ensuring its own robustness against
    exploitation, particularly in the critical final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_threshold_rate = 0.5
        self.learning_rate = 0.1
        self.min_threshold_rate = 0.1
        self.max_threshold_rate = 0.9
        self.current_cooperation_threshold_rate = self.initial_cooperation_threshold_rate

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_number - 1
        my_prev_contribution = int(history.my_actions[prev_round_idx])
        opponent_prev_contributions = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = my_prev_contribution + opponent_prev_contributions
        observed_group_cooperation_rate = total_cooperators_prev_round / n_players
        threshold_for_current_decision = self.current_cooperation_threshold_rate
        action_for_current_round = Action.D
        if observed_group_cooperation_rate >= threshold_for_current_decision:
            action_for_current_round = Action.C
        if observed_group_cooperation_rate > threshold_for_current_decision:
            self.current_cooperation_threshold_rate = max(self.min_threshold_rate, self.current_cooperation_threshold_rate - self.learning_rate)
        elif observed_group_cooperation_rate < threshold_for_current_decision:
            self.current_cooperation_threshold_rate = min(self.max_threshold_rate, self.current_cooperation_threshold_rate + self.learning_rate)
        return action_for_current_round

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by adapting its
    behavior based on observed historical contributions, while also protecting itself
    from exploitation, particularly in end-game scenarios. It dynamically adjusts its
    responsiveness based on the game's efficiency parameter 'k'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if self.n_rounds == 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        my_prev_action_contribution = 1 if history.my_actions[prev_round_index] else 0
        opponent_prev_contributions = np.sum(history.opponent_actions[prev_round_index, :])
        total_contributions_prev_round = my_prev_action_contribution + opponent_prev_contributions
        ACR = total_contributions_prev_round / self.n_players
        T = 1 / self.k_multiplier
        if ACR >= T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy is designed to foster and sustain group-wide cooperation
    in the Public Goods Game. It operates on principles of initial benevolence, conditional reciprocity based
    on group behavior, and rational adaptation to finite game conditions.

    Decision Rules:
    1.  **Round 1 (Initial Gesture):** Contribute (C).
    2.  **Rounds t from 2 to r-1 (Conditional Reciprocity):**
        *   Observe the actions of all 'n' players from the immediately preceding round (t-1).
        *   Calculate the proportion of cooperators in round t-1 (`P_cooperators_prev`).
        *   If `P_cooperators_prev >= 0.5` (Majority Rule): Contribute (C).
        *   Else (If `P_cooperators_prev < 0.5`): Defect (D).
    3.  **Round r (Last Round - Terminal Rationality):** Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      like n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on the ACC strategy.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past actions
                                           and payoffs for this player and opponents, or None
                                           if it's the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        total_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        prev_round_history_index = state.round_number - 1
        n_cooperators_prev = np.sum(history.opponent_actions[prev_round_history_index, :]) + int(history.my_actions[prev_round_history_index])
        p_cooperators_prev = n_cooperators_prev / total_players
        if p_cooperators_prev >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Adaptive Trust Builder: Aims to maximize collective welfare by proactively fostering cooperation while remaining robust against exploitation. It adapts its behavior based on the observed cooperation level of other players, with a bias towards promoting trust and collaboration.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THRESHOLD_HIGH_COOPERATION = 0.7
        self.THRESHOLD_LOW_COOPERATION = 0.3
        self.THRESHOLD_MODERATE_COOPERATION_FORGIVENESS = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_other_players = num_players - 1
        num_other_cooperators = np.sum(history.opponent_actions[-1, :])
        avg_others_c = num_other_cooperators / num_other_players
        if avg_others_c >= self.THRESHOLD_HIGH_COOPERATION:
            return Action.C
        elif avg_others_c <= self.THRESHOLD_LOW_COOPERATION:
            return Action.D
        elif avg_others_c >= self.THRESHOLD_MODERATE_COOPERATION_FORGIVENESS:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by starting with cooperation
    and then adapting its behavior based on the overall contribution level of the group
    in the previous round. It defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACR strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None for the first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        my_prev_action_is_cooperate = history.my_actions[previous_round_index]
        opponents_prev_actions = history.opponent_actions[previous_round_index, :]
        S_prev = int(my_prev_action_is_cooperate) + np.sum(opponents_prev_actions)
        if S_prev >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    Adaptive Reciprocity with Majority Rule strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by responding to the
    group's overall behavior. It balances the desire for collective welfare with the
    need for individual protection against exploitation, adapting its actions based on
    observed contributions while accounting for the game's boundaries.

    Decision Rules:
    1.  Round 1 (Initial Play): Contribute (C) to signal willingness to cooperate.
    2.  Intermediate Rounds (Round 2 up to Round r-1):
        - Observe N_C_prev (total number of cooperators in the previous round).
        - If N_C_prev >= n / 2 (at least half of players contributed), then Contribute (C).
        - Else (fewer than half contributed), then Defect (D).
    3.  Last Round (Round r): Defect (D) to prevent exploitation, as there are no
        future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        n_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if n_cooperators_prev >= n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain a high level of cooperation by
    demonstrating initial trust and then enforcing a minimum standard of collective
    contribution. It is designed to be robust against various opponent behaviors,
    adapt to observed group actions, and account for the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.sustainability_threshold = math.ceil(self.n_players / self.k_multiplier)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_val = 1 if history.my_actions[prev_round_idx] else 0
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev = my_prev_action_val + opponent_prev_cooperators
        if C_prev >= self.sustainability_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy fosters collective cooperation by initiating with a contribution,
    then adaptively reciprocating the observed cooperation level of other players.
    It incorporates caution towards the end of the game to prevent exploitation and
    defects in the final round based on backward induction.
    """
    COOPERATION_THRESHOLD: float = 0.75
    ENDGAME_BUFFER: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        number_of_other_cooperators = np.sum(history.opponent_actions[-1, :])
        n_other_players = self.n_players - 1
        other_players_cooperation_rate = number_of_other_cooperators / n_other_players
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        endgame_caution_start_round = total_rounds - self.ENDGAME_BUFFER - 1
        endgame_caution_end_round = total_rounds - 2
        if endgame_caution_start_round <= current_round_0_indexed <= endgame_caution_end_round:
            if other_players_cooperation_rate >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D
        if other_players_cooperation_rate >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    This strategy is designed to foster and maintain a high level of collective cooperation within the group,
    adapting to observed behavior while remaining robust against exploitation. It prioritizes the overall
    group payoff over short-term individual gains, while recognizing the realities of a tournament environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.aspiration_level: float = 1.0
        self.punishment_timer: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        total_contributions_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        observed_cooperation_rate = total_contributions_previous_round / n_players
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        else:
            aspiration_tolerance_threshold = self.aspiration_level - 1.0 / n_players
            if observed_cooperation_rate >= aspiration_tolerance_threshold:
                if observed_cooperation_rate == 1.0 and self.aspiration_level < 1.0:
                    self.aspiration_level = min(1.0, self.aspiration_level + 0.1)
                return Action.C
            else:
                self.punishment_timer = 2
                self.aspiration_level = max(0.5, observed_cooperation_rate)
                return Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    The Adaptive Forgiving Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by starting with a cooperative move
    and adapting its subsequent actions based on observed collective cooperation levels.
    It incorporates a 'forgiveness' mechanism to re-engage cooperation if an upward trend
    is observed after a prior defection. The strategy is also designed to be self-protective
    against exploitation and defects in the final round based on backward induction.

    Internal State Tracked:
    - my_last_action: The action (C or D) this strategy played in the immediately preceding round.
    - num_cooperators_last_round: Total number of cooperators observed in the immediately preceding round.
    - num_cooperators_two_rounds_ago: Total number of cooperators observed in the round before the preceding one.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.C
        self.num_cooperators_last_round: int = 0
        self.num_cooperators_two_rounds_ago: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if current_round_num == 0:
            self.my_last_action = Action.C
            return Action.C
        if current_round_num == n_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        prev_round_idx = current_round_num - 1
        my_action_prev_round_bool = history.my_actions[prev_round_idx]
        self.my_last_action = Action.C if my_action_prev_round_bool else Action.D
        opponents_cooperated_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        current_observation_num_cooperators = opponents_cooperated_prev_round + (1 if my_action_prev_round_bool else 0)
        self.num_cooperators_two_rounds_ago = self.num_cooperators_last_round
        self.num_cooperators_last_round = current_observation_num_cooperators
        final_action_this_round: Action
        CCR = self.num_cooperators_last_round / n_players
        k_over_n = k_factor / n_players
        if CCR >= k_over_n:
            final_action_this_round = Action.C
        elif current_round_num >= 2 and self.my_last_action == Action.D and (self.num_cooperators_last_round > self.num_cooperators_two_rounds_ago):
            final_action_this_round = Action.C
        else:
            final_action_this_round = Action.D
        self.my_last_action = final_action_this_round
        return final_action_this_round

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain high levels of cooperation by initiating
    cooperation, adaptively responding to observed collective behavior, and strategically
    withdrawing contributions to deter free-riding, while anticipating the dynamics of the end game.
    It balances the collective good with self-preservation in a competitive tournament environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.GTL: float = 1.0
        self.Learning_Rate_Gamma: float = 0.3
        self.Defection_Sensitivity_Multiplier_Delta: float = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        total_contributions_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        observed_cooperation_rate = total_contributions_last_round / self.n
        if observed_cooperation_rate >= self.GTL:
            self.GTL = (1 - self.Learning_Rate_Gamma) * self.GTL + self.Learning_Rate_Gamma * observed_cooperation_rate
        else:
            self.GTL = (1 - self.Learning_Rate_Gamma * self.Defection_Sensitivity_Multiplier_Delta) * self.GTL + self.Learning_Rate_Gamma * self.Defection_Sensitivity_Multiplier_Delta * observed_cooperation_rate
        self.GTL = max(0.0, min(1.0, self.GTL))
        base_threshold = 0.6
        k_n_adjustment = (1 - self.k / self.n) * 0.2
        end_game_adjustment = current_round_1_indexed / self.r * 0.3
        contribution_decision_threshold = base_threshold + k_n_adjustment + end_game_adjustment
        contribution_decision_threshold = min(0.95, max(0.1, contribution_decision_threshold))
        if self.GTL >= contribution_decision_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    The Adaptive Group Cooperator (AGC) strategy aims to foster and sustain collective
    cooperation in the N-Player Public Goods Game. It initiates cooperation in the first
    round and then adapts its behavior based on the observed collective contributions
    of the group in the previous round. It cooperates if a majority of players
    contributed, and defects otherwise. In the final round, it defects to prevent
    exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Group Cooperator strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing the history of actions and payoffs for
                     this player and opponents up to the previous round. It is None
                     for the very first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        my_previous_action_is_cooperate = history.my_actions[-1]
        opponent_previous_actions_is_cooperate = history.opponent_actions[-1, :]
        total_contributions_previous_round = int(my_previous_action_is_cooperate) + np.sum(opponent_previous_actions_is_cooperate)
        cooperation_threshold = math.ceil(n_players / 2)
        if total_contributions_previous_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a contribution,
    adaptively responding to the collective cooperation level in intermediate rounds,
    and defecting in the final round to prevent exploitation.

    Decision Rules:
    1. First Round (Round 0): Always Contribute (C).
    2. Intermediate Rounds (Round 1 to r-2):
       - Calculate Cooperation Index (CI) from the previous round (t-1).
       - CI = (Total cooperators in t-1) / n_players
       - If CI >= 0.5, Contribute (C).
       - Else (CI < 0.5), Defect (D).
    3. Last Round (Round r-1): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_number - 1
        total_cooperators_in_prev_round = history.my_actions[previous_round_idx] + sum(history.opponent_actions[previous_round_idx, :])
        cooperation_index = total_cooperators_in_prev_round / self.n_players
        if cooperation_index >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Adaptive Collective Reciprocation (ACR) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating, then adapts its behavior based on the historical
    collective cooperation rate relative to a dynamically calculated threshold. It aims
    to foster cooperation when beneficial and protect against exploitation, with a
    terminal defection in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.total_contributions_sum_history = 0
        self.rounds_completed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        prev_round_idx = current_round_number_0_indexed - 1
        my_prev_action_is_cooperate = history.my_actions[prev_round_idx]
        opponent_prev_actions_are_cooperate = history.opponent_actions[prev_round_idx, :]
        sum_j_c_j_prev = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate).item()
        self.total_contributions_sum_history += sum_j_c_j_prev
        self.rounds_completed += 1
        if current_round_number_0_indexed == self.n_rounds - 1:
            return Action.D
        H_C = self.total_contributions_sum_history / (self.n_players * self.rounds_completed)
        T_dynamic = 0.5 + 0.5 * (1 - self.k_multiplier / self.n_players)
        if H_C >= T_dynamic:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster and sustain collective
    cooperation in the Public Goods Game. It operates on the principle of conditional
    reciprocity, initiating cooperation and then adapting its behavior based on the
    observed collective commitment of other players, while also protecting itself
    from exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.endgame_defection_window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - self.endgame_defection_window:
            return Action.D
        num_other_players = self.game_description.n_players - 1
        cooperators_prev_round = sum(history.opponent_actions[-1, :])
        proportion_other_cooperation_prev_round = cooperators_prev_round / num_other_players
        if proportion_other_cooperation_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by reciprocating observed
    collective effort. It initiates cooperation in the first round to signal
    willingness to invest. In subsequent rounds, it observes the total number
    of cooperators in the previous round. If this count meets or exceeds the
    game's multiplier 'k' (used as a dynamic cooperation threshold), the strategy
    cooperates. Otherwise, it defects to discourage free-riding and incentivize
    higher contributions, protecting against exploitation. This adaptive
    approach applies consistently throughout all rounds, including the last.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if n_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to induce and sustain high levels of collective cooperation
    by starting cooperatively, reinforcing cooperation when the group performs well,
    and responsively defecting for a short duration when collective cooperation
    falls below a high threshold. It forgives after a period of punishment and
    defects in the final round based on game-theoretic backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_threshold = (self.n - 1) / self.n
        self.punishment_duration = 2
        self.consecutive_defections_by_self = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_defections_by_self = 0
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = (1 if my_action_prev_round_is_cooperate else 0) + opponent_cooperators_prev_round
        p_prev_round = total_cooperators_prev_round / self.n
        if p_prev_round >= self.cooperation_threshold:
            self.consecutive_defections_by_self = 0
            return Action.C
        elif self.consecutive_defections_by_self < self.punishment_duration:
            self.consecutive_defections_by_self += 1
            return Action.D
        else:
            self.consecutive_defections_by_self = 0
            return Action.C

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.

    This strategy observes the collective behavior of all players in previous rounds
    and adapts its own contribution decision based on a defined threshold. It aims
    to foster and sustain collective cooperation while remaining robust against
    exploitation.

    Decision Rules:
    1.  First Round (t=1, state.round_number=0): Always Contribute (C) to initiate
        cooperation and probe other players.
    2.  Intermediate Rounds (1 < t < r):
        Calculate the Average Group Contribution (AGC) from the immediately preceding round (t-1).
        If AGC >= 0.5: Contribute (C) to reinforce positive dynamics.
        If AGC < 0.5: Defect (D) to protect against exploitation and signal disapproval
                      of low collective effort.
    3.  Last Round (t=r, state.round_number=r-1): Always Defect (D) due to the
        absence of future interactions, ensuring individual rationality in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_prev_action = history.my_actions[previous_round_index]
        opponent_prev_actions = history.opponent_actions[previous_round_index, :]
        total_contributions_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        agc = total_contributions_prev_round / self.game_description.n_players
        if agc >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    The Adaptive Reciprocal Contributor (ARC) strategy aims to foster and sustain collective
    cooperation in the Public Goods Game. It starts with an initial cooperative stance,
    then adaptively reciprocates observed collective cooperation levels of other players.
    It strategically defects in the final round to optimize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Reciprocal Contributor strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Reciprocal Contributor strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the current player
                     and opponents. It is None for the very first round (round_number == 0).

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == total_rounds - 1:
            return Action.D
        sum_others_cooperated_prev = sum(history.opponent_actions[-1, :])
        num_other_players = n_players - 1
        P_others_cooperated_prev = sum_others_cooperated_prev / num_other_players
        if P_others_cooperated_prev >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy is designed to foster and maintain collective cooperation while
    being robust against exploitation. It starts by cooperating in the first round
    to signal willingness. In middle rounds, it cooperates if at least half of
    the players cooperated in the previous round, otherwise it defects. In the
    final round, it always defects, recognizing the absence of future incentives.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_index, :])
        N_coop_prev = num_opponent_cooperators_prev + history.my_actions[prev_round_index]
        cooperation_threshold = math.ceil(self.n_players / 2)
        if N_coop_prev >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by reciprocating observed collective effort
    while preventing exploitation. It operates on the principle that individual contribution is
    warranted when the group demonstrates sufficient commitment to the public good.

    Decision Rules:
    1. Round 1 (Initial Play): Always Contribute (C) to initiate cooperation.
    2. Rounds 2 to r-1 (Adaptive Play):
       - If total_contributions_prev >= k (the game's multiplier): Contribute (C).
         This rewards and reinforces observed collective effort.
       - Else (total_contributions_prev < k): Defect (D).
         This prevents exploitation and signals insufficient collective investment.
    3. Round r (Final Round): Always Defect (D), as there are no future interactions for reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACC strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for this player and opponents,
                     or None if it's the first round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round - 1
        my_prev_action_contribution = int(history.my_actions[prev_round_index])
        opponent_prev_contributions = sum(history.opponent_actions[prev_round_index, :])
        total_contributions_prev = my_prev_action_contribution + opponent_prev_contributions
        if total_contributions_prev >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation in the first round. In subsequent rounds,
    it acts as a conditional cooperator, contributing if a simple majority of
    players (including itself) contributed in the previous round, and defecting
    otherwise. This approach balances collective welfare with self-protection
    against exploitation, aiming to foster and sustain collective cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_prev_action = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        c_prev = my_prev_action + np.sum(opponent_prev_actions)
        if c_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy is designed to promote and sustain cooperation by initiating with trust,
    responding conditionally to the collective behavior of the group based on the 'k'
    multiplication factor, and protecting itself from exploitation by defecting in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_threshold = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round - 1
        my_action_last_round = history.my_actions[previous_round_index]
        opponent_actions_last_round = history.opponent_actions[previous_round_index, :]
        n_cooperators_last_round = int(my_action_last_round) + np.sum(opponent_actions_last_round)
        if n_cooperators_last_round >= self.k_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    This strategy, named "Collective Reciprocator," aims to foster and sustain high levels of cooperation within the group,
    adapting to observed collective behavior while remaining resilient to free-riding and promoting overall group welfare.
    It uses a blend of optimism, conditional cooperation, and measured forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.forgiveness_window_m = 3
        self.cooperation_threshold_t = 0.6
        self.tolerance_factor_f = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_past_rounds_to_consider = min(self.forgiveness_window_m, current_round_0_indexed)
        window_start_index = current_round_0_indexed - num_past_rounds_to_consider
        my_contributions_in_window = np.sum(history.my_actions[window_start_index:current_round_0_indexed])
        opponent_contributions_in_window = np.sum(history.opponent_actions[window_start_index:current_round_0_indexed, :])
        total_contributions_in_window = my_contributions_in_window + opponent_contributions_in_window
        denominator = self.n_players * num_past_rounds_to_consider
        average_group_cooperation_rate = total_contributions_in_window / denominator
        if average_group_cooperation_rate >= self.cooperation_threshold_t - self.tolerance_factor_f:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain high levels of cooperation by
    responding to the collective behavior observed in previous rounds. It operates
    in two modes: COOPERATIVE and PUNISHMENT.

    - Starts by contributing in Round 1.
    - In COOPERATIVE mode, it continues to contribute if a high proportion (>= 75%)
      of players cooperated in the previous round. If cooperation falls below this
      threshold, it defects and switches to PUNISHMENT mode.
    - In PUNISHMENT mode, it defects unless a very high proportion (>= 90%)
      of players cooperated in the previous round, signaling a strong collective
      effort to restore cooperation, at which point it contributes and switches
      back to COOPERATIVE mode.
    - Always defects in the final round due to the end-game effect.
    """
    COOPERATIVE_MODE = 'COOPERATIVE'
    PUNISHMENT_MODE = 'PUNISHMENT'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_mode: str = self.COOPERATIVE_MODE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        n_c_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperative_threshold = math.floor(self.n_players * 0.75)
        punishment_exit_threshold = math.floor(self.n_players * 0.9)
        if self.cooperation_mode == self.COOPERATIVE_MODE:
            if n_c_prev >= cooperative_threshold:
                return Action.C
            else:
                self.cooperation_mode = self.PUNISHMENT_MODE
                return Action.D
        elif self.cooperation_mode == self.PUNISHMENT_MODE:
            if n_c_prev >= punishment_exit_threshold:
                self.cooperation_mode = self.COOPERATIVE_MODE
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare by initially cooperating,
    then adapting its behavior based on the observed collective contribution
    of other players. It reciprocates significant cooperation and punishes
    widespread free-riding. In the final round, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None for the first round (round 0).

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_cooperated_others_prev = sum(history.opponent_actions[-1, :])
        cooperation_threshold = math.ceil((n_players - 1) / 2)
        if num_cooperated_others_prev >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to promote and sustain collective cooperation. It initiates cooperation,
    reciprocates observed cooperation, and employs a clear deterrent mechanism
    against persistent collective defection. It is also strategically aware of
    the 'endgame' effect in finite repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.InitialCooperationRounds = 2
        self.CooperationThreshold = max(0.5, self.k / self.n + 0.1)
        self.CriticalLowCooperationRate = 0.2
        self.DefectionStreakThreshold = 2
        self.permanent_defection_mode_active = False
        self.low_cooperation_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.permanent_defection_mode_active:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed <= self.InitialCooperationRounds:
            return Action.C
        past_round_total_contributions = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        cooperation_rate_prev_round = past_round_total_contributions / self.n
        if cooperation_rate_prev_round < self.CriticalLowCooperationRate:
            self.low_cooperation_streak += 1
        else:
            self.low_cooperation_streak = 0
        if self.low_cooperation_streak >= self.DefectionStreakThreshold:
            self.permanent_defection_mode_active = True
        if cooperation_rate_prev_round >= self.CooperationThreshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    Dynamic Collective Reciprocity (DCR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by adaptively responding
    to the group's observed behavior. It initiates cooperation in the first round and defects
    in the final round. In intermediate rounds, it calculates a 'cooperation_sentiment' based
    on recent group contributions and compares it to a threshold derived from the game's
    multiplication factor 'k'. If the sentiment is high enough, it cooperates; otherwise,
    it defects to protect individual interests.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_sentiment: float = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_sentiment = 1.0
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_contribution_prev_round = int(history.my_actions[-1])
            total_opponent_contributions_prev_round = np.sum(history.opponent_actions[-1, :])
            total_contributions_prev_round = my_contribution_prev_round + total_opponent_contributions_prev_round
            current_round_cooperation_rate = total_contributions_prev_round / self.game_description.n_players
            self.cooperation_sentiment = 0.8 * current_round_cooperation_rate + 0.2 * self.cooperation_sentiment
            required_cooperation_threshold = self.game_description.k / self.game_description.n_players
            if self.cooperation_sentiment >= required_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    It begins by cooperating, then adapts its behavior in intermediate rounds based
    on the observed majority cooperation of other players in the preceding round.
    It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        num_other_cooperators = sum(history.opponent_actions[-1, :])
        num_other_players = n_players - 1
        cooperation_rate_others = num_other_cooperators / num_other_players
        if cooperation_rate_others >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    The Adaptive Majority Cooperator (AMC) strategy aims to foster and sustain collective cooperation
    by reciprocating good behavior while punishing widespread defection. It starts with cooperation,
    then adapts based on whether a majority of players cooperated in the previous round, and finally
    defects in the last round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        
        Args:
            game_description: An object containing game parameters such as n_players and n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the AMC strategy rules.

        Args:
            state: The current state of the game, including the round number.
            history: A PlayerHistory object containing past actions and payoffs, or None for round 0.

        Returns:
            An Action (C for Cooperate, D for Defect) for the current round.
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_cooperators_previous_round >= self.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    The Adaptive Forgiver strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating trust, conditionally
    cooperating based on past group behavior, tolerating minor defections
    (especially in larger groups), punishing widespread defection, and
    defecting in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Forgiver strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Forgiver strategy's rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs.
                     It is None for the first round (round_number == 0).

        Returns:
            An Action enum member (Action.C or Action.D).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_was_D = not history.my_actions[prev_round_idx]
        num_opponent_defectors = np.sum(~history.opponent_actions[prev_round_idx, :])
        N_D_prev = num_opponent_defectors + (1 if my_prev_action_was_D else 0)
        if self.n_players == 2:
            if N_D_prev == 0:
                return Action.C
            else:
                return Action.D
        elif N_D_prev <= 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive High-Cooperation Enforcement (AHCE) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain maximum collective cooperation while being resilient to
    free-riding and adaptive to changing group behaviors. It operates under the principle that the group
    should strive for full contribution, but it offers a degree of forgiveness for temporary lapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_without_full_cooperation: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        cooperators_from_opponents_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        my_contribution_prev_round = 1 if my_action_prev_round_is_cooperate else 0
        total_contributions_prev_round = cooperators_from_opponents_prev_round + my_contribution_prev_round
        if total_contributions_prev_round == n:
            self.rounds_without_full_cooperation = 0
        else:
            self.rounds_without_full_cooperation += 1
        if state.round_number == r - 1:
            return Action.D
        if self.rounds_without_full_cooperation <= 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy operates in distinct phases, adapting its decision based on
    the round number, game parameters, and observed history of contributions.
    It balances the pursuit of collective welfare with the practical need for
    individual success in a competitive environment.

    - Phase 1 (Round 1): Always Cooperate (C) to signal willingness and gather data.
    - Phase 2 (Intermediate Rounds): Adapts based on the proportion of cooperators
      in the previous round. If this proportion meets or exceeds the game's
      inherent efficiency threshold (k/n), it Cooperates; otherwise, it Defects (D).
    - Phase 3 (Final Round): Always Defects (D) to prevent exploitation,
      following backward induction logic for finite games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Contributor strategy.

        Args:
            game_description: An object containing the game's parameters,
                              including the number of players (n_players),
                              total number of rounds (n_rounds), and the
                              multiplication factor (k).
        """
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.k / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Collective Contributor strategy's rules.

        Args:
            state: An object providing the current game state, primarily
                   the current_round_number (0-indexed).
            history: An object containing historical actions and payoffs for
                     this player and opponents. This will be None for round 0.

        Returns:
            Action.C if the strategy chooses to Cooperate, Action.D if it chooses to Defect.
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            total_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            proportion_cooperators_prev_round = total_cooperators_prev_round / n_players
            if proportion_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by contributing when the group shows
    sufficient willingness to do so, while protecting itself from exploitation by non-contributors.
    It adapts its behavior based on the observed contributions of all players in previous rounds.

    Summary of behavior per round:
    1.  First Round (Round 1): Always Contribute (C) to initiate cooperation and test the group.
    2.  Intermediate Rounds (Round 2 to Round `r-1`):
        Observes the actions of all players from the *previous* round. If the total number of
        cooperators in the previous round was greater than or equal to `n/2` (at least half),
        the strategy contributes (C). Otherwise, it defects (D). This adaptive threshold
        encourages sustained cooperation when the group demonstrates sufficient commitment.
    3.  Last Round (Round `r`): Always Defect (D), based on backward induction, as there are
        no future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev >= self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR) strategy for the N-Player Public Goods Game.

    This strategy fosters and maintains cooperation by:
    1.  **Initiating Cooperation:** Always cooperates in the first round to signal intent.
    2.  **Conditional Cooperation:** In intermediate rounds, cooperates if a strict majority
        (ceil(n/2)) of players cooperated in the previous round; otherwise, defects.
        This provides a dynamic, forgiving reciprocity mechanism.
    3.  **Nuanced End-Game:** In the final round, it cooperates only if *all* players
        cooperated in the penultimate round, otherwise defects to prevent exploitation.

    This approach balances fostering collective action, adapting to group behavior,
    and protecting against free-riding, especially in the critical final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            my_action_penultimate_round = history.my_actions[-1]
            opponent_actions_penultimate_round = history.opponent_actions[-1, :]
            total_contributions_penultimate_round = my_action_penultimate_round + sum(opponent_actions_penultimate_round)
            if total_contributions_penultimate_round == self.n_players:
                return Action.C
            else:
                return Action.D
        my_action_previous_round = history.my_actions[-1]
        opponent_actions_previous_round = history.opponent_actions[-1, :]
        total_contributions_previous_round = my_action_previous_round + sum(opponent_actions_previous_round)
        if total_contributions_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy is designed to foster and sustain high levels of
    cooperation in the Public Goods Game. It uses conditional reciprocity, starting with cooperation
    and adapting based on the average contribution rate of other players in the previous round.
    It strategically defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        sum_other_contributions_prev_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        average_other_contribution_rate = sum_other_contributions_prev_round / num_other_players
        if average_other_contribution_rate >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster and sustain collective
    cooperation in the Public Goods Game. It starts by cooperating, then adaptively
    cooperates if a sufficient number of players (at least 'k') contributed in the previous
    round, otherwise defects to protect itself from exploitation. In the final round, it
    always defects due to the absence of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            n_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if n_cooperators_previous_round >= self.k_multiplier:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    Evolving Collective Reciprocity (ECR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by adaptively responding
    to the collective behavior of all players in the previous round. It balances
    initial trust with conditional reciprocity, while protecting against exploitation.

    Internal State:
    - _required_cooperation_rate: The minimum proportion of players that must have
      contributed in the *previous* round for this strategy to contribute in the
      *current* round. It's adaptively updated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._required_cooperation_rate = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        total_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1].item()
        avg_contribution_last_round = total_cooperators_last_round / n_players
        action_for_current_round: Action
        if avg_contribution_last_round >= self._required_cooperation_rate:
            action_for_current_round = Action.C
        else:
            action_for_current_round = Action.D
        if avg_contribution_last_round == 1.0:
            self._required_cooperation_rate = max(0.5, self._required_cooperation_rate - 0.1)
        elif avg_contribution_last_round >= 0.75:
            self._required_cooperation_rate = max(0.6, self._required_cooperation_rate - 0.05)
        elif avg_contribution_last_round >= 0.5:
            self._required_cooperation_rate = min(0.9, self._required_cooperation_rate + 0.1)
        else:
            self._required_cooperation_rate = min(0.95, self._required_cooperation_rate + 0.2)
        return action_for_current_round

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    The Conditional Reciprocal Contributor strategy for the N-Player Public Goods Game.

    This strategy aims to initiate and sustain cooperation by starting cooperatively
    and then adapting its behavior based on the overall level of contributions
    observed in the group. It incorporates elements of reciprocity and conditional
    punishment to prevent exploitation, while also providing opportunities for
    re-establishing cooperation if collective effort improves.

    Decision Rules:
    1. First Round (t=0): Always Contribute (C) as an initial gesture of goodwill.
    2. Last Round (t=r-1): Always Defect (D) to avoid exploitation in the absence
       of future repercussions.
    3. Intermediate Rounds (0 < t < r-1):
       - Observe total contributions from the previous round (C_prev).
       - If C_prev < k (total contributions were less than the collective benefit `k`):
         Defect (D) in the current round, signaling insufficient collective effort.
       - Else (If C_prev >= k, indicating sufficient collective effort):
         Contribute (C) in the current round to reciprocate and encourage continued cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev_round < self.k_factor:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    Adaptive Collective Cooperator (ACC) strategy for the N-Player Public Goods Game.
    This strategy initializes with cooperation, then adapts its behavior based on the
    group's observed cooperation rate from the previous round, using a fixed
    Cooperation Target Threshold (CTT). In the final round, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_target_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        num_opponent_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        my_previous_action_int = int(history.my_actions[previous_round_index])
        total_cooperators_in_previous_round = num_opponent_cooperators + my_previous_action_int
        GCR = total_cooperators_in_previous_round / n_players
        if GCR >= self.cooperation_target_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Adaptive Reciprocity for Public Goods strategy.

    This strategy aims to maximize long-term collective welfare by promoting
    cooperation and deterring free-riding. It adapts to observed group behavior
    using an Exponentially Weighted Moving Average (EWMA) of total contributions.
    It initiates cooperation in the first round, reciprocates when the smoothed
    collective contributions meet a threshold (`k`), defects otherwise, and
    always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.EWMA_ALPHA = 0.25
        self.smoothed_total_contributions = float(self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            total_contributions_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            self.smoothed_total_contributions = self.EWMA_ALPHA * total_contributions_prev + (1 - self.EWMA_ALPHA) * self.smoothed_total_contributions
            if self.smoothed_total_contributions >= self.k:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy fosters collective cooperation by starting with a contribution,
    then adaptively responding to the group's cooperation rate in previous rounds.
    It cooperates if 60% or more of players cooperated in the last round, otherwise defects.
    It always defects in the final round based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players and n_rounds.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing the history of actions and payoffs for
                     all players up to the previous round, or None for the first round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[current_round - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[current_round - 1, :])
        total_cooperators_prev_round = int(my_prev_action_is_cooperate) + opponent_prev_cooperators
        gcr = total_cooperators_prev_round / n_players
        if gcr >= 0.6:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and sustain cooperation. It starts by cooperating
    in the first round. In intermediate rounds, it observes the proportion of cooperators
    in the immediately preceding round. If 50% or more players contributed, it continues
    to cooperate; otherwise, it defects as a collective punishment. In the final round,
    it defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[current_round - 1]
        opponents_prev_actions_were_cooperate = history.opponent_actions[current_round - 1, :]
        num_cooperators_prev_round = int(my_prev_action_was_cooperate) + sum(opponents_prev_actions_were_cooperate)
        p_coop = num_cooperators_prev_round / self.n_players
        if p_coop >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Adaptive Reciprocity: This strategy aims to foster and sustain collective cooperation.
    It initiates contributions in the first round, then reciprocates the observed cooperation rate
    of other players in intermediate rounds, and defects in the final round to protect against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Reciprocity strategy with the game's parameters.

        Args:
            game_description: A PublicGoodsDescription object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Reciprocity strategy.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs, or None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        total_game_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_game_rounds - 1:
            return Action.D
        total_others_cooperated_previous_round = np.sum(history.opponent_actions[-1, :])
        average_others_cooperation_rate = total_others_cooperated_previous_round / (num_players - 1)
        if average_others_cooperation_rate >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy balances initial trust with conditional reciprocity, adapting to the
    overall level of cooperation observed in the group, and acknowledges the unique
    dynamics of the end-game by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_trust_rounds = 1
        self.cooperation_threshold_fraction = 0.7
        self.endgame_defection_rounds = 2
        self.required_cooperators_threshold = math.ceil(self.game_description.n_players * self.cooperation_threshold_fraction)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_r = self.game_description.n_rounds
        if current_round_0_indexed < self.initial_trust_rounds:
            return Action.C
        if current_round_0_indexed >= total_rounds_r - self.endgame_defection_rounds:
            return Action.D
        num_C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_C_prev >= self.required_cooperators_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Dynamic Group Reciprocity strategy for N-Player Public Goods Game.
    This strategy aims to initiate and maintain the collectively optimal outcome
    (all players contributing) by being initially cooperative, adaptively responding
    to group contributions, and incorporating both punishment for widespread defection
    and forgiveness to re-establish cooperation. It explicitly accounts for the finite
    nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_rounds: int = 2
        self.punishment_duration: int = 1
        self.current_punishment_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number < self.initial_cooperation_rounds:
            return Action.C
        if current_round_number == r - 1:
            return Action.D
        total_contributions_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        acr_prev = total_contributions_prev / n
        if self.current_punishment_counter > 0:
            if self.current_punishment_counter < self.punishment_duration:
                self.current_punishment_counter += 1
                return Action.D
            else:
                self.current_punishment_counter = 0
                return Action.C
        elif acr_prev >= 0.9:
            return Action.C
        elif acr_prev < 0.5:
            self.current_punishment_counter = 1
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    Adaptive Collective Reciprocation (ACR) strategy for the N-Player Public Goods Game.

    Core Philosophy: This strategy aims to foster and sustain high levels of group
    cooperation. It begins with an act of trust, then adaptively reciprocates the
    observed cooperation levels within the group. It is designed to be forgiving of
    minor, non-persistent defections to prevent spirals of uncooperativeness, but
    will punish significant or persistent exploitation to protect itself and
    re-establish a collective norm. It prioritizes the collective mindset while
    remaining robust in a competitive tournament setting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_sustain_threshold = self.n_players - 2
        self.defection_streak_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        previous_round_zero_indexed = current_round_zero_indexed - 1
        num_other_cooperators_last_round = np.sum(history.opponent_actions[previous_round_zero_indexed, :])
        any_persistent_defector = False
        if current_round_zero_indexed >= self.defection_streak_threshold:
            two_rounds_ago_zero_indexed = current_round_zero_indexed - self.defection_streak_threshold
            num_opponents = self.n_players - 1
            for player_idx in range(num_opponents):
                if not history.opponent_actions[previous_round_zero_indexed, player_idx] and (not history.opponent_actions[two_rounds_ago_zero_indexed, player_idx]):
                    any_persistent_defector = True
                    break
        if current_round_zero_indexed < self.n_rounds - 1:
            if num_other_cooperators_last_round >= self.n_players - 1:
                return Action.C
            elif num_other_cooperators_last_round >= self.cooperation_sustain_threshold and (not any_persistent_defector):
                return Action.C
            else:
                return Action.D
        elif num_other_cooperators_last_round >= self.n_players - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    Dynamic Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain high levels of cooperation. It
    starts by cooperating, then adaptively responds to the collective
    cooperation rate from the previous round. It continues to cooperate
    even at moderate cooperation levels to encourage recovery, but defects
    if cooperation falls to very low levels to prevent exploitation. In the
    final round, it defects to maximize individual payoff due to the absence
    of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on
        the strategy's logic.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for
                     this player and opponents, or None for the very first round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
        cooperators_prev_round_my = int(history.my_actions[-1])
        total_cooperators_prev_round = cooperators_prev_round_opponents + cooperators_prev_round_my
        coop_rate = total_cooperators_prev_round / n_players
        if coop_rate >= 0.3:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster widespread cooperation by starting cooperatively
    and continuing to do so if a significant proportion of players (50% or more)
    cooperated in the previous round. It defects if collective cooperation falls
    below this threshold, acting as a signal of dissatisfaction and a self-protective
    measure. In the final round, it always defects to preempt exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.threshold_c = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.total_rounds - 1:
            return Action.D
        cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_c_rate = cooperators_previous_round / self.n_players
        if avg_c_rate >= self.threshold_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    The Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.
    It initiates cooperation, sustains it if a majority of others contribute, and
    defects in the final round to prevent exploitation. This strategy balances
    collective welfare with protection against individual exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        opponent_cooperations_last_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        proportion_others_cooperated = opponent_cooperations_last_round / num_other_players
        if proportion_others_cooperated >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating
    cooperation, adaptively reciprocating collective contributions based on a
    dynamic threshold (ceil(n/2)), and strategically defecting in the final round
    to prevent exploitation. It balances collective welfare with self-protection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_prev_action_value = int(history.my_actions[-1])
        total_opponent_contributions_prev = np.sum(history.opponent_actions[-1, :])
        c_prev = my_prev_action_value + total_opponent_contributions_prev
        if c_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy is designed for the N-Player
    Public Goods Game to encourage and sustain cooperation while being robust against exploitation.
    
    It operates in three distinct phases:
    1.  **Round 1 (Initialization & Signaling):** Contributes (C) to signal willingness to cooperate.
    2.  **Rounds 2 to (r-1) (Adaptive Conditional Play):** Calculates a 'Cooperation Threshold' T = ceil(n / k).
        If the total contributions in the previous round meet or exceed T, the strategy contributes (C).
        Otherwise, it defects (D) to protect against exploitation and signal dissatisfaction with
        the collective effort.
    3.  **Round r (Final Round - Endgame Effect):** Defects (D) to maximize individual payoff
        in the absence of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_action_bool = history.my_actions[state.round_number - 1]
        opponents_previous_actions_bool = history.opponent_actions[state.round_number - 1, :]
        total_contributions_previous_round = int(my_previous_action_bool) + np.sum(opponents_previous_actions_bool)
        if total_contributions_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    Adaptive Reciprocator with End-Game Logic for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating
    contributions and then adaptively responding to the observed level of
    cooperation from other players. It employs rational behavior in the final
    round to prevent exploitation.

    Decision Rules:
    1.  Round 0 (first round): Always Contribute (C).
    2.  Intermediate Rounds (rounds 1 to r-2):
        - Observe the total number of cooperators (N_C_prev) in the previous round.
        - If N_C_prev >= ceil(n_players / 2), then Contribute (C).
        - Else (N_C_prev < ceil(n_players / 2)), then Defect (D).
    3.  Final Round (round r-1): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for this player
                     and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == last_round_0_indexed:
            return Action.D
        else:
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev_was_C = history.my_actions[-1]
            n_c_prev = num_opponent_cooperators_prev + my_action_prev_was_C
            if n_c_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness:

    This strategy aims to foster and maintain collective cooperation by adapting to
    observed behaviors, while safeguarding against persistent exploitation. It
    prioritizes establishing cooperation, reacting reciprocally to the group's
    actions, and attempting to re-establish cooperation if it breaks down.

    - Starts with cooperation in the first round.
    - Always defects in the final round.
    - In middle rounds:
        - If this player has defected for a set 'FORGIVENESS_INTERVAL' of consecutive rounds,
          it attempts to cooperate as a gesture of re-engagement.
        - Otherwise, it observes the proportion of cooperators in the previous round.
          If this proportion meets a 'COOPERATION_THRESHOLD', it cooperates.
          Otherwise, it defects to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.my_rounds_since_last_cooperation = 0
        self.COOPERATION_THRESHOLD = 0.5
        self.FORGIVENESS_INTERVAL = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        chosen_action: Action
        if current_round_1_indexed == 1:
            chosen_action = Action.C
            self.my_rounds_since_last_cooperation = 0
            return chosen_action
        if current_round_1_indexed == self.n_rounds:
            chosen_action = Action.D
            self.my_rounds_since_last_cooperation += 1
            return chosen_action
        if self.my_rounds_since_last_cooperation >= self.FORGIVENESS_INTERVAL:
            chosen_action = Action.C
            self.my_rounds_since_last_cooperation = 0
            return chosen_action
        else:
            total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= self.COOPERATION_THRESHOLD:
                chosen_action = Action.C
                self.my_rounds_since_last_cooperation = 0
            else:
                chosen_action = Action.D
                self.my_rounds_since_last_cooperation += 1
            return chosen_action

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    The "Adaptive Collective Sustenance" strategy aims to foster and maintain collective welfare
    by promoting cooperation, adapting to observed behavior, and protecting against persistent free-riding.
    It relies on game parameters and historical actions to guide decisions.

    - Round 1: Contributes (C) to probe for initial cooperation.
    - Intermediate Rounds (2 to r-1): Observes the total number of cooperators (`N_C_prev`)
      in the previous round. If `N_C_prev` meets or exceeds `floor(k)`, it contributes (C);
      otherwise, it defects (D). This adapts to the collective's contribution level,
      scaling its tolerance based on the collective benefit multiplier `k`.
    - Final Round (r): Defects (D) due to endgame rationality, preventing exploitation
      when no future interactions exist.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponents_prev_cooperations = sum(history.opponent_actions[-1, :])
        n_c_prev = int(my_prev_action_was_cooperate) + opponents_prev_cooperations
        if n_c_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy is a conditional cooperator
    that leverages game parameters to dynamically set expectations for collective
    participation. It proactively initiates cooperation and sustains it as long
    as the group demonstrates a sufficient commitment to the public good, defined
    by the multiplier 'k'. It defects in the last round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[-1]
        opponent_previous_cooperators_count = np.sum(history.opponent_actions[-1, :])
        total_contributions_prev = my_previous_action_was_cooperate + opponent_previous_cooperators_count
        if total_contributions_prev >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    Collective Adaptive Reciprocity strategy for N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating with a
    cooperative stance and then dynamically adjusting its contribution based on the
    overall level of cooperation observed within the group in the previous round.
    It incorporates self-protective measures against persistent free-riding and
    accounts for endgame effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r_current = state.round_number + 1
        if r_current == 1:
            return Action.C
        if r_current == self.n_rounds:
            return Action.D
        previous_round_index = state.round_number - 1
        my_action_prev_round = history.my_actions[previous_round_index]
        opponents_actions_prev_round = history.opponent_actions[previous_round_index, :]
        total_cooperators_prev_round = int(my_action_prev_round) + sum(opponents_actions_prev_round)
        if total_cooperators_prev_round >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.

    This strategy combines an initial show of trust, adaptive reciprocity based on
    collective group behavior, and a pragmatic endgame. It explicitly considers
    game parameters (n, k, r) and historical actions to make its decisions.

    Core Principles:
    - Initial Trust: Begin by cooperating to signal willingness and explore the group's intent.
    - Adaptive Reciprocity: React to the collective behavior of other players, adjusting
      tolerance for defection based on the inherent difficulty of cooperation (as determined
      by 'k' and 'n').
    - Self-Preservation in Endgame: Prevent exploitation in the final rounds when future
      reciprocity is no longer possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        num_others_contributed_prev = sum(history.opponent_actions[-1, :])
        free_rider_incentive_factor = 1.0 - self.k_factor / self.n_players
        base_threshold = (self.n_players - 1) * free_rider_incentive_factor
        threshold_for_others = max(1, math.floor(base_threshold))
        if num_others_contributed_prev >= threshold_for_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    The Adaptive Conditional Cooperator strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation while remaining
    robust to exploitation. It initiates cooperation, reciprocates cooperative
    behavior, and deters free-riding by defecting when cooperation levels drop.
    It defects in the final round to maximize individual payoff given the
    finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + history.my_actions[prev_round_idx]
        proportion_cooperators = num_total_cooperators_prev_round / self.n_players
        if proportion_cooperators >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    The Adaptive Collective Trust (ACT) strategy operates on the principle of conditional cooperation.
    It starts by extending trust to the group, contributing in the first round to signal a willingness to cooperate.
    In subsequent rounds, it evaluates the collective contribution from the previous round against a clear majority threshold.
    If a sufficient number of players contributed, ACT continues to contribute, rewarding the collective effort.
    If contributions fall below this threshold, ACT temporarily defects to signal dissatisfaction and incentivize renewed collective effort.
    In the final round, recognizing the absence of future interactions, ACT prioritizes individual self-interest,
    as is rational in a finite game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.floor(self.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_prev_action_val = 1 if history.my_actions[-1] else 0
        opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
        S_prev = my_prev_action_val + opponent_prev_cooperators
        if S_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    The Adaptive Conditional Contributor strategy aims to foster and sustain collective
    cooperation in the Public Goods Game. It initiates cooperation, reacts adaptively
    to group behavior based on a cooperation threshold from the previous round, and
    protects itself against exploitation in the final stage of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.cooperation_threshold_T = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        c_prev_round_opponents = sum(history.opponent_actions[-1, :])
        c_prev_round_me = 1 if history.my_actions[-1] else 0
        total_cooperators_prev_round = c_prev_round_opponents + c_prev_round_me
        if total_cooperators_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    Adaptive Conditional Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation.
    It initiates cooperation, responds reciprocally based on the cooperation levels
    of other players in the preceding round, and ensures self-protection by defecting
    in the final round. The threshold for cooperation is dynamically set based on
    the number of players to balance forgiveness with protection against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        if self.n_players == 2:
            self.threshold_c = 0.5
        elif self.n_players > 2:
            self.threshold_c = (self.n_players - 2) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        previous_round_history_index = state.round_number - 1
        num_other_cooperators_prev_round = sum(history.opponent_actions[previous_round_history_index, :])
        num_other_players = self.n_players - 1
        ocr = num_other_cooperators_prev_round / num_other_players
        if ocr >= self.threshold_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    The Adaptive Majority Cooperator (AMC) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It initiates cooperation in the first round,
    then adaptively responds to the observed level of collective contribution in
    previous rounds. It rewards sufficient group cooperation by continuing to contribute,
    and withdraws contributions when collective effort falters. In the final round,
    it defects based on backward induction to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with the game parameters and calculates
        the cooperation threshold.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Majority Cooperator strategy rules.

        Args:
            state: An object containing the current game state, including the
                   current round number.
            history: An object containing the history of actions and payoffs from
                     previous rounds. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if total_contributions_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to promote and sustain cooperation in the Public Goods Game.
    It initiates with cooperation, maintains it if a sufficient number of players (based on n/k) cooperated in the previous round,
    and defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        n_coop_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if n_coop_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation. It initiates
    cooperation in the first round, conditionally maintains it based on the
    observed behavior of the group against a k/n threshold, and protects itself
    in the final round by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_T = self.k / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        c_other_t_minus_1 = sum(previous_round_opponent_actions)
        n_minus_1 = self.n_players - 1
        gcs = c_other_t_minus_1 / n_minus_1
        if gcs >= self.threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    Adaptive Collective Reciprocity Strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by:
    1.  **Initiating Cooperation (Round 1):** Always contributes to signal a willingness to contribute.
    2.  **Adaptive Core Play (Rounds 2 to r-1):** Makes a decision (Cooperate or Defect) based on
        the proportion of cooperators in the immediately preceding round. This observed proportion
        is compared against a dynamic threshold (1 - 1/k), which scales with the collective
        benefit multiplier `k`. This encourages higher cooperation when the public good is more valuable.
    3.  **End-Game Defection (Final Round):** Defects in the last round to protect against exploitation,
        as there are no future interactions for retaliation or reward, a common robust practice
        in finite repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.t_coop = 1 - 1 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        p_c_prev = total_cooperators_prev_round / n_players
        if p_c_prev >= self.t_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Parameter-Adaptive Reciprocity (PAR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating trust
    and then adapting its behavior based on observed collective contribution levels,
    using a threshold derived from the game's parameters.

    Decision Rules:
    1. Round 1 (Initial Trust): Always Contribute (C).
    2. Middle Rounds (Round 2 to r-1):
       - Calculate a Cooperation Threshold (T_C = ceil(n / k)).
       - If the total number of cooperators in the previous round (N_C_prev)
         was greater than or equal to T_C, then Contribute (C).
       - Otherwise (N_C_prev < T_C), then Defect (D).
    3. Final Round (r): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        N_C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if N_C_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    Adaptive Consensus Builder: A strategy for the N-Player Public Goods Game
    that prioritizes fostering cooperation. It starts cooperatively, maintains
    cooperation if a majority of players contributed in the previous round,
    and defects in response to widespread defection or in the final round
    to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Consensus Builder strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Consensus Builder strategy rules.

        Args:
            state: The current state of the game, including the round number.
            history: The historical actions and payoffs from this player's perspective.
                     This is None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Reciprocal Collective Contributor (RCC) strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by:
    1.  Starting cooperatively to signal willingness.
    2.  Adapting behavior in intermediate rounds based on the observed collective
        defection level of other players in the previous round. It cooperates
        if less than or equal to half of the other players defected, and defects
        otherwise.
    3.  Defecting in the final round due to the absence of future interactions.

    This approach balances fostering cooperation with robustness against exploitation
    and adaptability to various group behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        num_others = n - 1
        num_cooperators_others_prev = sum(history.opponent_actions[current_round - 1, :])
        n_defectors_others_prev = num_others - num_cooperators_others_prev
        defection_threshold = math.floor(num_others / 2)
        if n_defectors_others_prev <= defection_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    The Adaptive Public Good Steward strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation, offers forgiveness for minor deviations,
    and punishes persistent free-riding, while anticipating the endgame effect.
    It dynamically adjusts its expectations based on game parameters (n, k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k_factor = game_description.k
        self.n_rounds = game_description.n_rounds
        self.consecutive_low_cooperation_rounds: int = 0
        self.cooperation_threshold: int = math.floor(self.n_players * (1.0 - 1.0 / self.k_factor))
        self.cooperation_threshold = max(1, self.cooperation_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        previous_round_total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_total_contributions >= self.cooperation_threshold:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        else:
            self.consecutive_low_cooperation_rounds += 1
            if self.consecutive_low_cooperation_rounds == 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by initially cooperating,
    punishing deviations from full collective contribution, and offering forgiveness
    when full cooperation is re-established. It adapts to the game's finite horizon
    by defecting in the final round.
    """

    class _ACRState(Enum):
        COOPERATE = 1
        PUNISH = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.current_state = self._ACRState.COOPERATE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_contributors_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if self.current_state == self._ACRState.COOPERATE:
            if num_contributors_last_round == self.n_players:
                return Action.C
            else:
                self.current_state = self._ACRState.PUNISH
                return Action.D
        elif self.current_state == self._ACRState.PUNISH:
            if num_contributors_last_round == self.n_players:
                self.current_state = self._ACRState.COOPERATE
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for N-Player Public Goods Game.

    This strategy aims to establish and maintain collective contribution by starting cooperatively,
    then monitoring the group's contribution levels. It cooperates if the average contribution
    from other players in the previous round meets a COOPERATION_THRESHOLD. If the threshold is not
    met, it defects as a temporary "punishment" or disincentive for free-riding.
    The strategy is forgiving, returning to cooperation as soon as the threshold is met again.
    It always defects in the final round of the game due to backward induction to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = 0.5
        self.current_stance = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            self.current_stance = Action.C
            return Action.C
        total_contributions_others_last_round = sum(history.opponent_actions[-1, :])
        n_others = n_players - 1
        avg_others_contribution_last_round = total_contributions_others_last_round / n_others
        if avg_others_contribution_last_round >= self.COOPERATION_THRESHOLD:
            self.current_stance = Action.C
        else:
            self.current_stance = Action.D
        return self.current_stance

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    This strategy fosters and sustains collective cooperation while remaining adaptive and robust.
    It operates on the principle of conditional cooperation, reacting to the overall group's past
    contributions. It is a willing initial cooperator, maintains cooperation as long as a
    significant portion of the group contributes, and protects against exploitation by defecting
    when collective contributions significantly decline, or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        my_prev_action_cooperated = history.my_actions[prev_round_index]
        opponent_prev_cooperators = sum(history.opponent_actions[prev_round_index, :])
        s_prev = int(my_prev_action_cooperated) + opponent_prev_cooperators
        cooperation_threshold = math.ceil(self.n_players / 2)
        if s_prev >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by:
    1. Cooperating in the first round to signal willingness.
    2. In intermediate rounds, reciprocates collective cooperation:
       - Continues to cooperate if the total contributions in the previous round
         met or exceeded the game's multiplier `k`.
       - Defects if contributions fell below `k`, as a signal and self-protection.
    3. Defects in the final round, acknowledging the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        N_C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if N_C_prev >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by adaptively
    responding to the overall group's behavior, while protecting itself from
    exploitation and acknowledging the finite nature of the game. It embodies a
    collective mindset by initiating cooperation and being forgiving, but also
    a pragmatic robustness against free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_stance: str = 'Cooperative'
        self.Cooperation_Threshold: float = max(0.5, self.game_description.k / self.game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        P_t_minus_1 = total_cooperators_prev_round / n_players
        if self.current_stance == 'Cooperative':
            if P_t_minus_1 < self.Cooperation_Threshold:
                self.current_stance = 'Defensive'
                return Action.D
            else:
                return Action.C
        elif P_t_minus_1 >= self.Cooperation_Threshold:
            self.current_stance = 'Cooperative'
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by initiating generosity,
    rewarding group contributions, forgiving minor lapses, and protecting against
    sustained exploitation. Its adaptability is tied to the game's parameters,
    specifically the multiplier `k`, and the observed history of play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.required_cooperators: int = max(1, math.floor(self.game_description.n_players - (self.game_description.k - 1)))
        self.forgiveness_rounds: int = 1
        self.defection_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1])
        my_action_prev_round = 1 if history.my_actions[current_round - 1] else 0
        c_prev = opponent_cooperators_prev_round + my_action_prev_round
        if c_prev >= self.required_cooperators:
            self.defection_streak = 0
            return Action.C
        else:
            self.defection_streak += 1
            if self.defection_streak <= self.forgiveness_rounds:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation. It begins with a
    cooperative action and then adaptively responds to the observed level of group
    cooperation. Its objective is to promote overall welfare within the group while
    protecting itself from exploitation by consistently uncooperative players.
    The strategy balances the pursuit of collective benefit with the need for
    robustness in a competitive tournament environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        c_prev_round_count = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        cooperation_rate = c_prev_round_count / n
        tp = max(0.5, 0.25 + k / (2 * n))
        min_cooperators = math.ceil(tp * n)
        if c_prev_round_count >= min_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by initiating
    a pro-social action, then conditionally reciprocating based on the group's
    observed behavior. It recognizes the finite nature of the game and protects
    against exploitation while encouraging a high level of collective contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Conditional Cooperator strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for the player
                     and opponents. It is None for the very first round (round 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if state.round_number == n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_prev_action = history.my_actions[previous_round_index]
        opponent_prev_actions = history.opponent_actions[previous_round_index, :]
        c_prev = int(my_prev_action) + np.sum(opponent_prev_actions)
        p_c = c_prev / n_players
        if p_c >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster cooperation
    by starting with an initial contribution, then adapting its behavior based on
    the collective contribution rate of other players. It will continue cooperating
    if a majority (or more) of others contributed in the previous round, otherwise it defects.
    In the final round, it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        num_other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = n_players - 1
        p_others_cooperated = num_other_cooperators_prev_round / num_other_players
        if p_others_cooperated >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy:
    Starts by cooperating, then conditionally cooperates based on the collective cooperation of others
    in the previous round, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.cooperation_threshold_others = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev >= self.cooperation_threshold_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    The "Adaptive Collective Cooperator" strategy aims to establish and sustain
    a high level of cooperation. It starts by cooperating in the first round,
    defects in the last round, and in intermediate rounds, its decision is
    adaptive: it cooperates if the total number of cooperators in the previous
    round met a calculated threshold (ceil(n * 0.6)), otherwise it defects.
    This strategy seeks to reward and reinforce collective cooperation while
    protecting itself from exploitation when cooperation levels are insufficient.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players * 0.6)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        if current_t == self.n_rounds:
            return Action.D
        if current_t == 1:
            return Action.C
        my_action_prev = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperators_prev = my_action_prev + opponent_cooperators_prev
        if num_cooperators_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator: This strategy promotes cooperation when the collective
    contribution to the public good is sufficiently high, and temporarily withdraws cooperation
    when participation falls below a critical threshold. It starts by signaling a willingness
    to cooperate and accounts for the final round's unique incentives.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        n = self.game_description.n_players
        k = self.game_description.k
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponents_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = (1 if my_prev_action_is_cooperate else 0) + opponents_prev_cooperators
        T = max(k, math.ceil(n / 2))
        if C_prev >= T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    The Parametric Adaptive Contributor strategy is designed to foster and sustain
    collective cooperation in the N-Player Public Goods Game. It adapts its behavior
    based on the game's parameters (n, r, k) and observed history, aiming for high
    collective welfare while remaining robust against exploitation.

    This strategy operates as follows:
    1.  **Initialization**: Calculates a dynamic `C_threshold` based on the Marginal Per Capita Return (MPCR).
        This threshold determines the minimum number of cooperators required in the previous round
        for this strategy to continue cooperating. A lower MPCR (harder to cooperate) demands a
        higher `C_threshold`, and vice-versa.
    2.  **Round 1**: Always contributes (C) to proactively signal willingness to cooperate.
    3.  **Intermediate Rounds (2 to r-1)**: Observes the total number of cooperators (`C_prev`)
        in the previous round. If `C_prev` meets or exceeds `C_threshold`, it contributes (C).
        Otherwise, it defects (D) to discourage free-riding and signal a need for greater collective effort.
    4.  **Final Round (r)**: Always defects (D), as there are no future consequences or opportunities for reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        mpcr = self.k / self.n
        required_cooperation_proportion = 1 - mpcr
        self.C_threshold = math.ceil(self.n * required_cooperation_proportion)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        my_action_prev_round = history.my_actions[current_round - 1]
        C_prev = num_opponent_cooperators_prev_round + my_action_prev_round
        if C_prev >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    The "Adaptive Collective Effort" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize overall group utility, including its own, by
    promoting widespread cooperation. It starts by demonstrating a willingness
    to contribute and then adjusts its behavior based on the group's past actions,
    ensuring it doesn't become a consistent free-rider victim.

    Decision Rules:
    1. Round 1 (Initiation): Always Contribute (C).
    2. Rounds 2 to r-1 (Adaptive Cooperation):
       - If total cooperators in the previous round (N_C_prev) >= ceil(n / 2), Contribute (C).
       - Else (N_C_prev < ceil(n / 2)), Defect (D).
    3. Round r (Final Round): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Effort strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the strategy rules.

        Args:
            state: An object containing the current game state, including round_number.
            history: An object containing past game actions and payoffs for all players.
                     Will be None for the very first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_val = int(history.my_actions[-1])
        N_C_prev = num_opponent_cooperators_prev + my_action_prev_val
        if N_C_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    The Adaptive Group Reciprocator (AGR) strategy promotes and sustains collective cooperation
    by starting with an initial cooperative stance, then dynamically adjusting its cooperation
    threshold based on the group's observed cooperation rate from the immediately preceding round.
    It becomes more lenient when high cooperation is observed and stricter when low cooperation
    occurs. To ensure robustness and prevent exploitation, especially in a tournament setting,
    the strategy explicitly defects in the final round of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.dynamic_cooperation_threshold: float = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action_val = 1 if history.my_actions[prev_round_idx] else 0
        opponent_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = my_prev_action_val + opponent_prev_cooperators
        group_cooperation_rate_prev = total_cooperators_prev_round / n_players
        action_for_current_round: Action
        if group_cooperation_rate_prev >= self.dynamic_cooperation_threshold:
            action_for_current_round = Action.C
        else:
            action_for_current_round = Action.D
        if group_cooperation_rate_prev >= 0.75:
            self.dynamic_cooperation_threshold = max(0.4, self.dynamic_cooperation_threshold - 0.05)
        elif group_cooperation_rate_prev < 0.5:
            self.dynamic_cooperation_threshold = min(0.9, self.dynamic_cooperation_threshold + 0.1)
        return action_for_current_round

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    This strategy aims to foster and sustain collective cooperation by initiating goodwill, 
    adaptively responding to the group's contribution levels, and protecting against exploitation, 
    especially in the game's final stages. It is designed to encourage beneficial collective 
    outcomes without being a naive cooperator.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed >= total_rounds - 1:
            return Action.D
        W = min(5, state.round_number)
        total_contributions_others_over_W_rounds = np.sum(history.opponent_actions[state.round_number - W:state.round_number, :])
        n_others = n_players - 1
        P_others_avg = total_contributions_others_over_W_rounds / (W * n_others)
        if P_others_avg >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    The Conditional Collectiveist (CC) strategy for the N-Player Public Goods Game.

    This strategy is designed to foster and maintain collective cooperation by adapting
    its behavior based on the observed actions of the group in previous rounds, while
    also protecting against exploitation.

    Decision Rules:
    1.  First Round: Contributes (C) to initiate cooperation.
    2.  Intermediate Rounds: Observes the previous round's actions. If at least
        `ceil(n / 2.0)` players (including self) contributed, it contributes (C).
        Otherwise, it defects (D) to prevent exploitation.
    3.  Last Round: Defects (D) based on backward induction, as there are no
        future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        num_cooperators_previous_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by:
    1.  Initiating cooperation in the first round to signal willingness.
    2.  Conditionally cooperating in intermediate rounds: contributing if the
        total contributions in the previous round met or exceeded the game's
        multiplication factor `k`, otherwise defecting. This uses `k` as a
        threshold for sufficient collective effort.
    3.  Defecting in the final round as a rational exit strategy due to the
        absence of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the
                   current round number.
            history: An object containing the history of actions and payoffs
                     from previous rounds, or None if it's the first round.

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if total_contributions_prev_round >= self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.

    This strategy initiates cooperation in the first round as a trusting gesture.
    In intermediate rounds, it sustains collective contributions as long as a
    simple majority of other players contributed in the previous round, fostering
    collective action while tolerating some free-riding. In the final round,
    it defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        if self.n_players > 1:
            self.cooperation_threshold = math.ceil((self.n_players - 1) / 2)
        else:
            self.cooperation_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        n_other_c = sum(history.opponent_actions[-1, :])
        if n_other_c >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    The Adaptive Reciprocal Contributor (ARC) strategy aims to foster and sustain high
    levels of cooperation throughout the repeated Public Goods Game. It begins by
    offering cooperation as an olive branch and then dynamically adapts its behavior
    based on the collective actions observed in the previous round. It uses a specific
    tolerance threshold to decide when to reciprocate cooperation and when to signal
    the cost of widespread defection, while pragmatically protecting itself in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        if self.n_players == 2:
            self.cooperation_tolerance_threshold = 0.0
        elif self.n_players > 2:
            self.cooperation_tolerance_threshold = 1.0 / float(self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_cooperated_prev = history.my_actions[-1]
        opponent_cooperated_count_prev = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev = my_cooperated_prev + opponent_cooperated_count_prev
        total_defectors_prev = self.n_players - total_cooperators_prev
        P_d_prev = float(total_defectors_prev) / float(self.n_players)
        if P_d_prev <= self.cooperation_tolerance_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy makes decisions based on the current
    round number and the observed cooperation rate in the previous round. It starts cooperative,
    rewards collective cooperation, punishes defection, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = current_round_number - 1
            my_prev_action_was_C = history.my_actions[prev_round_idx]
            opponents_prev_actions = history.opponent_actions[prev_round_idx, :]
            total_cooperators_prev_round = int(my_prev_action_was_C) + np.sum(opponents_prev_actions)
            cooperation_rate = total_cooperators_prev_round / n_players
            if cooperation_rate >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.
    It aims to foster and sustain collective cooperation by starting cooperatively,
    responding to observed behavior with a mix of generosity and punishment,
    and adapting its endgame play based on the group's historical cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        t = current_round_0_indexed + 1
        if t == self.r:
            return Action.D
        if t == self.r - 1 and self.r >= 3:
            num_rounds_for_avg = self.r - 2
            historical_opponent_actions_slice = history.opponent_actions[0:num_rounds_for_avg, :]
            cooperation_rates_per_opponent = []
            for player_idx in range(self.n - 1):
                player_history_slice = historical_opponent_actions_slice[:, player_idx]
                P_j_overall = np.mean(player_history_slice)
                cooperation_rates_per_opponent.append(P_j_overall)
            Avg_P_others_overall = np.mean(cooperation_rates_per_opponent)
            if Avg_P_others_overall >= 0.75:
                return Action.C
            else:
                return Action.D
        if t == 1:
            return Action.C
        if t == 2:
            return Action.C
        last_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        num_other_cooperators_last_round = np.sum(last_round_opponent_actions)
        num_other_defectors_last_round = self.n - 1 - num_other_cooperators_last_round
        if num_other_defectors_last_round == 0:
            return Action.C
        elif num_other_defectors_last_round == self.n - 1:
            return Action.D
        else:
            defection_proportion = num_other_defectors_last_round / (self.n - 1)
            forgiveness_threshold = 0.5
            if defection_proportion < forgiveness_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    Adaptive Reciprocal Contributor (ARC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster high levels of cooperation within the Public Goods Game,
    maximizing collective welfare over the long run, while being robust against exploitation
    by other players. It operates on principles of conditional cooperation, signalling
    an initial willingness to contribute, and defecting in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if total_contributions_prev_round >= self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by being responsive to the observed
    behavior of all players, rewarding collective effort, punishing widespread defection, and
    offering opportunities for recovery. It starts with an assumption of goodwill (cooperate)
    and then adapts based on a cooperation threshold. It also incorporates a forgiveness mechanism
    to attempt to restart cooperation after severe breakdowns, but avoids naive exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD: float = 0.65
        self.FORGIVENESS_THRESHOLD: float = 0.25
        self.last_round_met_cooperation_threshold: bool = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if n_rounds == 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :]) + history.my_actions[current_round_0_indexed - 1]
        C_prev_round = int(total_cooperators_prev_round)
        P_coop_prev = C_prev_round / n_players
        if P_coop_prev >= self.COOPERATION_THRESHOLD:
            self.last_round_met_cooperation_threshold = True
            return Action.C
        elif self.FORGIVENESS_THRESHOLD <= P_coop_prev < self.COOPERATION_THRESHOLD:
            self.last_round_met_cooperation_threshold = False
            return Action.D
        elif P_coop_prev < self.FORGIVENESS_THRESHOLD:
            if self.last_round_met_cooperation_threshold:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
            self.last_round_met_cooperation_threshold = False
            return action_to_take
        return Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.

    This strategy is designed to foster and maintain cooperation by initiating trust,
    monitoring the collective level of contribution, and adaptively responding to
    ensure a fair and mutually beneficial outcome, while also protecting itself
    from exploitation. It aims to achieve the higher collective welfare derived
    from widespread cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = (1 + self.k / self.n_players) / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        total_contributions_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        P_contributed = total_contributions_prev_round / self.n_players
        if P_contributed >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy combines initial cooperation, conditional reciprocity based on a collective
    cooperation threshold, and a forgiving, temporary punishment mechanism, along with a
    rational defection in the final round. It aims to promote and sustain cooperation while
    being robust against various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_mode_active: bool = False
        self.punishment_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_previous_action = history.my_actions[state.round_number - 1]
            opponents_previous_actions = history.opponent_actions[state.round_number - 1, :]
            previous_total_contributions = my_previous_action + np.sum(opponents_previous_actions)
            if self.punishment_mode_active:
                action_for_current_round = Action.D
                self.punishment_rounds_remaining -= 1
                if self.punishment_rounds_remaining == 0:
                    self.punishment_mode_active = False
                return action_for_current_round
            else:
                n = self.game_description.n_players
                cooperation_threshold_T = max(2, math.floor(n / 2) + 1)
                if previous_total_contributions >= cooperation_threshold_T:
                    return Action.C
                else:
                    action_for_current_round = Action.D
                    self.punishment_mode_active = True
                    self.punishment_rounds_remaining = 2
                    return action_for_current_round

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    Adaptive Collective PGG (AC-PGG) strategy.

    This strategy aims to foster and sustain collective cooperation while being
    robust against various opponent behaviors and recognizing the game's
    structural incentives. It initiates cooperation in the first round (if
    not a one-shot game), adaptively responds to the group's contribution
    levels in subsequent rounds, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy by storing game parameters and calculating
        the dynamic cooperation threshold, T_cooperate.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        k = self.game_description.k
        self.T_cooperate = max(0.5, 1 / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the AC-PGG strategy rules and the game state/history.

        Args:
            state: The current state of the game, including the current round number.
            history: An object containing the history of actions and payoffs for
                     this player and opponents in previous rounds, or None for round 0.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == total_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        n_cooperators_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        n_cooperators_my_action_prev_round = int(history.my_actions[-1])
        total_cooperators_prev_round = n_cooperators_opponents_prev_round + n_cooperators_my_action_prev_round
        P_C_prev_round = total_cooperators_prev_round / n_players
        if P_C_prev_round >= self.T_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    Adaptive Collective Engagement (ACE) strategy for N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation while
    protecting against persistent exploitation. It adapts to observed group
    behavior by initiating cooperation, rewarding widespread cooperation,
    sanctioning widespread defection, and defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Collective Engagement (ACE) strategy rules.

        Args:
            state: An object containing the current game state, including
                   the current round number (0-indexed).
            history: An object containing the history of actions and payoffs
                     from previous rounds. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        n_cooperators_others_prev = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.game_description.n_players - 1
        p_cooperators_others_prev = n_cooperators_others_prev / num_other_players
        if p_cooperators_others_prev >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to establish and maintain a high level of cooperation. It demonstrates
    an initial willingness to contribute, then reciprocates the observed cooperation
    levels from other players, and strategically defects in the final round to prevent
    exploitation.

    Decision Rules:
    1. Round 1 (t=1, 0-indexed round 0): Always CONTRIBUTE (C) to signal trust.
    2. Intermediate Rounds (1 < t < r, 0-indexed rounds 1 to r-2):
       - Observe the proportion of other players who contributed in the previous round (P_coop).
       - If P_coop >= 0.5, then CONTRIBUTE (C).
       - Else (P_coop < 0.5), then DEFECT (D).
    3. Last Round (t=r, 0-indexed round r-1): Always DEFECT (D) due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        sum_other_contributions_prev_round = sum(history.opponent_actions[-1, :])
        num_other_players = num_players - 1
        p_coop_prev_round = sum_other_contributions_prev_round / num_other_players
        if p_coop_prev_round >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    This strategy, named "Adaptive Reciprocity with End-Game Defection,"
    is designed to foster and sustain collective cooperation in the Public Goods Game.
    It aims to maximize long-term collective welfare by initiating cooperation,
    rewarding sufficient group contributions, and punishing widespread defection,
    while remaining robust to various opponent behaviors.

    The strategy's decision rules are:
    1. For Round 1 (0-indexed round 0): Always Contribute (C).
    2. For Intermediate Rounds (0-indexed rounds 1 to r-2):
        a. Observe `S_prev`, the total number of cooperators in the previous round.
        b. Calculate `Threshold_Cooperators = ceil(n / k)`.
        c. If `S_prev >= Threshold_Cooperators`: Contribute (C).
        d. Else: Defect (D).
    3. For the Final Round (0-indexed round r-1): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)
        self.total_rounds = self.game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.total_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponent_prev_actions_cooperate = history.opponent_actions[-1, :]
        s_prev = int(my_prev_action_was_cooperate) + sum(opponent_prev_actions_cooperate)
        if s_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy aims to foster and sustain collective cooperation
    throughout the Public Goods Game. It starts by cooperating, then dynamically adjusts its behavior
    based on the observed overall level of cooperation in the group from the previous round.
    It incorporates a collective "punishment" mechanism to discourage widespread free-riding
    and strategically defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_ratio = 0.5
        self.end_game_horizon = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx >= n_rounds - self.end_game_horizon:
            return Action.D
        my_prev_action_was_C = history.my_actions[current_round_idx - 1]
        opponents_prev_cooperation_count = sum(history.opponent_actions[current_round_idx - 1, :])
        total_cooperators_prev_round = int(my_prev_action_was_C) + opponents_prev_cooperation_count
        observed_cooperation_ratio = total_cooperators_prev_round / n_players
        if observed_cooperation_ratio >= self.cooperation_threshold_ratio:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    Adaptive Collective Reciprocity (with Peer-Based Threshold) strategy for the N-Player Public Goods Game.

    This strategy balances the desire for collective welfare with protection against individual exploitation.
    It initiates cooperation in the first round, then adapts its behavior based on the observed
    actions of other players in subsequent rounds, aiming for a high collective payoff.
    It recognizes that sustaining cooperation requires mutual effort and will adjust its
    actions accordingly, ultimately defecting in the final round due to rational end-game considerations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_others = math.ceil((self.game_description.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.game_description.n_rounds - 1:
            return Action.D
        n_c_others_in_prev_round = sum(history.opponent_actions[-1, :])
        if n_c_others_in_prev_round >= self.cooperation_threshold_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    Adaptive k-Threshold Reciprocity strategy for the N-Player Public Goods Game.

    This strategy balances individual incentives with collective benefit by adapting
    its cooperation threshold based on the public good's efficiency (k) and
    the observed level of cooperation in the previous round.
    It cooperates in the first round, defects in the last, and adjusts
    behavior in intermediate rounds based on a dynamic threshold T.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive k-Threshold Reciprocity strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = max(1, self.n_players - math.floor(self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs. Will be None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_prev_cooperation = np.sum(history.my_actions[current_round_number - 1])
        opponent_prev_cooperations = np.sum(history.opponent_actions[current_round_number - 1, :])
        C_prev = my_prev_cooperation + opponent_prev_cooperations
        if C_prev >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    Adaptive Collective Steward strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation within the group.
    It balances the ambition for high collective payoffs with a pragmatic need to avoid exploitation,
    adapting its behavior based on the observed willingness of the group to contribute to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Steward strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number (0-indexed).
            history: Historical actions and payoffs for the player and opponents.
                     This will be None for the first round (state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if self.n_rounds == 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_0_indexed = current_round_0_indexed - 1
        my_action_prev = history.my_actions[prev_round_0_indexed]
        opponent_actions_prev = history.opponent_actions[prev_round_0_indexed, :]
        C_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        group_cooperation_rate = C_prev / self.n_players
        tolerated_defectors = max(1, math.floor(self.n_players / 4))
        Cooperation_Threshold = (self.n_players - tolerated_defectors) / self.n_players
        if group_cooperation_rate >= Cooperation_Threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain collective cooperation while being
    robust against exploitation and adaptable to varying group sizes. It balances an
    initial cooperative stance with conditional responses to defection, aiming for
    the highest possible collective welfare within the tournament context.

    Decision Rules:
    1. Round 1 (state.round_number == 0): Always Contribute (C).
       Rationale: Signal willingness to cooperate and set a positive precedent.

    2. Adaptive Phase (Rounds 2 to r-1):
       Observe total defectors (D_prev_round) in the previous round.
       - If D_prev_round == 0 (Full Cooperation): Contribute (C).
         Rationale: Maintain optimal collective outcome.
       - If D_prev_round == 1 (One Defector):
         - If n_players > 2: Contribute (C).
           Rationale: Forgive a single defector to promote robust cooperation,
                      preventing collapse from minor deviations.
         - If n_players == 2: Defect (D).
           Rationale: In a 2-player game, single defection means the only
                      other player defected, which would be exploitation.
       - If D_prev_round >= 2 (Two or More Defectors): Defect (D).
         Rationale: Punish significant free-riding to discourage widespread defection.

    3. Last Round (state.round_number == n_rounds - 1): Always Defect (D).
       Rationale: Rational play dictates defection to maximize immediate gain
                  as there are no future consequences. This prevents exploitation.

    Edge Case (r=1): If n_rounds == 1, state.round_number will be 0. The first
    round rule (always C) takes precedence, as per the strategy description,
    resolving the potential conflict with the 'last round' rule.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_cooperation_prev_round = history.my_actions[-1]
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_cooperation_prev_round else 0)
        total_defectors_prev_round = self.n_players - total_cooperators_prev_round
        if total_defectors_prev_round == 0:
            return Action.C
        elif total_defectors_prev_round == 1:
            if self.n_players > 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Forgiveness strategy for the N-Player Public Goods Game.

    This strategy aims to encourage and sustain cooperation by starting with cooperation,
    adaptively responding to collective contribution levels, and providing a mechanism for
    forgiveness. It prioritizes collective welfare but includes a final-round defection
    to prevent exploitation in finite games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_stance: bool = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            self.current_stance = True
            return Action.C
        if current_round_idx == self.r - 1:
            return Action.D
        total_contributions_prev = int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))
        if self.current_stance:
            T_C = max(2, math.floor(self.n * 0.75))
            if total_contributions_prev >= T_C:
                return Action.C
            else:
                self.current_stance = False
                return Action.D
        else:
            T_F = self.n
            if total_contributions_prev == T_F:
                self.current_stance = True
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain high levels of cooperation by:
    1.  Starting with an optimistic and cooperative stance for the first two rounds.
    2.  Adapting its behavior based on the collective cooperation rate of other players
        in the previous round during intermediate rounds. If at least 50% of others
        cooperated, it cooperates; otherwise, it defects to deter free-riding.
    3.  Anticipating end-game rationality by defecting in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1 or current_round_t == 2:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        n_others = self.n_players - 1
        avg_contrib_rate_others = c_others_prev / n_others
        if avg_contrib_rate_others >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by initiating cooperation,
    rewarding sufficient collective effort, and deterring exploitation. It adapts
    its behavior based on a dynamic cooperation threshold derived from game
    parameters (n and k), ensuring that contributions are tied to the efficiency
    of the public good itself. It defects in the final round based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_multiplier)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_contribution_prev_round = history.my_actions[current_round_number - 1]
        opponent_contributions_prev_round = history.opponent_actions[current_round_number - 1, :]
        total_cooperators_prev_round = int(my_contribution_prev_round) + sum(opponent_contributions_prev_round)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    Adaptive Collective Enforcement (ACE) strategy for the N-Player Public Goods Game.
    This strategy aims to establish and maintain full collective cooperation through
    proactive signals, punitive defection, and end-game adaptation.
    """

    class GroupCooperationStateEnum(Enum):
        COOPERATE = 1
        PUNISH = 2
    _PUNISHMENT_DURATION_DEFAULT = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r_total = game_description.n_rounds
        self.PUNISHMENT_DURATION = self._PUNISHMENT_DURATION_DEFAULT
        self._end_game_start_round = self.r_total - self.PUNISHMENT_DURATION
        self.GroupCooperationState = self.GroupCooperationStateEnum.COOPERATE
        self.PunishmentRoundsRemaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number > 0:
            total_contributions_last_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if self.GroupCooperationState == self.GroupCooperationStateEnum.PUNISH:
                self.PunishmentRoundsRemaining -= 1
                if self.PunishmentRoundsRemaining == 0:
                    self.GroupCooperationState = self.GroupCooperationStateEnum.COOPERATE
            is_last_round_within_punishable_period = current_round_number - 1 < self._end_game_start_round
            if self.GroupCooperationState == self.GroupCooperationStateEnum.COOPERATE and is_last_round_within_punishable_period:
                if total_contributions_last_round < self.n_players:
                    self.GroupCooperationState = self.GroupCooperationStateEnum.PUNISH
                    self.PunishmentRoundsRemaining = self.PUNISHMENT_DURATION
        if current_round_number >= self._end_game_start_round:
            return Action.D
        elif self.GroupCooperationState == self.GroupCooperationStateEnum.COOPERATE:
            return Action.C
        elif self.GroupCooperationState == self.GroupCooperationStateEnum.PUNISH:
            return Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    Adaptive Collective Strategy with Majority Threshold:

    This strategy aims to foster and sustain collective cooperation while protecting
    individual interests from exploitation. It prioritizes the collective good by
    initiating cooperation and rewarding widespread participation, but it also
    enforces a norm by defecting when collective effort is insufficient.

    Decision Rules:
    1.  First Round (Round 0): Contribute (C) to initiate cooperation.
    2.  Intermediate Rounds: Observe the total number of cooperators from the previous
        round (C_past). If C_past is a strict majority (floor(n/2) + 1 or more),
        then Contribute (C). Otherwise, Defect (D) to punish insufficient collective effort.
    3.  Last Round: Defect (D) due to backward induction, as there are no future
        interactions to influence.

    Edge Cases:
    -   If the game length is 1 round, the "Last Round" rule takes precedence,
        and the strategy will Defect (D).
    -   The majority threshold (floor(n/2) + 1) adapts naturally for small 'n'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.majority_threshold = math.floor(self.game_description.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_round_cooperators >= self.majority_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    Initiates with cooperation, then adapts based on the previous round's collective
    contribution. Requires a substantial majority (75%) of players to contribute
    to continue its own contribution. Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.required_cooperators_threshold = math.floor(self.game_description.n_players * 0.75)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_idx - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        opponent_prev_cooperators = sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = my_prev_action_was_C + opponent_prev_cooperators
        if total_cooperators_prev_round >= self.required_cooperators_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy.

    This strategy aims to establish and maintain a cooperative equilibrium by
    initiating cooperation and then adapting its behavior based on the observed
    collective contribution rate among other players. It seeks to encourage
    mutual benefit (collective welfare) while protecting against exploitation.

    Decision Rules:
    1. First Round (t=1): Always Contribute (C).
    2. Intermediate Rounds (1 < t < r):
       - Observe the cooperation rate among other players in the immediately
         preceding round (t-1).
       - If this rate is 0.5 or higher, Contribute (C).
       - Otherwise (rate < 0.5), Defect (D).
    3. Last Round (t=r): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        according to the Adaptive Collective Reciprocity (ACR) strategy.

        Args:
            state: The current state of the game, including the current round number.
                   `state.round_number` is 0-indexed.
            history: The history of actions and payoffs from previous rounds.
                     This will be `None` for the very first round (round_number = 0).

        Returns:
            An `Action` enum member (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_other_players = self.game_description.n_players - 1
            others_contributed_count = sum(history.opponent_actions[-1, :])
            cooperation_rate_among_others = others_contributed_count / num_other_players
            if cooperation_rate_among_others >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    Adaptive Group Reciprocity with Forgiveness strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare by promoting cooperation while protecting
    against exploitation. It starts with cooperation, rewards continued high collective
    cooperation (tolerating up to one defector), punishes significant free-riding by defecting,
    and defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[-1]
        opponent_previous_cooperators_count = np.sum(history.opponent_actions[-1, :])
        c_t_minus_1 = int(my_previous_action_was_cooperate) + int(opponent_previous_cooperators_count)
        if c_t_minus_1 >= self.n_players - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Dynamic Collective Reciprocity (DCR) strategy for the N-Player Public Goods Game.

    This strategy promotes cooperation by starting with good faith, exhibiting conditional
    generosity, proportional punishment, and a clear exit strategy for the endgame.
    It balances fostering collective welfare with self-preservation against exploitation,
    aiming to guide the group towards sustained high contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.INITIAL_COOPERATION_THRESHOLD = 0.7
        self.PATIENCE_ROUNDS_BEFORE_PUNISH = max(2, math.floor(self.r / 5))
        self.FORGIVENESS_COOPERATION_RATE = 0.9
        self.current_mode = 'Cooperate_Mode'
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        total_cooperators_last_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        p_cooperators_last_round = total_cooperators_last_round / self.n
        if self.current_mode == 'Cooperate_Mode':
            if p_cooperators_last_round >= self.INITIAL_COOPERATION_THRESHOLD:
                self.consecutive_low_cooperation_rounds = 0
                return Action.C
            else:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds >= self.PATIENCE_ROUNDS_BEFORE_PUNISH:
                    self.current_mode = 'Punish_Mode'
                    return Action.D
                else:
                    return Action.C
        elif self.current_mode == 'Punish_Mode':
            if p_cooperators_last_round >= self.FORGIVENESS_COOPERATION_RATE:
                self.current_mode = 'Cooperate_Mode'
                self.consecutive_low_cooperation_rounds = 0
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    Dynamic Collective Reciprocator: Fosters cooperation using conditional cooperation,
    patience (Forgiveness_Factor), and end-game rationality. It adapts its optimism
    threshold based on game parameters (k/n) and historical cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_ECR: float = 1.0
        self.F: float = 0.7
        self.BOT: float = 0.4
        self.K_N_Factor_Weight: float = 0.1
        self.k_n_ratio: float = self.game_description.k / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        elif current_round_t == 1:
            return Action.C
        my_last_action_was_C = history.my_actions[-1]
        opponent_last_actions_were_C = history.opponent_actions[-1, :]
        total_cooperators_last_round = int(my_last_action_was_C) + np.sum(opponent_last_actions_were_C)
        ocr_t_minus_1 = total_cooperators_last_round / self.game_description.n_players
        self.current_ECR = self.F * self.current_ECR + (1 - self.F) * ocr_t_minus_1
        aot = max(self.BOT, self.k_n_ratio + self.K_N_Factor_Weight)
        if ocr_t_minus_1 >= self.k_n_ratio:
            return Action.C
        elif self.current_ECR >= aot:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    Forgiving Collective Reciprocity (FCR) strategy for the N-Player Public Goods Game.
    This strategy aims to establish and maintain full cooperation by starting cooperatively,
    demanding unanimous cooperation from the group to continue cooperating, and defecting
    if any player defects. It is "forgiving" in that it immediately reverts to cooperation
    if universal cooperation is re-established, allowing the group to recover from breakdowns.
    In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the FCR strategy with the game's parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the FCR strategy rules.

        Args:
            state: The current state of the game (e.g., round number).
            history: A PlayerHistory object containing past actions and payoffs
                     from this player's perspective. Is None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        total_contributions_last_round = history.my_actions[-1] + sum(history.opponent_actions[-1, :])
        if total_contributions_last_round == n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare by adapting its cooperation threshold
    based on recent group contribution levels. It begins with a cooperative signal in the
    first round. In subsequent rounds (up to the last), it dynamically adjusts its
    cooperation threshold: becoming more lenient with sustained high cooperation
    (lowering the threshold) and stricter with sustained low cooperation (raising it).
    This adaptive mechanism encourages and sustains collective action. In the final
    round, it reverts to individual rationality by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold: float = 0.6
        self.consecutive_high_cooperation_rounds: int = 0
        self.consecutive_low_cooperation_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n_players = self.game_description.n_players
        n_total_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == n_total_rounds - 1:
            return Action.D
        previous_round_total_cooperators = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        P_prev = previous_round_total_cooperators / n_players
        if P_prev >= 0.9:
            self.consecutive_high_cooperation_rounds += 1
            self.consecutive_low_cooperation_rounds = 0
            if self.consecutive_high_cooperation_rounds >= 3:
                self.cooperation_threshold -= 0.05
                self.cooperation_threshold = max(self.cooperation_threshold, 0.3)
                self.consecutive_high_cooperation_rounds = 0
        elif P_prev <= 0.3:
            self.consecutive_low_cooperation_rounds += 1
            self.consecutive_high_cooperation_rounds = 0
            if self.consecutive_low_cooperation_rounds >= 3:
                self.cooperation_threshold += 0.05
                self.cooperation_threshold = min(self.cooperation_threshold, 0.8)
                self.consecutive_low_cooperation_rounds = 0
        else:
            self.consecutive_high_cooperation_rounds = 0
            self.consecutive_low_cooperation_rounds = 0
        if P_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    Adaptive Proportional Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by adaptively responding to the
    collective behavior observed in previous rounds, while also protecting
    itself from exploitation. It starts with cooperation, adapts its behavior
    based on a 'cooperation threshold' derived from the game's efficiency (k),
    and defects in the final round to avoid end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop = 1.0 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == r - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        proportion_of_cooperators_prev_round = total_cooperators_prev_round / n
        if proportion_of_cooperators_prev_round >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster and sustain collective cooperation
    within the Public Goods Game. It operates on the principle of conditional cooperation:
    demonstrating an initial willingness to contribute and then adapting its behavior based on the
    observed collective contribution level of the group in previous rounds. This approach seeks to
    maximize collective welfare when possible, while pragmatically protecting against exploitation
    when collective contributions are insufficient.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_contribution = int(history.my_actions[state.round_number - 1])
        num_opponent_contributions_previous = np.sum(history.opponent_actions[state.round_number - 1, :])
        c_previous = my_previous_contribution + num_opponent_contributions_previous
        if c_previous >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    Adaptive Reciprocity (AR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by initiating with a
    contribution in the first round. In subsequent intermediate rounds, it
    observes the collective cooperation level from the previous round. If the
    proportion of cooperators meets or exceeds a defined threshold (e.g., 60%),
    the strategy reciprocates by contributing. Otherwise, it defects to signal
    disapproval of insufficient cooperation. In the final round of the game,
    it always defects, recognizing the absence of future interactions.
    """
    _DEFAULT_COOPERATION_THRESHOLD: float = 0.6

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.__class__._DEFAULT_COOPERATION_THRESHOLD

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        my_prev_action_count = 1 if history.my_actions[-1] else 0
        opponent_prev_action_count = np.sum(history.opponent_actions[-1, :])
        s_prev = my_prev_action_count + opponent_prev_action_count
        if s_prev / n_players >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation. It starts by cooperating
    to signal willingness. In subsequent rounds, it adaptively responds to the
    observed collective behavior: it cooperates if a clear majority (more than
    half) cooperated in the previous round, and defects otherwise. In the final
    round, it defects based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        else:
            return Action.D
        cooperation_threshold_value = self.game_description.n_players / 2.0
        if total_cooperators_previous_round > cooperation_threshold_value:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.
    This strategy dictates actions based on the observed collective contribution level
    in the previous round, aiming to foster and sustain cooperation.
    It contributes in the first round, defects in the last round (due to backward induction),
    and in intermediate rounds, it cooperates if the total number of
    cooperators in the previous round met or exceeded a dynamic threshold T = ceil(k).
    Otherwise, it defects to punish insufficient collective effort.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        k_multiplier = self.game_description.k
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        my_previous_action_cooperated = history.my_actions[-1]
        opponent_previous_cooperators = sum(history.opponent_actions[-1, :])
        N_C_prev = (1 if my_previous_action_cooperated else 0) + opponent_previous_cooperators
        T = math.ceil(k_multiplier)
        if N_C_prev >= T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    Adaptive Free-Rider with Revival Mechanic strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily defecting and free-riding
    on the contributions of others. It strategically contributes only when cooperation
    levels are critically low and a cooldown period has passed, with the goal of
    "re-seeding" cooperation for future exploitation, not genuine altruism. It also
    unconditionally defects in the endgame phase due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.Exploitation_Threshold = 0.4
        self.Revival_Threshold = 0.1
        self.Revival_Cooldown = max(3, math.floor(n / 3))
        self.Endgame_Rounds = max(1, math.floor(r / 4))
        self.rounds_since_my_last_contribution = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            self.rounds_since_my_last_contribution = 1
            return Action.D
        if current_round_0_indexed >= r - self.Endgame_Rounds:
            self.rounds_since_my_last_contribution += 1
            return Action.D
        N_others_contributed_last_round = sum(history.opponent_actions[-1, :])
        APCR = N_others_contributed_last_round / (n - 1)
        if APCR >= self.Exploitation_Threshold:
            self.rounds_since_my_last_contribution += 1
            return Action.D
        elif APCR < self.Revival_Threshold and self.rounds_since_my_last_contribution >= self.Revival_Cooldown:
            self.rounds_since_my_last_contribution = 0
            return Action.C
        else:
            self.rounds_since_my_last_contribution += 1
            return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy prioritizes free-riding and strategically contributes only when necessary
    to prevent a complete collapse of the public good or to maintain unpredictability,
    always reverting to defection when opportunities arise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action: Action = Action.D
        self.my_defection_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        action_to_take: Action = Action.D
        others_contributions_prev_round: int = 0
        others_avg_contribution_rate: float = 0.0
        if state.round_number > 0:
            others_contributions_prev_round = int(np.sum(history.opponent_actions[state.round_number - 1, :]))
            num_other_players = self.game_description.n_players - 1
            if num_other_players > 0:
                others_avg_contribution_rate = others_contributions_prev_round / num_other_players
            if self.my_previous_action == Action.D:
                self.my_defection_streak += 1
            else:
                self.my_defection_streak = 0
        if state.round_number == self.game_description.n_rounds - 1:
            action_to_take = Action.D
        elif state.round_number == 0:
            action_to_take = Action.D
        elif others_contributions_prev_round == 0:
            action_to_take = Action.D
        elif others_avg_contribution_rate >= 0.7:
            action_to_take = Action.D
        elif others_avg_contribution_rate < 0.3 and self.my_defection_streak >= 3:
            action_to_take = Action.C
        elif 0.3 <= others_avg_contribution_rate < 0.7 and self.my_defection_streak >= 5:
            if random.random() < 0.3:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        else:
            action_to_take = Action.D
        self.my_previous_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by defaulting to defection,
    while judiciously making strategic contributions only when there is a calculated
    expectation of inducing or maintaining a higher level of cooperation from other players
    that can then be free-ridden upon. It adapts to opponent behavior and game parameters,
    including handling edge cases.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.N_short = max(2, min(self.n, 5))
        self.N_long = max(5, int(min(self.r / 2, 2 * self.n)))
        self.ReseedAttemptsCount = 0
        self.MaxReseedAttempts = int(min(self.r / 5, 3))
        self.k_n_ratio = self.k / self.n
        self.extreme_k_n_ratio = self.k_n_ratio < 0.2

    def _get_opponent_coop_rate(self, window_size: int, history: PlayerHistory, current_round_idx: int) -> float:
        """
        Calculates the average contribution rate of opponents over a specified window of rounds.
        Handles cases with no history or empty windows.
        """
        if current_round_idx == 0 or window_size == 0:
            return 0.0
        start_idx = max(0, current_round_idx - window_size)
        relevant_opponent_actions = history.opponent_actions[start_idx:current_round_idx, :]
        if relevant_opponent_actions.size == 0:
            return 0.0
        return np.mean(relevant_opponent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        current_round_num = current_round_idx + 1
        if self.extreme_k_n_ratio:
            return Action.D
        my_last_action = int(history.my_actions[-1]) if current_round_idx > 0 else None
        if current_round_num <= self.N_short:
            if current_round_idx == 0:
                return Action.C
            else:
                last_round_others_avg_c = np.mean(history.opponent_actions[-1, :])
                if last_round_others_avg_c >= 0.6:
                    return Action.D
                else:
                    return Action.C
        endgame_start_round_num = self.r - max(1, self.n) + 1
        if current_round_num >= endgame_start_round_num:
            return Action.D
        others_cooperation_rate_short_term = self._get_opponent_coop_rate(self.N_short, history, current_round_idx)
        others_cooperation_rate_long_term = self._get_opponent_coop_rate(self.N_long, history, current_round_idx)
        if others_cooperation_rate_short_term >= 0.6:
            return Action.D
        elif others_cooperation_rate_short_term < 0.2:
            if my_last_action == Action.D.value and self.ReseedAttemptsCount < self.MaxReseedAttempts and (others_cooperation_rate_long_term > 0.1):
                self.ReseedAttemptsCount += 1
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal gain by strategically free-riding
    on the contributions of others. It carefully balances the desire to exploit
    with the need to prevent a complete collapse of cooperation, which would
    ultimately diminish its own payoff. It adapts its behavior based on
    observed cooperation levels and always optimizes for the final round.
    """
    INITIAL_COOPERATE_ROUNDS = 2
    EXPLOITATION_THRESHOLD_PERCENTAGE = 0.5
    MINIMUM_SUSTAINABLE_CONTRIBUTIONS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_number == total_rounds - 1:
            return Action.D
        if current_round_number < self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        my_prev_action_contributed = history.my_actions[current_round_number - 1]
        opponent_prev_contributions_sum = np.sum(history.opponent_actions[current_round_number - 1, :])
        total_cooperators_prev = int(my_prev_action_contributed) + int(opponent_prev_contributions_sum)
        if total_cooperators_prev <= self.MINIMUM_SUSTAINABLE_CONTRIBUTIONS:
            return Action.D
        cooperation_rate_prev = total_cooperators_prev / num_players
        if cooperation_rate_prev >= self.EXPLOITATION_THRESHOLD_PERCENTAGE:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    The Opportunistic Leech strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily free-riding
    on the contributions of other players. It employs a minimal initial
    investment to "bait" others, then defaults to defection, only contributing
    strategically when the collective good is critically low to prevent its
    complete collapse and ensure future exploitation opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Reinvestment_Threshold = 0.2
        self.M_History_Window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        num_rounds_for_calc = min(current_round, self.M_History_Window)
        total_other_cooperators_sum = np.sum(history.opponent_actions[-num_rounds_for_calc:, :])
        denominator = (n_players - 1) * num_rounds_for_calc
        avg_other_coop_rate_recent = total_other_cooperators_sum / denominator
        if avg_other_coop_rate_recent < self.Reinvestment_Threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    The Opportunistic Parasite strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically inducing and
    then free-riding on the cooperation of others, while minimizing losses when
    cooperation is absent or too low to be beneficial. It adapts to opponent behavior
    and leverages knowledge of game parameters, especially the number of rounds.

    Decision Rules Precedence:
    1.  End-Game Defection Horizon: Defect unconditionally in the last 3 rounds
        (or all available rounds if the total number of rounds (r) is 3 or less).
    2.  Initial Cooperation (Round 1): Cooperate unconditionally in the first round,
        unless this round falls within the end-game defection horizon.
    3.  Initial Cooperation (Round 2): Cooperate unconditionally in the second round,
        unless this round falls within the end-game defection horizon.
    4.  Main Exploitation and Re-seeding Phase:
        a.  If the average contribution rate from all other players in the previous
            round drops below 30%, temporarily Cooperate (C) to re-seed cooperation.
        b.  Otherwise (if the average contribution rate is 30% or above), Defect (D)
            to exploit the sufficient existing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._end_game_defection_horizon = 3
        self._initial_cooperation_duration = 2
        self._re_seeding_cooperation_threshold = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed >= total_rounds - min(self._end_game_defection_horizon, total_rounds):
            return Action.D
        elif current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == 1:
            return Action.C
        else:
            C_others_prev = sum(history.opponent_actions[-1, :])
            average_other_contribution_rate = C_others_prev / (n_players - 1)
            if average_other_contribution_rate < self._re_seeding_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    Adaptive Free-Rider with Endgame Exploitation strategy.

    This strategy aims to maximize its own payoff by strategically using limited "bait"
    contributions to encourage cooperation from others, primarily free-riding on their
    contributions, and always defecting in the game's final stages.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.initial_bait_duration = max(1, min(3, math.floor(r / 4)))
        self.endgame_horizon = max(1, min(3, math.floor(r / 4)))
        self.cooperation_threshold_others = max(1, math.floor((n - 1) / 2))
        self.max_bait_attempts_if_low_cooperation = 2
        self.my_consecutive_bait_contributions = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed > total_rounds - self.endgame_horizon:
            self.my_consecutive_bait_contributions = 0
            return Action.D
        if current_round_1_indexed <= self.initial_bait_duration:
            self.my_consecutive_bait_contributions = 0
            return Action.C
        num_others_contributed_last_round = np.sum(history.opponent_actions[-1, :])
        if num_others_contributed_last_round >= self.cooperation_threshold_others:
            self.my_consecutive_bait_contributions = 0
            return Action.D
        elif self.my_consecutive_bait_contributions < self.max_bait_attempts_if_low_cooperation:
            self.my_consecutive_bait_contributions += 1
            return Action.C
        else:
            self.my_consecutive_bait_contributions = 0
            return Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    This strategy aims to maximize its own payoff by free-riding on others' contributions.
    It strategically contributes only to prevent the collapse of the public good, thereby
    preserving future exploitation opportunities. It adapts to the overall cooperation level
    of other players, acting as a calculated exploiter.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the CalculatedFreeRider strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, k.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the strategy's decision rules.

        Args:
            state: The current state of the game, including the current round number.
            history: An object containing a record of past actions and payoffs for
                     this player and opponents. None for the very first round.

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_completed_rounds = state.round_number
        individual_avg_rates = []
        for i in range(self.n_players - 1):
            total_contributions_by_opponent_i = np.sum(history.opponent_actions[:, i])
            individual_avg_rates.append(total_contributions_by_opponent_i / num_completed_rounds)
        Overall_Avg_Other_Contribution_Rate = np.mean(individual_avg_rates)
        if Overall_Avg_Other_Contribution_Rate >= 0.6:
            return Action.D
        elif 0.3 <= Overall_Avg_Other_Contribution_Rate < 0.6:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    The Opportunistic Leech strategy for the Public Goods Game.
    This strategy aims to maximize individual payoff by primarily free-riding on the contributions of others,
    only contributing when strategically necessary to prevent a collective collapse that would ultimately
    reduce its own gains, or to re-establish an exploitable environment. It is adaptive, robust against
    varying opponent behaviors, and relies solely on game parameters and observed history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_defection_rounds = 2
        self.cooperation_recovery_threshold = 0.25
        self.recent_history_window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_number < self.initial_defection_rounds:
            return Action.D
        if current_round_number >= n_rounds - 3:
            return Action.D
        history_start_idx = max(self.initial_defection_rounds, current_round_number - self.recent_history_window)
        history_end_idx = current_round_number
        recent_opponent_actions_slice = history.opponent_actions[history_start_idx:history_end_idx, :]
        action_to_take = Action.D
        if recent_opponent_actions_slice.size > 0:
            average_others_contribution_rate_recent = np.mean(recent_opponent_actions_slice)
            my_last_action = history.my_actions[-1]
            if average_others_contribution_rate_recent < self.cooperation_recovery_threshold and (not my_last_action):
                action_to_take = Action.C
        return action_to_take

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    The Pragmatic Free-Rider strategy for N-Player Public Goods Game.

    This strategy is highly exploitative, consistently aiming to maximize its own payoff by
    free-riding on the contributions of others. However, it is pragmatic enough to
    make a calculated, temporary contribution if the collective pool of contributions
    is severely depleted, and there's sufficient game remaining to potentially revive
    the common good for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.critical_cooperation_threshold: float = 0.3
        self.reseed_rounds_remaining_minimum: int = max(3, math.ceil(self.game_description.n_rounds / 4))
        self.low_cooperation_streak_trigger: int = 2
        self.consecutive_low_cooperation_rounds: int = 0
        self.contributed_in_previous_round: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_num == 0:
            self.contributed_in_previous_round = False
            return Action.D
        if current_round_num == total_rounds - 1:
            self.contributed_in_previous_round = False
            return Action.D
        action_for_current_round = Action.D
        contributed_in_this_round = False
        previous_round_index = current_round_num - 1
        c_others_prev = np.sum(history.opponent_actions[previous_round_index, :])
        avg_c_others_prev = c_others_prev / (num_players - 1)
        if avg_c_others_prev < self.critical_cooperation_threshold:
            self.consecutive_low_cooperation_rounds += 1
        else:
            self.consecutive_low_cooperation_rounds = 0
        is_cooperation_critically_low = self.consecutive_low_cooperation_rounds >= self.low_cooperation_streak_trigger
        rounds_remaining_including_current = total_rounds - current_round_num
        is_enough_time_left = rounds_remaining_including_current >= self.reseed_rounds_remaining_minimum
        did_not_contribute_last_round = not self.contributed_in_previous_round
        if is_cooperation_critically_low and is_enough_time_left and did_not_contribute_last_round:
            action_for_current_round = Action.C
            contributed_in_this_round = True
        else:
            action_for_current_round = Action.D
            contributed_in_this_round = False
        self.contributed_in_previous_round = contributed_in_this_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    The Calculated Predator strategy aims to maximize individual payoff by strategically
    alternating between cooperative signals (baiting) and free-riding. It only contributes
    when necessary to maintain an environment conducive to future exploitation or prevent
    a complete collapse of the public good. It is adaptive, robust, and explicitly exploitative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.calibration_rounds = min(3, max(0, self.r - 1))
        remaining_rounds_after_calibration = self.r - self.calibration_rounds
        self.end_game_horizon = max(1, min(3, remaining_rounds_after_calibration))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number < self.calibration_rounds:
            if current_round_number == 0:
                return Action.C
            else:
                previous_round_opponent_actions = history.opponent_actions[current_round_number - 1, :]
                num_other_players = self.n - 1
                sum_others_prev_c = np.sum(previous_round_opponent_actions)
                p_others_prev = sum_others_prev_c / num_other_players
                if p_others_prev > 0.5:
                    return Action.C
                else:
                    return Action.D
        elif current_round_number >= self.r - self.end_game_horizon:
            return Action.D
        else:
            end_history_idx = current_round_number - 1
            start_history_idx = max(0, current_round_number - 3)
            actual_window_size = end_history_idx - start_history_idx + 1
            num_other_players = self.n - 1
            opponent_actions_in_window = history.opponent_actions[start_history_idx:end_history_idx + 1, :]
            total_cooperators_in_window = np.sum(opponent_actions_in_window)
            p_others_avg_window = total_cooperators_in_window / (actual_window_size * num_other_players)
            if p_others_avg_window >= 0.4:
                return Action.D
            elif p_others_avg_window <= 0.1:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    The Opportunistic Defector strategy aims to free-ride on others' contributions.
    It defaults to defection, but includes a minimal, calculated re-seeding mechanism
    to prevent the public good pool from completely collapsing, from which it benefits.
    It avoids re-seeding in early rounds (round 0), and completely defects in the final L rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.last_reseeding_round = -self.r
        self.L = max(1, math.floor(self.r / 3))
        self.threshold_low_cooperation = max(1, math.floor((self.n - 1) / 3))
        self.cool_down_rounds = max(1, math.floor(self.r / 10), 2)
        self.min_future_rounds = self.cool_down_rounds + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round >= self.r - self.L:
            return Action.D
        action_to_take = Action.D
        if current_round >= 2:
            s_other_prev = np.sum(history.opponent_actions[current_round - 1, :])
            s_other_prev_prev = np.sum(history.opponent_actions[current_round - 2, :])
            cond1 = s_other_prev < self.threshold_low_cooperation
            cond2 = s_other_prev_prev < self.threshold_low_cooperation
            cond3 = current_round > self.last_reseeding_round + self.cool_down_rounds
            cond4 = self.r - current_round > self.min_future_rounds
            if cond1 and cond2 and cond3 and cond4:
                action_to_take = Action.C
                self.last_reseeding_round = current_round
        return action_to_take

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by strategically leveraging
    the contributions of others. It primarily seeks to free-ride, only contributing
    as a calculated investment to prevent the collapse of the public good,
    thus preserving a pool for future exploitation. It abandons cooperation
    in the endgame phase to secure immediate gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.r_endgame_start = max(1, self.n_rounds - 3)
        self.strategy_history_window_param = 3
        self.optimal_free_ride_threshold = (self.n_players - 1) * 0.5
        self.collapse_risk_threshold = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_1_indexed >= self.r_endgame_start:
            return Action.D
        N_cooperated_others_prev = np.sum(history.opponent_actions[-1, :])
        if N_cooperated_others_prev >= self.optimal_free_ride_threshold:
            return Action.D
        effective_history_window = min(current_round_1_indexed - 1, self.strategy_history_window_param)
        Avg_cooperation_others_recent = np.mean(history.opponent_actions[-effective_history_window:, :])
        if Avg_cooperation_others_recent < self.collapse_risk_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to extract maximum benefit from other players' contributions
    while minimizing its own cost, primarily by defecting to free-ride. It
    incorporates a calculated "nudge" contribution only when cooperation has
    completely broken down (all other players defected in the previous round)
    and there are sufficient future rounds for this investment to potentially
    yield exploitable returns. It always prioritizes individual gain and avoids
    unrecoverable losses, particularly in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the strategy.

        Args:
            state (GameState): The current state of the game (e.g., round number).
            history (None | PlayerHistory): The history of actions and payoffs
                                            for all players up to the previous round.
                                            This is None for the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed >= n_rounds - 2:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev > 0:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    The Prudent Exploiter strategy maximizes individual payoff by predominantly defecting,
    while strategically contributing only to establish or cautiously revive collective
    cooperation for future free-riding opportunities in the N-Player Public Goods Game.

    It starts with an initial contribution to probe the group's cooperativeness.
    In subsequent rounds, it will defect if a majority of other players contribute,
    free-riding on their efforts. If cooperation is low but historically present,
    and the strategy hasn't exhausted its limited re-engagement attempts (max 2),
    it will contribute to try and stimulate further cooperation. Otherwise, it defaults
    to defection. In the final round, it always defects to maximize terminal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.has_observed_cooperation: bool = False
        self.re_engagement_attempts_made: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        C_others_prev = sum(history.opponent_actions[-1, :])
        My_Contribution_prev = history.my_actions[-1]
        if C_others_prev > 1:
            self.has_observed_cooperation = True
        majority_others_cooperated_threshold = math.ceil((n - 1) / 2)
        if C_others_prev >= majority_others_cooperated_threshold:
            return Action.D
        if C_others_prev <= 1 and My_Contribution_prev == False and (self.has_observed_cooperation == True) and (self.re_engagement_attempts_made < 2) and (state.round_number < r - 2):
            self.re_engagement_attempts_made += 1
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    The Adaptive Predator with Failsafe strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by primarily free-riding,
    but includes mechanisms to re-seed cooperation if it collapses and a failsafe
    to prevent endless losses against consistently non-cooperative opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.COOPERATION_THRESHOLD = max(1, math.floor((n - 1) * 0.2))
        self.RESEED_DURATION_ROUNDS = max(2, math.floor(r / 8))
        self.MAX_FAILED_RESEED_ATTEMPTS = 3
        self.reseed_countdown = 0
        self.failed_reseed_attempts = 0
        self.permanent_defection_mode = False
        self.last_round_was_reseed_end = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number > 0 and self.last_round_was_reseed_end:
            C_others_prev_round = sum(history.opponent_actions[current_round_number - 1, :])
            if C_others_prev_round < self.COOPERATION_THRESHOLD:
                self.failed_reseed_attempts += 1
            self.last_round_was_reseed_end = False
        if self.failed_reseed_attempts >= self.MAX_FAILED_RESEED_ATTEMPTS:
            self.permanent_defection_mode = True
        if current_round_number == self.game_description.n_rounds - 1:
            self.last_round_was_reseed_end = False
            return Action.D
        if self.permanent_defection_mode:
            self.last_round_was_reseed_end = False
            return Action.D
        if self.reseed_countdown > 0:
            action_to_take = Action.C
            self.reseed_countdown -= 1
            if self.reseed_countdown == 0:
                self.last_round_was_reseed_end = True
            return action_to_take
        elif current_round_number == 0:
            return Action.D
        else:
            C_others_prev_round = sum(history.opponent_actions[current_round_number - 1, :])
            if C_others_prev_round >= self.COOPERATION_THRESHOLD:
                self.failed_reseed_attempts = 0
                return Action.D
            else:
                self.reseed_countdown = self.RESEED_DURATION_ROUNDS - 1
                return Action.C

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    The Adaptive Opportunist strategy is designed to maximize its own payoff in the Public Goods Game
    by primarily free-riding on others' contributions. It employs a nuanced approach that balances
    initial efforts to stimulate cooperation with aggressive exploitation and robust self-protection
    mechanisms to avoid being consistently exploited itself. The strategy prioritizes defection
    as its default action, only contributing when a calculated benefit (such as initiating or
    re-establishing a pool for future exploitation) is perceived, or during an initial "seeding" phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.ENDGAME_ROUNDS = 2
        self.INITIAL_SEED_ROUNDS = max(1, min(3, math.floor(self.r / 10)))
        self.REPRIMING_WINDOW = 5
        self.REPRIMING_THRESHOLD = 0.3
        self.SOLO_CONTRIBUTION_SHUTDOWN_STREAK = 2
        self.solo_contribution_count = 0
        self.permanent_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        current_round_1_indexed = current_round_idx + 1
        if history is not None and current_round_idx > 0:
            last_round_idx = current_round_idx - 1
            my_last_action = history.my_actions[last_round_idx]
            total_contributions_last_round = int(my_last_action) + np.sum(history.opponent_actions[last_round_idx, :])
            if my_last_action == Action.C.value:
                if total_contributions_last_round == 1:
                    self.solo_contribution_count += 1
                else:
                    self.solo_contribution_count = 0
                if self.solo_contribution_count >= self.SOLO_CONTRIBUTION_SHUTDOWN_STREAK:
                    self.permanent_defection_mode = True
            else:
                self.solo_contribution_count = 0
        if self.permanent_defection_mode:
            return Action.D
        if current_round_1_indexed > self.r - self.ENDGAME_ROUNDS:
            return Action.D
        if current_round_1_indexed <= self.INITIAL_SEED_ROUNDS:
            return Action.C
        past_others_avg_coop_rate = 0.0
        num_rounds_to_consider = min(current_round_idx, self.REPRIMING_WINDOW)
        if num_rounds_to_consider > 0:
            relevant_opponent_actions = history.opponent_actions[current_round_idx - num_rounds_to_consider:current_round_idx, :]
            past_others_total_contributions = np.sum(relevant_opponent_actions)
            denominator = (self.n - 1) * num_rounds_to_consider
            past_others_avg_coop_rate = past_others_total_contributions / denominator
        if past_others_avg_coop_rate >= self.REPRIMING_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    Adaptive Free-Rider with Calculated Reciprocity Bait strategy for the N-Player Public Goods Game.
    This strategy primarily defects to free-ride on others' contributions. It adaptively
    adjusts behavior based on observed cooperation levels, making strategic contributions
    ("baiting") only to encourage future cooperation for further exploitation.
    It defects in the first and endgame rounds to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.endgame_window = max(1, min(3, math.ceil(self.n_rounds / 5.0)))
        self.n_others = self.n_players - 1
        if self.n_others <= 0:
            self.high_cooperation_threshold = 0
            self.low_cooperation_threshold = 0
        else:
            self.high_cooperation_threshold = math.ceil(self.n_others * 0.6)
            self.low_cooperation_threshold = math.ceil(self.n_others * 0.2)
        self.bait_trigger_threshold = max(3, math.ceil(self.n_players / self.k))
        self.my_last_action = Action.D
        self.consecutive_defect_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        next_action = Action.D
        if current_round_t == 1:
            next_action = Action.D
        elif current_round_t > self.n_rounds - self.endgame_window:
            next_action = Action.D
        else:
            num_others_contributed_prev = np.sum(history.opponent_actions[-1, :])
            if num_others_contributed_prev >= self.high_cooperation_threshold:
                next_action = Action.D
            elif num_others_contributed_prev >= self.low_cooperation_threshold and num_others_contributed_prev < self.high_cooperation_threshold:
                if self.consecutive_defect_rounds >= self.bait_trigger_threshold:
                    next_action = Action.C
                else:
                    next_action = Action.D
            else:
                next_action = Action.D
        self.my_last_action = next_action
        if next_action == Action.D:
            self.consecutive_defect_rounds += 1
        else:
            self.consecutive_defect_rounds = 0
        return next_action

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    Opportunistic Pumper strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding,
    but strategically contributing when cooperation levels among opponents
    are critically low, to "pump" the system and encourage renewed contributions,
    after which it reverts to free-riding. It is designed to be exploitative by
    defaulting to defection, only contributing when necessary to sustain the
    public good for future exploitation, and always defecting when the future
    is irrelevant (initial and endgame rounds).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.low_cooperation_threshold_ratio = 0.4
        self.min_effective_contributors = max(1, math.floor((self.n - 1) / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round >= self.r - 1:
            return Action.D
        n_others_c_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev = history.my_actions[-1]
        avg_others_contribution_prev_round = n_others_c_prev / (self.n - 1)
        condition_1_low_cooperation = avg_others_contribution_prev_round < self.low_cooperation_threshold_ratio
        condition_2_sufficient_potential = n_others_c_prev >= self.min_effective_contributors
        condition_3_my_recent_action_was_defection = my_action_prev == False
        if condition_1_low_cooperation and condition_2_sufficient_potential and condition_3_my_recent_action_was_defection:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize its own payoff in the Public Goods Game
    by strategically leveraging the cooperation of other players. It employs an initial probing phase
    to test for cooperation, an adaptive middle game for free-riding or re-engagement, and a definitive
    end-game defection based on backward induction. The strategy dynamically adjusts its behavior
    based on observed cooperation levels from other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.probe_rounds = max(1, min(math.floor(self.n_rounds / 4), 3))
        self.end_game_rounds = max(1, min(math.floor(self.n_rounds / 5), 3))
        self.cooperation_threshold_high = 0.6
        self.cooperation_threshold_low = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed <= self.probe_rounds:
            return Action.C
        if current_round_1_indexed > self.n_rounds - self.end_game_rounds:
            return Action.D
        history_window_size = min(current_round_0_indexed, 5)
        recent_opponent_actions = history.opponent_actions[-history_window_size:, :]
        total_other_contributions = np.sum(recent_opponent_actions)
        denominator = (self.n_players - 1) * history_window_size
        avg_other_contribution_rate = total_other_contributions / denominator
        if avg_other_contribution_rate >= self.cooperation_threshold_high:
            return Action.D
        elif avg_other_contribution_rate <= self.cooperation_threshold_low:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    Calculated Opportunist: A strategy that aims to maximize individual payoff
    by primarily free-riding on others' contributions. It strategically makes
    catalytic contributions only when collective contributions are too low
    to be profitable for free-riding, followed by a committed period of
    exploitation. It ruthlessly defects in the first round and during the end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_reset_cooperation: float = 0.25
        self.FREE_RIDE_DURATION: int = 5
        r = self.game_description.n_rounds
        self.END_GAME_WINDOW_SIZE: int = max(1, min(5, r // 3))
        self.free_ride_rounds_left: int = 0
        self.n_players = self.game_description.n_players
        self.total_rounds = self.game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed >= self.total_rounds - self.END_GAME_WINDOW_SIZE:
            return Action.D
        if self.free_ride_rounds_left > 0:
            self.free_ride_rounds_left -= 1
            return Action.D
        num_other_players = self.n_players - 1
        sum_c_others_prev_round = np.sum(history.opponent_actions[-1, :])
        avg_c_others_prev_round = sum_c_others_prev_round / num_other_players
        if avg_c_others_prev_round < self.T_reset_cooperation:
            self.free_ride_rounds_left = self.FREE_RIDE_DURATION
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.
    It aims to maximize individual payoff by strategically leveraging the
    cooperation of other players. It induces cooperation when beneficial,
    exploits it when present, and defends against being exploited when
    cooperation is low.

    This strategy makes decisions based on the observed cooperation level
    from other players in the previous round, categorized into high, low,
    and moderate. It includes an initial probing phase, an adaptive
    exploitation phase, and a definitive defection in the end-game rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_high_cooperation: float = 0.6
        self.threshold_low_cooperation: float = 0.2
        self.max_re_inducement_streak: int = 2
        self.my_last_action: Action | None = None
        self.current_re_inducement_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_t == 1:
            self.my_last_action = Action.C
            return Action.C
        if current_round_t == r - 1 or current_round_t == r:
            self.my_last_action = Action.D
            self.current_re_inducement_streak = 0
            return Action.D
        previous_round_index = state.round_number - 1
        c_others_t_minus_1 = np.sum(history.opponent_actions[previous_round_index, :])
        p_others_cooperated_t_minus_1 = c_others_t_minus_1 / (n - 1)
        action_to_take: Action
        if p_others_cooperated_t_minus_1 >= self.threshold_high_cooperation:
            action_to_take = Action.D
            self.current_re_inducement_streak = 0
        elif p_others_cooperated_t_minus_1 <= self.threshold_low_cooperation:
            action_to_take = Action.D
            self.current_re_inducement_streak = 0
        elif self.current_re_inducement_streak < self.max_re_inducement_streak:
            action_to_take = Action.C
            self.current_re_inducement_streak += 1
        else:
            action_to_take = Action.D
            self.current_re_inducement_streak = 0
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    The Calculated Parasite strategy aims to maximize its own payoff by free-riding on others' contributions.
    It strategically intervenes with cooperation only when collective contributions are at risk of collapsing,
    or at the very beginning to establish a cooperative baseline. It exploits robust cooperation and
    defects in endgame scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_critical = 0.25
        self.T_high_cooperation = 0.75
        self.Intervention_Rounds = 2
        self.Endgame_Horizon = max(1, math.floor(self.r * 0.1))
        self.intervention_active = False
        self.intervention_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        sum_c_others_prev = np.sum(prev_round_opponent_actions)
        num_others = self.n - 1
        AvgC_others_prev = sum_c_others_prev / num_others
        if current_round_0_indexed >= self.r - self.Endgame_Horizon:
            return Action.D
        if self.intervention_active:
            if self.intervention_countdown > 0:
                self.intervention_countdown -= 1
                return Action.C
            else:
                self.intervention_active = False
        if AvgC_others_prev < self.T_critical:
            self.intervention_active = True
            self.intervention_countdown = self.Intervention_Rounds - 1
            return Action.C
        elif AvgC_others_prev >= self.T_high_cooperation:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    "The Calculated Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative, adaptive, and robust.
    It aims to maximize its own payoff by generally free-riding on the contributions of others,
    only contributing strategically to initiate cooperation, prevent its complete collapse,
    or react to specific circumstances, while protecting itself from being continually exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.LURE_ROUNDS = max(0, min(2, r - 1))
        self.ASSESSMENT_WINDOW = 4
        self.REVIVE_COOPERATION_THRESHOLD = 0.35
        self.ABANDONMENT_FAILURE_THRESHOLD = 0.2
        self.is_in_abandonment_mode: bool = False
        self.is_currently_attempting_revive: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_idx == n_rounds - 1:
            self.is_currently_attempting_revive = False
            return Action.D
        if self.is_in_abandonment_mode:
            return Action.D
        if current_round_idx < self.LURE_ROUNDS:
            self.is_currently_attempting_revive = False
            return Action.C
        window_start_idx = max(self.LURE_ROUNDS, current_round_idx - self.ASSESSMENT_WINDOW)
        window_end_idx = current_round_idx - 1
        num_history_rounds_in_window = window_end_idx - window_start_idx + 1
        if num_history_rounds_in_window <= 0 or history is None:
            average_other_contribution_rate = 0.0
        else:
            relevant_history_actions = history.opponent_actions[window_start_idx:window_end_idx + 1, :]
            total_other_contributions = np.sum(relevant_history_actions)
            max_possible_contributions_in_window = (n_players - 1) * num_history_rounds_in_window
            average_other_contribution_rate = total_other_contributions / max_possible_contributions_in_window
        if self.is_currently_attempting_revive:
            if average_other_contribution_rate < self.ABANDONMENT_FAILURE_THRESHOLD:
                self.is_in_abandonment_mode = True
                self.is_currently_attempting_revive = False
                return Action.D
            else:
                self.is_currently_attempting_revive = False
        if self.is_in_abandonment_mode:
            return Action.D
        elif average_other_contribution_rate < self.REVIVE_COOPERATION_THRESHOLD:
            self.is_currently_attempting_revive = True
            return Action.C
        else:
            self.is_currently_attempting_revive = False
            return Action.D

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    Core Philosophy: Prioritizes immediate personal gain through defection. Only
    contributes strategically when it's a necessary, calculated investment to prevent
    the public good from entirely collapsing, thereby preserving future free-riding
    opportunities. This strategy is driven by self-interest, not genuine cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_defections_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        chosen_action: Action
        if current_round == self.n_rounds:
            chosen_action = Action.D
            self.consecutive_defections_count += 1
            return chosen_action
        if current_round <= 2:
            chosen_action = Action.D
            self.consecutive_defections_count += 1
            return chosen_action
        total_cooperations_by_opponents = np.sum(history.opponent_actions)
        denominator = state.round_number * (self.n_players - 1)
        average_cooperation_rate_others = total_cooperations_by_opponents / denominator
        if average_cooperation_rate_others >= 0.6:
            chosen_action = Action.D
            self.consecutive_defections_count += 1
            return chosen_action
        elif average_cooperation_rate_others < 0.3:
            if self.consecutive_defections_count >= 3:
                chosen_action = Action.C
                self.consecutive_defections_count = 0
                return chosen_action
            else:
                chosen_action = Action.D
                self.consecutive_defections_count += 1
                return chosen_action
        else:
            chosen_action = Action.D
            self.consecutive_defections_count += 1
            return chosen_action

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    The Opportunistic Shepherd strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically oscillating
    between minimal cooperation (to induce or maintain contributions from others)
    and aggressive defection (to free-ride on those contributions). Its primary
    goal is individual payoff maximization, not collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.LURING_ROUNDS_INITIAL = max(1, min(3, math.floor(r * 0.1)))
        self.LURING_ROUNDS_RESET = 1
        self.DEFECTION_THRESHOLD_HIGH = 0.6
        self.DEFECTION_THRESHOLD_LOW = 0.2
        self.ENDGAME_ROUNDS = max(1, min(3, math.floor(r * 0.1)))
        self.current_state = 'INITIAL_LURE'
        self.lure_countdown = self.LURING_ROUNDS_INITIAL
        self.num_opponents = n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round >= total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        if self.current_state == 'INITIAL_LURE':
            action = Action.C
            self.lure_countdown -= 1
            if self.lure_countdown == 0:
                self.current_state = 'EXPLOIT'
            return action
        if self.current_state == 'RESET_LURE':
            action = Action.C
            self.lure_countdown -= 1
            if self.lure_countdown == 0:
                self.current_state = 'EXPLOIT'
            return action
        if self.num_opponents > 0:
            total_others_contrib = np.sum(history.opponent_actions[-1, :])
            avg_others_contrib = total_others_contrib / self.num_opponents
        else:
            avg_others_contrib = 0.0
        if avg_others_contrib >= self.DEFECTION_THRESHOLD_HIGH:
            return Action.D
        elif avg_others_contrib <= self.DEFECTION_THRESHOLD_LOW:
            if self.LURING_ROUNDS_RESET == 1:
                self.current_state = 'EXPLOIT'
            else:
                self.current_state = 'RESET_LURE'
                self.lure_countdown = self.LURING_ROUNDS_RESET - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by dynamically adjusting its
    cooperation level based on observed opponent behavior. It operates in three
    phases:
    1. Initial Probing: Contributes to encourage others' cooperation.
    2. Adaptive Exploitation: Free-rides if others cooperate sufficiently, otherwise
       contributes to 're-educate' and stimulate cooperation.
    3. Endgame: Always defects to maximize final round payoffs.
    """
    PROBE_ROUNDS: int = 3
    ASSESSMENT_WINDOW: int = 5
    COOPERATION_THRESHOLD: float = 0.5
    ENDGAME_ROUNDS: int = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed >= self.n_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        elif current_round_0_indexed < self.PROBE_ROUNDS:
            return Action.C
        else:
            start_idx = max(0, current_round_0_indexed - self.ASSESSMENT_WINDOW)
            end_idx = current_round_0_indexed
            relevant_opponent_actions = history.opponent_actions[start_idx:end_idx, :]
            total_contributions_by_others = np.sum(relevant_opponent_actions)
            num_rounds_in_window = end_idx - start_idx
            max_possible_contributions = (self.n_players - 1) * num_rounds_in_window
            average_others_cooperation_rate = total_contributions_by_others / max_possible_contributions
            if average_others_cooperation_rate >= self.COOPERATION_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    The Adaptive Exploiter strategy maximizes personal gain in the Public Goods Game
    by dynamically balancing initial cooperation ('baiting') with subsequent defection
    ('exploitation'), adapting to the collective behavior of other players.

    It operates on a dynamic "bait-exploit-re-bait" cycle:
    1.  **Initial Baiting:** Contributes for a few initial rounds to signal cooperation and
        encourage others to contribute, establishing a public good pool.
    2.  **Adaptive Exploitation:** Once the baiting phase is over, it monitors the
        contributions of other players. If enough other players contribute, it defects
        to maximize its private payoff by free-riding.
    3.  **Re-Baiting:** If its previous defection caused a significant drop in other
        players' contributions (below a calculated threshold), it will contribute
        again in the current round to "re-bait" them, attempting to restore
        a cooperative environment for future exploitation.
    4.  **Final Round Defection:** Always defects in the last round as there are no
        future interactions to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.InitialBaitRounds = max(1, min(3, math.floor(self.r / 3)))
        self.ExploitationThreshold_Ratio = 0.5
        self.ExploitationThreshold = max(1, math.floor((self.n - 1) * self.ExploitationThreshold_Ratio))
        self.my_last_action_was_C = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        action_for_current_round = Action.D
        if current_round_number_1_indexed == self.r:
            action_for_current_round = Action.D
            return action_for_current_round
        if current_round_number_1_indexed == 1:
            action_for_current_round = Action.C
            self.my_last_action_was_C = True
            return action_for_current_round
        S_others_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        if current_round_number_1_indexed <= self.InitialBaitRounds:
            action_for_current_round = Action.C
            self.my_last_action_was_C = True
        elif not self.my_last_action_was_C and S_others_prev_round < self.ExploitationThreshold:
            action_for_current_round = Action.C
            self.my_last_action_was_C = True
        elif S_others_prev_round >= self.ExploitationThreshold:
            action_for_current_round = Action.D
            self.my_last_action_was_C = False
        else:
            action_for_current_round = Action.D
            self.my_last_action_was_C = False
        return action_for_current_round

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize individual payoff by predominantly free-riding.
    It monitors the collective behavior of other players, only contributing when strategically necessary
    to prevent a collapse of the public good that would eliminate free-riding opportunities,
    and then quickly reverts to defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.window_for_average = 3
        self.expl_threshold_count = max(1, math.ceil(0.6 * (n - 1)))
        self.collapse_threshold_count = math.floor(0.2 * (n - 1))
        self.lure_duration = 1
        self.rounds_to_lure = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_game_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.D
        if current_round_number == total_game_rounds - 1:
            return Action.D
        if self.rounds_to_lure > 0:
            self.rounds_to_lure -= 1
        if self.rounds_to_lure > 0:
            return Action.C
        else:
            current_other_contributions = np.sum(history.opponent_actions[current_round_number - 1, :])
            actual_window_size = min(current_round_number, self.window_for_average)
            opponent_actions_in_window = history.opponent_actions[current_round_number - actual_window_size:current_round_number, :]
            avg_other_contributions = np.mean(np.sum(opponent_actions_in_window, axis=1))
            if avg_other_contributions >= self.expl_threshold_count:
                return Action.D
            elif avg_other_contributions <= self.collapse_threshold_count:
                return Action.D
            elif current_other_contributions < avg_other_contributions:
                self.rounds_to_lure = self.lure_duration
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    The Opportunistic Defector with a Trigger strategy for the N-Player Public Goods Game.

    This strategy leverages historical opponent actions to dynamically choose between free-riding
    (defecting) and strategically contributing. It aims to maximize individual payoff by defecting
    when collective cooperation from others is high or too low, and contributing strategically
    to encourage more cooperation when it is in a moderate range.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The historical actions and payoffs from previous rounds, or None if it's the first round.

        Returns:
            An Action (C or D) for the current round.
        """
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        num_other_players = num_players - 1
        contributions_from_others_prev_round = np.sum(history.opponent_actions[-1, :])
        ecro = contributions_from_others_prev_round / num_other_players
        if ecro >= 0.75:
            return Action.D
        elif ecro < 0.25:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    The Conditional Predator strategy is designed to exploit cooperative environments.
    It initiates with a single, calculated act of cooperation (C) in the first round
    to probe for potential cooperators. In all subsequent rounds, it consistently
    defects (D). The strategy rationalizes this defection: either as active
    free-riding when others are sufficiently cooperative (above `CooperationThreshold`),
    or as defensive defection to minimize losses when cooperation is low.
    Despite the detailed monitoring description, the action taken in intermediate
    and final rounds is always Defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold: float = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    Adaptive Predator strategy for the N-Player Public Goods Game.
    Aims to maximize individual payoff by free-riding, contributing only to prevent
    the complete collapse of the common pool and preserve future exploitation opportunities.
    It adapts to opponent cooperation levels across different game phases.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_previous_action: Action = Action.D
        self.others_contributions_history: list[int] = []
        self.max_probe_round: int = max(3, math.floor(self.r / 10))
        self.start_endgame_round: int = self.r - max(3, math.floor(self.r / 10)) + 1
        self.MIN_VIABLE_COOPERATION: int = max(1, math.floor((self.n - 1) * 0.1))
        self.CRITICAL_DROP_FACTOR: float = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        actual_round_number: int = state.round_number + 1
        if state.round_number == 0:
            chosen_action = Action.D
            self.my_previous_action = chosen_action
            return chosen_action
        others_contributions_prev_round: int = int(np.sum(history.opponent_actions[-1, :]))
        if actual_round_number <= self.max_probe_round:
            chosen_action = Action.D
        elif actual_round_number >= self.start_endgame_round:
            chosen_action = Action.D
        else:
            num_avg_rounds_to_consider: int = min(5, len(self.others_contributions_history))
            others_contributions_recent_avg: float = np.mean(self.others_contributions_history[-num_avg_rounds_to_consider:])
            if others_contributions_prev_round <= self.MIN_VIABLE_COOPERATION:
                chosen_action = Action.D
            elif others_contributions_prev_round < others_contributions_recent_avg * self.CRITICAL_DROP_FACTOR and others_contributions_prev_round > self.MIN_VIABLE_COOPERATION and (self.my_previous_action == Action.D):
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self.my_previous_action = chosen_action
        self.others_contributions_history.append(others_contributions_prev_round)
        return chosen_action

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy is designed to maximize its own payoff by consistently attempting to free-ride on the contributions of other players.
    It employs a dynamic approach: defaulting to defection, making strategic contributions only when necessary to prevent
    the complete collapse of the public good (thereby maintaining a pool to free-ride from), and exploiting the game's
    finite nature by defecting unconditionally in the final rounds. The strategy adapts based on the observed actions
    of other players.
    """
    _REACTIVATION_THRESHOLD_PROPORTION = 0.25
    _ENDGAME_START_OFFSET = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.t_endgame_start_round = max(1, self.n_rounds - self._ENDGAME_START_OFFSET)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t >= self.t_endgame_start_round:
            return Action.D
        sum_others_contributed_last_round = sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        P_others_contributed = sum_others_contributed_last_round / num_other_players
        if P_others_contributed >= self._REACTIVATION_THRESHOLD_PROPORTION:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    The Predatory Stabilizer strategy for the N-Player Public Goods Game.
    This strategy primarily free-rides, defecting by default. It makes
    minimal, calculated contributions (cooperating) only when the observed
    level of cooperation from others falls below a specific threshold,
    attempting to "stabilize" the public good for future exploitation.
    It strategically exits all cooperation in the endgame phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        my_last_action = Action.C if history.my_actions[-1] else Action.D
        sum_other_contributions_prev = sum(history.opponent_actions[-1, :])
        if self.n - 1 > 0:
            c_others_rate_prev = sum_other_contributions_prev / (self.n - 1)
        else:
            c_others_rate_prev = 0.0
        if current_round_number >= self.r - 2:
            return Action.D
        T_rate_exploit = 0.3
        if c_others_rate_prev >= T_rate_exploit:
            return Action.D
        elif my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to exploit cooperative
    tendencies in other players while minimizing personal contributions.
    It probes for cooperation, free-rides when possible, and strategically
    re-initiates cooperation only to resume exploitation. It adapts to the
    observed average behavior of other players.
    """

    class AFRMode(Enum):
        PROBE = 'Probe'
        EXPLOIT = 'Exploit'
        RECALIBRATE = 'Recalibrate'
    N_PROBE_ROUNDS = 1
    N_RECALIBRATE_ROUNDS = 2
    THRESHOLD_HIGH = 0.7
    THRESHOLD_LOW = 0.4

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode: Strategy.AFRMode = self.AFRMode.PROBE
        self.rounds_in_current_mode: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round == n_rounds - 1:
            return Action.D
        if current_round == 0:
            self.mode = self.AFRMode.PROBE
            self.rounds_in_current_mode = 0
            self.rounds_in_current_mode += 1
            return Action.C
        num_other_players = n_players - 1
        p_others_prev = 0.0
        if num_other_players > 0:
            sum_opponent_cooperations_prev_round = sum(history.opponent_actions[-1, :])
            p_others_prev = sum_opponent_cooperations_prev_round / num_other_players
        action_for_this_round = Action.D
        if self.mode == self.AFRMode.PROBE:
            if self.rounds_in_current_mode < self.N_PROBE_ROUNDS:
                action_for_this_round = Action.C
                self.rounds_in_current_mode += 1
            elif p_others_prev >= self.THRESHOLD_HIGH:
                self.mode = self.AFRMode.EXPLOIT
                self.rounds_in_current_mode = 0
                action_for_this_round = Action.D
            else:
                self.mode = self.AFRMode.RECALIBRATE
                self.rounds_in_current_mode = 0
                action_for_this_round = Action.C
        elif self.mode == self.AFRMode.EXPLOIT:
            if p_others_prev >= self.THRESHOLD_LOW:
                action_for_this_round = Action.D
                self.rounds_in_current_mode += 1
            else:
                self.mode = self.AFRMode.RECALIBRATE
                self.rounds_in_current_mode = 0
                action_for_this_round = Action.C
        elif self.mode == self.AFRMode.RECALIBRATE:
            if self.rounds_in_current_mode < self.N_RECALIBRATE_ROUNDS:
                action_for_this_round = Action.C
                self.rounds_in_current_mode += 1
            elif p_others_prev >= self.THRESHOLD_HIGH:
                self.mode = self.AFRMode.EXPLOIT
                self.rounds_in_current_mode = 0
                action_for_this_round = Action.D
            else:
                self.mode = self.AFRMode.EXPLOIT
                self.rounds_in_current_mode = 0
                action_for_this_round = Action.D
        return action_for_this_round

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    The Responsive Manipulator strategy aims to exploit other players' willingness to contribute,
    only contributing itself when necessary to "prime the pump" or prevent the public good
    from entirely collapsing, thus preserving a pool to free-ride from in subsequent rounds.
    It is adaptive to the observed behavior of opponents and includes mechanisms to prevent
    itself from being continuously exploited by persistent defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold: float = 0.3
        self.patience_threshold: int = 2
        self.consecutive_low_ocr_contributions: int = 0
        self.have_given_up_on_cooperation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        if current_round_index == 0:
            return Action.D
        if current_round_index == self.game_description.n_rounds - 1:
            return Action.D
        if self.have_given_up_on_cooperation:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_index - 1, :]
        num_other_cooperators = np.sum(previous_round_opponent_actions)
        num_opponents = self.game_description.n_players - 1
        ocr = num_other_cooperators / num_opponents
        if ocr >= self.exploitation_threshold:
            self.consecutive_low_ocr_contributions = 0
            return Action.D
        elif self.consecutive_low_ocr_contributions >= self.patience_threshold:
            self.have_given_up_on_cooperation = True
            return Action.D
        else:
            self.consecutive_low_ocr_contributions += 1
            return Action.C

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    The Opportunistic Freerider strategy prioritizes defecting to maximize immediate personal gain.
    It incorporates a mechanism to strategically "breathe life" back into the public good if it
    approaches a critical state of collapse, ensuring there remains a pool to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_contribution_round_self: int = 0
        self.initial_defection_rounds: int = 3
        self.final_defection_rounds: int = 3
        self.collapse_threshold_ratio: float = 0.35
        self.cooldown_after_contribution: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round > n_rounds - self.final_defection_rounds:
            return Action.D
        elif current_round <= self.initial_defection_rounds:
            return Action.D
        past_contributions_others = np.sum(history.opponent_actions[current_round - 1, :])
        cooperation_ratio_others = past_contributions_others / (n_players - 1)
        if current_round - self.last_contribution_round_self <= self.cooldown_after_contribution:
            return Action.D
        elif cooperation_ratio_others < self.collapse_threshold_ratio:
            self.last_contribution_round_self = current_round
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically leveraging and
    exploiting the contributions of other players. It starts by cooperating
    to "bait" others, then defects whenever others contribute. If no one
    else contributes, it occasionally "probes" for renewed cooperation,
    otherwise it defects. In the final round, it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_probe = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev > 0:
            return Action.D
        elif random.random() < self.p_probe:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    Adaptive Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to free-riding
    while adaptively contributing only when necessary to prevent the public good
    from collapsing entirely, thus preserving future opportunities for exploitation.
    It prioritizes self-interest and minimal contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.N_others = self.game_description.n_players - 1
        self.T_collapse = max(1, math.floor(self.N_others / 3))
        self.Defect_Streak_Required = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.D
        if current_round == n_rounds - 1:
            return Action.D
        N_C_others_prev = sum(history.opponent_actions[current_round - 1, :])
        my_action_prev_was_defect = history.my_actions[current_round - 1] == False
        my_action_two_prev_was_defect = False
        if current_round == 1:
            my_action_two_prev_was_defect = True
        else:
            my_action_two_prev_was_defect = history.my_actions[current_round - 2] == False
        if N_C_others_prev < self.T_collapse:
            if my_action_prev_was_defect and my_action_two_prev_was_defect:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    Adaptive Free-Rider strategy for N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on the contributions of other players,
    while adapting its behavior to maintain an environment conducive to future exploitation.
    It considers an endgame phase, an initial probing phase, and an adaptive intermediate phase based on
    recent cooperation rates of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.PROBE_ROUNDS = 1
        self.WINDOW_SIZE_FOR_HISTORY = min(max(1, math.floor(self.n_rounds / 4)), 5)
        self.EXPLOIT_THRESHOLD = 0.7
        self.REBUILD_THRESHOLD = 0.4
        self.LAST_ROUND_BUFFER = min(self.n_rounds, max(1, 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed >= self.n_rounds - self.LAST_ROUND_BUFFER:
            return Action.D
        if current_round_0_indexed < self.PROBE_ROUNDS:
            return Action.C
        num_history_rounds_available = current_round_0_indexed
        effective_window_size = min(self.WINDOW_SIZE_FOR_HISTORY, num_history_rounds_available)
        if effective_window_size == 0:
            return Action.C
        opponent_actions_in_window = history.opponent_actions[-effective_window_size:, :]
        total_cooperations_from_others = np.sum(opponent_actions_in_window)
        total_possible_cooperations_from_others = (self.n_players - 1) * effective_window_size
        average_cooperation_from_others = total_cooperations_from_others / total_possible_cooperations_from_others
        if average_cooperation_from_others >= self.EXPLOIT_THRESHOLD:
            return Action.D
        elif average_cooperation_from_others <= self.REBUILD_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    The Strategic Parasite strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on
    the contributions of other players, while contributing only when necessary to
    prevent the collapse of the public good and ensure future free-riding opportunities.
    It adapts to observed cooperation levels and exploits end-game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Strategic Parasite strategy with game parameters and
        derived internal parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.defection_horizon_rounds = max(1, math.floor(r / 5))
        self.cooperation_trigger_threshold = 0.5 - k / n * 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Strategic Parasite strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs. Is None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round >= n_rounds - self.defection_horizon_rounds:
            return Action.D
        num_others_contributed_last_round = sum(history.opponent_actions[-1, :])
        others_contribution_rate = num_others_contributed_last_round / (n_players - 1)
        if others_contribution_rate < self.cooperation_trigger_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by defaulting to defection,
    but adaptively contributes when necessary to encourage or re-stimulate
    cooperation from other players, thereby creating opportunities for future free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reseed_counter = 0
        self.ENDGAME_BUFFER_ROUNDS = 3
        self.COOPERATION_REBIRTH_THRESHOLD = 0.5
        self.RESEED_DURATION = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if n_rounds <= self.ENDGAME_BUFFER_ROUNDS:
            return Action.D
        if current_round_1_indexed > n_rounds - self.ENDGAME_BUFFER_ROUNDS:
            self.reseed_counter = 0
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_other_cooperators = np.sum(history.opponent_actions[-1, :])
        n_minus_1 = n_players - 1
        proportion_other_contributions = num_other_cooperators / n_minus_1
        if self.reseed_counter > 0:
            self.reseed_counter -= 1
            return Action.C
        elif proportion_other_contributions < self.COOPERATION_REBIRTH_THRESHOLD:
            self.reseed_counter = self.RESEED_DURATION - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    The Opportunistic Forager strategy aims to maximize personal payoff by primarily
    free-riding on the contributions of others. It is designed to be highly exploitative,
    only contributing a token when the collective contribution level from others falls
    to a critically low point, threatening the existence of a viable public good pool
    to exploit. This strategic contribution acts as a "lure" or "investment" to encourage
    renewed cooperation, after which the strategy reverts to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        others_total_contrib_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        PGVT = math.floor((self.n - 1) / 3)
        if others_total_contrib_prev_round < PGVT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    The "Opportunistic Free-Rider" strategy is designed to maximize individual payoff in the
    Public Goods Game. It begins by contributing to prime initial cooperation among players.
    During the adaptive middle game, it predominantly defects to free-ride, but strategically
    contributes if the average cooperation of others drops below a 're-seed' threshold, aiming
    to prevent a complete collapse of the public good. If cooperation completely collapses or
    in the final rounds, it defaults to defection to minimize losses or exploit the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.initial_cooperate_rounds = max(1, math.floor(r / 10))
        self.endgame_defect_rounds = max(1, math.floor(r / 10))
        self.cooperation_reseed_threshold = 0.3
        self.persistent_defection_threshold = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_0_indexed < self.initial_cooperate_rounds:
            return Action.C
        if current_round_0_indexed >= total_rounds - self.endgame_defect_rounds:
            return Action.D
        total_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
        avg_others_contributions_prev_round = total_others_cooperated_prev_round / (num_players - 1)
        if avg_others_contributions_prev_round < self.persistent_defection_threshold:
            return Action.D
        elif avg_others_contributions_prev_round < self.cooperation_reseed_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff by primarily defecting
    but adaptively contributes when necessary to encourage continued cooperation from other players,
    thus preserving a public good from which to benefit. It learns from past interactions
    and adjusts its level of exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.Defection_Ceiling = 3
        self.Exploitation_Threshold = 0.7
        self.Reactivation_Threshold = 0.2
        self.Significant_Drop_Factor = 0.7
        self.my_consecutive_defections = 0
        self.last_round_others_avg_contribution = 0.0
        self.total_contributions_history = []
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            if r == 1:
                return Action.D
            else:
                return Action.C
        if current_round == r - 1:
            return Action.D
        previous_round_my_action_is_C = history.my_actions[current_round - 1]
        previous_round_opponent_actions_are_C = history.opponent_actions[current_round - 1, :]
        self.last_round_others_avg_contribution = np.sum(previous_round_opponent_actions_are_C) / (n - 1)
        total_contributions_previous_round = np.sum(previous_round_opponent_actions_are_C) + int(previous_round_my_action_is_C)
        self.total_contributions_history.append(total_contributions_previous_round)
        action_for_current_round = Action.D
        forced_reactivation = False
        if self.my_consecutive_defections >= self.Defection_Ceiling:
            forced_reactivation = True
        elif self.last_round_others_avg_contribution < self.Reactivation_Threshold:
            forced_reactivation = True
        elif current_round > 1 and len(self.total_contributions_history) >= 2:
            if self.total_contributions_history[-1] < self.total_contributions_history[-2] * self.Significant_Drop_Factor:
                forced_reactivation = True
        if forced_reactivation:
            action_for_current_round = Action.C
        elif self.last_round_others_avg_contribution >= self.Exploitation_Threshold:
            action_for_current_round = Action.D
        else:
            action_for_current_round = Action.D
        if action_for_current_round == Action.C:
            self.my_consecutive_defections = 0
        else:
            self.my_consecutive_defections += 1
        return action_for_current_round

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by initially cooperating to
    lure others, then free-riding, and attempting to re-lure cooperation if
    contributions dwindle, before finally defecting in the game's final rounds.
    It adapts to opponent behavior and focuses on exploitation over sustained
    mutual cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        if r == 1:
            self._initial_lure_rounds = 0
        elif r == 2:
            self._initial_lure_rounds = 1
        else:
            self._initial_lure_rounds = 2
        self._cooperation_threshold = 0.5
        self._tried_to_re_lure_in_previous_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        action_to_take = Action.D
        current_round_was_re_lure_attempt = False
        if r == 1:
            action_to_take = Action.D
            current_round_was_re_lure_attempt = False
        elif current_round <= self._initial_lure_rounds and r > 1:
            action_to_take = Action.C
            current_round_was_re_lure_attempt = False
        elif current_round == r and r > 1:
            action_to_take = Action.D
            current_round_was_re_lure_attempt = False
        else:
            my_action_last_round_bool = history.my_actions[state.round_number - 1]
            opponent_actions_last_round = history.opponent_actions[state.round_number - 1, :]
            total_contributions_last_round = int(my_action_last_round_bool) + np.sum(opponent_actions_last_round)
            my_contribution_last_round = int(my_action_last_round_bool)
            avg_other_c = 0.0
            avg_other_c = (total_contributions_last_round - my_contribution_last_round) / (n - 1)
            tried_to_re_lure_last_round_flag = self._tried_to_re_lure_in_previous_round
            if avg_other_c >= self._cooperation_threshold:
                action_to_take = Action.D
                current_round_was_re_lure_attempt = False
            elif avg_other_c == 0:
                action_to_take = Action.D
                current_round_was_re_lure_attempt = False
            elif tried_to_re_lure_last_round_flag:
                action_to_take = Action.D
                current_round_was_re_lure_attempt = False
            else:
                action_to_take = Action.C
                current_round_was_re_lure_attempt = True
        self._tried_to_re_lure_in_previous_round = current_round_was_re_lure_attempt
        return action_to_take

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    Adaptive Conditional Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily free-riding on the
    contributions of other players. It defects whenever a sufficient proportion
    of others contribute, but adaptively contributes itself when the level of
    cooperation among others drops too low. This calculated contribution acts as
    an 'investment' to revive the public good, ensuring future opportunities for
    exploitation. The willingness to contribute is adjusted based on the game's
    multiplier (k/n) to strategically maintain the public good's activity.

    It starts with a cooperation in the first round to "seed" cooperation,
    and defects unconditionally in the final round to maximize terminal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and calculates the
        dynamic revival threshold.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.revival_threshold = max(0.2, 0.75 * self.k / self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: GameState object containing the current round number.
            history: PlayerHistory object containing past actions and payoffs.
                     Will be None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_others_contributed_count = np.sum(history.opponent_actions[-1, :])
        prop_others_cooperated = prev_others_contributed_count / (self.n_players - 1)
        if prop_others_cooperated >= self.revival_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize individual payoff by primarily defecting.
    It adaptively free-rides on others' contributions when cooperation levels are sufficient.
    When cooperation severely collapses, it strategically contributes to "re-ignite" collective action,
    but only if sufficient rounds remain to exploit renewed cooperation and after a cooldown period
    since its last re-ignition attempt. In the final rounds, it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = game_description.n_rounds
        self.exploit_threshold = 0.6
        self.reignition_threshold = 0.1
        self.reignition_cooldown_rounds = max(3, math.floor(r / 5))
        self.endgame_window = max(1, min(3, math.floor(r * 0.2)))
        self.last_reignition_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        t = current_round_0_indexed + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if t > n_rounds - self.endgame_window:
            return Action.D
        if t == 1:
            return Action.D
        num_cooperating_opponents_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if n_players - 1 == 0:
            p_others_prev_round = 0.0
        else:
            p_others_prev_round = num_cooperating_opponents_prev_round / (n_players - 1)
        if p_others_prev_round >= self.exploit_threshold:
            return Action.D
        elif p_others_prev_round <= self.reignition_threshold:
            if t - self.last_reignition_round > self.reignition_cooldown_rounds and n_rounds - t >= self.reignition_cooldown_rounds:
                self.last_reignition_round = t
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    The Opportunistic Harvester strategy is designed to be exploitative, adaptive,
    and robust in the N-Player Public Goods Game. It aims to maximize its own
    payoff by strategically encouraging and then exploiting cooperation from
    other players, while protecting its endowment when cooperation is low.

    It seeds cooperation in the first round, then exploits cooperative environments
    for a limited number of rounds (ExploitationLimit) before re-investing (contributing)
    to sustain cooperation. If cooperation from others falls below a threshold
    (CooperationThreshold), it defects to protect its endowment and signal
    disapproval of low cooperation. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.TH = math.ceil((self.game_description.n_players - 1) * 0.6)
        self.EL = 2
        self.my_consecutive_defections_in_cooperative_env: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.my_consecutive_defections_in_cooperative_env = 0
            return Action.C
        prev_non_me_contributors = np.sum(history.opponent_actions[-1, :])
        if prev_non_me_contributors >= self.TH:
            if self.my_consecutive_defections_in_cooperative_env < self.EL:
                self.my_consecutive_defections_in_cooperative_env += 1
                return Action.D
            else:
                self.my_consecutive_defections_in_cooperative_env = 0
                return Action.C
        else:
            self.my_consecutive_defections_in_cooperative_env = 0
            return Action.D

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by predominantly free-riding
    on the contributions of other players. It strategically contributes only when
    necessary to prevent a complete collapse of cooperation that it could
    otherwise exploit, always prioritizing self-interest, especially in the
    game's final stages.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.observed_cooperation_rate = 0.0
        self.learning_rate_alpha = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.n_rounds - 1:
            return Action.D
        s_others_last_round = sum(history.opponent_actions[-1, :])
        current_others_cooperation_ratio = s_others_last_round / (self.n_players - 1)
        self.observed_cooperation_rate = self.learning_rate_alpha * self.observed_cooperation_rate + (1 - self.learning_rate_alpha) * current_others_cooperation_ratio
        if self.observed_cooperation_rate >= 0.5:
            return Action.D
        elif self.observed_cooperation_rate >= 0.2:
            return Action.D
        else:
            s_total_last_round = history.my_actions[-1] + s_others_last_round
            if s_total_last_round == 0:
                return Action.D
            elif random.random() < 0.2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    The Opportunistic Prowler strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding on
    others' contributions. It defects in initial probing rounds and final
    endgame rounds. In the adaptive middle game, it monitors the average
    contribution of other players. If this contribution falls below a threshold,
    indicating a risk to the public good (and thus to free-riding opportunities),
    it temporarily contributes to "re-stimulate" cooperation before reverting
    to defection. Cooperation is a calculated, short-term investment, not an
    altruistic act.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.N_PROBE: int = 2
        self.N_ENDGAME: int = max(2, math.ceil(self.n_rounds * 0.1))
        self.FREE_RIDE_THRESHOLD: float = 0.4
        self.STIMULATE_DURATION: int = 1
        self.stimulating_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.N_PROBE:
            return Action.D
        if current_round_1_indexed > self.n_rounds - self.N_ENDGAME:
            return Action.D
        num_other_players = self.n_players - 1
        previous_round_opponent_contributions_sum = np.sum(history.opponent_actions[-1, :])
        average_other_contribution = previous_round_opponent_contributions_sum / num_other_players
        if self.stimulating_counter > 0:
            self.stimulating_counter -= 1
            return Action.C
        elif average_other_contribution < self.FREE_RIDE_THRESHOLD:
            self.stimulating_counter = self.STIMULATE_DURATION - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    Calculated Lure: A strategy designed to maximize its own payoff in the Public Goods Game
    by primarily free-riding on the contributions of other players. It operates on an
    exploitative principle, only engaging in temporary cooperation (a "lure") when the
    overall level of contributions from others drops so low that it threatens future
    free-riding opportunities, and there are sufficient rounds remaining for such a
    temporary gesture to potentially stimulate renewed cooperation from the group.
    """
    _CRITICAL_COOPERATION_THRESHOLD: float = 0.33
    _MIN_REMAINING_ROUNDS_FOR_LURE: int = 5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.my_action_in_prev_round_was_contribute: bool = False
        self.CRITICAL_COOPERATION_THRESHOLD = self._CRITICAL_COOPERATION_THRESHOLD
        self.MIN_REMAINING_ROUNDS_FOR_LURE = self._MIN_REMAINING_ROUNDS_FOR_LURE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        chosen_action = Action.D
        if current_round_0_indexed == 0:
            chosen_action = Action.D
        elif current_round_0_indexed == self.n_rounds - 1:
            chosen_action = Action.D
        elif self.my_action_in_prev_round_was_contribute:
            chosen_action = Action.D
        else:
            num_other_cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
            num_other_players = self.n_players - 1
            cooperation_ratio_others = num_other_cooperators_in_prev_round / num_other_players
            cooperation_critically_low = cooperation_ratio_others < self.CRITICAL_COOPERATION_THRESHOLD
            enough_rounds_left = self.n_rounds - current_round_1_indexed > self.MIN_REMAINING_ROUNDS_FOR_LURE
            if cooperation_critically_low and enough_rounds_left:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self.my_action_in_prev_round_was_contribute = chosen_action == Action.C
        return chosen_action

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff by defecting when
    sufficient cooperation from others exists, and selectively cooperating to 're-prime'
    the public good if collective contributions fall below a critical threshold.
    It begins and ends with defection for maximum exploitation, reserving conditional
    cooperation for intermediate rounds to prevent the collapse of the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.T = math.floor(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        N_others_contributed_prev_round = sum(history.opponent_actions[-1, :])
        if N_others_contributed_prev_round < self.T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    Adaptive Free-Rider with Strategic Prodding strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding, employing
    limited, strategic cooperation only when necessary to stimulate or sustain a public goods
    pool that can then be exploited. It always reverts to defection for maximum benefit
    and exploits the endgame by defecting unconditionally.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_rounds = 2
        self.exploitation_threshold_low = 0.25
        self.prodding_cooldown_rounds = 5
        self.final_defection_rounds = 2
        self.last_prodding_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_game_round > r - self.final_defection_rounds:
            return Action.D
        elif current_game_round <= self.initial_cooperation_rounds:
            if current_game_round == self.initial_cooperation_rounds:
                self.last_prodding_round = current_game_round
            return Action.C
        else:
            action_to_take = Action.D
            if history is not None and history.opponent_actions.shape[0] > 0:
                sum_other_contributions_prev = sum(history.opponent_actions[-1, :])
                num_other_players = n - 1
                average_other_contributions_prev = sum_other_contributions_prev / num_other_players
                condition_low_pool = average_other_contributions_prev < self.exploitation_threshold_low
                condition_cooldown_passed = current_game_round - self.last_prodding_round > self.prodding_cooldown_rounds
                condition_future_benefit_possible = current_game_round <= r - self.final_defection_rounds - 1
                if condition_low_pool and condition_cooldown_passed and condition_future_benefit_possible:
                    action_to_take = Action.C
                    self.last_prodding_round = current_game_round
            return action_to_take

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    Adaptive Free-Rider with Strategic Lure strategy for the N-Player Public Goods Game.
    This strategy aims to exploit cooperation by defecting when enough others contribute,
    but strategically contributes to "re-seed" cooperation if it drops too low, for a
    limited number of rounds. It also contributes in the first round to "lure" and
    ruthlessly defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_floor: float = 0.4
        self.PatienceRounds: int = 2
        self.consecutive_low_coop_contributions: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            self.consecutive_low_coop_contributions = 0
            return Action.C
        if current_round_zero_indexed == n_rounds - 1:
            return Action.D
        N_C_others_prev = np.sum(history.opponent_actions[-1, :])
        n_other_players = n_players - 1
        P_C_others_prev = N_C_others_prev / n_other_players
        if P_C_others_prev >= self.T_floor:
            self.consecutive_low_coop_contributions = 0
            return Action.D
        elif self.consecutive_low_coop_contributions < self.PatienceRounds:
            self.consecutive_low_coop_contributions += 1
            return Action.C
        else:
            self.consecutive_low_coop_contributions = 0
            return Action.D

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    The "Adaptive Predator" strategy is designed to maximize individual payoff in the repeated Public Goods Game
    by primarily free-riding on the contributions of others. It operates in distinct phases:
    1.  Endgame Phase: Defects unconditionally in the final rounds to secure immediate payoff.
    2.  Initial Round: Starts by defecting to exploit naive cooperators and observe initial behavior.
    3.  Adaptive Exploitation:
        -   If other players' cooperation is sufficiently high, it defects to free-ride.
        -   If other players' cooperation is consistently critically low, it cooperates once as a "stimulus"
            to prevent the public good from collapsing entirely, which would diminish future exploitation opportunities.
        -   Otherwise (moderate cooperation), it defaults to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.exploitation_threshold_ratio = 0.6
        self.stimulus_threshold_ratio = 0.2
        self.stimulus_consecutive_low_rounds = 3
        self.endgame_rounds = max(2, min(5, round(self.r * 0.2)))
        self.stimulus_low_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.r - self.endgame_rounds:
            return Action.D
        elif current_round_1_indexed == 1:
            return Action.D
        else:
            num_others_contributed = sum(history.opponent_actions[-1, :])
            exploitation_cooperation_threshold = round((self.n - 1) * self.exploitation_threshold_ratio)
            stimulus_cooperation_threshold = round((self.n - 1) * self.stimulus_threshold_ratio)
            if num_others_contributed >= exploitation_cooperation_threshold:
                self.stimulus_low_count = 0
                return Action.D
            elif num_others_contributed <= stimulus_cooperation_threshold:
                self.stimulus_low_count += 1
                if self.stimulus_low_count >= self.stimulus_consecutive_low_rounds:
                    self.stimulus_low_count = 0
                    return Action.C
                else:
                    return Action.D
            else:
                self.stimulus_low_count = 0
                return Action.D

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize its own payoff in the Public Goods Game
    by strategically balancing initial cooperation to foster a common pool, aggressive defection
    to free-ride, re-engagement when collective cooperation wanes, and outright exploitation
    in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_lure_rounds_count = max(1, min(2, math.floor(self.r / 5)))
        self.endgame_defect_rounds_count = max(1, min(3, math.floor(self.r / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.r == 1:
            return Action.D
        if current_round_1_indexed > self.r - self.endgame_defect_rounds_count:
            return Action.D
        if current_round_1_indexed <= self.initial_lure_rounds_count:
            return Action.C
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        n_others = self.n - 1
        if c_others_prev < math.floor(n_others / 2):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    The Adaptive Free-Rider with Probing (AFRP) strategy aims to maximize individual payoff
    by primarily free-riding on the contributions of others. It balances the immediate
    incentive to defect with the need to maintain a sufficiently high level of overall
    cooperation to ensure there's something substantial to free-ride on. It is adaptive
    to opponent behavior and robust across various game settings, employing initial
    cooperation, adaptive exploitation with occasional probes, and endgame defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperate_rounds = 2
        self.endgame_rounds = 1
        self.high_cooperation_threshold = 0.6
        self.low_cooperation_threshold = 0.2
        self.probe_interval = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round < self.initial_cooperate_rounds:
            return Action.C
        elif current_round >= n_rounds - self.endgame_rounds:
            return Action.D
        past_round_contributions_of_others = sum(history.opponent_actions[-1])
        other_players_count = n_players - 1
        other_cooperation_rate = past_round_contributions_of_others / other_players_count
        if other_cooperation_rate >= self.high_cooperation_threshold:
            return Action.D
        elif (current_round + 1) % self.probe_interval == 0 and other_cooperation_rate <= self.low_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy adapts to observed opponent behavior,
    free-riding when possible and strategically minimizing contributions while preventing
    the complete collapse of the public good when beneficial. It leverages game phases
    (initial, adaptive, end-game) to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.num_end_game_rounds = max(1, math.floor(self.r / 4))
        self.end_game_start_round_0_idx = self.r - self.num_end_game_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        current_round_t = current_round_0_idx + 1
        if current_round_t == 1:
            return Action.D
        elif current_round_0_idx >= self.end_game_start_round_0_idx:
            return Action.D
        else:
            n_other_players = self.n - 1
            if n_other_players == 0:
                return Action.D
            num_others_cooperated_prev = sum(history.opponent_actions[-1, :])
            P_others_prev = num_others_cooperated_prev / n_other_players
            if P_others_prev < 0.2:
                return Action.D
            elif P_others_prev >= 0.75:
                return Action.D
            else:
                p = P_others_prev * (self.k / self.n)
                if random.random() < p:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    The Self-Preserving Parasite strategy.
    This strategy is highly exploitative, defaulting to free-riding. It is designed
    to make minimal, calculated contributions only when necessary to prevent the
    complete collapse of the common good, thereby preserving long-term
    opportunities for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.exploitation_threshold = 0.6
        self.collapse_threshold = 0.2
        self.reactivation_trigger = 3
        self.consecutive_defections_in_critical_zone = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        c_others_prev = sum(history.opponent_actions[current_round - 1, :])
        fraction_others_coop = c_others_prev / (self.n - 1)
        if fraction_others_coop >= self.exploitation_threshold:
            self.consecutive_defections_in_critical_zone = 0
            return Action.D
        elif fraction_others_coop < self.collapse_threshold:
            self.consecutive_defections_in_critical_zone = 0
            return Action.D
        else:
            self.consecutive_defections_in_critical_zone += 1
            if self.consecutive_defections_in_critical_zone >= self.reactivation_trigger:
                self.consecutive_defections_in_critical_zone = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy maximizes individual payoff by aggressively
    free-riding on the contributions of others. It strategically makes minimal
    contributions only when necessary to prevent the complete collapse of the
    public good, thereby preserving opportunities for future exploitation.

    Decision Rules:
    - Round 1 (Initial Bait Phase): Contribute (C) to signal cooperation.
    - Intermediate Rounds:
        - Observe the average cooperation of other players in the previous round.
        - If average cooperation is >= 0.35, Defect (D) to free-ride.
        - If average cooperation is < 0.35, Contribute (C) to reactivate cooperation
          and prevent collapse, thereby preserving future free-riding opportunities.
    - Final Round: Defect (D) as there are no future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THRESHOLD_TO_REACTIVATE_COOPERATION = 0.35

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_number_of_rounds = self.game_description.n_rounds
        total_number_of_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_number_of_rounds - 1:
            return Action.D
        sum_of_contributions_by_other_players_prev_round = sum(history.opponent_actions[-1, :])
        num_other_players = total_number_of_players - 1
        if num_other_players == 0:
            return Action.D
        average_cooperation_other_players_prev_round = sum_of_contributions_by_other_players_prev_round / num_other_players
        if average_cooperation_other_players_prev_round >= self.THRESHOLD_TO_REACTIVATE_COOPERATION:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff in the
    N-Player Public Goods Game by strategically free-riding on others' contributions
    while selectively contributing to maintain a sufficient pool of cooperators
    for future exploitation, and to avoid being endlessly exploited itself.

    It begins with a probing contribution in the first round and defects in the
    final round. In intermediate rounds, it defects if others are sufficiently
    cooperative (above a calculated Exploitation_Threshold), or if it has been a
    solo contributor for two or more consecutive rounds without sufficient
    collective effort from others. Otherwise, it contributes to re-seed cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.my_consecutive_solo_contributions: int = 0
        self.exploitation_threshold = max(1, math.floor((self.n_players - 1) * self.k / self.n_players))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        c_others_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        my_prev_action_was_c = history.my_actions[state.round_number - 1]
        if c_others_prev_round == 0 and my_prev_action_was_c:
            self.my_consecutive_solo_contributions += 1
        else:
            self.my_consecutive_solo_contributions = 0
        if c_others_prev_round >= self.exploitation_threshold:
            return Action.D
        elif self.my_consecutive_solo_contributions >= 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal gain by free-riding on others' contributions.
    It initiates with a "probe" contribution, then defaults to defection as long as
    there's sufficient cooperation from others. If cooperation completely dries up,
    it attempts to "reboot" the system with a limited number of contributions
    to re-establish an exploitable environment. In the final round, it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._free_ride_threshold = 1
        self._zero_coop_tolerance_rounds = 3
        self._max_reboot_attempts = 2
        self._rounds_of_zero_other_contributions_consecutive = 0
        self._current_reboot_attempts_made = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            self._rounds_of_zero_other_contributions_consecutive = 0
            self._current_reboot_attempts_made = 0
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        s_c_others_t_minus_1 = sum(history.opponent_actions[-1, :])
        if s_c_others_t_minus_1 >= self._free_ride_threshold:
            self._rounds_of_zero_other_contributions_consecutive = 0
            return Action.D
        else:
            self._rounds_of_zero_other_contributions_consecutive += 1
            if self._rounds_of_zero_other_contributions_consecutive >= self._zero_coop_tolerance_rounds and self._current_reboot_attempts_made < self._max_reboot_attempts:
                self._current_reboot_attempts_made += 1
                self._rounds_of_zero_other_contributions_consecutive = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    The Adaptive Opportunist strategy is designed to be highly exploitative by strategically manipulating
    cooperation levels among other players to maximize its own free-riding opportunities, while
    safeguarding against the complete collapse of the public good. It adapts to observed behavior,
    using contributions as an investment or a bait, and defecting whenever a sufficient public good
    has been established by others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Initial_Probe_Rounds = 2
        self.Endgame_Defection_Rounds = 1
        self.High_Cooperation_Threshold = 0.5
        self.Low_Cooperation_Threshold = 0.2
        self.my_last_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        action: Action
        if current_round_1_indexed <= self.Initial_Probe_Rounds:
            action = Action.C
        elif current_round_1_indexed > total_rounds - self.Endgame_Defection_Rounds:
            action = Action.D
        else:
            n_other_players = n_players - 1
            sum_other_contributions_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            avg_other_contributions_prev_round = sum_other_contributions_prev_round / n_other_players
            if avg_other_contributions_prev_round >= self.High_Cooperation_Threshold:
                action = Action.D
            elif avg_other_contributions_prev_round < self.Low_Cooperation_Threshold:
                action = Action.C
            elif self.my_last_action == Action.C:
                action = Action.D
            else:
                action = Action.C
        self.my_last_action = action
        return action

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    The Adaptive Predator strategy for the N-Player Public Goods Game.
    It aims to maximize its own payoff by strategically free-riding on the contributions of others.
    It uses limited, calculated cooperation as a tool for inducing or re-establishing a public good,
    then reverts to free-riding, always prioritizing its own immediate and long-term gains.
    It adapts to falling cooperation by temporarily contributing and gives up on cooperation
    if others consistently defect at very low levels.
    """

    class Phase(Enum):
        INITIAL_BAIT = 'INITIAL_BAIT'
        EXPLOITING = 'EXPLOITING'
        REINDUCING_COOP = 'REINDUCING_COOP'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.COOP_BAIT_ROUNDS = 2
        self.ENDGAME_DEFECT_ROUNDS = 3
        self.COOP_MAINTENANCE_THRESHOLD = 0.5
        self.REIND_COOP_DURATION = 2
        self.FUTILITY_THRESHOLD = 0.1
        self.FUTILITY_ROUNDS_GIVE_UP = 3
        self.current_phase = self.Phase.INITIAL_BAIT
        self.re_induce_counter = 0
        self.futility_counter = 0
        self.num_other_players = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.r - self.ENDGAME_DEFECT_ROUNDS:
            return Action.D
        avg_others_c_prev_round = 0.0
        if state.round_number > 0:
            others_prev_actions = history.opponent_actions[-1, :]
            total_others_c_prev_round = np.sum(others_prev_actions)
            avg_others_c_prev_round = total_others_c_prev_round / self.num_other_players
        if state.round_number > 0:
            if avg_others_c_prev_round < self.FUTILITY_THRESHOLD:
                self.futility_counter += 1
            else:
                self.futility_counter = 0
        if self.current_phase is not self.Phase.INITIAL_BAIT and self.futility_counter >= self.FUTILITY_ROUNDS_GIVE_UP:
            self.current_phase = self.Phase.EXPLOITING
            self.re_induce_counter = 0
            return Action.D
        if self.current_phase is self.Phase.INITIAL_BAIT:
            if current_round_1_indexed <= self.COOP_BAIT_ROUNDS:
                return Action.C
            else:
                self.current_phase = self.Phase.EXPLOITING
                return Action.D
        elif self.current_phase is self.Phase.REINDUCING_COOP:
            if self.re_induce_counter < self.REIND_COOP_DURATION:
                self.re_induce_counter += 1
                return Action.C
            else:
                self.current_phase = self.Phase.EXPLOITING
                self.re_induce_counter = 0
                return Action.D
        elif self.current_phase is self.Phase.EXPLOITING:
            if avg_others_c_prev_round < self.COOP_MAINTENANCE_THRESHOLD:
                self.current_phase = self.Phase.REINDUCING_COOP
                self.re_induce_counter = 1
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    "The Calculated Free-Rider" strategy. This strategy is designed for
    exploitation in the repeated Public Goods Game, aiming to maximize
    individual payoff by strategically leveraging the contributions of others
    while minimizing its own contributions. It adapts to observed opponent
    behavior and game parameters, ensuring robustness across various scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.initial_r_rounds_count = max(1, min(3, r // 5))
        self.hopeless_threshold_factor = 0.15
        self.adaptive_strategic_threshold_factor = max(0.25, min(0.5, 0.2 + k / n * 0.3))
        self.n_minus_1 = n - 1
        if self.n_minus_1 > 0:
            self.min_hopeless_others = max(1, round(self.n_minus_1 * self.hopeless_threshold_factor))
            self.min_strategic_others = max(1, round(self.n_minus_1 * self.adaptive_strategic_threshold_factor))
        else:
            self.min_hopeless_others = 0
            self.min_strategic_others = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round == n_rounds - 1:
            return Action.D
        if current_round < self.initial_r_rounds_count:
            return Action.C
        s_others_prev = sum(history.opponent_actions[-1, :])
        if self.n_minus_1 == 0:
            return Action.D
        if s_others_prev < self.min_hopeless_others:
            return Action.D
        elif s_others_prev < self.min_strategic_others:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically leveraging
    the cooperation of other players. It employs an initial "lure" phase to
    encourage collective contributions, followed by an "exploitation" phase
    where it free-rides on others' contributions, and a final "retreat" phase
    where it always defects. Its adaptation comes from monitoring the collective
    cooperation level of opponents during the exploitation phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.LureRounds = 2
        self.ExploitThreshold = 0.5
        self.EndGameRounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.n_rounds == 1:
            return Action.D
        if current_round_1_indexed <= self.LureRounds:
            return Action.C
        if current_round_1_indexed > self.n_rounds - self.EndGameRounds:
            return Action.D
        num_other_players = self.n_players - 1
        sum_others_contributing_prev = np.sum(history.opponent_actions[-1, :])
        percentage_others_contributing_prev = sum_others_contributing_prev / num_other_players
        if percentage_others_contributing_prev >= self.ExploitThreshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    The Opportunistic Scavenger is an adaptive and exploitative strategy designed for the Public Goods Game.
    Its primary goal is to maximize personal payoff by primarily free-riding on the contributions of others.
    It strategically contributes only when necessary to prevent a complete collapse of the public good,
    which would eliminate future exploitation opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.exploitation_threshold = self.k_factor / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == self.n_rounds:
            return Action.D
        if current_round == 1:
            return Action.C
        c_others_t_minus_1 = np.sum(history.opponent_actions[state.round_number - 1, :])
        acro = c_others_t_minus_1 / (self.n_players - 1)
        if acro == 0:
            return Action.D
        elif acro >= self.exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.

    This strategy dynamically adjusts its contribution behavior based on observed collective cooperation.
    It aims to maximize its own payoff by strategically encouraging and then free-riding on others'
    contributions, while protecting itself from being a "sucker." It combines:
    1. An initial contribution to foster cooperation.
    2. A core adaptive free-riding mechanism that defects when others cooperate sufficiently,
       but contributes when cooperation wanes to "re-prime" the public good.
    3. A definitive end-game defection in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Opportunist strategy with game parameters.

        Args:
            game_description: A PublicGoodsDescription object containing n_players, n_rounds, and k.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None for the very first round (state.round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed > self.n_rounds - 3:
            return Action.D
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        if c_others_prev >= self.cooperation_threshold_T:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    The Scavenger strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on others' contributions.
    It defects by default, only contributing strategically when the public good is
    on the verge of collapse, to preserve future exploitation opportunities, and
    only if specific conditions regarding remaining rounds and recent defection are met.
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.rounds_since_my_last_contribution: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        chosen_action: Action
        if current_round_0_idx == 0:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            self.rounds_since_my_last_contribution = 1
            return chosen_action
        elif current_round_0_idx == n_rounds - 1:
            chosen_action = Action.D
            return chosen_action
        else:
            total_other_contributions_prev = np.sum(history.opponent_actions[current_round_0_idx - 1, :])
            chosen_action = Action.D
            current_round_1_idx = current_round_0_idx + 1
            critical_collapse_condition = total_other_contributions_prev < 2
            remaining_rounds = n_rounds - current_round_1_idx
            min_remaining_for_priming = math.ceil(0.1 * n_rounds)
            sufficient_remaining_game_condition = remaining_rounds > min_remaining_for_priming
            my_previous_action_was_defect_condition = self.my_last_action == Action.D
            not_recently_primed_condition = self.rounds_since_my_last_contribution >= 2
            if critical_collapse_condition and sufficient_remaining_game_condition and my_previous_action_was_defect_condition and not_recently_primed_condition:
                chosen_action = Action.C
            if chosen_action == Action.C:
                self.my_last_action = Action.C
                self.rounds_since_my_last_contribution = 0
            else:
                self.my_last_action = Action.D
                self.rounds_since_my_last_contribution += 1
            return chosen_action

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy aims to maximize individual payoff by predominantly free-riding.
    It strategically contributes only when necessary to induce or sustain a collective pool of
    contributions, which it then exploits. It operates in three phases:
    1.  Initial Seeding: Contributes for a few initial rounds to encourage cooperation.
    2.  Adaptive Exploitation: Dynamically decides to defect (free-ride) or contribute (re-seed)
        based on the proportion of other players' contributions in the previous round.
    3.  End Game Defection: Defects unconditionally in the final rounds due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r_total = game_description.n_rounds
        if self.r_total > 1:
            self.initial_cooperation_rounds = max(0, math.floor(self.r_total / 5))
        else:
            self.initial_cooperation_rounds = 0
        self.end_game_start_round = self.r_total - min(self.r_total, 3) + 1
        self.high_contribution_threshold_ratio = 0.5
        self.low_contribution_threshold_ratio = 0.25
        self.num_other_players = self.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round >= self.end_game_start_round:
            return Action.D
        if current_round <= self.initial_cooperation_rounds:
            return Action.C
        sum_other_contributions_last_round = np.sum(history.opponent_actions[-1, :])
        proportion_other_contributions = sum_other_contributions_last_round / self.num_other_players
        if proportion_other_contributions >= self.high_contribution_threshold_ratio:
            return Action.D
        elif proportion_other_contributions < self.low_contribution_threshold_ratio:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its individual payoff by consistently attempting to free-ride
    on the contributions of other players. It only contributes itself as a calculated,
    self-interested investment to prevent the complete collapse of the public good,
    which would eliminate future opportunities for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes The Adaptive Free-Rider strategy.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.COOPERATION_MAINTENANCE_THRESHOLD = 0.4
        self.DEFECTION_PUNISHMENT_WINDOW = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Free-Rider strategy.

        Args:
            state: The current game state, including the round number (0-indexed).
            history: The history of actions and payoffs from previous rounds for this player and opponents.
                     `history` will be None for round_number == 0.

        Returns:
            An Action enum (C or D).
        """
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if state.round_number == 0:
            return Action.D
        if state.round_number >= n_rounds - 2:
            return Action.D
        effective_window_size = min(state.round_number, self.DEFECTION_PUNISHMENT_WINDOW)
        recent_opponent_actions = history.opponent_actions[-effective_window_size:]
        total_contributions_by_others_in_window = np.sum(recent_opponent_actions)
        n_other_players = n_players - 1
        average_other_cooperation_rate = total_contributions_by_others_in_window / (n_other_players * effective_window_size)
        if average_other_cooperation_rate > self.COOPERATION_MAINTENANCE_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize individual payoff by primarily free-riding,
    while adaptively contributing only when necessary to prevent the collapse of the public good.
    It continuously adjusts its 'Exploitation_Threshold' based on past interactions to determine
    when to defect and when to contribute.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ET = 0.3
        self.ET_Min = 0.1
        self.ET_Max = 0.7
        self.ET_Adjustment_Step = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        total_other_contributions_last_round = np.sum(history.opponent_actions[-1, :])
        average_other_contributions_last_round = total_other_contributions_last_round / (n_players - 1)
        if average_other_contributions_last_round >= self.ET:
            my_action_for_current_round = Action.D
        else:
            my_action_for_current_round = Action.C
        my_action_last_round_was_C = history.my_actions[-1]
        if not my_action_last_round_was_C:
            if average_other_contributions_last_round >= self.ET:
                self.ET = min(self.ET_Max, self.ET + self.ET_Adjustment_Step)
            else:
                self.ET = max(self.ET_Min, self.ET - self.ET_Adjustment_Step)
        elif average_other_contributions_last_round >= self.ET:
            self.ET = min(self.ET_Max, self.ET + self.ET_Adjustment_Step)
        else:
            self.ET = max(self.ET_Min, self.ET - self.ET_Adjustment_Step)
        return my_action_for_current_round

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy prioritizes immediate self-interest by free-riding on others'
    contributions. It defaults to defection to maximize gains and only contributes
    when the level of cooperation from others falls critically low, threatening
    the existence of a public good worth exploiting. It is designed to never
    become a lone cooperator (a "sucker").
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        ratio_k_n = self.k / self.n_players
        base_threshold = (self.n_players - 1) * (1 - ratio_k_n)
        self.cooperation_revival_threshold = max(1, math.floor(base_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        c_other_last = np.sum(history.opponent_actions[-1, :])
        if c_other_last == 0:
            return Action.D
        elif c_other_last < self.cooperation_revival_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    The Adaptive Predator strategy for the N-Player Public Goods Game.

    This strategy is designed to maximize individual payoff by strategically exploiting
    cooperative tendencies in opponents while minimizing personal contributions when
    cooperation is weak or absent. It adapts to observed opponent behavior and
    prioritizes self-interest.

    Decision phases:
    1. Single Round Game: Always Defect (D).
    2. Initial Assessment Phase (rounds 1 to min(3, r-1)):
        - Round 1: Cooperate (C) as a "probe" to gather data.
        - Subsequent rounds in this phase: If P_others_prev (previous round's
          opponent cooperation proportion) >= 0.5, Cooperate (C); else Defect (D).
    3. Exploitation Phase (rounds min(3, r-1)+1 to r-1):
        - Always Defect (D). The strategy notes when conditions for strong
          exploitation (P_others_recent_avg >= 0.6) are met or not, but in both
          cases, it defects to free-ride or minimize loss.
    4. Final Round (r): Always Defect (D) due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if n_rounds == 1:
            return Action.D
        if current_round_1_indexed == n_rounds:
            return Action.D
        min_rounds_for_initial_phase = min(3, n_rounds - 1)
        if 1 <= current_round_1_indexed <= min_rounds_for_initial_phase:
            if current_round_1_indexed == 1:
                return Action.C
            else:
                prev_round_idx = state.round_number - 1
                total_prev_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
                num_other_players = n_players - 1
                P_others_prev = total_prev_cooperators / num_other_players
                if P_others_prev >= 0.5:
                    return Action.C
                else:
                    return Action.D
        start_history_idx = max(0, state.round_number - 3)
        recent_opponent_actions = history.opponent_actions[start_history_idx:state.round_number, :]
        total_recent_cooperators = np.sum(recent_opponent_actions)
        num_observation_rounds = recent_opponent_actions.shape[0]
        num_other_players = n_players - 1
        P_others_recent_avg = 0.0
        if num_observation_rounds > 0:
            P_others_recent_avg = total_recent_cooperators / (num_observation_rounds * num_other_players)
        if P_others_recent_avg >= 0.6:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed to be highly exploitative by maximizing its own payoff.
    It operates by constantly assessing the cooperative behavior of other players and adjusting its actions
    to secure the highest personal gain. Its core principle is to default to defection unless a strategic
    reason to contribute emerges, always prioritizing self-interest.

    It employs a calculated initial contribution to 'prime the pump', aggressively defects in endgame
    rounds to maximize final payouts, and in mid-game, it free-rides when cooperation is sufficient
    or makes temporary investments to prevent the collapse of collective contributions, always aiming
    to maintain an environment conducive to future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_cooperation = 0.5
        self.N_endgame_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round >= n_rounds - self.N_endgame_rounds:
            return Action.D
        if current_round == 0:
            return Action.C
        c_others_previous_round = sum(history.opponent_actions[-1, :])
        num_others = n_players - 1
        avg_c_others_previous_round = c_others_previous_round / num_others
        if avg_c_others_previous_round >= self.T_cooperation:
            return Action.D
        elif avg_c_others_previous_round > 0 and avg_c_others_previous_round < self.T_cooperation:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    The "Adaptive Predator" strategy aims to maximize individual payoff by primarily exploiting the contributions of others.
    It employs a default defection policy, strategically contributes only when the public good pool is completely depleted
    to "bait" new contributions, and ruthlessly defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_zero_contributions_count: int = 0
        self.test_contribution_limit: int = 3
        self.endgame_defection_rounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.D
        if current_round_number >= total_rounds - self.endgame_defection_rounds:
            self.consecutive_zero_contributions_count = 0
            return Action.D
        total_contributions_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if total_contributions_prev == 0:
            self.consecutive_zero_contributions_count += 1
        else:
            self.consecutive_zero_contributions_count = 0
        if total_contributions_prev > 0:
            return Action.D
        elif self.consecutive_zero_contributions_count <= self.test_contribution_limit:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy is designed to maximize individual payoff by strategically exploiting
    other players' contributions while maintaining a sufficient level of collective
    cooperation to free-ride upon. It adapts to opponent behaviors and is robust
    across varying game parameters.

    Phases (prioritized):
    1.  **Final Round Phase:** Defect (D) in the very last round to maximize terminal payoff.
    2.  **Lure Phase:** Contribute (C) in the initial `LURE_ROUNDS` to encourage cooperation.
    3.  **Exploit and Maintain Phase:** In intermediate rounds, evaluate past opponent contributions.
        If contributions are high enough (`>= EXPLOIT_THRESHOLD`), defect (D) to free-ride.
        Otherwise, contribute (C) to re-lure or maintain cooperation.
    """
    LURE_ROUNDS: int = 2
    EXPLOIT_THRESHOLD: float = 0.6

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == n_rounds:
            return Action.D
        elif current_round_1_indexed <= self.LURE_ROUNDS:
            return Action.C
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            sum_others_contributions = np.sum(previous_round_opponent_actions)
            n_others = n_players - 1
            avg_others_contribution_prev = sum_others_contributions / n_others
            if avg_others_contribution_prev >= self.EXPLOIT_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    Adaptive Free-Rider with Calculated Inducement.

    This strategy aims to maximize its own payoff in a repeated Public Goods Game
    by primarily free-riding. It only contributes when the observed contribution
    rate from others falls below a calculated threshold, signaling a risk of
    public good collapse. These contributions are strategic, designed to induce
    others to contribute again, allowing the strategy to resume free-riding.
    The strategy always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_induction_counter: int = 0
        k_over_n_ratio = self.k / self.n
        self.exploitation_threshold_low: float = max(0.1, min(0.7, 0.5 * k_over_n_ratio + 0.1))
        self.cooperation_induction_duration: int = max(1, min(3, math.floor(self.r / 5)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        total_other_contributions_prev_round = sum(history.opponent_actions[-1, :])
        proportion_other_contributors_prev_round = total_other_contributions_prev_round / (self.n - 1)
        if self.cooperation_induction_counter > 0:
            self.cooperation_induction_counter -= 1
            return Action.C
        elif proportion_other_contributors_prev_round < self.exploitation_threshold_low:
            self.cooperation_induction_counter = self.cooperation_induction_duration - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    The Conditional Exploiter strategy for the N-Player Public Goods Game.
    This strategy adaptively responds to the collective behavior of the group,
    attempting to stimulate contributions when they drop too low, only to
    revert to free-riding when cooperation is sufficient. It is designed
    to maximize individual payoff by strategically leveraging cooperation.
    """
    OBSERVATION_WINDOW = 3
    COOPERATION_THRESHOLD = 0.6
    LURE_DURATION = 2
    END_GAME_DEFECT_ROUNDS = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.lure_in_progress: bool = False
        self.lure_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > r - self.END_GAME_DEFECT_ROUNDS:
            return Action.D
        if self.lure_in_progress:
            if self.lure_rounds_remaining > 0:
                self.lure_rounds_remaining -= 1
                return Action.C
            else:
                self.lure_in_progress = False
        if current_round_1_indexed == 1:
            return Action.C
        rounds_to_observe = min(state.round_number, self.OBSERVATION_WINDOW)
        opponent_actions_in_window = history.opponent_actions[state.round_number - rounds_to_observe:state.round_number, :]
        total_other_contributions_in_window = np.sum(opponent_actions_in_window)
        max_possible_contributions = (n - 1) * rounds_to_observe
        current_avg_other_c_rate = total_other_contributions_in_window / max_possible_contributions
        if current_avg_other_c_rate >= self.COOPERATION_THRESHOLD:
            return Action.D
        else:
            self.lure_in_progress = True
            self.lure_rounds_remaining = self.LURE_DURATION - 1
            return Action.C

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    The Strategic Opportunist strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative by primarily free-riding
    on others' contributions. It adaptively attempts to re-establish cooperation
    if the public good collapses, but only for a limited number of "probes"
    before resorting to permanent defection, thus ensuring robustness against
    universally uncooperative environments.
    """
    _R_ENDGAME_DEFECTION_HORIZON = 2
    _COOPERATION_DEFECTION_THRESHOLD = 0.5
    _PROBE_LENGTH = 2
    _MAX_FAILED_PROBES = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action_was_probe: bool = False
        self.probe_rounds_left: int = 0
        self.consecutive_failed_probes: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.game_description.n_rounds - self._R_ENDGAME_DEFECTION_HORIZON:
            self.my_last_action_was_probe = False
            self.probe_rounds_left = 0
            self.consecutive_failed_probes = 0
            return Action.D
        if self.my_last_action_was_probe:
            if self.probe_rounds_left > 0:
                self.probe_rounds_left -= 1
                return Action.C
            else:
                self.my_last_action_was_probe = False
        if state.round_number == 0:
            self.my_last_action_was_probe = True
            self.probe_rounds_left = self._PROBE_LENGTH - 1
            self.consecutive_failed_probes += 1
            return Action.C
        s_others_prev_round = sum(history.opponent_actions[-1, :])
        n_others = self.game_description.n_players - 1
        avg_c_rate_others = s_others_prev_round / n_others
        if avg_c_rate_others >= self._COOPERATION_DEFECTION_THRESHOLD:
            self.consecutive_failed_probes = 0
            return Action.D
        elif self.consecutive_failed_probes < self._MAX_FAILED_PROBES:
            self.my_last_action_was_probe = True
            self.probe_rounds_left = self._PROBE_LENGTH - 1
            self.consecutive_failed_probes += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    The Adaptive Exploit (AE) strategy aims to maximize individual payoff in the Public Goods Game
    by primarily free-riding while adaptively making strategic contributions only when necessary
    to sustain a sufficiently cooperative environment from which to benefit.
    It identifies cooperators in early rounds, exploits a healthy cooperative pool,
    and re-engages with contributions if cooperation from others is declining but not yet "dead,"
    all while avoiding wasted contributions on persistent defectors or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.warmup_rounds = 2
        self.minimum_active_cooperators = 1
        calculated_threshold = self.game_description.k / self.game_description.n_players / 2
        self.re_engagement_threshold = max(0.05, calculated_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if history is None:
            return Action.D
        my_prev_contribution_int = int(history.my_actions[-1])
        others_prev_contributions_sum = np.sum(history.opponent_actions[-1, :])
        total_prev_contributions = my_prev_contribution_int + others_prev_contributions_sum
        if current_round < self.warmup_rounds:
            if total_prev_contributions > 0:
                return Action.C
            else:
                return Action.D
        if current_round == n_rounds - 1:
            return Action.D
        n_other_players = n_players - 1
        num_other_cooperators_prev = others_prev_contributions_sum
        avg_others_contrib_rate = num_other_cooperators_prev / n_other_players
        if num_other_cooperators_prev < self.minimum_active_cooperators:
            return Action.D
        elif avg_others_contrib_rate < self.re_engagement_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on others' contributions.
    It encourages cooperation only when it's beneficial for future exploitation and quickly reverts to
    defection when cooperation is not profitable or sustainable.

    The strategy operates in three main phases:
    1. Initial Bait Phase: Contributes to encourage cooperation from others.
    2. Endgame Phase: Always defects to maximize final round payoff.
    3. Exploitation & Maintenance Phase: Adapts its action based on the average contribution of other
       players in the previous round, aiming to free-ride or re-induce cooperation strategically.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.BAIT_ROUNDS = 2
        self.EXPLOIT_THRESHOLD = 0.5
        self.REINDUCEMENT_THRESHOLD = 0.2
        self.ENDGAME_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        if current_round < self.BAIT_ROUNDS:
            return Action.C
        elif current_round >= total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        else:
            total_other_contributions_prev_round = np.sum(history.opponent_actions[-1, :])
            avg_other_contributions_prev_round = total_other_contributions_prev_round / (n_players - 1)
            if avg_other_contributions_prev_round >= self.EXPLOIT_THRESHOLD:
                return Action.D
            elif avg_other_contributions_prev_round >= self.REINDUCEMENT_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    The Calculated Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by primarily free-riding on others' contributions,
    while strategically contributing only when necessary to initiate or revive a level of group
    cooperation that can then be exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.defection_threshold = max(0.2, k / n * 0.8)
        self.re_cooperate_interval = 3
        self.my_consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == 0:
            self.my_consecutive_defections = 0
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        sum_c_others = np.sum(history.opponent_actions[-1, :])
        n_others = n_players - 1
        avg_c_others = sum_c_others / n_others
        action_to_take = Action.D
        if avg_c_others >= self.defection_threshold:
            action_to_take = Action.D
        elif self.my_consecutive_defections >= self.re_cooperate_interval and avg_c_others > 0:
            action_to_take = Action.C
        else:
            action_to_take = Action.D
        if action_to_take == Action.C:
            self.my_consecutive_defections = 0
        else:
            self.my_consecutive_defections += 1
        return action_to_take

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    The Adaptive Free-Rider strategy.

    This strategy aims to maximize its own payoff by selectively cooperating
    to encourage others, free-riding on their contributions, and exploiting
    the endgame. It operates in three distinct phases:

    1.  Initial Lure Phase: In the first few rounds, the strategy cooperates
        to establish a reputation and encourage other players to contribute
        to the public good.

    2.  Adaptive Exploitation Phase: During the main part of the game, the
        strategy's default action is to defect, thus free-riding on others'
        contributions. However, if the observed average level of cooperation
        from other players in recent rounds drops below a calculated threshold,
        the strategy will temporarily cooperate. This "investment" aims to
        signal renewed cooperation, attempting to re-engage others and restore
        a higher level of public good for future free-riding opportunities.

    3.  Endgame Defection Phase: In the final rounds, the strategy defects
        universally. As there are no future consequences to consider, this
        phase prioritizes immediate self-maximization by contributing nothing
        and free-riding on any remaining contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_initial_rounds = max(1, min(3, math.floor(self.r / 4)))
        self.T_endgame_rounds = max(1, min(3, math.floor(self.r / 4)))
        self.Cooperation_Revival_Threshold = max(1, math.floor((self.n - 1) * 0.5))
        self.W_recent_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.r - self.T_endgame_rounds:
            return Action.D
        elif current_round_1_indexed <= self.T_initial_rounds:
            return Action.C
        else:
            lookback_start_round_1_indexed = max(self.T_initial_rounds + 1, current_round_1_indexed - self.W_recent_rounds)
            history_slice_start_idx = lookback_start_round_1_indexed - 1
            history_slice_end_idx = state.round_number
            relevant_history_actions = history.opponent_actions[history_slice_start_idx:history_slice_end_idx]
            num_rounds_in_window = relevant_history_actions.shape[0]
            average_others_contributions = 0.0
            if num_rounds_in_window > 0:
                total_others_contributions_sum = np.sum(relevant_history_actions)
                average_others_contributions = total_others_contributions_sum / num_rounds_in_window
            else:
                total_others_contributions_sum_prev_round = np.sum(history.opponent_actions[self.T_initial_rounds - 1])
                average_others_contributions = total_others_contributions_sum_prev_round / 1.0
            if average_others_contributions < self.Cooperation_Revival_Threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    The Adaptive Exploiter strategy for N-Player Public Goods Game.
    Primarily free-rides, but cooperates to maintain profitable common pool
    or prevent collapse, and defects in end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if self.r >= 2 and (current_round_number == self.r - 1 or current_round_number == self.r - 2):
            return Action.D
        previous_round_cooperation_score_others = np.sum(history.opponent_actions[current_round_number - 1, :])
        overall_sum_others_contributions = np.sum(history.opponent_actions)
        num_past_rounds = current_round_number
        total_possible_others_contributions = (self.n - 1) * num_past_rounds
        overall_average_cooperation_rate_others = overall_sum_others_contributions / total_possible_others_contributions
        if overall_average_cooperation_rate_others >= 0.75:
            return Action.D
        elif previous_round_cooperation_score_others >= math.ceil(self.n / self.k):
            return Action.C
        elif previous_round_cooperation_score_others > 0 and overall_average_cooperation_rate_others >= 0.3:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    The Cunning Free-Rider strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain by free-riding on the contributions
    of others, only contributing when absolutely necessary to prevent the complete
    collapse of the public good pool, and then immediately reverting to free-riding.
    It is adaptive to opponent behavior and robust across various game settings.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.just_baited_last_round: bool = False
        self.r_endgame_start_1_indexed: int = max(2, self.n_rounds - 3)
        self.coop_recharge_threshold: int = max(1, math.floor((self.n_players - 1) / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.just_baited_last_round = False
            return Action.D
        elif current_round_1_indexed >= self.r_endgame_start_1_indexed:
            self.just_baited_last_round = False
            return Action.D
        else:
            if self.just_baited_last_round:
                self.just_baited_last_round = False
                return Action.D
            c_others_prev = sum(history.opponent_actions[-1, :])
            if c_others_prev >= self.coop_recharge_threshold:
                return Action.D
            elif c_others_prev == 0:
                self.just_baited_last_round = True
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize individual payoff by
    strategically free-riding on the contributions of others while ensuring the
    public good doesn't collapse entirely. It balances aggressive exploitation
    with a pragmatic approach to maintaining a minimal level of cooperation.

    It operates in three phases:
    1. Initial Investment: Contributes unconditionally in early rounds to signal
       cooperation and establish a public good.
    2. Endgame: Defects unconditionally in the final rounds to maximize
       short-term gains when future interactions are not possible.
    3. Adaptive Exploitation: In intermediate rounds, it defects if a sufficient
       number of other players contributed in the previous round, otherwise, it
       contributes to re-seed cooperation and prevent collapse.
    """
    _INITIAL_INVESTMENT_ROUNDS = 2
    _ENDGAME_ROUNDS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._CONTRIBUTION_MAINTENANCE_THRESHOLD = math.ceil((self.n - 1) * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round < self._INITIAL_INVESTMENT_ROUNDS:
            return Action.C
        if current_round >= self.r - self._ENDGAME_ROUNDS:
            return Action.D
        other_contributions_prev_round = sum(history.opponent_actions[-1, :])
        if other_contributions_prev_round >= self._CONTRIBUTION_MAINTENANCE_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    "The Adaptive Opportunist" is designed to maximize individual payoff in the
    Public Goods Game by primarily free-riding on others' contributions. It
    defects by default and contributes only when the observed cooperation rate
    of other players falls below a predefined threshold, acting as a strategic
    "re-prime" to stimulate more cooperation. After any such contribution, it
    immediately reverts to defection. It always defects in the very first and
    very last rounds of the game to maximize immediate gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.cooperation_history: list[int] = []
        self.lookback_rounds: int = 3
        self.cooperation_revival_threshold: float = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_0_indexed > 0:
            sum_other_contributions_prev_round = int(np.sum(history.opponent_actions[-1, :]))
            self.cooperation_history.append(sum_other_contributions_prev_round)
        if current_round_0_indexed == 0:
            action = Action.D
            self.my_last_action = action
            return action
        if current_round_0_indexed == r - 1:
            action = Action.D
            self.my_last_action = action
            return action
        effective_lookback = min(current_round_0_indexed, self.lookback_rounds)
        recent_cooperation_rate = 0.0
        if effective_lookback > 0:
            sum_other_contributions_recent = sum(self.cooperation_history[-effective_lookback:])
            possible_max_contributions = (n - 1) * effective_lookback
            recent_cooperation_rate = sum_other_contributions_recent / possible_max_contributions
        if self.my_last_action == Action.C:
            action = Action.D
        elif recent_cooperation_rate < self.cooperation_revival_threshold:
            action = Action.C
        else:
            action = Action.D
        self.my_last_action = action
        return action

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    The Calculated Predator strategy: A highly exploitative strategy designed to maximize individual gain
    by strategically leveraging or disincentivizing cooperation from others. It free-rides when possible,
    only contributing as a calculated investment to re-establish an exploitable cooperative environment,
    or to avoid being a complete "sucker" when cooperation has entirely collapsed.
    """
    INITIAL_PROBE_ROUNDS: int = 3
    END_GAME_ROUNDS: int = 5
    HIGH_COOPERATION_THRESHOLD: float = 0.7
    CRITICAL_COOPERATION_THRESHOLD: float = 0.3
    RESET_CONTRIBUTION_INTERVAL: int = 5
    MEMORY_ROUNDS: int = 5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds_total = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.rounds_since_last_C: int = 0
        self.my_last_action: Action | None = None
        self.opponent_cooperation_history: list[float] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        chosen_action: Action
        if current_round < self.INITIAL_PROBE_ROUNDS:
            chosen_action = Action.C
        elif current_round >= self.n_rounds_total - self.END_GAME_ROUNDS:
            chosen_action = Action.D
        else:
            if history is not None and current_round > 0:
                previous_round_opponent_actions = history.opponent_actions[-1, :]
                num_opponents = self.n_players - 1
                current_opponent_cooperation_rate = sum(previous_round_opponent_actions) / num_opponents
                self.opponent_cooperation_history.append(current_opponent_cooperation_rate)
                if len(self.opponent_cooperation_history) > self.MEMORY_ROUNDS:
                    self.opponent_cooperation_history.pop(0)
            avg_opponent_cooperation_rate_longterm = np.mean(self.opponent_cooperation_history) if self.opponent_cooperation_history else 0.0
            if avg_opponent_cooperation_rate_longterm >= self.HIGH_COOPERATION_THRESHOLD:
                chosen_action = Action.D
            elif avg_opponent_cooperation_rate_longterm <= self.CRITICAL_COOPERATION_THRESHOLD:
                chosen_action = Action.D
            elif self.my_last_action == Action.D and self.rounds_since_last_C >= self.RESET_CONTRIBUTION_INTERVAL:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        if chosen_action == Action.C:
            self.rounds_since_last_C = 0
        else:
            self.rounds_since_last_C += 1
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    The Adaptive Extractor strategy for the N-Player Public Goods Game.

    This strategy focuses on free-riding by default, making calculated "investments"
    only when necessary to sustain highly profitable levels of collective cooperation,
    and aggressively defecting when there's no longer a strategic benefit to contributing.

    It operates in three phases:
    1.  Endgame Phase: Aggressively defects in final rounds to maximize immediate payoff,
        leveraging backward induction.
    2.  Initial Assessment Phase: Defects in the first two rounds to observe opponent behavior
        and identify free-riding opportunities without contributing.
    3.  Adaptive Exploitation Phase: In mid-game, it defaults to defection, but will
        contribute (cooperate) if the observed collective cooperation from other players
        in the previous round was exceptionally high (75% or more of other players).
        This contribution is a strategic investment to sustain a highly profitable public good pool.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_contributions_so_far = 0
        self.last_round_others_contributions = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        C_others_in_prev_round = 0
        if history is not None and state.round_number > 0:
            C_others_in_prev_round = sum(history.opponent_actions[-1, :])
        r = self.game_description.n_rounds
        endgame_horizon = max(1, min(math.floor(r / 3), 5))
        if current_round_t > r - endgame_horizon:
            return Action.D
        elif current_round_t <= 2:
            return Action.D
        else:
            n_players = self.game_description.n_players
            n_minus_1 = n_players - 1
            cooperation_threshold = round(0.75 * n_minus_1)
            if C_others_in_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    The "Strategic Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own cumulative payoff by primarily free-riding,
    interspersed with minimal, calculated contributions only as strategic
    investments to encourage or re-establish a sufficiently large pool of
    public goods from others, which it can then exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.k_over_n = self.k / self.n
        self.re_bait_0_indexed_upper_bound = self.r - math.floor(self.r / 4) - 1
        self.W = max(3, math.floor(self.r / 5))
        self.num_opponents = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            if self.r == 1:
                return Action.D
            else:
                return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        default_action = Action.D
        condition1_enough_rounds_remaining = current_round_0_indexed < self.re_bait_0_indexed_upper_bound
        my_last_action = history.my_actions[current_round_0_indexed - 1]
        condition2_last_action_was_defect = not my_last_action
        others_contributed_last_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        condition3_low_recent_cooperation = others_contributed_last_round < self.n / 4
        average_others_contribution_rate = 0.0
        start_index_for_W_history = max(0, current_round_0_indexed - self.W)
        actual_rounds_in_window = current_round_0_indexed - start_index_for_W_history
        if actual_rounds_in_window > 0 and self.num_opponents > 0:
            relevant_opponent_actions_for_rate = history.opponent_actions[start_index_for_W_history:current_round_0_indexed, :]
            total_contributions_from_others_in_W = np.sum(relevant_opponent_actions_for_rate)
            total_possible_contributions_from_others_in_W = actual_rounds_in_window * self.num_opponents
            if total_possible_contributions_from_others_in_W > 0:
                average_others_contribution_rate = total_contributions_from_others_in_W / total_possible_contributions_from_others_in_W
        condition4_low_sustained_cooperation = average_others_contribution_rate < 0.3
        condition5_public_good_efficiency_high_enough = self.k_over_n > 0.3
        if condition1_enough_rounds_remaining and condition2_last_action_was_defect and condition3_low_recent_cooperation and condition4_low_sustained_cooperation and condition5_public_good_efficiency_high_enough:
            return Action.C
        else:
            return default_action

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to exploit the cooperation of other players
    while intelligently re-engaging or disengaging to prevent the game from collapsing
    entirely. It is highly adaptive to observed opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        n_others = self.game_description.n_players - 1
        cooperators_from_others_prev_round = sum(history.opponent_actions[prev_round_idx, :])
        P_coop_others = cooperators_from_others_prev_round / n_others
        my_last_action_was_cooperate = history.my_actions[prev_round_idx]
        if P_coop_others >= 0.6:
            return Action.D
        elif P_coop_others <= 0.2:
            return Action.D
        elif not my_last_action_was_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    The "Calculated Predator" Strategy

    This strategy is designed to be highly exploitative, consistently aiming to free-ride
    on the contributions of other players while strategically intervening only when
    absolutely necessary to prevent a complete collapse of cooperation, thereby
    preserving future exploitation opportunities. It prioritizes maximizing its own payoff
    and minimizes its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploit_threshold_high_cooperation = 0.6
        self.re_engagement_threshold_low_cooperation = 0.2
        self.re_engagement_max_rounds = 2
        self.re_engagement_cooldown_rounds = 3
        self.in_re_engagement_phase = False
        self.rounds_contributed_in_phase = 0
        self.cooldown_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == total_rounds - 1:
            return Action.D
        self.cooldown_timer += 1
        if self.in_re_engagement_phase:
            if self.rounds_contributed_in_phase < self.re_engagement_max_rounds:
                self.rounds_contributed_in_phase += 1
                return Action.C
            else:
                self.in_re_engagement_phase = False
                self.rounds_contributed_in_phase = 0
                return Action.D
        else:
            num_other_players = n_players - 1
            if num_other_players == 0:
                proportion_cooperating_others = 0.0
            else:
                sum_of_other_contributions = np.sum(history.opponent_actions[-1, :])
                proportion_cooperating_others = sum_of_other_contributions / num_other_players
            if proportion_cooperating_others >= self.exploit_threshold_high_cooperation:
                return Action.D
            elif proportion_cooperating_others < self.re_engagement_threshold_low_cooperation and self.cooldown_timer >= self.re_engagement_cooldown_rounds:
                self.in_re_engagement_phase = True
                self.rounds_contributed_in_phase = 1
                self.cooldown_timer = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize individual payoff in the
    Public Goods Game by strategically oscillating between contribution and defection.
    It fosters initial cooperation, exploits others when their cooperation is high,
    and contributes only when the public good is critically low to prevent collapse,
    ensuring continued opportunities for free-riding. It always defects in the final round.
    The strategy dynamically adjusts its thresholds for exploitation and revival
    based on the k/n ratio, making it robust across different game parameter settings.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the Adaptive Free-Rider strategy.
        Pre-calculates dynamic thresholds based on game parameters k and n.
        """
        self.game_description = game_description
        self.k_over_n_ratio = self.game_description.k / self.game_description.n_players
        self.exploit_threshold_upper = min(0.9, 0.4 + 0.5 * self.k_over_n_ratio)
        self.revive_threshold_lower = max(0.1, 0.1 + 0.2 * self.k_over_n_ratio)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Free-Rider strategy rules.
        """
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        c_others_prev = sum(history.opponent_actions[state.round_number - 1, :])
        num_other_players = n_players - 1
        current_cooperation_ratio = c_others_prev / num_other_players
        if current_cooperation_ratio >= self.exploit_threshold_upper:
            return Action.D
        elif current_cooperation_ratio <= self.revive_threshold_lower:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    The Adaptive Leech strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by generally free-riding on
    the contributions of others. It only contributes when strategically necessary
    to cultivate or rescue the public good for future exploitation, or as an
    initial investment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.terminal_exploitation_rounds = max(1, min(3, math.floor(self.n_rounds / 5)))
        self.high_cooperation_threshold_fraction = 0.7
        self.low_cooperation_threshold_fraction = 0.3
        self.other_player_effort_threshold = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round >= self.n_rounds - self.terminal_exploitation_rounds:
            return Action.D
        my_c_prev = int(history.my_actions[current_round - 1])
        S_others_prev = np.sum(history.opponent_actions[current_round - 1, :])
        S_prev = my_c_prev + S_others_prev
        avg_others_prev = 0.0
        avg_others_prev = S_others_prev / (self.n_players - 1)
        if S_prev >= self.n_players * self.high_cooperation_threshold_fraction:
            return Action.D
        elif S_prev < self.n_players * self.low_cooperation_threshold_fraction:
            if avg_others_prev >= self.other_player_effort_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy is designed to be exploitative by maximizing free-riding
    opportunities while adaptively responding to maintain a sufficient level
    of cooperation from others. It defaults to defection and only contributes
    strategically to prevent a complete collapse of the public good, thereby
    preserving future free-riding potential.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.assessment_window: int = 4
        self.min_other_cooperation_rate: float = 0.3
        self.my_d_streak_limit: int = 3
        self.end_game_horizon: int = 1
        self.my_consecutive_defections: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            chosen_action = Action.D
            self.my_consecutive_defections = 1
            return chosen_action
        if current_round >= n_rounds - self.end_game_horizon:
            chosen_action = Action.D
            self.my_consecutive_defections = 0
            return chosen_action
        relevant_opponent_actions = history.opponent_actions[-self.assessment_window:]
        total_other_contributions = np.sum(relevant_opponent_actions)
        num_other_players = n_players - 1
        num_relevant_rounds = relevant_opponent_actions.shape[0]
        denominator = num_other_players * num_relevant_rounds
        if denominator == 0:
            avg_other_contributions_rate = 0.0
        else:
            avg_other_contributions_rate = total_other_contributions / denominator
        if avg_other_contributions_rate >= self.min_other_cooperation_rate:
            chosen_action = Action.D
            self.my_consecutive_defections += 1
        elif self.my_consecutive_defections >= self.my_d_streak_limit:
            chosen_action = Action.C
            self.my_consecutive_defections = 0
        else:
            chosen_action = Action.D
            self.my_consecutive_defections += 1
        return chosen_action

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    The Opportunistic Provocateur strategy for the N-Player Public Goods Game.
    This strategy defaults to defection, contributing only when necessary
    to prevent the collapse of the common pool, aiming to exploit others' contributions.
    It strategically places "bait" contributions to stimulate cooperation from others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.last_action_was_bait_contribution = False
        self.exploitation_threshold_T = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.last_action_was_bait_contribution = False
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        n_others_contributed_prev = sum(history.opponent_actions[-1, :])
        if n_others_contributed_prev >= self.exploitation_threshold_T:
            self.last_action_was_bait_contribution = False
            return Action.D
        elif self.last_action_was_bait_contribution:
            self.last_action_was_bait_contribution = False
            return Action.D
        else:
            self.last_action_was_bait_contribution = True
            return Action.C

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    This strategy, "Calculated Coaxer and Ruthless Free-Rider," is designed to maximize
    individual payoff by strategically influencing other players to contribute to the
    public good, on which it then ruthlessly free-rides. It is adaptive, robust, and
    explicitly adheres to an exploitative mindset, only contributing when such actions
    are calculated to yield greater personal benefit through the induced cooperation
    of others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self._initial_coax_rounds = 1
        self._coax_responsiveness_threshold = 0.4
        self._re_coax_evaluation_window = 5
        self._cooperation_collapse_threshold = 0.2
        self._final_defect_horizon = 3
        self._initial_check_performed = False
        self._permanent_defection_mode = False
        self._last_contribution_round_number = -self._re_coax_evaluation_window
        self._effective_initial_coax_rounds = max(0, min(self._initial_coax_rounds, self.n_rounds - self._final_defect_horizon))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round >= self.n_rounds - self._final_defect_horizon:
            return Action.D
        if current_round < self._effective_initial_coax_rounds:
            self._last_contribution_round_number = current_round
            return Action.C
        if not self._initial_check_performed and current_round == self._effective_initial_coax_rounds:
            self._initial_check_performed = True
            initial_others_avg_contribution = 0.0
            if self._effective_initial_coax_rounds > 0 and history is not None and (history.opponent_actions.shape[0] >= self._effective_initial_coax_rounds):
                total_opponent_actions_in_window = self._effective_initial_coax_rounds * (self.n_players - 1)
                if total_opponent_actions_in_window > 0:
                    initial_others_avg_contribution = np.sum(history.opponent_actions[0:self._effective_initial_coax_rounds, :]) / total_opponent_actions_in_window
            if initial_others_avg_contribution < self._coax_responsiveness_threshold:
                self._permanent_defection_mode = True
        if self._permanent_defection_mode:
            return Action.D
        action_to_take = Action.D
        if history is not None and history.opponent_actions.shape[0] >= self._re_coax_evaluation_window:
            total_opponent_actions_in_window = self._re_coax_evaluation_window * (self.n_players - 1)
            if total_opponent_actions_in_window > 0:
                current_others_avg_contribution = np.sum(history.opponent_actions[-self._re_coax_evaluation_window:, :]) / total_opponent_actions_in_window
            else:
                current_others_avg_contribution = 0.0
            if current_others_avg_contribution < self._cooperation_collapse_threshold:
                if current_round - self._last_contribution_round_number >= self._re_coax_evaluation_window:
                    action_to_take = Action.C
                    self._last_contribution_round_number = current_round
        return action_to_take

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    The Opportunistic Defector strategy is designed to maximize individual payoff
    by strategically leveraging initial cooperation to enable subsequent free-riding,
    while being robust to a wide range of opponent behaviors and quickly cutting
    losses when cooperation is not forthcoming.

    Core Principles:
    1.  Seed Cooperation: Initiate cooperation to "prime the pump" and encourage
        others to contribute, creating a pool to exploit.
    2.  Exploit Willingness: Once a baseline of cooperation is established,
        predominantly defect to free-ride on others' contributions.
    3.  Cut Losses: If overall cooperation significantly degrades, cease all
        contributions to avoid being exploited, defaulting to defection.
    4.  Endgame Exploitation: Always defect in the final rounds, as there are no
        future consequences to consider.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.initial_cooperation_rounds = min(3, self.n_rounds - 1) if self.n_rounds > 1 else 0
        self.tolerance_threshold = 0.5
        self.endgame_defection_horizon = max(1, math.floor(self.n_rounds * 0.1))
        self.my_current_strategy_mode = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.my_current_strategy_mode = 'SEEDING'
        if current_round_1_indexed > self.n_rounds - self.endgame_defection_horizon:
            return Action.D
        while True:
            if self.my_current_strategy_mode == 'SEEDING':
                if current_round_1_indexed <= self.initial_cooperation_rounds:
                    return Action.C
                else:
                    self.my_current_strategy_mode = 'EXPLOITING'
            elif self.my_current_strategy_mode == 'EXPLOITING':
                if state.round_number > 0:
                    s_others_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
                    n_minus_1 = self.n_players - 1
                    ocl_prev = s_others_prev / n_minus_1
                    if ocl_prev >= self.tolerance_threshold:
                        return Action.D
                    else:
                        self.my_current_strategy_mode = 'DEGRADED'
                else:
                    self.my_current_strategy_mode = 'DEGRADED'
            elif self.my_current_strategy_mode == 'DEGRADED':
                return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    The Calculated Free-Rider strategy: Primarily defects to free-ride, but will make a minimal, 
    calculated "investment" (a single contribution) if group cooperation has demonstrably 
    declined after having previously existed. It never attempts to initiate cooperation 
    from a state of universal defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.kickstart_contribution_duration = 1
        self.cooperation_low_threshold = 0.2
        self.history_window = 5
        self.endgame_horizon = 2
        self.rounds_contributed_self_this_cycle = 0
        self.total_other_contributions_ever = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        contributions_from_others_last_round = np.sum(history.opponent_actions[-1, :])
        self.total_other_contributions_ever += contributions_from_others_last_round
        if state.round_number >= self.game_description.n_rounds - self.endgame_horizon:
            return Action.D
        if self.rounds_contributed_self_this_cycle > 0:
            if self.rounds_contributed_self_this_cycle < self.kickstart_contribution_duration:
                self.rounds_contributed_self_this_cycle += 1
                return Action.C
            else:
                self.rounds_contributed_self_this_cycle = 0
                return Action.D
        if self.total_other_contributions_ever == 0:
            return Action.D
        else:
            num_rounds_in_window = min(state.round_number, self.history_window)
            if num_rounds_in_window == 0:
                return Action.D
            window_opponent_actions = history.opponent_actions[-num_rounds_in_window:, :]
            sum_other_contributions_in_window = np.sum(window_opponent_actions)
            total_possible_other_contributions_in_window = (self.game_description.n_players - 1) * num_rounds_in_window
            recent_average_other_contribution = sum_other_contributions_in_window / total_possible_other_contributions_in_window
            if recent_average_other_contribution < self.cooperation_low_threshold:
                self.rounds_contributed_self_this_cycle = 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on
    other players' contributions. It uses an initial "probe" phase to encourage
    cooperation, then defaults to defection if others are contributing sufficiently.
    It includes a "re-probe" mechanism to re-initiate cooperation if the shared
    pool dwindles too much, and always defects in the final rounds due to backward
    induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Initial_Probe_Rounds = 2
        self.Cooperation_Threshold = 0.4
        self.Re_Probe_Interval = 5
        self.History_Window_Size = 3
        self.Endgame_Window_Size = max(1, min(5, self.game_description.n_rounds // 4))
        self.My_Consecutive_Defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        chosen_action: Action
        if current_round < self.Initial_Probe_Rounds:
            chosen_action = Action.C
        elif current_round >= n_rounds - self.Endgame_Window_Size:
            chosen_action = Action.D
        else:
            num_rounds_in_history = current_round
            num_rounds_to_consider = min(self.History_Window_Size, num_rounds_in_history)
            total_possible_contributions = (n_players - 1) * num_rounds_to_consider
            sum_opponent_cooperation = np.sum(history.opponent_actions[-num_rounds_to_consider:, :])
            avg_others_cooperation_rate = sum_opponent_cooperation / total_possible_contributions
            if avg_others_cooperation_rate >= self.Cooperation_Threshold:
                chosen_action = Action.D
            elif self.My_Consecutive_Defections >= self.Re_Probe_Interval:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        if chosen_action == Action.D:
            self.My_Consecutive_Defections += 1
        else:
            self.My_Consecutive_Defections = 0
        return chosen_action

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed to maximize individual payoff in a
    repeated Public Goods Game by selectively free-riding on others' contributions
    while strategically managing its own actions to prevent a collapse of the
    public good. It adapts based on observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_multiplier = game_description.k
        self.probe_rounds: int = 2
        self.exploit_duration: int = 2
        self.cooperation_threshold_upper: float = 0.7
        self.cooperation_threshold_lower: float = 0.3
        self.my_last_action: Action | None = None
        self.exploit_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        current_action: Action
        n_other_players = self.n_players - 1
        if current_round < self.probe_rounds:
            current_action = Action.C
            self.exploit_counter = 0
        elif current_round == self.n_rounds - 1:
            current_action = Action.D
            self.exploit_counter = 0
        else:
            num_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
            p_others_cooperated_prev_round = num_others_cooperated_prev_round / n_other_players
            if p_others_cooperated_prev_round >= self.cooperation_threshold_upper:
                if self.exploit_counter < self.exploit_duration:
                    current_action = Action.D
                    self.exploit_counter += 1
                else:
                    current_action = Action.C
                    self.exploit_counter = 0
            elif p_others_cooperated_prev_round <= self.cooperation_threshold_lower:
                current_action = Action.D
                self.exploit_counter = 0
            else:
                if self.my_last_action == Action.C:
                    current_action = Action.D
                else:
                    current_action = Action.C
                self.exploit_counter = 0
        self.my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    The Opportunistic Shark: An exploitative strategy for the N-Player Public Goods Game.
    It probes for cooperation in the first round, free-rides when others contribute,
    and attempts to re-ignite cooperation (up to a limit) when it falters.
    It defaults to defection in the final round and after failed re-seeding attempts,
    prioritizing its own payoff by dynamically adapting to observed opponent actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_solo_contributions: int = 0
        self.max_seed_attempts: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round == n_rounds - 1:
            return Action.D
        if current_round == 0:
            return Action.C
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev >= 1:
            self.consecutive_solo_contributions = 0
            return Action.D
        elif self.consecutive_solo_contributions < self.max_seed_attempts:
            self.consecutive_solo_contributions += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    The Opportunistic Parasite strategy aims to free-ride on others' contributions,
    only cooperating in the first round as an initial probe, and strategically
    contributing in intermediate rounds only when cooperation from other players
    drops to a critically low level, to prevent a total collapse of the shared
    public good. It defects in the final round to maximize immediate payoff,
    exploiting the finite horizon of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self._cooperation_floor_threshold = 0.25
        self._min_others_for_nudge = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        others_contributions_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        num_other_players = self.n_players - 1
        others_cooperation_rate_prev = others_contributions_prev / num_other_players
        if others_cooperation_rate_prev < self._cooperation_floor_threshold:
            if others_contributions_prev >= self._min_others_for_nudge:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by strategically free-riding on the contributions of others
    while adaptively responding to group cooperation levels. It proceeds in distinct phases depending on the current round number.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        prev_round_index = state.round_number - 1
        c_others_prev = sum(history.opponent_actions[prev_round_index, :])
        n_others = self.n_players - 1
        avg_c_others_prev = c_others_prev / n_others
        threshold_k_n = self.k / self.n_players
        if avg_c_others_prev >= threshold_k_n:
            return Action.D
        else:
            my_action_prev = history.my_actions[prev_round_index]
            if my_action_prev == True:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    "The Manipulative Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by strategically encouraging 
    others to contribute while minimizing its own contributions. It adapts to 
    observed behavior, trying to maintain a level of collective contribution 
    that it can free-ride upon, and disengaging when exploitation is no 
    longer profitable or when the game concludes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.free_ride_threshold: float = 0.5
        self.max_consecutive_free_rides: int = 3
        self.recovery_contribution_rounds: int = 2
        self.endgame_horizon: int = max(1, min(r // 4, 3))
        self.futility_check_rounds: int = min(n - 1, 3)
        self.my_consecutive_defections: int = 0
        self.recovery_countdown: int = 0
        self.consecutive_rounds_zero_contribution_others: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        others_avg_contribution_last_round = 0.0
        if state.round_number > 0:
            opponent_cooperators_last_round = sum(history.opponent_actions[-1, :])
            if n > 1:
                others_avg_contribution_last_round = opponent_cooperators_last_round / (n - 1)
            if others_avg_contribution_last_round == 0:
                self.consecutive_rounds_zero_contribution_others += 1
            else:
                self.consecutive_rounds_zero_contribution_others = 0
        if current_round > r - self.endgame_horizon:
            return Action.D
        if self.consecutive_rounds_zero_contribution_others >= self.futility_check_rounds and current_round > 1:
            return Action.D
        if current_round == 1:
            self.my_consecutive_defections = 0
            self.recovery_countdown = 0
            return Action.C
        if self.recovery_countdown > 0:
            self.recovery_countdown -= 1
            self.my_consecutive_defections = 0
            return Action.C
        if others_avg_contribution_last_round >= self.free_ride_threshold and self.my_consecutive_defections < self.max_consecutive_free_rides:
            self.my_consecutive_defections += 1
            return Action.D
        else:
            self.my_consecutive_defections = 0
            self.recovery_countdown = self.recovery_contribution_rounds - 1
            return Action.C

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    The Calculated Freerider strategy is designed to maximize individual payoff in the N-Player
    Public Goods Game. It strategically exploits cooperative behavior by operating in three distinct
    phases: Probing, Sustained Exploitation, and Endgame. The strategy balances immediate free-riding
    with tactical contributions to encourage and sustain an exploitable environment, adapting its
    actions based on observed cooperation levels from previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.probing_rounds = max(1, min(3, math.floor(r / 4)))
        self.endgame_rounds = max(1, min(5, math.floor(r / 4)))
        self.exploit_threshold = max(1, math.floor(n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        endgame_start_round_1_indexed = self.game_description.n_rounds - self.endgame_rounds + 1
        if current_round_1_indexed >= endgame_start_round_1_indexed:
            return Action.D
        c_prev = 0
        if state.round_number > 0:
            c_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if current_round_1_indexed <= self.probing_rounds:
            if current_round_1_indexed == 1:
                return Action.C
            elif c_prev >= self.exploit_threshold:
                return Action.D
            else:
                return Action.C
        if c_prev >= self.exploit_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize individual payoff by free-riding on the
    contributions of others. It adaptively responds to opponents' behavior, contributing only when
    necessary to prevent a collapse of cooperation, and aggressively free-rides otherwise, especially
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CIT = 0.5
        self.CT = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.game_description.n_rounds - 1:
            return Action.D
        num_other_players = self.game_description.n_players - 1
        sum_other_contributions_prev_round = sum(history.opponent_actions[-1, :])
        aoc_prev = sum_other_contributions_prev_round / num_other_players
        if aoc_prev > self.CIT:
            return Action.D
        elif aoc_prev < self.CT:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    Adaptive Provocateur Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily free-riding on
    the contributions of others. It strategically contributes only when necessary
    to prevent the collapse of overall cooperation, thereby preserving an
    environment ripe for future exploitation. It is designed to be adaptive
    to diverse opponent behaviors and robust across various game parameter settings.
    It exploits the finite nature of the game in its final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for
                     this player and opponents, or None for the first round (round 0).

        Returns:
            An Action enum (C for Cooperate, D for Defect).
        """
        current_round_0_indexed = state.round_number
        n = self.n_players
        r = self.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed >= r - 2:
            return Action.D
        num_others_last_round = np.sum(history.opponent_actions[-1, :])
        n_minus_1 = n - 1
        p_others_last_round = num_others_last_round / n_minus_1
        num_relevant_history_rounds = min(current_round_0_indexed, 5)
        recent_opponent_actions = history.opponent_actions[-num_relevant_history_rounds:]
        p_others_avg_recent = np.mean(recent_opponent_actions)
        if p_others_last_round >= 0.6 and p_others_avg_recent >= 0.5:
            return Action.D
        elif p_others_last_round <= 0.2 and p_others_avg_recent <= 0.3:
            return Action.C
        elif p_others_last_round >= 0.4:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy is designed to maximize its own payoff in the
    Public Goods Game by consistently seeking opportunities to free-ride. It defects
    when others are sufficiently cooperative, thus benefiting from the public good
    without personal cost. When cooperation falters, it makes calculated, strategic
    investments (contributing 1) to "prime the pump" and re-establish a collective
    good for future exploitation. However, it avoids being a "sucker" by ceasing
    contributions if collective cooperation becomes critically low or in the final round.
    """
    COOPERATION_THRESHOLD = 0.5
    MIN_INVESTMENT_VIABILITY = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = self.COOPERATION_THRESHOLD
        self.min_investment_viability = self.MIN_INVESTMENT_VIABILITY

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_prev_action = history.my_actions[current_round_number - 1]
        opponent_prev_actions = history.opponent_actions[current_round_number - 1, :]
        total_prev_contributions = int(my_prev_action) + np.sum(opponent_prev_actions)
        avg_cooperation_rate = total_prev_contributions / self.n_players
        if avg_cooperation_rate >= self.cooperation_threshold:
            return Action.D
        elif total_prev_contributions < self.min_investment_viability:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    The Resilient Parasite strategy aims to maximize payoff by generally free-riding
    on others' contributions, making strategic small investments only when the public good
    is at risk of collapse to preserve future exploitation opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.critical_cooperation_threshold = max(1, math.floor((self.n_players - 1) * 0.2))
        self.re_invest_threshold = math.ceil((self.n_players - 1) * 0.4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_last_action_was_cooperate = history.my_actions[current_round_0_indexed - 1]
        others_contributed_last_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if others_contributed_last_round <= self.critical_cooperation_threshold:
            return Action.D
        elif others_contributed_last_round < self.re_invest_threshold:
            if not my_last_action_was_cooperate:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    The Cunning Free-Rider strategy exploits the public goods game by primarily free-riding
    on the contributions of others. It defaults to defecting, making a single, calculated
    'cooperative' move only as a last resort to revive a completely collapsed public good
    (total_contributions_prev == 0), thereby creating new opportunities for further exploitation.
    It never contributes if the public good is already providing returns or if its single
    re-seeding attempt has already been made. It consistently defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.my_c_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        total_contributions_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if total_contributions_prev == 0 and self.my_c_count == 0:
            self.my_c_count = 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    The Shark is an exploitative strategy for the N-Player Public Goods Game.
    It aims to maximize individual payoff by strategically leveraging others' contributions.
    The strategy begins with a single round of cooperation as an initial probe to potentially
    induce cooperation in others. Following this, it consistently defects for the majority
    of the game to free-ride on any existing cooperation. Finally, it ensures defection
    in the last 'endgame_horizon' rounds, applying backward induction principles to maximize
    final gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.probe_rounds = 1
        self.endgame_horizon = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= total_rounds - self.endgame_horizon:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by systematically free-riding
    on the contributions of others. It only contributes strategically and minimally
    to prevent a complete collapse of the public good that would eliminate
    future free-riding opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.FREE_RIDE_THRESHOLD: float = 0.6
        self.RE_SEED_THRESHOLD: float = 0.25
        self.RE_SEED_DURATION: int = 2
        self.rounds_remaining_in_re_seed_commitment: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == n_rounds - 1:
            self.rounds_remaining_in_re_seed_commitment = 0
            return Action.D
        if self.rounds_remaining_in_re_seed_commitment > 0:
            self.rounds_remaining_in_re_seed_commitment -= 1
            return Action.C
        total_other_players = self.game_description.n_players - 1
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        avg_others_contribution_prev_round = cooperators_prev_round / total_other_players
        if avg_others_contribution_prev_round >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        elif avg_others_contribution_prev_round < self.RE_SEED_THRESHOLD:
            self.rounds_remaining_in_re_seed_commitment = self.RE_SEED_DURATION - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to exploit cooperative
    behavior from other players while minimizing its own contributions and
    protecting against being consistently "suckered." It leverages historical
    data to adapt its contribution strategy and maximizes end-game payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.end_game_horizon = 2
        self.cooperation_threshold = 0.5
        self.sucker_prevention_attempts = 2
        self.my_contributions_since_last_others_contribution = 0
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.num_other_players = self.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed >= self.n_rounds - self.end_game_horizon:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        C_others_last_round = sum(history.opponent_actions[-1, :])
        my_action_last_round = history.my_actions[-1]
        if C_others_last_round > 0:
            self.my_contributions_since_last_others_contribution = 0
        elif my_action_last_round == Action.C:
            self.my_contributions_since_last_others_contribution += 1
        else:
            self.my_contributions_since_last_others_contribution = 0
        average_others_contribution_rate = C_others_last_round / self.num_other_players
        if average_others_contribution_rate >= self.cooperation_threshold:
            return Action.D
        elif self.my_contributions_since_last_others_contribution >= self.sucker_prevention_attempts:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    The Calculated Opportunist: Primarily free-rides, cooperating only in the first round (if >1 total rounds)
    or when the public good completely collapses, to enable future free-riding opportunities.
    Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            if total_rounds == 1:
                return Action.D
            else:
                return Action.C
        elif current_round == total_rounds - 1:
            return Action.D
        else:
            my_contribution_last_round = int(history.my_actions[current_round - 1])
            opponent_contributions_last_round = np.sum(history.opponent_actions[current_round - 1, :])
            total_contributions_last_round = my_contribution_last_round + opponent_contributions_last_round
            if total_contributions_last_round == 0:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    The Opportunistic Shepherd strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by largely free-riding on the
    contributions of other players. It contributes only when necessary to prevent
    a complete collapse of cooperation, or in an initial phase to probe for
    cooperative partners, thereby ensuring a pool of public goods to exploit in
    future rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.HEALTHY_COOPERATION_THRESHOLD = math.floor((self.n_players - 1) * 0.6)
        self.CRITICAL_COOPERATION_THRESHOLD = math.floor((self.n_players - 1) * 0.25)
        self.PERSONAL_CONTRIBUTION_RATE_LIMIT = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        C_others_prev = int(np.sum(history.opponent_actions[-1, :]))
        if C_others_prev >= self.HEALTHY_COOPERATION_THRESHOLD:
            return Action.D
        elif C_others_prev <= self.CRITICAL_COOPERATION_THRESHOLD:
            return Action.D
        else:
            my_total_contributions_so_far = int(np.sum(history.my_actions))
            total_rounds_played_so_far = state.round_number
            my_current_contribution_rate = my_total_contributions_so_far / total_rounds_played_so_far
            my_last_action = history.my_actions[-1]
            if not my_last_action and my_current_contribution_rate <= self.PERSONAL_CONTRIBUTION_RATE_LIMIT:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    The "Adaptive Leech" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by adaptively free-riding on
    opponent cooperation, strategically contributing to sustain cooperation, and
    exploiting end-game effects.

    It prioritizes free-riding when overall contributions are robust but makes
    calculated "bait" contributions to sustain cooperation when it risks collapsing.
    It also capitalizes on end-game effects through unconditional defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Leech strategy with game parameters and internal strategy settings.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.OBSERVATION_WINDOW = 3
        self.COOPERATION_THRESHOLD_LOW = 0.25
        self.COOPERATION_THRESHOLD_HIGH = 0.75
        self.endgame_defection_rounds_val = min(2, self.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Leech decision rules.

        The decision rules are applied in order of precedence.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (state.round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.n_rounds - self.endgame_defection_rounds_val:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        actual_observation_window_size = min(self.OBSERVATION_WINDOW, state.round_number)
        start_index = state.round_number - actual_observation_window_size
        relevant_history_slice = history.opponent_actions[start_index:state.round_number, :]
        sum_other_contributions = np.sum(relevant_history_slice)
        num_possible_contributions = actual_observation_window_size * (self.n_players - 1)
        avg_proportion_other_contributions = sum_other_contributions / num_possible_contributions
        if avg_proportion_other_contributions >= self.COOPERATION_THRESHOLD_HIGH:
            return Action.D
        elif avg_proportion_other_contributions >= self.COOPERATION_THRESHOLD_LOW:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    The Responsive Exploiter strategy aims to maximize its own payoff by strategically encouraging
    cooperation from others to create a "pool" of contributions, then aggressively free-riding
    on that pool. It adapts its behavior based on the observed cooperation levels of others
    and its own success, always prioritizing its individual gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_coop_ratio_target: float = 0.5
        self.rounds_of_low_payoff: int = 0
        self.permanent_defect_mode: bool = False
        self.initial_priming_rounds: int = min(2, game_description.n_rounds - 1)
        self.leniency_up_step: float = 0.05
        self.leniency_down_step: float = 0.15
        self.min_payoff_for_reprime: float = 1.05
        self.reprime_rounds_threshold: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed > 0:
            last_round_my_payoff = history.my_payoffs[-1]
            if last_round_my_payoff < self.min_payoff_for_reprime:
                self.rounds_of_low_payoff += 1
            else:
                self.rounds_of_low_payoff = 0
        is_last_round = current_round_0_indexed == n_rounds - 1
        if self.permanent_defect_mode or is_last_round:
            return Action.D
        if current_round_0_indexed == 0:
            if n_rounds == 1:
                return Action.D
            else:
                return Action.C
        s_others_prev = sum(history.opponent_actions[-1, :])
        if 1 <= current_round_0_indexed <= self.initial_priming_rounds - 1:
            if s_others_prev == 0:
                self.permanent_defect_mode = True
                return Action.D
            else:
                return Action.C
        s_others_prev_ratio = s_others_prev / (n_players - 1)
        if self.rounds_of_low_payoff >= self.reprime_rounds_threshold:
            return_action = Action.C
            self.rounds_of_low_payoff = 0
            self.current_coop_ratio_target = 0.5
            return return_action
        if s_others_prev_ratio >= self.current_coop_ratio_target:
            return_action = Action.D
            self.current_coop_ratio_target = min(0.95, self.current_coop_ratio_target + self.leniency_up_step)
        else:
            return_action = Action.C
            self.current_coop_ratio_target = max(0.05, self.current_coop_ratio_target - self.leniency_down_step)
        return return_action

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by encouraging others to
    contribute, then free-riding on their contributions. It adaptively adjusts
    its behavior based on the observed cooperation level of other players
    and the game's remaining rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.initial_coop_rounds = 1
        self.coop_window_size = min(max(2, int(self.n_rounds / 5)), self.n_rounds - 1)
        self.endgame_horizon = max(1, int(self.n_rounds / 5))
        self.initial_cooperation_threshold_param = 0.6
        self.threshold_adjustment_factor = 0.1
        self.current_cooperation_threshold = self.initial_cooperation_threshold_param

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if self.n_rounds - current_round_1_indexed < self.endgame_horizon:
            return Action.D
        elif current_round_1_indexed <= self.initial_coop_rounds:
            return Action.C
        else:
            num_rounds_in_history = state.round_number
            actual_window_size = min(self.coop_window_size, num_rounds_in_history)
            average_others_cooperation_rate: float
            if actual_window_size == 0:
                average_others_cooperation_rate = 0.0
            else:
                relevant_actions = history.opponent_actions[num_rounds_in_history - actual_window_size:num_rounds_in_history, :]
                total_contributions_in_window = np.sum(relevant_actions)
                total_possible_contributions_in_window = actual_window_size * relevant_actions.shape[1]
                average_others_cooperation_rate = total_contributions_in_window / total_possible_contributions_in_window
            if average_others_cooperation_rate >= self.current_cooperation_threshold:
                action = Action.D
                self.current_cooperation_threshold = min(1.0, self.current_cooperation_threshold + self.threshold_adjustment_factor)
            else:
                action = Action.C
                self.current_cooperation_threshold = max(0.0, self.current_cooperation_threshold - self.threshold_adjustment_factor)
            return action

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    The Strategic Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by free-riding on others'
    contributions while strategically intervening to prevent the collapse of
    collective cooperation, thereby preserving future free-riding opportunities.
    It is adaptive, robust, and explicitly exploitative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.re_induce_active = False
        self.re_induce_rounds_left = 0
        self.r_probe_initial_cooperate = max(1, min(3, math.floor(self.n_rounds / 4)))
        self.r_endgame_defect = max(1, min(2, math.floor(self.n_rounds / 5)))
        self.cooperation_maintenance_threshold = 0.5
        self.re_induce_duration = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round < self.r_probe_initial_cooperate:
            return Action.C
        elif current_round >= self.n_rounds - self.r_endgame_defect:
            return Action.D
        else:
            prev_round_overall_cooperation_rate: float
            if current_round == self.r_probe_initial_cooperate:
                my_contribs_initial = history.my_actions[0:self.r_probe_initial_cooperate]
                opp_contribs_initial = history.opponent_actions[0:self.r_probe_initial_cooperate, :]
                total_cooperators_sum = np.sum(my_contribs_initial) + np.sum(opp_contribs_initial)
                total_possible_cooperators = self.n_players * self.r_probe_initial_cooperate
                prev_round_overall_cooperation_rate = total_cooperators_sum / total_possible_cooperators
            else:
                my_prev_action = history.my_actions[current_round - 1]
                opponent_prev_actions = history.opponent_actions[current_round - 1, :]
                total_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
                prev_round_overall_cooperation_rate = total_cooperators_prev_round / self.n_players
            if self.re_induce_active and self.re_induce_rounds_left > 0:
                self.re_induce_rounds_left -= 1
                if self.re_induce_rounds_left == 0:
                    self.re_induce_active = False
                return Action.C
            elif prev_round_overall_cooperation_rate < self.cooperation_maintenance_threshold:
                self.re_induce_active = True
                self.re_induce_rounds_left = self.re_induce_duration - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by strategically free-riding on others' contributions
    while adaptively maintaining a cooperative environment for continued exploitation. It relies
    solely on observed past actions and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Opportunist strategy with game parameters and internal state.
        """
        self.game_description = game_description
        self.exploit_counter: int = 0
        self.exploit_limit: int = 3
        self.high_coop_threshold: float = 0.75
        self.low_coop_threshold: float = 0.25
        self.overall_coop_pivot: float = 0.5
        self.n_opponents: int = self.game_description.n_players - 1

    def _calculate_coop_rates(self, state: GameState, history: PlayerHistory) -> tuple[float, float]:
        """
        Calculates the previous round's cooperation rate and the overall average cooperation rate
        among other players. This function is called for intermediate rounds, where history is
        guaranteed to exist and `state.round_number` is at least 1.
        """
        previous_round_coop_rate = 0.0
        overall_avg_coop_rate = 0.0
        num_cooperated_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        previous_round_coop_rate = num_cooperated_t_minus_1 / self.n_opponents
        total_possible_cooperations = state.round_number * self.n_opponents
        total_cooperated_overall = np.sum(history.opponent_actions)
        overall_avg_coop_rate = total_cooperated_overall / total_possible_cooperations
        return (previous_round_coop_rate, overall_avg_coop_rate)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on the strategy rules.
        """
        if state.round_number == 0:
            self.exploit_counter = 0
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        previous_round_coop_rate, overall_avg_coop_rate = self._calculate_coop_rates(state, history)
        if previous_round_coop_rate >= self.high_coop_threshold:
            if self.exploit_counter < self.exploit_limit:
                self.exploit_counter += 1
                return Action.D
            else:
                self.exploit_counter = 0
                return Action.C
        elif self.low_coop_threshold <= previous_round_coop_rate < self.high_coop_threshold:
            if overall_avg_coop_rate >= self.overall_coop_pivot:
                self.exploit_counter = 0
                return Action.C
            else:
                self.exploit_counter += 1
                return Action.D
        else:
            self.exploit_counter += 1
            return Action.D

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    The "Calculated Instigator" strategy aims to maximize personal payoff by predominantly free-riding.
    It defects in the first and last rounds. In intermediate rounds, it adapts to the observed
    level of cooperation. It strategically contributes only when total contributions are critically
    low (between 0 and 25% of players, inclusive of 25% but exclusive of 0) to prevent total collapse
    of the public good, thereby preserving future exploitation opportunities. In all other intermediate
    scenarios (complete collapse, high cooperation, or moderate cooperation), it defaults to defection
    to maximize immediate gain from others' contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_number - 1
        my_prev_action_val = 1 if history.my_actions[previous_round_idx] else 0
        opponents_prev_actions_sum = np.sum(history.opponent_actions[previous_round_idx, :])
        C_prev = my_prev_action_val + opponents_prev_actions_sum
        critical_cooperation_threshold = round(self.n_players * 0.25)
        if C_prev == 0:
            return Action.D
        elif C_prev >= self.n_players - 1:
            return Action.D
        elif C_prev <= critical_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    The Opportunistic Revivalist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by primarily free-riding on
    the contributions of others. It strategically contributes only when the
    overall level of cooperation is dwindling to the point where continued
    free-riding becomes less profitable, attempting to "revive" the collective
    contributions for future exploitation. It is robust to persistent defection
    by opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_contribution_round: int = 0
        self.consecutive_low_cooperation_rounds: int = 0
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = self.k / self.n_players
        self.n_minus_1 = self.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t > self.n_rounds - 2:
            return Action.D
        else:
            contributions_from_others_prev_round = np.sum(history.opponent_actions[-1, :])
            other_cooperation_rate = contributions_from_others_prev_round / self.n_minus_1
            if other_cooperation_rate < self.cooperation_threshold:
                self.consecutive_low_cooperation_rounds += 1
            else:
                self.consecutive_low_cooperation_rounds = 0
            if other_cooperation_rate >= self.cooperation_threshold:
                return Action.D
            elif self.consecutive_low_cooperation_rounds > 3:
                return Action.D
            elif current_round_t - self.my_last_contribution_round <= 2:
                return Action.D
            else:
                self.my_last_contribution_round = current_round_t
                return Action.C

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by extensively free-riding
    on the contributions of others, while strategically making minimal contributions
    only when necessary to prevent the complete collapse of the public good,
    thereby preserving future free-riding opportunities. It adapts to observed
    levels of cooperation and leverages end-game effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_defects_by_me: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        decision = Action.D
        if current_round_1_indexed == 1:
            decision = Action.D
        else:
            end_game_horizon = max(1, math.floor(self.r / 5))
            if current_round_1_indexed > self.r - end_game_horizon:
                decision = Action.D
            else:
                N_others = self.n - 1
                N_others_contrib_prev = int(np.sum(history.opponent_actions[state.round_number - 1, :]))
                High_Coop_Threshold = math.ceil(N_others * 0.6)
                Low_Coop_Threshold = max(1, math.floor(N_others * 0.2))
                if N_others_contrib_prev >= High_Coop_Threshold:
                    decision = Action.D
                elif N_others_contrib_prev < Low_Coop_Threshold:
                    if self.consecutive_defects_by_me >= 3 and current_round_1_indexed < self.r - 2:
                        decision = Action.C
                    else:
                        decision = Action.D
                elif self.consecutive_defects_by_me >= 2:
                    decision = Action.C
                else:
                    decision = Action.D
        if decision == Action.C:
            self.consecutive_defects_by_me = 0
        else:
            self.consecutive_defects_by_me += 1
        return decision

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to maximize individual payoff by exploiting the contributions of other players
    while strategically intervening to prevent the total collective contribution from collapsing entirely,
    which would diminish personal returns. It prioritizes free-riding but includes mechanisms to 're-seed' cooperation
    when necessary, ensuring a pool for continued exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.initial_cooperate_rounds_count = max(1, min(math.floor(self.n_rounds / 5), 3))
        self.cooperation_threshold_high = 0.7
        self.cooperation_threshold_low = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_cooperate_rounds_count:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_other_players = self.n_players - 1
        my_previous_contribution_is_cooperate = history.my_actions[-1]
        previous_round_others_contributions = np.sum(history.opponent_actions[-1, :])
        average_other_contributions_rate = previous_round_others_contributions / num_other_players
        if average_other_contributions_rate >= self.cooperation_threshold_high:
            return Action.D
        elif average_other_contributions_rate < self.cooperation_threshold_low:
            return Action.D
        elif not my_previous_contribution_is_cooperate and average_other_contributions_rate < self.cooperation_threshold_high:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    Adaptive Free-Rider with Self-Preservation strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by defaulting to defection,
    selectively contributing only when necessary to prevent the collapse of the
    public good (thereby preserving future free-riding opportunities) or to
    induce greater cooperation from others. It is designed to avoid being a
    "sucker" against persistent defectors.

    Decision rules are based on the current round number and the proportion of
    other players who cooperated in the immediately preceding round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: A PublicGoodsDescription dataclass containing
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the game state and historical data.

        Args:
            state: A GameState dataclass with the current round_number.
            history: A PlayerHistory dataclass containing past actions and payoffs
                     from this player's and opponents' perspective. Will be None
                     for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        n_others_cooperated_last_round = sum(history.opponent_actions[-1, :])
        num_other_players = n - 1
        p_others_cooperated = n_others_cooperated_last_round / num_other_players
        if n_others_cooperated_last_round == 0:
            return Action.D
        elif p_others_cooperated >= 0.5:
            return Action.D
        else:
            return Action.C