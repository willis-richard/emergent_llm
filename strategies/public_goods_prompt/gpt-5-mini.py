"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
- Game: public_goods_prompt
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState




class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Thresholded Reciprocal with Individual Reputation and Forgiveness (TRIF)

    - Keeps per-player exponential-smoothed cooperation scores.
    - Starts cooperative, tolerates small slips, punishes clear breakdowns for a short fixed length,
      and forgives when cooperation is re-established. Applies endgame caution in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(5, self.r)
        self.M = math.ceil(self.n / 2)
        self.D_allow = max(1, math.floor(self.n / 8))
        self.P = 2
        self.S = 2
        self.E = min(2, self.r)
        self.s_scores = [1.0 for _ in range(self.n)]
        self.punish_count = 0
        self.good_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        last_opponent_actions = history.opponent_actions[-1]
        last_my_action = bool(history.my_actions[-1])
        for i in range(self.n - 1):
            a_i = 1 if bool(last_opponent_actions[i]) else 0
            self.s_scores[i] = 0.6 * self.s_scores[i] + 0.4 * a_i
        self_idx = self.n - 1
        a_self = 1 if last_my_action else 0
        self.s_scores[self_idx] = 0.6 * self.s_scores[self_idx] + 0.4 * a_self
        cooperators_opponents = int(last_opponent_actions.sum()) if hasattr(last_opponent_actions, 'sum') else sum((1 for x in last_opponent_actions if x))
        C_prev = cooperators_opponents + (1 if last_my_action else 0)
        if self.n - 1 > 0:
            avg_other_score = sum(self.s_scores[0:self.n - 1]) / float(self.n - 1)
        else:
            avg_other_score = 0.0
        persistent_defectors = [i for i in range(self.n - 1) if self.s_scores[i] < 0.25]
        if self.punish_count > 0:
            self.punish_count = max(0, self.punish_count - 1)
            self.good_streak = 0
            return Action.D
        in_endgame_window = state.round_number >= self.r - self.E
        if in_endgame_window:
            if state.round_number == self.r - 1:
                self.good_streak = 0
                return Action.D
            if self.E >= 2 and state.round_number == self.r - 2:
                if C_prev == self.n or (avg_other_score >= 0.9 and len(persistent_defectors) == 0):
                    self.good_streak += 1
                    if self.good_streak >= self.S:
                        self.punish_count = 0
                    return Action.C
                else:
                    self.good_streak = 0
                    return Action.D
        choose_C = False
        if C_prev >= self.n - self.D_allow:
            choose_C = True
        elif C_prev >= self.M:
            choose_C = True
        else:
            choose_C = False
        if choose_C:
            self.good_streak += 1
            if self.good_streak >= self.S:
                self.punish_count = 0
            return Action.C
        else:
            remaining_after_current = max(0, self.P - 1)
            self.punish_count = remaining_after_current
            self.good_streak = 0
            return Action.D

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    Adaptive Collective Win-Stay / Lose-Shift (AC-WSLS)

    - Start by cooperating in round 0.
    - For non-final rounds (1 .. r-2), compute the payoff last round and the
      counterfactual payoff had you flipped your last action (holding others fixed).
      * If everyone cooperated last round (S_last == n), play C to reward full cooperation.
      * If your last-round payoff equals the counterfactual (within numerical tolerance),
        choose C (tie-break bias toward cooperation).
      * If your last-round payoff > counterfactual, repeat your previous action (win-stay).
      * If your last-round payoff < counterfactual, switch action (lose-shift).
    - In the final round (round_number == n_rounds - 1), defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        if round_idx == 0 or history is None:
            return Action.C
        if round_idx >= max(0, self.n_rounds - 1):
            return Action.D
        try:
            my_last_action_bool = bool(history.my_actions[-1])
        except Exception:
            return Action.C
        try:
            opp_last_row = history.opponent_actions[-1, :]
            opp_coops = int(np.sum(opp_last_row))
        except Exception:
            opp_coops = 0
        c_last = 1 if my_last_action_bool else 0
        S_last = c_last + opp_coops
        if S_last == self.n_players:
            return Action.C
        try:
            pi_last = float(history.my_payoffs[-1])
        except Exception:
            pi_last = 1 - c_last + self.k / float(self.n_players) * float(S_last)
        S_others = S_last - c_last
        c_flip = 1 - c_last
        counterfactual = 1 - c_flip + self.k / float(self.n_players) * float(S_others + c_flip)
        if math.isclose(pi_last, counterfactual, rel_tol=1e-09, abs_tol=1e-12):
            return Action.C
        if pi_last > counterfactual:
            return Action.C if c_last == 1 else Action.D
        else:
            return Action.D if c_last == 1 else Action.C

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Collective Conditional Cooperator with Proportional Retaliation and Forgiveness (CCCPR-F).

    - Starts cooperative.
    - Uses a recent-window memory W to estimate each opponent's cooperation rate S_j and
      the group cooperation rate G (average of S_j).
    - Cooperates whenever G >= tau (threshold tuned by k/n).
    - Forgives isolated slips (small-noise tolerance epsilon).
    - If recent defections exceed tolerance, triggers a short, proportional group punishment
      targeted at recent defectors and persistent defectors (S_j <= 0.30).
    - Punishments last a bounded number of rounds and are ended early if the group
      cooperation rate recovers above tau.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        self.W = min(10, max(1, math.floor(r / 5)))
        ratio = k / float(n) if n > 0 else 0.0
        if ratio >= 0.75:
            self.tau = 0.45
        elif ratio >= 0.5:
            self.tau = 0.5
        else:
            self.tau = 0.6
        self.epsilon = 0.2
        self.P_base = 2
        self.P_max = min(4, max(1, math.floor(r / 10) + 1))
        self.PunishTargets = set()
        self.PunishUntilRound = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        curr_round = int(state.round_number)
        n = int(self.game_description.n_players)
        if curr_round == 0 or history is None:
            return Action.C
        if curr_round == self.game_description.n_rounds - 1:
            return Action.D
        window_len = min(self.W, curr_round)
        opp_actions = history.opponent_actions
        num_others = max(1, n - 1)
        if window_len <= 0:
            S_j = np.zeros(num_others, dtype=float)
        else:
            try:
                window_actions = opp_actions[-window_len:, :]
            except Exception:
                window_actions = opp_actions.copy()
                if window_actions.ndim == 1:
                    window_actions = window_actions.reshape(-1, 1)
            if window_actions.size == 0:
                S_j = np.zeros(num_others, dtype=float)
            else:
                S_j = np.mean(window_actions.astype(float), axis=0)
        if S_j.size == 0:
            G = 0.0
        else:
            G = float(np.mean(S_j))
        try:
            last_row = opp_actions[-1, :]
            num_defected_last = int(np.sum(~last_row))
            f_last = num_defected_last / float(num_others)
        except Exception:
            f_last = 0.0
        if self.PunishUntilRound >= curr_round:
            action = Action.D
            if G >= self.tau:
                self.PunishTargets.clear()
                self.PunishUntilRound = -1
            return action
        if G >= self.tau:
            return Action.C
        if f_last == 0.0:
            return Action.C
        if 0.0 < f_last <= self.epsilon:
            return Action.C
        chronic_mask = S_j <= 0.3
        m = int(np.sum(chronic_mask))
        P = min(self.P_max, self.P_base + math.floor(m / 2))
        punish_targets = set()
        try:
            last_row_bool = opp_actions[-1, :].astype(bool)
            for idx in range(last_row_bool.shape[0]):
                if not bool(last_row_bool[idx]):
                    punish_targets.add(int(idx))
        except Exception:
            pass
        for idx, is_chronic in enumerate(chronic_mask):
            if bool(is_chronic):
                punish_targets.add(int(idx))
        self.PunishTargets = punish_targets
        self.PunishUntilRound = curr_round + P - 1
        return Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperation (ACCC)

    - Starts by cooperating (unless single-shot).
    - Tracks recent average group cooperation over a window W based on game length.
    - If recent average < T_low (0.5) it switches to a short, bounded Punish-mode (group-level defection).
    - After a punishment block it enters a short Rehabilitation observation window H where it cooperates while observing.
      If the group shows strong recovery (average >= T_high = 0.8 over the H rounds), it returns to Cooperate-mode;
      otherwise it re-enters punish with a modestly escalated but bounded punishment length.
    - Always defects in the final round.
    - Shortens punishment and rehab near the end of the game to avoid wasting last rounds.
    - Robust to noise by averaging over a window and requiring thresholds to be crossed before changing modes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r_total = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_low = 0.5
        self.T_high = 0.8

    def _compute_recent_fraction(self, history: PlayerHistory, upto_round: int, W: int) -> float:
        """
        Compute RecentFraction prior to round index `upto_round` (0-based).
        Uses up to W most recent rounds: indices max(0, upto_round-W) .. upto_round-1.
        If there are no prior rounds (upto_round == 0) returns 1.0 per spec.
        """
        if upto_round <= 0:
            return 1.0
        start = max(0, upto_round - W)
        end = upto_round - 1
        if history is None:
            return 1.0
        num_rounds_available = history.my_actions.shape[0]
        end = min(end, num_rounds_available - 1)
        if end < start:
            return 1.0
        fracs = []
        for idx in range(start, end + 1):
            my_a = bool(history.my_actions[idx]) if history.my_actions.size > 0 else False
            opp_row = history.opponent_actions[idx, :] if history.opponent_actions.size > 0 else np.array([], dtype=bool)
            total_coop = (1 if my_a else 0) + int(np.sum(opp_row))
            fracs.append(total_coop / float(max(1, self.n)))
        if not fracs:
            return 1.0
        return float(np.mean(fracs))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r_total <= 1:
            return Action.D
        if t == 0:
            return Action.C
        if t == self.r_total - 1:
            return Action.D
        W = min(4, max(1, self.r_total - 1))
        H_default = min(2, max(1, math.floor(W / 2)))
        if history is None:
            return Action.D

        def most_opponents_always_cooperated() -> bool:
            rounds_observed = t
            if rounds_observed <= 0:
                return False
            if history.opponent_actions.size == 0:
                return False
            opp_matrix = history.opponent_actions[:rounds_observed, :]
            if opp_matrix.size == 0:
                return True
            per_opponent_all = np.all(opp_matrix, axis=0)
            count_all = int(np.sum(per_opponent_all))
            num_opponents = opp_matrix.shape[1]
            return count_all >= math.ceil(num_opponents / 2)
        if most_opponents_always_cooperated():
            return Action.C
        mode = 'Cooperate'
        punish_counter = 0
        current_P = 0
        rehab_counter = 0
        rehab_start_idx = None
        for s in range(0, t):
            recent = self._compute_recent_fraction(history, s, W)
            R_at_s = self.r_total - s
            P_base = min(3, max(1, math.floor(R_at_s / 4)))
            H = H_default
            if R_at_s <= 2:
                P_base = 1
                H = 1
            if mode == 'Cooperate':
                if recent < self.T_low:
                    mode = 'Punish'
                    current_P = P_base
                    punish_counter = current_P
                    punish_counter -= 1
                    if punish_counter <= 0:
                        mode = 'Rehab'
                        rehab_counter = H
                        rehab_start_idx = s
                else:
                    pass
            elif mode == 'Punish':
                if punish_counter > 0:
                    punish_counter -= 1
                    if punish_counter <= 0:
                        mode = 'Rehab'
                        rehab_counter = H
                        rehab_start_idx = s + 1
                else:
                    mode = 'Rehab'
                    rehab_counter = H
                    rehab_start_idx = s + 1
            elif mode == 'Rehab':
                rehab_counter -= 1
                if rehab_counter <= 0:
                    if rehab_start_idx is None or rehab_start_idx > s:
                        avg = 0.0
                    else:
                        frs = []
                        for ii in range(rehab_start_idx, s + 1):
                            if ii < 0 or ii >= history.my_actions.shape[0]:
                                continue
                            my_a = bool(history.my_actions[ii])
                            opp_row = history.opponent_actions[ii, :] if history.opponent_actions.size > 0 else np.array([], dtype=bool)
                            total_coop = (1 if my_a else 0) + int(np.sum(opp_row))
                            frs.append(total_coop / float(max(1, self.n)))
                        avg = float(np.mean(frs)) if frs else 0.0
                    if avg >= self.T_high:
                        mode = 'Cooperate'
                        punish_counter = 0
                        current_P = 0
                    else:
                        R_remaining_after_s = max(1, self.r_total - (s + 1))
                        P_prime = min(max(1, current_P + 1), math.floor(R_remaining_after_s / 2) if R_remaining_after_s >= 2 else 1)
                        P_prime = max(1, P_prime)
                        current_P = P_prime
                        punish_counter = current_P
                        mode = 'Punish'
        recent_t = self._compute_recent_fraction(history, t, W)
        R = self.r_total - t
        P_now = min(3, max(1, math.floor(R / 4)))
        H_now = H_default
        if R <= 2:
            P_now = 1
            H_now = 1
        if mode == 'Cooperate':
            if recent_t < self.T_low:
                return Action.D
            else:
                return Action.C
        if mode == 'Punish':
            if punish_counter > 0:
                return Action.D
            else:
                return Action.C
        if mode == 'Rehab':
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    Collective conditional-cooperation with graduated punishment and forgiveness.

    - Start cooperating.
    - Use a simple majority threshold to judge rounds as cooperative.
    - If a minority cooperated in the previous round, start a short proportional punishment.
    - Punishments are capped and short; forgiveness occurs as soon as majority cooperation is observed.
    - If cooperation remains very low over a short window, switch to resigned mode and only probe occasionally.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.M = math.ceil(self.n / 2)
        self.P_max = min(4, max(1, math.floor(self.r / 10)))
        self.W = min(self.r, 6)
        self.R_th = 0.3
        self.L = max(4, math.floor(self.r / 6))
        self.mode = 'cooperative'
        self.punishment_timer = 0
        self.last_punish_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def coop_count_in_round(idx: int) -> int:
            if history is None:
                return 0
            my = int(bool(history.my_actions[idx]))
            opp = int(np.count_nonzero(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            return my + opp

        def recent_coop_fraction(window: int=None) -> float:
            if history is None:
                return 0.0
            if window is None:
                window = self.W
            hist_len = t
            if hist_len <= 0:
                return 0.0
            start = max(0, hist_len - window)
            counts = []
            for idx in range(start, hist_len):
                counts.append(coop_count_in_round(idx))
            if not counts:
                return 0.0
            mean_coop_count = float(sum(counts)) / len(counts)
            return mean_coop_count / max(1, self.n)
        if t == 0:
            self.mode = 'cooperative'
            self.punishment_timer = 0
            self.last_punish_round = None
            return Action.C
        if t == max(0, self.r - 1):
            return Action.D
        hist_len = t
        if self.mode == 'resigned':
            do_probe = self.L > 0 and t % self.L == 0
            action = Action.C if do_probe else Action.D
            frac = recent_coop_fraction(self.W)
            if frac >= self.M / max(1, self.n):
                self.mode = 'cooperative'
                self.punishment_timer = 0
                self.last_punish_round = None
            return action
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if self.punishment_timer == 0:
                if hist_len >= 2:
                    coop_last = coop_count_in_round(hist_len - 1)
                    coop_prev = coop_count_in_round(hist_len - 2)
                    mean_coop_last_two = (coop_last + coop_prev) / 2.0
                    if mean_coop_last_two >= self.M:
                        self.mode = 'cooperative'
                        self.punishment_timer = 0
                        self.last_punish_round = None
                    else:
                        defectors_last = max(0, self.n - coop_last)
                        self.punishment_timer = min(self.P_max, 1 + defectors_last)
                        self.mode = 'punishing'
                        self.last_punish_round = t
                else:
                    self.mode = 'cooperative'
                    self.punishment_timer = 0
                    self.last_punish_round = None
            if recent_coop_fraction(self.W) < self.R_th:
                self.mode = 'resigned'
                self.punishment_timer = 0
                self.last_punish_round = None
            return action
        if hist_len <= 0:
            return Action.C
        c_last = coop_count_in_round(hist_len - 1)
        if c_last >= self.M:
            return Action.C
        defectors_last = max(0, self.n - c_last)
        self.mode = 'punishing'
        self.punishment_timer = min(self.P_max, 1 + defectors_last)
        self.last_punish_round = t
        if recent_coop_fraction(self.W) < self.R_th:
            self.mode = 'resigned'
            self.punishment_timer = 0
            self.last_punish_round = None
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    Collective Conditional-Cooperation (CCC)

    - Starts cooperating to signal willingness to form a cooperative norm.
    - Uses a short memory window W to estimate recent cooperation rates of each opponent.
    - If the group shows a 'bad round' (fraction cooperating among others last round < theta),
      initiates a bounded punishment phase whose length scales with the number of moderately
      uncooperative players. Punishment is forgiving and conditional on observed improvement.
    - After punishment ends, requires R consecutive 'good' rounds before fully resuming normal play.
    - Defects in the final round to avoid endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        w = round(math.sqrt(max(1, self.r)))
        self.W = min(5, max(1, w))
        self.R_forgive = 2
        self.P_max = max(1, round(0.2 * self.r))
        ratio = self.k / max(1.0, self.n)
        if ratio >= 0.6:
            self.theta = 0.4
        elif ratio <= 0.4:
            self.theta = 0.6
        else:
            self.theta = 0.5
        self.t_punish = 0
        self.punish_started_at = None
        self.pre_punish_F_avg = None
        self.punish_targets = tuple()
        self.punish_end_round = None
        self._last_updated_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def _compute_stats(up_to_rounds=None):
            if history is None:
                return (0.0, 0.0, np.array([], dtype=float), 0)
            opp_actions = history.opponent_actions
            L = opp_actions.shape[0]
            if L == 0:
                return (0.0, 0.0, np.zeros((0,), dtype=float), 0)
            m = opp_actions.shape[1]
            last_row = opp_actions[-1, :]
            F_last = float(np.mean(last_row)) if m > 0 else 0.0
            window = min(self.W, L)
            slice_start = max(0, L - window)
            window_slice = opp_actions[slice_start:L, :]
            if window_slice.size == 0:
                f_j = np.zeros((m,), dtype=float)
            else:
                f_j = np.sum(window_slice, axis=0) / float(window)
            F_avg = float(np.mean(f_j)) if f_j.size > 0 else 0.0
            return (F_last, F_avg, f_j, m)
        if t == 0 or history is None:
            self.t_punish = 0
            self.punish_started_at = None
            self.pre_punish_F_avg = None
            self.punish_targets = tuple()
            self.punish_end_round = None
            self._last_updated_round = -1
            return Action.C
        if t == self.r - 1:
            pass
        if self._last_updated_round < t - 1:
            F_last_prev, F_avg_curr, f_j_curr, m = _compute_stats()
            if self.t_punish > 0 and self.punish_started_at is not None and (self.punish_started_at <= t - 1):
                improved = False
                if F_last_prev >= self.theta:
                    improved = True
                elif self.pre_punish_F_avg is not None and F_avg_curr > self.pre_punish_F_avg + 1e-12:
                    improved = True
                if improved:
                    self.t_punish = max(0, self.t_punish - 1)
                if self.punish_targets:
                    target_fj = []
                    for idx in self.punish_targets:
                        if 0 <= idx < f_j_curr.size:
                            target_fj.append(float(f_j_curr[idx]))
                    if any((fj > 0.5 for fj in target_fj)):
                        self.t_punish = max(0, self.t_punish - 1)
                    if all((fj > 0.0 for fj in target_fj)) and len(target_fj) > 0:
                        self.t_punish = max(0, self.t_punish - 1)
                if self.t_punish == 0:
                    self.punish_end_round = t - 1
                    self.punish_started_at = None
            self._last_updated_round = t - 1
        F_last, F_avg, f_j, m = _compute_stats()
        persistent_mask = np.zeros((0,), dtype=bool)
        if f_j.size > 0:
            last_row = history.opponent_actions[-1, :]
            persistent_mask = (f_j < 0.25) & (last_row == False)
            persistent_indices = tuple((int(i) for i in np.nonzero(persistent_mask)[0]))
        else:
            persistent_indices = tuple()

        def _start_punishment():
            num_moderately_bad = int(np.sum(f_j < 0.5)) if f_j.size > 0 else 0
            length = 1 + num_moderately_bad
            length = min(self.P_max, max(1, length))
            self.t_punish = int(length)
            self.punish_started_at = t
            self.pre_punish_F_avg = float(F_avg)
            self.punish_targets = persistent_indices
            self.punish_end_round = None
            self._last_updated_round = t - 1
            return
        if self.t_punish > 0:
            return Action.D
        if t == self.r - 1:
            return Action.D
        in_cooldown = False
        if self.punish_end_round is not None and t > self.punish_end_round:
            start = self.punish_end_round + 1
            opp_actions = history.opponent_actions
            L = opp_actions.shape[0]
            consecutive_good = 0
            for rr in range(L - 1, start - 1, -1):
                row = opp_actions[rr, :]
                frac_coop = float(np.mean(row)) if row.size > 0 else 0.0
                if frac_coop >= self.theta:
                    consecutive_good += 1
                else:
                    break
            if consecutive_good < self.R_forgive:
                in_cooldown = True
            else:
                in_cooldown = False
        if in_cooldown:
            if F_last >= self.theta:
                return Action.C
            else:
                _start_punishment()
                return Action.D
        if F_last >= self.theta or F_avg >= self.theta:
            return Action.C
        _start_punishment()
        return Action.D

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperation (ACCC)

    Starts cooperative, rewards groups that show high recent cooperation,
    targets short, proportional punishments at clear defectors, forgives
    quickly when defectors reform, and defects if cooperation collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.m = max(1, math.floor(self.n_rounds / 5))
        self.T_indiv = 0.5
        self.T_high = 0.75
        self.T_low = 0.4
        self.p_base = 1
        self._punished: dict[int, int] = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number
        window = min(self.m, t) if t > 0 else 0
        if window <= 0:
            return Action.C
        recent_my = history.my_actions[-window:]
        coop_rate_self = float(np.mean(recent_my)) if recent_my.size > 0 else 0.0
        recent_opponents = history.opponent_actions[-window:, :]
        if recent_opponents.size == 0:
            coop_rate_opponents = np.array([], dtype=float)
        else:
            coop_rate_opponents = np.mean(recent_opponents.astype(float), axis=0)
        if coop_rate_opponents.size > 0:
            Group_recent_rate = float(np.mean(coop_rate_opponents))
        else:
            Group_recent_rate = 0.0
        last_round_my = bool(history.my_actions[-1])
        last_round_opp = history.opponent_actions[-1, :]
        Most_recent_round_contributions = int(last_round_my) + int(np.sum(last_round_opp.astype(int)))
        if t == self.n_rounds - 1:
            unanimous_window_self = coop_rate_self >= 1.0
            unanimous_window_opps = coop_rate_opponents.size == 0 or np.all(coop_rate_opponents >= 1.0)
            if unanimous_window_self and unanimous_window_opps:
                return Action.C
            return Action.D
        if coop_rate_opponents.size > 0 and np.all(coop_rate_opponents >= 1.0):
            self._punished.clear()
            return Action.C
        if Group_recent_rate >= self.T_high:
            self._punished.clear()
            return Action.C
        if Group_recent_rate <= self.T_low:
            self._punished.clear()
            return Action.D
        Defectors = set((int(i) for i, rate in enumerate(coop_rate_opponents) if rate < self.T_indiv))
        to_remove = []
        for opp_idx, remaining in list(self._punished.items()):
            if coop_rate_opponents[opp_idx] >= self.T_indiv:
                to_remove.append(opp_idx)
        for idx in to_remove:
            self._punished.pop(idx, None)
        if Group_recent_rate >= self.T_high:
            self._punished.clear()
        for opp_idx in list(self._punished.keys()):
            if opp_idx < 0 or opp_idx >= coop_rate_opponents.size:
                self._punished.pop(opp_idx, None)
                continue
            if bool(history.opponent_actions[-1, opp_idx]):
                self._punished[opp_idx] = max(0, self._punished.get(opp_idx, 0) - 1)
                if self._punished[opp_idx] <= 0:
                    self._punished.pop(opp_idx, None)
        for opp_idx in Defectors:
            if opp_idx not in self._punished:
                shortfall = max(0.0, self.T_indiv - float(coop_rate_opponents[opp_idx]))
                proportional = int(math.ceil(shortfall * window))
                initial_len = max(self.p_base, proportional)
                self._punished[opp_idx] = initial_len
        if len(Defectors) > self.n_players / 2.0:
            self._punished.clear()
            return Action.D
        if len(self._punished) > 0:
            return Action.D
        if len(Defectors) == 0:
            majority_threshold = self.n_players / 2.0
            if bool(history.my_actions[-1]) and Most_recent_round_contributions > majority_threshold:
                return Action.C
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Collective Adaptive Pavlov (CAP)

    - Starts cooperative.
    - Defects in final round.
    - Punishes for P rounds if personally exploited (would have gained by defecting last round).
    - Otherwise cooperates when group cooperation fraction >= theta_coop, defects otherwise.
    - Small exploration/forgiveness probability epsilon to probe or recover cooperation.
    - Uses only observable history and game params.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.P = 2
        self.epsilon = 0.05
        self.delta = 0.0
        self.theta_coop = max(0.5, self.k / self.n if self.n > 0 else 0.5)
        self.punishment_counter = 0
        self.last_round_contributions = 0
        self.my_last_action = 0
        self.my_last_round_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        last_index = t - 1
        if t == 0:
            return Action.C
        if history is None:
            return Action.D
        have_last = last_index >= 0 and last_index < len(history.my_actions)
        if not have_last:
            return Action.D
        try:
            my_last_bool = bool(history.my_actions[last_index])
        except Exception:
            my_last_bool = False
        my_last_action = 1 if my_last_bool else 0
        self.my_last_action = my_last_action
        try:
            opps_last = history.opponent_actions[last_index, :]
            opps_coops = int(np.sum(opps_last))
        except Exception:
            opps_coops = 0
        last_round_contributions = opps_coops + my_last_action
        self.last_round_contributions = int(last_round_contributions)
        n = max(1, self.n)
        pi_actual = 1.0 - float(my_last_action) + self.k / n * float(last_round_contributions)
        self.my_last_round_payoff = float(pi_actual)
        pi_if_defect = 0.0 + self.k / n * float(last_round_contributions - my_last_action)
        if t == self.r - 1:
            return Action.D
        if self.punishment_counter > 0:
            probe = random.random() < self.epsilon
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.C if probe else Action.D
        exploited = pi_if_defect > pi_actual + self.delta
        if exploited:
            self.punishment_counter = int(self.P)
            return Action.D
        coop_fraction = float(last_round_contributions) / float(n) if n > 0 else 0.0
        if coop_fraction >= self.theta_coop:
            base_action = Action.C
        else:
            base_action = Action.D
        if base_action == Action.D and random.random() < self.epsilon:
            return Action.C
        return base_action

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    Collective-Conditional-Cooperator with Graduated Punishment and Forgiveness (CCC-GPF)

    - Signals cooperation on first round (unless single-round game).
    - Maintains cooperation while recent group cooperation >= qc.
    - On shortfall, enters a short, graduated punishment (defection) phase,
      followed by a one-round probationary cooperation test. If the group
      recovers, resume cooperation; otherwise repeat a (capped) punishment.
    - Adapts sensitivity qc based on k/n, uses a short memory window m,
      and keeps punishments short and proportional to observed shortfall or
      number of repeat defectors.
    - Always defects in the final round and in single-round games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.m = max(3, min(10, math.ceil(self.r / 10))) if self.r > 0 else 3
        self.qc = 0.5 + 0.2 * (1.0 - self.k / max(1.0, self.n_players))
        self.p_base = 1
        self.p_max = min(3, max(0, self.r - 1))
        self.cooperative_state = True
        self.punishment_countdown = 0
        self.in_probation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r <= 1:
            return Action.D
        t_idx = state.round_number
        if t_idx == self.r - 1:
            self.cooperative_state = False
            self.punishment_countdown = 0
            self.in_probation = False
            return Action.D
        if t_idx == 0:
            self.cooperative_state = True
            self.punishment_countdown = 0
            self.in_probation = False
            return Action.C
        if history is None:
            return Action.D
        past_rounds = t_idx
        recent_len = min(self.m, past_rounds) if past_rounds > 0 else 0
        coop_counts = []
        for i in range(past_rounds):
            my_coop = 1 if bool(history.my_actions[i]) else 0
            opp_coops = int(np.sum(history.opponent_actions[i, :])) if history.opponent_actions.size else 0
            coop_counts.append(my_coop + opp_coops)
        coop_counts = np.array(coop_counts, dtype=float) if len(coop_counts) > 0 else np.array([], dtype=float)
        if recent_len > 0:
            window = coop_counts[-recent_len:]
            recent_group_fraction = float(np.mean(window) / max(1.0, self.n_players))
            last_round_fraction = float(coop_counts[-1] / max(1.0, self.n_players))
        else:
            recent_group_fraction = 0.0
            last_round_fraction = 0.0
        s = 0
        if recent_len > 0 and history.opponent_actions.size:
            opp_window = history.opponent_actions[past_rounds - recent_len:past_rounds, :]
            opp_counts = np.sum(opp_window, axis=0)
            opp_frac = opp_counts / float(recent_len)
            s = int(np.sum(opp_frac < 0.5))
        else:
            s = 0
        remaining_nonfinal = max(0, self.r - 1 - t_idx)
        if self.cooperative_state:
            if recent_group_fraction >= self.qc:
                self.in_probation = False
                return Action.C
            self.cooperative_state = False
            self.in_probation = False
            if s > 0 and s <= 2:
                p = min(self.p_max, max(1, math.ceil(s / 2)))
            else:
                shortfall = max(0.0, self.qc - recent_group_fraction)
                p = min(self.p_max, self.p_base + math.ceil(shortfall * self.n_players))
            p = min(p, remaining_nonfinal if remaining_nonfinal >= 1 else 1)
            self.punishment_countdown = max(0, p - 1)
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        if not self.in_probation:
            self.in_probation = True
            return Action.C
        else:
            recent_len_eval = min(self.m, past_rounds) if past_rounds > 0 else 0
            if recent_len_eval > 0:
                window_eval = coop_counts[-recent_len_eval:]
                recent_group_fraction_eval = float(np.mean(window_eval) / max(1.0, self.n_players))
            else:
                recent_group_fraction_eval = 0.0
            if recent_group_fraction_eval >= self.qc:
                self.cooperative_state = True
                self.in_probation = False
                self.punishment_countdown = 0
                return Action.C
            if s > 0 and s <= 2:
                p = min(self.p_max, max(1, math.ceil(s / 2)))
            else:
                shortfall = max(0.0, self.qc - recent_group_fraction_eval)
                p = min(self.p_max, self.p_base + math.ceil(shortfall * self.n_players))
            p = min(p, remaining_nonfinal if remaining_nonfinal >= 1 else 1)
            self.in_probation = False
            self.cooperative_state = False
            self.punishment_countdown = max(0, p - 1)
            return Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Collective-First Conditional Cooperator (CFCC)

    - Cooperates on the first round.
    - Uses a short memory window (up to 5 rounds) to compute the recent average fraction of contributors.
    - If the recent cooperation rate R >= T_frac (computed from n and k), cooperates.
    - If R < T_frac, defects and triggers a 1-round punishment (set punishment_counter = 1).
    - While punishment_counter > 0, defects (punishment lasts that round) and the counter counts down;
      after observing recovered cooperation (R >= T_frac) the counter is cleared (forgiveness).
    - Always defects in the very last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r))
        raw = 1.0 - (self.k - 1.0) / float(max(1, self.n - 1))
        self.T_frac = max(0.5, min(0.9, raw))
        self.S = math.ceil(self.T_frac * self.n)
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        if round_idx == 0:
            return Action.C
        if round_idx == self.r - 1:
            return Action.D
        m = min(self.w, round_idx)
        if m <= 0 or history is None:
            R = 1.0
        else:
            try:
                my_slice = history.my_actions[-m:]
                opp_slice = history.opponent_actions[-m:, :]
                my_contribs = int(np.sum(my_slice))
                opp_contribs = int(np.sum(opp_slice))
                total_contribs = my_contribs + opp_contribs
                R = float(total_contribs) / float(self.n * m)
            except Exception:
                R = 1.0
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            if R >= self.T_frac:
                self.punishment_counter = 0
            return Action.D
        if R >= self.T_frac:
            return Action.C
        else:
            self.punishment_counter = 1
            return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Collective-Forgiving Majority (CFM)

    Start by cooperating (except in a one-shot game), sustain cooperation when the
    group shows it, punish defections briefly and proportionally, forgive quickly
    to restore group gains; defect in the known final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.threshold_fraction = max(0.5, min(0.8, self.k / max(1.0, self.n)))
        self.max_punish = min(3, max(1, math.floor(self.r / 10)))
        self.history_window = min(10, max(1, self.r))
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r == 1:
            return Action.D
        if state.round_number == self.r - 1:
            if self.punish_timer > 0:
                self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        try:
            last_opponent_row = history.opponent_actions[-1, :]
            last_my = bool(history.my_actions[-1])
        except Exception:
            last_opponent_row = np.zeros((max(0, self.n - 1),), dtype=bool)
            last_my = False
        last_round_cooperators = int(np.sum(last_opponent_row)) + (1 if last_my else 0)
        f_prev = last_round_cooperators / float(self.n) if self.n > 0 else 0.0
        available_rounds = int(history.my_actions.shape[0])
        window = min(self.history_window, available_rounds)
        if window <= 0:
            persistent_count = 0
        else:
            opp_window = history.opponent_actions[-window:, :]
            coop_counts = np.sum(opp_window.astype(np.int64), axis=0)
            defect_counts = window - coop_counts
            persist_threshold = math.ceil(self.history_window / 2)
            persistent_count = int(np.sum(defect_counts >= persist_threshold))
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if f_prev >= self.threshold_fraction:
            return Action.C
        if f_prev == 0.0:
            self.punish_timer = min(self.max_punish, 1 + persistent_count)
            return Action.D
        p = f_prev
        coop_now = random.random() < p
        action = Action.C if coop_now else Action.D
        defectors_last = self.n - last_round_cooperators
        if defectors_last > (1.0 - self.threshold_fraction) * self.n:
            self.punish_timer = min(self.max_punish, 1 + persistent_count)
        return action

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    Conditional Collective Cooperator with Proportional Punishment (CCCPP).

    - Start by cooperating.
    - Monitor recent cooperation over a short window.
    - Tolerate a small fraction of defectors.
    - If defections exceed tolerance and cooperation drops noticeably, enact a short
      proportional punishment (defect for a few rounds).
    - After punishment, require a short recovery (F consecutive cooperative-window rounds)
      before resuming unconditional cooperation.
    - Defect in the final round unless near-unanimous cooperation has been observed recently.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.T_small = 0.2
        self.P_max = 3
        self.F = 2
        self.L = min(10, max(1, self.r - 1))
        self.m = min(3, self.L)
        self.U = 0.95
        self.punishment_timer = 0
        self.recover_counter = 0
        self.post_punishment_wait = 0
        self.was_punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        idx = int(state.round_number)
        t = idx + 1
        n = self.n
        r = self.r
        if idx == 0 or history is None:
            self.punishment_timer = 0
            self.recover_counter = 0
            self.post_punishment_wait = 0
            self.was_punishing = False
            return Action.C
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            self.was_punishing = True
            return action
        if self.was_punishing and self.punishment_timer == 0:
            self.post_punishment_wait = self.F
            self.recover_counter = 0
            self.was_punishing = False
        prev_rounds = idx
        window_len = min(self.L, prev_rounds)
        coop_fracs = []
        start = prev_rounds - window_len
        for s in range(start, prev_rounds):
            opp_coops = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
            my_coop = int(bool(history.my_actions[s])) if history.my_actions.size else 0
            S_s = opp_coops + my_coop
            coop_fracs.append(float(S_s) / float(n))
        recent_coop = float(np.mean(coop_fracs)) if coop_fracs else 0.0
        last_opp_coops = int(np.sum(history.opponent_actions[idx - 1, :])) if history.opponent_actions.size else 0
        last_my_coop = int(bool(history.my_actions[idx - 1])) if history.my_actions.size else 0
        S_last = last_opp_coops + last_my_coop
        last_coop = float(S_last) / float(n)
        prior_m = min(self.m, prev_rounds - 1)
        if prior_m <= 0:
            trend = 0.0
        else:
            prior_start = idx - 1 - prior_m
            prior_fracs = []
            for s in range(prior_start, idx - 1):
                opp = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
                me = int(bool(history.my_actions[s])) if history.my_actions.size else 0
                prior_fracs.append(float(opp + me) / float(n))
            prior_avg = float(np.mean(prior_fracs)) if prior_fracs else last_coop
            trend = last_coop - prior_avg
        defectors_last = n - S_last
        tolerated_defectors = math.ceil(n * self.T_small)
        if t == r:
            check_len = min(3, prev_rounds)
            unanimous_ok = True
            if check_len <= 0:
                unanimous_ok = False
            else:
                start_u = prev_rounds - check_len
                for s in range(start_u, prev_rounds):
                    opp = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
                    me = int(bool(history.my_actions[s])) if history.my_actions.size else 0
                    frac = float(opp + me) / float(n)
                    if frac < self.U:
                        unanimous_ok = False
                        break
            if unanimous_ok:
                return Action.C
            else:
                return Action.D
        action: Action
        high_threshold = 1.0 - self.T_small
        low_threshold = self.T_small
        if self.post_punishment_wait <= 0:
            if recent_coop >= high_threshold:
                action = Action.C
            elif recent_coop <= low_threshold:
                action = Action.D
            elif trend >= 0.0:
                action = Action.C
            else:
                action = Action.D
        elif recent_coop <= low_threshold:
            action = Action.D
        elif trend >= 0.0:
            action = Action.C
        else:
            action = Action.D
        remaining = r - idx
        delta_threshold = 0.1 if remaining <= self.P_max else 0.05
        if self.punishment_timer == 0 and last_coop < high_threshold and (last_coop < recent_coop - delta_threshold) and (remaining > 0):
            excess_defectors = max(0, defectors_last - tolerated_defectors)
            punishment_length = min(self.P_max, max(1, excess_defectors))
            punishment_length = min(punishment_length, remaining)
            self.punishment_timer = punishment_length
            self.punishment_timer = max(0, self.punishment_timer - 1)
            self.was_punishing = True
            return Action.D
        if self.post_punishment_wait > 0:
            if recent_coop >= high_threshold:
                self.recover_counter += 1
            else:
                self.recover_counter = 0
            if self.recover_counter >= self.F:
                self.post_punishment_wait = 0
                self.recover_counter = 0
        return action

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Collective Conditional Cooperator with Forgiveness (CCCF)

    - Signals cooperation on the first move.
    - Reciprocates when recent group cooperation is high.
    - Triggers short, proportional punishment when recent cooperation is low.
    - Forgives immediately if the group returns to cooperation (or shows a strong spike).
    - Defects defensively in the final L_end rounds to avoid endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.m = k / n if n > 0 else 0.0
        self.W = min(5, max(1, math.floor(r / 10)))
        raw = 0.6 - 0.4 * ((k - 1.0) / max(1.0, n - 1.0))
        self.theta_coop = min(0.6, max(0.4, raw))
        self.theta_punish = max(0.1, self.theta_coop - 0.2)
        self.P = int(min(3, max(1, round(2.0 * (1.0 - self.m)))))
        self.L_end = min(3, r)
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            self.punish_remaining = 0
            return Action.C
        r = int(self.game_description.n_rounds)
        rem = r - t
        if rem <= self.L_end:
            self.punish_remaining = 0
            return Action.D
        opp_actions = history.opponent_actions
        past_rounds = 0
        try:
            past_rounds = int(opp_actions.shape[0])
        except Exception:
            past_rounds = 0
        if past_rounds == 0:
            return Action.D
        use_W = min(self.W, past_rounds)
        recent_slice = opp_actions[-use_W:, :]
        try:
            per_round_fracs = np.mean(recent_slice.astype(float), axis=1)
            avg_others = float(np.mean(per_round_fracs)) if per_round_fracs.size > 0 else 0.0
        except Exception:
            avg_others = float(np.mean(recent_slice)) if recent_slice.size > 0 else 0.0
        try:
            last_round = opp_actions[-1, :]
            last_round_frac = float(np.mean(last_round.astype(float)))
        except Exception:
            last_round_frac = 0.0
        if last_round_frac >= 0.8:
            self.punish_remaining = 0
            return Action.C
        if self.punish_remaining and self.punish_remaining > 0:
            if avg_others > self.theta_coop:
                self.punish_remaining = 0
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if avg_others >= self.theta_coop:
            return Action.C
        if avg_others <= self.theta_punish:
            self.punish_remaining = int(self.P)
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if last_round_frac >= 0.5:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Collective Conditional Cooperation with Proportional Punishment and Stochastic Forgiveness (CCPPSF).

    - Starts cooperatively (unless single-round game).
    - Monitors recent cooperation among other players using a window W.
    - If group cooperation fraction f_w >= alpha -> cooperate.
    - If f_w < alpha -> start/continue a proportional punishment of length L (counting current round),
      with small-probability cooperation probes (epsilon) during punishment to test recovery.
    - Punishment lengths are capped so they never extend into the final round.
    - Last round always defects. In final G rounds (excluding the last) only cooperate if recent
      history shows near-unanimous cooperation (all other players cooperated in the sampled window).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = 0.6
        self.beta = 0.5
        self.epsilon = 0.05
        self.W = min(5, max(1, math.floor(self.r / 5)))
        self.G = min(3, math.floor(self.r / 10))
        self.punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def compute_f_w():
            if history is None:
                return 0.0
            opp_actions = history.opponent_actions
            if opp_actions is None:
                return 0.0
            rounds_recorded = opp_actions.shape[0]
            if rounds_recorded <= 0:
                return 0.0
            m = min(self.W, rounds_recorded)
            recent = opp_actions[-m:, :]
            per_round_counts = np.sum(recent, axis=1)
            mean_coops = float(np.mean(per_round_counts)) if per_round_counts.size > 0 else 0.0
            denom = max(1, self.n - 1)
            return mean_coops / denom
        t_index = int(state.round_number)
        t = t_index + 1
        if t_index == 0:
            if self.r == 1:
                return Action.D
            self.punishment_remaining = 0
            return Action.C
        if t == self.r:
            self.punishment_remaining = 0
            return Action.D
        f_w = compute_f_w()
        if self.G > 0 and t > self.r - self.G and (t < self.r):
            if f_w >= 1.0 - 1e-12:
                self.punishment_remaining = 0
                return Action.C
            else:
                self.punishment_remaining = 0
                return Action.D
        if self.punishment_remaining > 0:
            if f_w >= self.alpha:
                self.punishment_remaining = 0
                return Action.C
            probe = random.random() < self.epsilon
            action = Action.C if probe else Action.D
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return action
        if f_w >= self.alpha:
            return Action.C
        p = 1.0 - f_w
        suggested = max(1, math.ceil(self.beta * p * self.r))
        max_allowed = max(0, self.r - t)
        L = min(suggested, max_allowed)
        if L <= 0:
            return Action.D
        self.punishment_remaining = L
        probe = random.random() < self.epsilon
        action = Action.C if probe else Action.D
        self.punishment_remaining = max(0, self.punishment_remaining - 1)
        return action

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    Proportional Reciprocity with Forgiveness (PRF)

    - Starts cooperative.
    - Uses a sliding window of recent rounds to compute other-players'
      cooperation rates and a group cooperation rate.
    - Tolerates small lapses, punishes proportionally (temporary punishment
      length scales with how low cooperation is), forgives immediately if
      the group recovers, and switches to permanent defection if at least
      half of the other players are persistent defectors.
    - Applies an endgame rule: when <= 2 rounds remain, defect unless recent
      rounds show unanimous cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        return

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def ceil(x: float) -> int:
            return math.ceil(x)
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number + 1
        remaining = r - t + 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        rounds_played = opp_actions.shape[0]
        if rounds_played != state.round_number:
            return Action.C

        def compute_group_coop_rate_at(s: int) -> float:
            available_past = s - 1
            W = min(4, available_past)
            if W <= 0:
                return 1.0
            start = max(0, available_past - W)
            window = opp_actions[start:start + W, :]
            if window.size == 0:
                return 1.0
            return float(window.astype(float).mean())

        def count_persistent_defectors_at(s: int) -> int:
            if s < 6:
                return 0
            available_past = s - 1
            window_p = min(6, available_past)
            if window_p <= 0:
                return 0
            start = max(0, available_past - window_p)
            window = opp_actions[start:start + window_p, :]
            if window.size == 0:
                return 0
            coop_j = window.astype(float).mean(axis=0)
            flags = coop_j <= 0.2
            return int(flags.sum())
        P_count_sim = 0
        permanent_defect = False
        other_half_threshold = ceil((n - 1) / 2)
        for s in range(1, t):
            rem_s = r - s + 1
            if rem_s <= 2:
                last_k = min(2, s - 1)
                all_coop_recent = True
                if last_k <= 0:
                    all_coop_recent = True
                else:
                    start = s - 1 - last_k
                    opp_segment = opp_actions[start:start + last_k, :]
                    my_segment = my_actions[start:start + last_k]
                    for rr in range(last_k):
                        if not my_segment[rr]:
                            all_coop_recent = False
                            break
                        if not opp_segment[rr, :].all():
                            all_coop_recent = False
                            break
                action_s = Action.C if all_coop_recent else Action.D
                continue
            pers_count = count_persistent_defectors_at(s)
            if pers_count >= other_half_threshold:
                permanent_defect = True
                P_count_sim = 0
                continue
            group_rate = compute_group_coop_rate_at(s)
            if P_count_sim > 0:
                if group_rate >= 0.6:
                    P_count_sim = 0
                    action_s = Action.C
                else:
                    P_count_sim = max(0, P_count_sim - 1)
                    action_s = Action.D
            elif group_rate >= 0.6:
                action_s = Action.C
            elif 0.3 <= group_rate < 0.6:
                action_s = Action.C
            else:
                extra = ceil((0.6 - group_rate) * 3)
                P_count_sim = 1 + extra
                action_s = Action.D
        if remaining <= 2:
            last_k = min(2, t - 1)
            all_coop_recent = True
            if last_k <= 0:
                all_coop_recent = True
            else:
                start = t - 1 - last_k
                opp_segment = opp_actions[start:start + last_k, :]
                my_segment = my_actions[start:start + last_k]
                for rr in range(last_k):
                    if not my_segment[rr]:
                        all_coop_recent = False
                        break
                    if not opp_segment[rr, :].all():
                        all_coop_recent = False
                        break
            return Action.C if all_coop_recent else Action.D
        pers_count_now = count_persistent_defectors_at(t)
        if pers_count_now >= other_half_threshold:
            return Action.D
        group_rate_now = compute_group_coop_rate_at(t)
        if P_count_sim > 0:
            if group_rate_now >= 0.6:
                P_count_sim = 0
                return Action.C
            else:
                return Action.D
        else:
            if group_rate_now >= 0.6:
                return Action.C
            if 0.3 <= group_rate_now < 0.6:
                return Action.C
            _ = 1 + ceil((0.6 - group_rate_now) * 3)
            return Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    Collective-First Conditional Cooperation (CFCC)

    - Tries to sustain near-universal contributions early/mid-game.
    - Uses a short memory window w to judge recent group cooperation.
    - If cooperation falls below threshold T (0.60) in a sustained way,
      initiates a calibrated collective punishment (defect L rounds).
    - After punishment runs, performs one cooperative TEST round; if the test
      shows cooperation restored, return to NORMAL; otherwise punish again
      with increased severity.
    - Always defects in the final round to avoid last-round exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.w = min(5, max(1, math.floor(self.r / 10)))
        self.T = 0.6
        self.base_punish = max(1, math.ceil(self.r / 10))
        self.state = 'NORMAL'
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == max(0, self.r - 1):
            return Action.D
        if state.round_number == 0:
            if self.r > 1:
                self.state = 'NORMAL'
                self.punish_remaining = 0
                return Action.C
        if history is None:
            return Action.C
        rounds_played = history.my_actions.shape[0]

        def coop_counts_last_m(m: int):
            if m <= 0:
                return np.array([], dtype=int)
            opp_slice = history.opponent_actions[-m:, :]
            opp_counts = opp_slice.sum(axis=1).astype(int)
            my_slice = history.my_actions[-m:].astype(int)
            total = opp_counts + my_slice
            return total
        if self.state == 'POST_TEST':
            if rounds_played >= 1:
                last_round_idx = rounds_played - 1
                opp_last = history.opponent_actions[last_round_idx, :].sum().astype(int)
                my_last = int(history.my_actions[last_round_idx])
                frac_last = (opp_last + my_last) / max(1, self.n)
                if frac_last >= self.T:
                    self.state = 'NORMAL'
                    self.punish_remaining = 0
                else:
                    window = min(self.w, rounds_played)
                    counts = coop_counts_last_m(window)
                    recent_coop_fraction = float(counts.sum()) / (window * self.n) if window > 0 else 0.0
                    severity = 1.0 - recent_coop_fraction
                    max_punish = max(1, self.r - state.round_number - 1)
                    new_punish = max(1, math.ceil(self.base_punish * severity * 1.5))
                    new_punish = min(max_punish, new_punish)
                    self.state = 'PUNISH'
                    self.punish_remaining = new_punish
        if self.state == 'PUNISH' and self.punish_remaining > 0:
            max_punish = max(1, self.r - state.round_number - 1)
            if self.punish_remaining > max_punish:
                self.punish_remaining = max_punish
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining <= 0:
                self.state = 'TEST'
                self.punish_remaining = 0
            else:
                self.state = 'PUNISH'
            return action
        if self.state == 'TEST':
            self.state = 'POST_TEST'
            return Action.C
        window = min(self.w, rounds_played)
        counts = coop_counts_last_m(window)
        recent_coop_fraction = float(counts.sum()) / (window * self.n) if window > 0 else 0.0
        if recent_coop_fraction >= self.T:
            self.state = 'NORMAL'
            return Action.C
        per_round_frac = counts / float(self.n) if counts.size > 0 else np.array([], dtype=float)
        required_low = 1 if self.w == 1 else 2
        low_count = int((per_round_frac < self.T).sum())
        if low_count < required_low:
            self.state = 'NORMAL'
            return Action.C
        max_punish = max(1, self.r - state.round_number - 1)
        punish_length = max(1, math.ceil(self.base_punish * (1.0 - recent_coop_fraction)))
        punish_length = min(max_punish, punish_length)
        self.state = 'PUNISH'
        self.punish_remaining = punish_length
        action = Action.D
        self.punish_remaining -= 1
        if self.punish_remaining <= 0:
            self.state = 'TEST'
            self.punish_remaining = 0
        else:
            self.state = 'PUNISH'
        return action

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    Adaptive Collective Cooperator (ACC)

    - Starts cooperatively (except in single-round games), tries to sustain cooperation
      when others reciprocate, punishes short and proportionally when clear exploitation
      is observed, and forgives quickly so cooperation can recover.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(0, self.r - 1))
        self.high_multiplier = self.k / self.n >= 0.6
        self.q_req = 0.5 if self.high_multiplier else 0.7
        self.tol = 0.15
        self.L0 = 2
        self.Lmax = min(5, self.r)
        self.P = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if state.round_number == 0 or history is None:
            if self.r == 1:
                return Action.D
            return Action.C
        prev_idx = state.round_number - 1
        opp_actions = history.opponent_actions
        try:
            prev_row = opp_actions[prev_idx]
        except Exception:
            prev_row = np.array([], dtype=bool)
        others = max(1, self.n - 1)
        if prev_row.size == 0:
            q_last = 0.0
            cooperators_last = 0
        else:
            cooperators_last = int(np.sum(prev_row))
            q_last = cooperators_last / float(others)
        window_size = min(self.w, state.round_number)
        if window_size <= 0:
            p_recent = q_last
        else:
            start = max(0, prev_idx - window_size + 1)
            rows = opp_actions[start:prev_idx + 1]
            if rows.size == 0:
                p_recent = q_last
            else:
                per_round_fracs = np.mean(rows.astype(float), axis=1)
                p_recent = float(np.mean(per_round_fracs))
        round_cooperators_est = int(round(others * q_last))
        defectors = max(0, others - round_cooperators_est)
        if self.P == 0:
            if q_last <= self.q_req - self.tol and p_recent < self.q_req and (defectors >= 1):
                denom = max(1, int(round(others / 4.0)))
                added = math.floor(defectors / denom)
                P_new = self.L0 + added
                self.P = min(self.Lmax, P_new)
        if self.P > 0:
            self.P = max(0, self.P - 1)
            return Action.D
        if p_recent >= self.q_req:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    Forgiving Group-Conditional Cooperation (FGCC)

    - Cooperates when recent group cooperation (fraction of other players cooperating)
      in a window W meets or exceeds threshold theta.
    - When recent cooperation falls below theta, triggers a short collective punishment
      of length up to P rounds (including the round the punishment starts), bounded
      by remaining rounds. Punishments are non-targeted and temporary.
    - Always defects in the final round. Handles very short games (r <= 2)
      with simplified rules.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = min(5, max(1, r // 5))
        self.theta = 0.6
        self.P = 2
        self.inPunish = False
        self.punishLeft = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t_index = int(state.round_number)
        if r <= 1:
            return Action.D
        if r == 2:
            if t_index == 0:
                return Action.C
            return Action.D
        if t_index == r - 1:
            return Action.D
        if self.inPunish:
            action = Action.D
            if self.punishLeft > 0:
                self.punishLeft -= 1
            if self.punishLeft <= 0:
                self.inPunish = False
                self.punishLeft = 0
            return action
        p = 1.0
        if history is not None and t_index > 0:
            num_rounds_to_consider = min(self.W, t_index)
            start_idx = t_index - num_rounds_to_consider
            try:
                recent = history.opponent_actions[start_idx:t_index, :]
                if recent.size == 0:
                    p = 1.0
                else:
                    denom = max(1, n - 1)
                    per_round_frac = np.sum(recent, axis=1) / float(denom)
                    p = float(np.mean(per_round_frac))
            except Exception:
                p = 1.0
        if p >= self.theta:
            return Action.C
        remaining_including_current = max(0, r - t_index)
        initial_punish = min(self.P, remaining_including_current)
        self.inPunish = True
        self.punishLeft = max(0, int(initial_punish) - 1)
        if self.punishLeft <= 0:
            self.inPunish = False
            self.punishLeft = 0
        return Action.D

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Collective Conditional Cooperator with Lenient Reciprocity and Probing.

    - Starts cooperative.
    - Uses a sliding-window cooperation score per other player to compute group cooperation r_bar.
    - Adjustable cooperation threshold tau that increases with public-good multiplier k.
    - Lenient ambiguous-zone behavior: give short grace period before punishing.
    - Short, proportional punishments whose length depends on recent r_bar.
    - Occasional cooperative probes after sustained mutual defection to attempt to restart cooperation.
    - Conservative near endgame: defect in final round; in last K rounds cooperate only if near-unanimous cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_remaining: int = 0
        self.pending_ambiguous_start: None | int = None
        self.pending_ambiguous_W: int = 0
        self.pending_probe_start: None | int = None
        self._rand = random.Random()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def clamp(x, lo, hi):
            return max(lo, min(hi, x))
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        k = self.game_description.k
        t_idx = state.round_number
        t1 = t_idx + 1
        if t_idx == 0:
            self.punishment_remaining = 0
            self.pending_ambiguous_start = None
            self.pending_ambiguous_W = 0
            self.pending_probe_start = None
            return Action.C
        if history is None:
            return Action.C
        past_rounds = history.my_actions.shape[0]
        W = min(10, t1 - 1)
        opp_actions = history.opponent_actions
        if W > 0:
            last_W = opp_actions[-W:, :]
            r_j = np.mean(last_W.astype(float), axis=0) if n - 1 > 0 else np.array([])
        else:
            r_j = np.zeros(n - 1, dtype=float)
        r_bar = float(np.mean(r_j)) if r_j.size > 0 else 0.0
        tau_base = 0.6
        adj = 0.15 * (k - 1.0) / max(1, n - 1)
        tau = clamp(tau_base - adj, 0.35, 0.75)
        tau_low = max(0.1, tau - 0.25)
        P = int(clamp(2 + math.floor(3 * (1.0 - r_bar)), 1, 6))
        S = 4
        K = min(3, max(1, math.floor(r_total / 10))) if r_total > 0 else 1
        if self.pending_ambiguous_start is not None:
            start = self.pending_ambiguous_start
            Wp = self.pending_ambiguous_W
            if past_rounds - 1 >= start + Wp:
                obs_start = start + 1
                obs_end = start + Wp + 1
                if obs_start < obs_end and obs_end <= past_rounds:
                    obs = opp_actions[obs_start:obs_end, :]
                    if obs.size == 0:
                        frac_failures = 1.0
                    else:
                        responded = np.any(obs, axis=0)
                        failures = ~responded
                        frac_failures = float(np.sum(failures)) / max(1, n - 1)
                    if frac_failures > 0.4:
                        self.punishment_remaining = max(0, P - 1)
                self.pending_ambiguous_start = None
                self.pending_ambiguous_W = 0
        if self.pending_probe_start is not None:
            start = self.pending_probe_start
            if past_rounds - 1 >= start + 2:
                obs_start = start + 1
                obs_end = start + 3
                if obs_start < obs_end and obs_end <= past_rounds:
                    obs = opp_actions[obs_start:obs_end, :]
                    if obs.size == 0:
                        frac_responders = 0.0
                    else:
                        responders = np.any(obs, axis=0)
                        frac_responders = float(np.sum(responders)) / max(1, n - 1)
                    if frac_responders <= 0.4:
                        self.punishment_remaining = max(0, P - 1)
                self.pending_probe_start = None
        if state.round_number >= r_total - K and state.round_number < r_total - 1:
            last_m = min(2, t1 - 1)
            unanimous_recent = False
            if last_m > 0:
                recent = opp_actions[-last_m:, :]
                unanimous_recent = bool(np.all(recent))
            if r_bar >= 0.9 and unanimous_recent:
                return Action.C
            else:
                return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return Action.D
        my_actions = history.my_actions
        defection_streak = 0
        for a in my_actions[::-1]:
            if not bool(a):
                defection_streak += 1
            else:
                break
        if defection_streak >= S and state.round_number < r_total - K and (self.pending_probe_start is None) and (self.pending_ambiguous_start is None):
            self.pending_probe_start = t_idx
            return Action.C
        if r_bar >= tau:
            self.pending_ambiguous_start = None
            self.pending_ambiguous_W = 0
            return Action.C
        if r_bar <= tau_low:
            self.punishment_remaining = max(0, P - 1)
            return Action.D
        W_prime = min(3, t1 - 1)
        if W_prime > 0:
            if self.pending_ambiguous_start is None:
                self.pending_ambiguous_start = t_idx
                self.pending_ambiguous_W = W_prime
        return Action.C

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Collective-First Conditional-Cooperator (CFCC)

    - Start by cooperating.
    - Use a window of recent rounds to judge others (W = min(3, r-1)).
    - If the recent fraction of other cooperators >= theta (0.60) remain cooperating.
    - If cooperation falls below theta, enter a finite Punish phase of length L (15% of r),
      capped if there are too few rounds left before the final round.
    - After Punish finishes, enter a short Recover phase of length R (min(2, max(1, r-2)))
      where we play C explicitly. Observe others' responses in that recover round:
         - If others reciprocate (fraction >= theta) switch back to Cooperate.
         - Otherwise re-enter Punish.
    - Always defect in the final round.
    - Be forgiving (finite punishments) and conservative near the end (cap punish/recover).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(3, max(1, self.r - 1))
        self.theta = 0.6
        self.L = max(1, math.ceil(0.15 * self.r))
        self.R = min(2, max(1, self.r - 2))
        self.mode = 'Cooperate'
        self.punish_counter = 0
        self.recover_counter = 0
        self._last_was_recover = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n

        def remaining_before_last_at(round_index: int) -> int:
            val = r - 1 - round_index
            return int(val) if val >= 0 else 0

        def remaining_before_last_next(round_index: int) -> int:
            val = r - 2 - round_index
            return int(val) if val >= 0 else 0
        if t == 0 or history is None:
            self.mode = 'Cooperate'
            self.punish_counter = 0
            self.recover_counter = 0
            self._last_was_recover = False
            return Action.C
        if self._last_was_recover:
            prev_idx = t - 1
            if prev_idx >= 0 and prev_idx < history.opponent_actions.shape[0]:
                opp_row = history.opponent_actions[prev_idx]
                try:
                    f_now = float(np.mean(opp_row)) if opp_row.size > 0 else 0.0
                except Exception:
                    f_now = 0.0
            else:
                f_now = 0.0
            if f_now >= self.theta:
                self.mode = 'Cooperate'
                self.punish_counter = 0
                self.recover_counter = 0
            else:
                self.mode = 'Punish'
                rem_before_last = remaining_before_last_at(t)
                if rem_before_last < self.L:
                    self.punish_counter = max(1, rem_before_last)
                else:
                    self.punish_counter = self.L
            self._last_was_recover = False
        if t == r - 1:
            return Action.D
        if self.mode == 'Cooperate':
            available_rounds = history.opponent_actions.shape[0]
            window_size = min(self.W, max(0, t))
            if window_size <= 0:
                f_recent = 1.0
            else:
                rows = history.opponent_actions[-window_size:]
                try:
                    per_round = np.mean(rows, axis=1) if rows.size > 0 else np.array([1.0] * window_size)
                    f_recent = float(np.mean(per_round))
                except Exception:
                    f_recent = 1.0
            if f_recent >= self.theta:
                return Action.C
            else:
                rem_before_last = remaining_before_last_at(t)
                if rem_before_last < self.L:
                    self.punish_counter = max(1, rem_before_last)
                else:
                    self.punish_counter = self.L
                self.punish_counter = max(0, int(self.punish_counter))
                if self.punish_counter > 0:
                    self.punish_counter -= 1
                if self.punish_counter == 0:
                    rem_next = remaining_before_last_next(t)
                    if rem_next <= 1:
                        last_observed_fraction = 0.0
                        if history.opponent_actions.shape[0] >= 1:
                            try:
                                last_observed_fraction = float(np.mean(history.opponent_actions[-1]))
                            except Exception:
                                last_observed_fraction = 0.0
                        if last_observed_fraction >= 0.95:
                            self.mode = 'Cooperate'
                            self.punish_counter = 0
                            self.recover_counter = 0
                        else:
                            self.mode = 'Punish'
                            self.punish_counter = max(1, rem_next)
                    else:
                        self.mode = 'Recover'
                        self.recover_counter = min(self.R, rem_next)
                else:
                    self.mode = 'Punish'
                return Action.D
        elif self.mode == 'Punish':
            self.punish_counter = max(0, int(self.punish_counter))
            if self.punish_counter > 0:
                self.punish_counter -= 1
            if self.punish_counter == 0:
                rem_next = remaining_before_last_next(t)
                if rem_next <= 1:
                    last_observed_fraction = 0.0
                    if history.opponent_actions.shape[0] >= 1:
                        try:
                            last_observed_fraction = float(np.mean(history.opponent_actions[-1]))
                        except Exception:
                            last_observed_fraction = 0.0
                    if last_observed_fraction >= 0.95:
                        self.mode = 'Cooperate'
                        self.punish_counter = 0
                        self.recover_counter = 0
                    else:
                        self.mode = 'Punish'
                        self.punish_counter = max(1, rem_next)
                else:
                    self.mode = 'Recover'
                    self.recover_counter = min(self.R, rem_next)
            else:
                self.mode = 'Punish'
            return Action.D
        elif self.mode == 'Recover':
            self.recover_counter = max(0, int(self.recover_counter))
            if self.recover_counter > 0:
                self.recover_counter -= 1
            self._last_was_recover = True
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    Collective Conditional Cooperate with Forgiving Punishment (C3FP)

    - Start by cooperating.
    - Use a recent-window estimate of others' cooperation to decide.
    - If recent cooperation >= theta -> cooperate.
    - If recent cooperation < theta -> enter a short punishment of length L (defect),
      but end punishment early if group cooperation recovers.
    - Always defect in the final round.
    - Optionally avoid punishing when only a very small set of persistent defectors
      exist but the rest of the group is cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.w = min(5, r)
        self.L = min(5, max(1, math.ceil(r / 10)))
        self.theta = max(0.5, 1.0 - (k - 1.0) / float(max(1, n - 1)))
        self.remaining_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        w = int(self.w)
        L = int(self.L)
        theta = float(self.theta)
        round_idx = int(state.round_number)
        if round_idx == 0 or history is None:
            if round_idx == 0:
                self.remaining_punish = 0
            return Action.C
        try:
            last_round_others_coop = float(history.opponent_actions[-1, :].sum()) / float(max(1, n - 1))
        except Exception:
            last_round_others_coop = 1.0
        if self.remaining_punish > 0:
            if last_round_others_coop >= theta:
                self.remaining_punish = 0
            else:
                self.remaining_punish = max(0, self.remaining_punish - 1)
        if round_idx == r - 1:
            self.remaining_punish = 0
            return Action.D
        if self.remaining_punish > 0:
            return Action.D
        prior_rounds = round_idx
        m = min(w, prior_rounds)
        if m <= 0:
            recent_fraction = 1.0
        else:
            start = prior_rounds - m
            try:
                window = history.opponent_actions[start:start + m, :]
                per_round_counts = window.sum(axis=1).astype(float)
                per_round_fracs = per_round_counts / float(max(1, n - 1))
                recent_fraction = float(np.mean(per_round_fracs))
            except Exception:
                recent_fraction = 1.0
        if recent_fraction >= theta:
            return Action.C
        if m > 0:
            try:
                window = history.opponent_actions[prior_rounds - m:prior_rounds, :]
                defect_counts = np.logical_not(window).sum(axis=0).astype(float)
                defect_fracs = defect_counts / float(m)
                persistent_mask = defect_fracs >= 0.9
                persistent_count = int(persistent_mask.sum())
            except Exception:
                persistent_mask = np.zeros((n - 1,), dtype=bool)
                persistent_count = 0
        else:
            persistent_mask = np.zeros((n - 1,), dtype=bool)
            persistent_count = 0
        if persistent_count > 0 and m > 0:
            non_persistent_count = n - 1 - persistent_count
            if non_persistent_count > 0:
                try:
                    window_excl = history.opponent_actions[prior_rounds - m:prior_rounds, :][:, ~persistent_mask]
                    per_round_counts_excl = window_excl.sum(axis=1).astype(float)
                    per_round_fracs_excl = per_round_counts_excl / float(non_persistent_count)
                    recent_fraction_excl = float(np.mean(per_round_fracs_excl))
                except Exception:
                    recent_fraction_excl = 0.0
            else:
                recent_fraction_excl = 0.0
            if recent_fraction_excl >= theta:
                return Action.C
        self.remaining_punish = L
        return Action.D

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Adaptive Collective Conditional Contributor (AC3)

    - Starts by cooperating to signal willingness to form a cooperative convention.
    - Uses a recent-lookback window to estimate each opponent's cooperation rate p_j and the group
      cooperation level P (average of p_j).
    - Rewards high recent cooperation, uses probabilistic reciprocity in mixed environments,
      and enters proportional, time-limited punishment when the group is largely non-cooperative.
    - Forgives when cooperation rebounds and uses low-probability probes during punishment and endgame
      to test for recovery. Becomes cautious near the end of a finite game and more conservative
      in very short games (r <= 5).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = max(3, math.floor(self.r / 10))
        self.Theta_high = 0.6
        self.Theta_low = 0.3
        self.Probe_prob = 0.1
        self.Final_rounds = min(3, math.floor(self.r / 4))
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        R = self.r - t
        if t == 0 or history is None:
            self.punish_remaining = 0
            return Action.C
        rounds_played = int(history.opponent_actions.shape[0])
        try:
            opp_last = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            opp_last = 0
        my_last = 1 if bool(history.my_actions[-1]) else 0
        S_last = opp_last + my_last
        short_game = self.r <= 5
        denom = min(self.L, rounds_played)
        if denom <= 0:
            P = 0.0
        else:
            recent = history.opponent_actions[-denom:, :]
            per_opponent_counts = np.sum(recent, axis=0).astype(float)
            p_js = per_opponent_counts / float(denom)
            if p_js.size == 0:
                P = 0.0
            else:
                P = float(np.mean(p_js))
        if short_game and t >= 1:
            if S_last == self.n and P >= 0.9:
                self.punish_remaining = 0
                return Action.C
            else:
                if random.random() < self.Probe_prob:
                    return Action.C
                return Action.D
        if R <= 1:
            self.punish_remaining = 0
            return Action.D
        if 2 <= R <= self.Final_rounds:
            if S_last == self.n and P >= 0.9:
                self.punish_remaining = 0
                return Action.C
            else:
                if random.random() < self.Probe_prob:
                    return Action.C
                return Action.D
        if P >= self.Theta_high:
            self.punish_remaining = 0
            return Action.C
        if self.punish_remaining > 0:
            action = Action.C if random.random() < self.Probe_prob else Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return action
        if P <= self.Theta_low:
            try:
                severity_steps = math.ceil((self.Theta_high - P) / 0.1)
            except Exception:
                severity_steps = 1
            L_punish = int(min(max(0, R - 1), 1 + max(0, severity_steps)))
            if L_punish <= 0:
                if random.random() < self.Probe_prob:
                    return Action.C
                return Action.D
            self.punish_remaining = L_punish
            action = Action.C if random.random() < self.Probe_prob else Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return action
        if self.Theta_low < P and P < self.Theta_high:
            if S_last >= math.ceil(self.n / 2):
                return Action.C
            elif random.random() < P:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Collective-Conditional Cooperator with Proportional Punishment and Forgiveness (CCP).

    - Signals cooperation on first round (unless the game length is 1, in which case it defects).
    - Uses a sliding memory window w = min(5, r) to evaluate recent cooperation by others.
    - If a clear majority of others (>= 50%) have cooperated recently, cooperate.
    - If majority threshold is not met, enter a short proportional punishment:
        punishment_length = max(1, round(P * (gamma - avg_others) / gamma))
      punishments are short and scaled. After applying the immediate punishment round, the
      stored punish_counter counts remaining future punishment rounds before re-evaluation.
    - In PUNISHING mode the strategy defects for the stored punishment rounds, then
      re-evaluates. If the group is cooperative again it exits punishment; otherwise it
      applies another one-round punishment.
    - Rapid recovery: if after leaving punishment the group meets the cooperation threshold
      for two consecutive decision points, remain cooperative and clear punishment memory.
    - In the final L rounds (endgame safety window) the strategy defects to avoid endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.w = min(5, self.n_rounds)
        self.gamma = 0.5
        base_P = max(1, math.floor(self.n_rounds / 10))
        self.P = min(3, base_P)
        base_L = max(1, math.floor(self.n_rounds / 10))
        self.L = min(3, base_L)
        self.mode = 'COOPERATIVE'
        self.punish_counter = 0
        self.recently_exited_punish = False
        self.post_exit_good_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_index = state.round_number
        r = self.n_rounds
        n = self.n_players

        def compute_avg_others() -> float:
            if history is None:
                return 0.0
            available_rounds = min(self.w, t_index)
            if available_rounds <= 0:
                return 0.0
            opp_acts = history.opponent_actions
            if opp_acts.size == 0:
                return 0.0
            recent = opp_acts[-available_rounds:, :]
            total_coops = int(np.sum(recent))
            denom = available_rounds * max(1, recent.shape[1])
            if denom == 0:
                return 0.0
            return float(total_coops) / float(denom)
        if t_index == 0:
            if r == 1:
                return Action.D
            return Action.C
        if t_index >= r - self.L:
            return Action.D
        avg_others = compute_avg_others()
        if self.mode == 'COOPERATIVE':
            if self.recently_exited_punish:
                if avg_others >= self.gamma:
                    self.post_exit_good_streak += 1
                    if self.post_exit_good_streak >= 2:
                        self.recently_exited_punish = False
                        self.post_exit_good_streak = 0
                        self.punish_counter = 0
                    return Action.C
                else:
                    self.recently_exited_punish = False
                    self.post_exit_good_streak = 0
            if avg_others >= self.gamma:
                return Action.C
            severity = max(0.0, (self.gamma - avg_others) / self.gamma)
            computed_len = max(1, round(self.P * severity))
            self.mode = 'PUNISHING'
            self.punish_counter = max(0, computed_len - 1)
            self.recently_exited_punish = False
            self.post_exit_good_streak = 0
            return Action.D
        if self.mode == 'PUNISHING':
            if self.punish_counter > 0:
                self.punish_counter -= 1
                return Action.D
            avg_others_re = compute_avg_others()
            if avg_others_re >= self.gamma:
                self.mode = 'COOPERATIVE'
                self.recently_exited_punish = True
                self.post_exit_good_streak = 1
                return Action.C
            else:
                self.punish_counter = 0
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Collective Conditional Cooperation with Graduated Punishment (C3GP).

    - Starts by cooperating, defects on the final round.
    - Uses a lookback window (up to 5 rounds) to compute recent cooperation among others.
    - If recent cooperation is above a threshold H (depends on k), cooperates.
    - If recent cooperation falls below H, enters a proportional, temporary punishment
      whose length scales with severity, group size, and k. Punishment is cancellable
      immediately if others restore cooperation. Small deviations near the threshold
      bias toward cooperation. Early-game history gives extra weight to the most
      recent round. In extremely short tournaments (r <= 2) punish cautiously.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        n_rounds = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        rnd = int(state.round_number)
        if rnd == 0 or history is None:
            if not hasattr(self, 'P'):
                self.P = 0
            return Action.C
        if rnd == n_rounds - 1:
            return Action.D
        L = min(5, rnd)
        opp_actions = history.opponent_actions
        recent_slice = opp_actions[-L:, :] if L > 0 else opp_actions[:0, :]
        sum_recent = int(np.sum(recent_slice).item()) if recent_slice.size > 0 else 0
        if L < 5 and L >= 1:
            sum_last = int(np.sum(opp_actions[-1, :]).item())
            sum_others = sum_recent - sum_last
            weighted_denominator = (n - 1) * (L + 1)
            if weighted_denominator > 0:
                avg_recent = (sum_others + 2 * sum_last) / weighted_denominator
            else:
                avg_recent = 0.0
        else:
            denom = (n - 1) * L if L > 0 else 1
            avg_recent = sum_recent / denom
        if k >= n / 2:
            H = 0.6
        else:
            H = 0.7
        near_threshold = abs(avg_recent - H) <= 0.03
        coop_condition = avg_recent >= H or near_threshold
        short_tournament_penalty_block = False
        if n_rounds <= 2 and rnd == max(0, n_rounds - 2):
            last_opponents_coop = int(np.sum(opp_actions[-1, :]).item())
            my_last = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
            total_coop_last = last_opponents_coop + (1 if my_last else 0)
            total_coop_fraction = total_coop_last / max(1, n)
            if total_coop_fraction >= 0.5:
                short_tournament_penalty_block = True
        if getattr(self, 'P', 0) > 0:
            if coop_condition:
                self.P = 0
                return Action.C
            self.P = max(0, self.P - 1)
            return Action.D
        if coop_condition:
            return Action.C
        if short_tournament_penalty_block:
            return Action.C
        defection_severity = max(0.0, H - avg_recent)
        denom_k = max(1.0, k)
        raw_punish = 1 + math.ceil(defection_severity * 5.0 * (n / denom_k))
        remaining_after_current = max(0, n_rounds - (rnd + 1))
        P_new = int(min(remaining_after_current, int(raw_punish)))
        self.P = P_new
        return Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    Proportional Reciprocal Collective (PRC)

    - Starts by cooperating (unless r == 1).
    - Uses a sliding window W and endgame horizon H derived from total rounds r.
    - Tracks a short, proportional punishment phase (punish_remaining) triggered
      when the recent average cooperation of others (S_others) falls below tau.
    - Punishment length P = 1 + ceil(f_last * W), where f_last is the fraction
      of other players who defected in the immediately preceding round.
    - While punishing, the strategy defects each round, but will cancel remaining
      punishment immediately if S_others recovers to >= tau.
    - In the final H rounds, the strategy requires a high cooperation history
      (tau_high) to keep cooperating; otherwise it defects (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = max(1, math.floor(r / 5))
        self.H = min(3, max(1, math.floor(r / 10)))
        self.tau = 0.6
        self.tau_high = 0.85
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        n_players = int(self.game_description.n_players)
        opponents_count = max(0, n_players - 1)
        t_index = int(state.round_number)
        t = t_index + 1
        if t_index == 0:
            if r == 1:
                self.punish_remaining = 0
                return Action.D
            self.punish_remaining = 0
            return Action.C
        if history is None:
            return Action.C
        history_rounds = history.opponent_actions.shape[0]
        considered_rounds = min(self.W, history_rounds)
        if considered_rounds <= 0:
            S_others = 0.0
        else:
            recent_opponents = history.opponent_actions[-considered_rounds:, :]
            coop_counts = np.sum(recent_opponents, axis=0).astype(float)
            s_j = coop_counts / float(considered_rounds)
            if opponents_count > 0:
                S_others = float(np.mean(s_j))
            else:
                S_others = 0.0
        if history_rounds <= 0 or opponents_count == 0:
            f_last = 0.0
        else:
            last_row = history.opponent_actions[-1, :]
            defectors = float(np.sum(~last_row))
            f_last = defectors / float(opponents_count)
        if t > r - self.H:
            self.punish_remaining = 0
            if S_others >= self.tau_high:
                return Action.C
            else:
                return Action.D
        if S_others >= self.tau:
            self.punish_remaining = 0
            return Action.C
        if self.punish_remaining > 0:
            if S_others >= self.tau:
                self.punish_remaining = 0
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        P = 1 + math.ceil(f_last * float(self.W))
        self.punish_remaining = max(0, int(P) - 1)
        return Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Collective-minded adaptive strategy for N-player Public Goods Game.

    - Cooperates by default when a clear majority of others have recently cooperated.
    - Uses short, targeted, proportional punishments against abrupt/persistent defectors.
    - Forgives when punished players resume cooperation.
    - Periodically probes during defensive phases to check for recovery.
    - Defects in the final round (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.n_opponents = max(0, self.n_players - 1)
        self.m = int(min(10, max(3, math.floor(self.n_rounds / 4))))
        self.tau = 0.6
        self.tau_ind = 0.5
        self.p0 = 2
        self.probe_interval = int(max(6, math.ceil(self.n_rounds / 6)))
        self.recent_coop_counts = [0] * self.n_opponents
        self.punishment_timers = [0] * self.n_opponents
        self.last_probe_round = -self.probe_interval

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == self.n_rounds - 1:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.C
        rounds_so_far = history.opponent_actions.shape[0]
        window = min(self.m, rounds_so_far) if rounds_so_far > 0 else 1
        window = max(1, window)
        opp_actions = history.opponent_actions.astype(int)
        recent_counts = [0] * self.n_opponents
        recent_rates = [0.0] * self.n_opponents
        for i in range(self.n_opponents):
            if rounds_so_far >= 1:
                recent_slice = opp_actions[-window:, i]
                cnt = int(recent_slice.sum())
            else:
                cnt = 0
            recent_counts[i] = cnt
            recent_rates[i] = cnt / float(window)
        self.recent_coop_counts = recent_counts
        for i in range(self.n_opponents):
            if self.punishment_timers[i] > 0:
                self.punishment_timers[i] -= 1
                if self.punishment_timers[i] < 0:
                    self.punishment_timers[i] = 0
        if self.n_opponents > 0:
            group_coop_rate = float(np.mean(np.array(recent_rates)))
        else:
            group_coop_rate = 1.0
        if rounds_so_far >= 1:
            last_round_cooperators = int(opp_actions[-1, :].sum())
        else:
            last_round_cooperators = 0

        def abrupt_defection(i: int) -> bool:
            if rounds_so_far < 1:
                return False
            last = int(opp_actions[-1, i])
            if last == 1:
                return False
            if rounds_so_far >= 2 and int(opp_actions[-2, i]) == 1:
                return True
            if rounds_so_far >= 3:
                if int(opp_actions[-2, i]) == 1 or int(opp_actions[-3, i]) == 1:
                    return True
            return False
        for i in range(self.n_opponents):
            if recent_rates[i] < self.tau_ind and abrupt_defection(i):
                deficit = max(0.0, self.tau_ind - recent_rates[i])
                extra = math.ceil(deficit * self.m)
                new_timer = int(self.p0 + extra)
                if new_timer > self.punishment_timers[i]:
                    self.punishment_timers[i] = new_timer
        for i in range(self.n_opponents):
            if recent_rates[i] >= self.tau_ind and self.punishment_timers[i] > 0:
                self.punishment_timers[i] = 0
        high_coop_count = sum((1 for rate in recent_rates if rate >= 0.9))
        generous_threshold = max(0, self.n_players - 2)
        tau_local = self.tau
        if high_coop_count >= generous_threshold:
            tau_local = 0.55
        near_band = 0.05
        if abs(group_coop_rate - tau_local) <= near_band:
            if rounds_so_far >= 2:
                prev_cooperators = int(opp_actions[-2, :].sum())
                if last_round_cooperators > prev_cooperators:
                    return Action.C
                if last_round_cooperators < prev_cooperators:
                    return Action.D
            else:
                return Action.C
        for i in range(self.n_opponents):
            if self.punishment_timers[i] > 0:
                last_defected = False
                if rounds_so_far >= 1:
                    last_defected = int(opp_actions[-1, i]) == 0
                if recent_rates[i] < self.tau_ind or last_defected:
                    return Action.D
        if group_coop_rate >= tau_local:
            return Action.C
        defensive = False
        lookback = min(3, rounds_so_far)
        if lookback >= 2:
            bad_rounds = 0
            for r_idx in range(-lookback, 0, 1):
                round_coop_frac = float(opp_actions[r_idx, :].sum()) / float(self.n_opponents) if self.n_opponents > 0 else 1.0
                if round_coop_frac < tau_local:
                    bad_rounds += 1
            if bad_rounds >= 2:
                defensive = True
        elif group_coop_rate < tau_local:
            defensive = True
        if defensive:
            if t - self.last_probe_round >= self.probe_interval:
                self.last_probe_round = t
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    Collective-Graded Reciprocity (CGR)

    - Starts by cooperating, defects in final round.
    - Monitors recent cooperation rates over a memory window W.
    - Initiates targeted short punishments against persistent defectors (based on PR_j and recent defection).
    - When no targeted punishments are active, uses group-level graded short punishments
      depending on recent collapse severity (d_last).
    - Fast forgiveness: offenders that cooperate for G consecutive rounds are rehabilitated.
    - Deterministic, uses only public history and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(0, self.r - 1))

        def _clamp(x: float, lo: float, hi: float) -> float:
            if x < lo:
                return lo
            if x > hi:
                return hi
            return x
        denom = max(1, self.n - 1)
        high_base = 0.65 + 0.25 * (self.k - 1.0) / denom
        self.high_coop_threshold = _clamp(high_base, 0.5, 0.95)
        low_base = 0.45 + 0.2 * (self.k - 1.0) / denom
        self.low_coop_threshold = _clamp(low_base, 0.2, min(1.0, self.high_coop_threshold))
        self.G = 2
        self.Pmax = min(4, self.r)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def _clamp(x: float, lo: float, hi: float) -> float:
            if x < lo:
                return lo
            if x > hi:
                return hi
            return x
        round_idx = state.round_number
        if history is None or round_idx == 0:
            return Action.C
        if round_idx == max(0, self.r - 1):
            return Action.D
        t = history.my_actions.shape[0]
        if t != round_idx:
            t = round_idx
        last_w = min(self.W, t)
        if last_w == 0:
            return Action.C
        n_opponents = 0
        if hasattr(history, 'opponent_actions') and history.opponent_actions is not None:
            n_opponents = history.opponent_actions.shape[1] if history.opponent_actions.ndim == 2 else 0
        PR_list = []
        my_recent = history.my_actions[-last_w:] if last_w <= history.my_actions.shape[0] else history.my_actions
        if my_recent.size == 0:
            PR_self = 1.0
        else:
            PR_self = float(np.mean(my_recent.astype(float)))
        PR_list.append(PR_self)
        for j in range(self.n - 1):
            if n_opponents == 0 or j >= n_opponents:
                PR_list.append(1.0)
            else:
                opp_hist = history.opponent_actions
                opp_recent = opp_hist[-last_w:, j] if last_w <= opp_hist.shape[0] else opp_hist[:, j]
                if opp_recent.size == 0:
                    PR_j = 1.0
                else:
                    PR_j = float(np.mean(opp_recent.astype(float)))
                PR_list.append(PR_j)
        RC = float(np.mean(np.array(PR_list, dtype=float))) if len(PR_list) > 0 else 1.0
        punish_rem = [0] * (self.n - 1)
        any_targeted = False
        for j in range(self.n - 1):
            PR_j = PR_list[1 + j]
            if n_opponents == 0 or j >= n_opponents:
                defections_last_w = 0
            else:
                opp_recent_bool = history.opponent_actions[-last_w:, j] if last_w <= history.opponent_actions.shape[0] else history.opponent_actions[:, j]
                defections_last_w = int(np.sum(~opp_recent_bool))
            last_two = min(2, t)
            defected_in_last_two = False
            if n_opponents != 0 and j < n_opponents:
                opp_recent_last_two = history.opponent_actions[-last_two:, j] if last_two <= history.opponent_actions.shape[0] else history.opponent_actions[:, j]
                if opp_recent_last_two.size > 0 and np.any(~opp_recent_last_two):
                    defected_in_last_two = True
            rehabilitated = False
            if self.G <= t and n_opponents != 0 and (j < n_opponents):
                g_check = min(self.G, history.opponent_actions.shape[0])
                if g_check > 0:
                    last_g = history.opponent_actions[-g_check:, j]
                    if np.all(last_g):
                        rehabilitated = True
            elif n_opponents == 0 or j >= n_opponents:
                rehabilitated = True
            if PR_j <= 0.4 and defected_in_last_two and (not rehabilitated):
                rem = 1 + defections_last_w
                rem = int(min(self.Pmax, rem))
                if rem > 0:
                    punish_rem[j] = rem
                    any_targeted = True
            else:
                punish_rem[j] = 0
        if any((p > 0 for p in punish_rem)):
            return Action.D
        if RC >= self.high_coop_threshold:
            return Action.C
        if RC <= self.low_coop_threshold:
            return Action.D
        d_last = 0
        if t >= 1:
            if n_opponents > 0 and history.opponent_actions.shape[0] >= 1:
                opp_last = history.opponent_actions[-1, :min(n_opponents, history.opponent_actions.shape[1])]
                d_opp = int(np.sum(~opp_last))
            else:
                d_opp = 0
            my_last = False
            if history.my_actions.size >= 1:
                my_last = history.my_actions[-1]
            d_self = 0 if my_last else 1
            d_last = d_opp + d_self
        else:
            d_last = 0
        if d_last == 0:
            return Action.C
        if d_last == 1:
            return Action.C
        cutoff = math.ceil(0.25 * self.n)
        if 2 <= d_last <= cutoff:
            return Action.D
        if d_last > cutoff:
            P = min(self.Pmax, 1 + math.floor(d_last / 2))
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Punishment and Forgiveness (C3-GPF).

    - Starts cooperatively.
    - Keeps cooperating while the recent fraction of cooperators >= target.
    - If recent cooperation falls below target, initiates a short, proportional group punishment
      (withholds contributions for a small number of rounds).
    - Forgives early if the group restores cooperation for forgiveness_required consecutive rounds.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(3, self.r)
        raw_target = 0.5 + 0.2 * (self.k - 1.0) / max(1, self.n - 1)
        if raw_target < 0.5:
            self.target = 0.5
        elif raw_target > 0.9:
            self.target = 0.9
        else:
            self.target = raw_target
        self.P_max = max(1, math.ceil(self.r / 6))
        self.forgiveness_required = 2
        self.endgame_defect_round = self.r
        self.endgame_soften = math.ceil(self.r * 0.1)
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        idx = int(state.round_number)
        r = self.r
        n = self.n
        if idx == r - 1:
            return Action.D
        completed_rounds = idx
        if completed_rounds == 0 or history is None:
            recent_rate = 1.0
        else:
            window = min(self.m, completed_rounds)
            start = completed_rounds - window
            fractions = []
            for ri in range(start, completed_rounds):
                opp_sum = int(np.sum(history.opponent_actions[ri, :])) if history.opponent_actions.size > 0 else 0
                my_coop = 1 if bool(history.my_actions[ri]) else 0
                total_coop = opp_sum + my_coop
                fractions.append(total_coop / float(n))
            recent_rate = float(np.mean(np.array(fractions))) if len(fractions) > 0 else 1.0

        def forgiveness_condition_met() -> bool:
            if completed_rounds < self.forgiveness_required:
                return False
            start_idx = completed_rounds - self.forgiveness_required
            for ri in range(start_idx, completed_rounds):
                opp_sum = int(np.sum(history.opponent_actions[ri, :])) if history.opponent_actions.size > 0 else 0
                my_coop = 1 if bool(history.my_actions[ri]) else 0
                total_coop = opp_sum + my_coop
                frac = total_coop / float(n)
                if frac < self.target:
                    return False
            return True
        if self.punish_counter > 0:
            if history is not None and forgiveness_condition_met():
                self.punish_counter = 0
            else:
                action = Action.D
                self.punish_counter = max(0, self.punish_counter - 1)
                return action
        if recent_rate >= self.target:
            return Action.C
        deficit = max(0.0, self.target - recent_rate)
        planned_length = min(self.P_max, max(1, math.ceil(deficit * r)))
        t = idx + 1
        if t > r - self.endgame_soften:
            planned_length = min(planned_length, 1)
        remaining_after_now = max(0, planned_length - 1)
        self.punish_counter = remaining_after_now
        return Action.D

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Threshold-Grace Reciprocity (TGR)

    - Starts by cooperating.
    - Uses a majority threshold T = max(2, ceil(n/2)) to detect a cooperative norm.
    - Maintains an offense_streak (consecutive rounds with cooperators < T) reconstructed
      from observed aggregate cooperation counts.
    - When offense_streak == 0, cooperates; when > 0, punishes by defecting (with small
      epsilon probes to test restoration).
    - When observing a recovery round (total cooperators >= T while currently punished),
      schedules one probation cooperation round. If the probation round itself meets the
      threshold, offense_streak resets to 0 and cooperation resumes.
    - Always defects in the final round.
    - Avoids initiating a punishment if there is not enough remaining horizon to enforce it:
      if starting a punishment now would leave <= 1 rounds to enforce it (remaining_rounds <= 2),
      prefer to defect rather than start such a punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T = max(2, math.ceil(self.n / 2))
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        remaining_rounds = self.r - t
        if remaining_rounds <= 1:
            return Action.D
        last_index = t - 1
        try:
            opps_last = history.opponent_actions[last_index, :]
        except Exception:
            opps_last = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=bool)
        my_last = bool(history.my_actions[last_index]) if history.my_actions.size else False
        last_cooperators = int(np.sum(opps_last) + (1 if my_last else 0))
        offense_streak = 0
        scheduled_probation = False
        n_rounds_observed = history.my_actions.shape[0]
        for rr in range(n_rounds_observed):
            opps_rr = history.opponent_actions[rr, :] if history.opponent_actions.size else np.array([], dtype=bool)
            my_rr = bool(history.my_actions[rr]) if history.my_actions.size else False
            total_rr = int(np.sum(opps_rr) + (1 if my_rr else 0))
            if scheduled_probation:
                if total_rr >= self.T:
                    offense_streak = 0
                else:
                    offense_streak += 1
                scheduled_probation = False
            elif total_rr >= self.T:
                if offense_streak > 0:
                    scheduled_probation = True
                offense_streak = max(0, offense_streak - 1)
            else:
                offense_streak += 1
        if scheduled_probation and remaining_rounds > 1:
            return Action.C
        if offense_streak == 0:
            if last_cooperators < self.T and remaining_rounds <= 2:
                return Action.D
            return Action.C
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    Reciprocal Collective with Graded Forgiveness (RC-GF)

    - Starts cooperative.
    - Uses a recent window of rounds to judge behavior (L = min(5, r)).
    - Maintains a punish counter (number of future rounds to punish).
    - Applies graded punishment for isolated, multiple, or sustained defections.
    - Forgives after limited punishment if cooperation rebounds.
    - Treats persistent defectors specially by excluding them when deciding whether to keep cooperating.
    - Uses endgame vigilance in final E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.L = min(5, self.r)
        self.S = 1
        self.Pmax = min(3, self.r)
        self.M = 2
        self.E = min(2, self.r)
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number + 1
        n = self.n
        r = self.r
        start_idx = max(0, state.round_number - self.L)
        end_idx = state.round_number - 1
        num_recent = end_idx - start_idx + 1
        if num_recent <= 0:
            return Action.C
        recent_my = history.my_actions[start_idx:end_idx + 1]
        recent_opp = history.opponent_actions[start_idx:end_idx + 1, :]
        my_sum = int(np.sum(recent_my))
        opp_sum = int(np.sum(recent_opp))
        denom_total = n * num_recent
        group_coop_rate_recent = (my_sum + opp_sum) / denom_total if denom_total > 0 else 0.0
        last_idx = state.round_number - 1
        last_my = bool(history.my_actions[last_idx])
        last_opp_row = history.opponent_actions[last_idx, :] if history.opponent_actions.shape[0] > last_idx else np.zeros(0, dtype=bool)
        last_cooperators = (1 if last_my else 0) + int(np.sum(last_opp_row))
        last_round_defectors = n - last_cooperators
        personal_rates = []
        personal_rates.append(float(np.mean(recent_my)) if num_recent > 0 else 1.0)
        if recent_opp.size == 0:
            pass
        else:
            cols = recent_opp.shape[1]
            for c in range(cols):
                personal_rates.append(float(np.mean(recent_opp[:, c])) if num_recent > 0 else 1.0)
        persistent_mask = [rate <= 0.2 for rate in personal_rates]
        num_persistent = sum((1 for x in persistent_mask if x))

        def trigger_punishment(total_rounds: int):
            T = max(0, int(total_rounds))
            self.punish_counter = max(0, T - 1)
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if t > r - self.E:
            if last_cooperators == n:
                return Action.C
            else:
                return Action.D
        if num_persistent > 0:
            non_persistent_count = n - num_persistent
            if non_persistent_count <= 0:
                return Action.D
            coop_excluding = 0
            if not persistent_mask[0]:
                coop_excluding += int(np.sum(recent_my))
            for opp_idx, is_persistent in enumerate(persistent_mask[1:]):
                if not is_persistent:
                    coop_excluding += int(np.sum(recent_opp[:, opp_idx]))
            denom_excl = non_persistent_count * num_recent
            coop_rate_excluding_persistent = coop_excluding / denom_excl if denom_excl > 0 else 0.0
            if coop_rate_excluding_persistent >= 0.75:
                return Action.C
            else:
                return Action.D
        if group_coop_rate_recent >= 0.9:
            return Action.C
        if 0.6 <= group_coop_rate_recent < 0.9:
            if last_round_defectors == 0:
                return Action.C
            elif last_round_defectors == 1:
                if random.random() < 0.01:
                    return Action.C
                return trigger_punishment(self.S)
            else:
                total_puns = min(self.Pmax, int(self.M * last_round_defectors))
                total_puns = max(1, total_puns)
                return trigger_punishment(total_puns)
        if group_coop_rate_recent < 0.6:
            severity = math.ceil((1.0 - group_coop_rate_recent) * n)
            total_puns = min(self.Pmax, int(self.M * severity))
            total_puns = max(1, total_puns)
            return trigger_punishment(total_puns)
        return Action.C

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    Proportional Forgiving Trigger (PFT) strategy for the N-player Public Goods Game.

    - Start by cooperating (round 0).
    - In the final round always defect.
    - If currently in a punishment phase (punishment_timer > 0), defect and decrement the timer.
    - Otherwise, look at last round's total cooperators (including yourself).
      - If cooperators >= majority_threshold (ceil(n/2)), cooperate.
      - Else enter a proportionate punishment: set punishment_timer = min(remaining_rounds_after_this, defect_count_last_round)
        and defect this round. The timer counts remaining punishment rounds after the current one.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.R = int(game_description.n_rounds)
        self.majority_threshold = math.ceil(self.n / 2)
        self.punishment_timer: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        s = int(state.round_number)
        if s == self.R - 1:
            return Action.D
        if s == 0 or history is None:
            return Action.C
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :]
        coop_count_last = int(my_last) + int(np.count_nonzero(opp_last))
        if coop_count_last >= self.majority_threshold:
            return Action.C
        defect_count = self.n - coop_count_last
        remaining_after_current = max(0, self.R - (s + 1))
        self.punishment_timer = min(remaining_after_current, int(defect_count))
        return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    Opportunistic Conditional Cooperator (OCC)

    - Seed cooperation in round 0.
    - Exploit when others are reliably cooperating (everyone else cooperated last round and recent cooperation is very high).
    - Defect when the environment looks hopeless (low recent cooperation) or when most others defect.
    - If my exploitation appears to trigger punishment (group cooperation drops by >= 0.15 after my defection),
      immediately repair by cooperating for two non-endgame rounds.
    - Always defect in the last E rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.E = max(1, math.ceil(self.r / 4))
        self.M = math.ceil((self.n - 1) / 2)
        self.T_low = 0.3
        self.T_high = 0.9
        self.retaliated = False
        self.g_before = None
        self.repair_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        cur = int(state.round_number)
        if cur >= self.r - self.E:
            return Action.D
        if cur == 0 or history is None:
            return Action.C
        window_end = cur
        window_start = max(0, cur - self.W)
        try:
            opp_window = history.opponent_actions[window_start:window_end, :]
        except Exception:
            opp_window = np.zeros((0, self.n - 1), dtype=bool)
        if opp_window.size == 0:
            G_recent = 0.0
        else:
            G_recent = float(np.mean(opp_window.astype(float)))
        try:
            last_round_others = int(np.sum(history.opponent_actions[cur - 1, :]))
        except Exception:
            if history.opponent_actions.shape[0] > 0:
                last_round_others = int(np.sum(history.opponent_actions[-1, :]))
            else:
                last_round_others = 0
        try:
            last_action_self = bool(history.my_actions[cur - 1])
        except Exception:
            last_action_self = False
        if not last_action_self and self.retaliated and (self.g_before is not None):
            drop = self.g_before - G_recent
            if drop >= 0.15:
                if self.repair_counter < 2:
                    self.repair_counter = 2
                self.g_before = None
            else:
                self.g_before = None
        if self.repair_counter > 0:
            self.repair_counter -= 1
            if self.repair_counter == 0:
                self.retaliated = False
            return Action.C
        if G_recent <= self.T_low:
            return Action.D
        if last_round_others == self.n - 1 and G_recent >= self.T_high:
            if last_action_self:
                self.retaliated = True
                self.g_before = G_recent
            return Action.D
        if last_round_others >= self.M:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    Adaptive Proportional Reciprocity with Forgiveness (APRF)

    - Starts cooperatively (except in trivial 1-round games).
    - Uses short (w) and long (W) windows to compute recent and trend cooperation rates among others.
    - If recent cooperation falls below a target T, punish proportionally for a short number of rounds,
      then probe with a single cooperative TEST round. If the TEST succeeds (recent cooperation >= T),
      return to COOP; otherwise escalate punish length (doubling) up to a cap (P_max).
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        self.W = min(15, self.r)
        if self.k / max(1.0, self.n) >= 0.7:
            self.T = 0.5
        else:
            self.T = 0.6
        self.P_max = max(1, min(max(0, self.r - 1), 1 + math.floor(self.W / 3)))
        self.mode = 'COOP'
        self.punish_remaining = 0
        self.last_punish_length = 0
        self.test_awaiting_eval = False
        self.test_round_index = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        w = self.w
        W = self.W
        T = self.T
        P_max = self.P_max

        def compute_rate(window_size: int) -> float:
            if history is None or history.opponent_actions.size == 0:
                return 0.0
            hist = history.opponent_actions
            available_rounds = hist.shape[0]
            if available_rounds == 0:
                return 0.0
            use = min(window_size, available_rounds)
            recent_slice = hist[-use:, :]
            coop_counts = np.sum(recent_slice.astype(np.int64), axis=1)
            per_round_fractions = coop_counts / max(1, n - 1)
            return float(np.mean(per_round_fractions))
        if state.round_number == 0:
            if r == 1:
                return Action.D
            return Action.C
        if self.test_awaiting_eval and self.test_round_index is not None and (state.round_number > self.test_round_index):
            recent_rate = compute_rate(w)
            if recent_rate >= T:
                self.mode = 'COOP'
                self.test_awaiting_eval = False
                self.test_round_index = None
            else:
                escalated = max(1, int(self.last_punish_length * 2))
                self.last_punish_length = min(P_max, escalated)
                self.punish_remaining = self.last_punish_length
                self.mode = 'PUNISH'
                self.test_awaiting_eval = False
                self.test_round_index = None
        if state.round_number == r - 1:
            return Action.D
        if self.mode == 'PUNISH':
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
                if self.punish_remaining == 0:
                    self.mode = 'TEST'
                return Action.D
            else:
                self.mode = 'TEST'
        if self.mode == 'TEST':
            if not self.test_awaiting_eval:
                self.test_awaiting_eval = True
                self.test_round_index = state.round_number
                return Action.C
            else:
                return Action.C
        if self.mode == 'COOP':
            recent_rate = compute_rate(w)
            trend_rate = compute_rate(W)
            if recent_rate >= T:
                return Action.C
            d = T - recent_rate
            if trend_rate >= 0.9 and d <= 0.1:
                return Action.C
            tentative_length = 1 + math.ceil(d * (n - 1))
            punish_length = min(P_max, tentative_length)
            self.last_punish_length = int(punish_length)
            remaining_after_immediate = max(0, self.last_punish_length - 1)
            self.punish_remaining = remaining_after_immediate
            self.mode = 'PUNISH' if self.punish_remaining > 0 else 'TEST'
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    Adaptive exploitative-public-goods strategy.

    - Probes briefly to identify naive cooperators and punishers.
    - Exploits nearly-constant cooperators when punishers are rare.
    - Switches to reciprocal cooperation when enough punishers and remaining rounds make collapse costly.
    - Uses short, targeted punishments with rapid forgiveness (punish timers).
    - Uses moving-window statistics to avoid overreacting to noise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.punish_timers = None
        self.appease_timer = 0
        self.ProbeRounds = max(0, min(3, self.r - 1))
        self.WindowCap = 10
        self.PunishDelta = 0.3
        self.NaiveCoopThresh = 0.95
        self.ForgiveLength = 2

    def _ensure_state(self):
        if self.punish_timers is None:
            opp = max(0, self.n - 1)
            self.punish_timers = [0] * opp

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        k = self.k
        if state.round_number == 0 or history is None or len(history.my_actions) == 0:
            if k / max(1.0, n) >= 1.0:
                return Action.D
            if self.ProbeRounds >= 1:
                return Action.C
            return Action.D
        self._ensure_state()
        opp_count = max(0, n - 1)
        roundsPlayed = int(len(history.my_actions))
        t = state.round_number + 1
        rem = max(0, r - t + 1)
        if t == r:
            for i in range(len(self.punish_timers)):
                if self.punish_timers[i] > 0:
                    self.punish_timers[i] = max(0, self.punish_timers[i] - 1)
            return Action.D
        if self.ProbeRounds >= 1 and t <= self.ProbeRounds:
            if k / max(1.0, n) >= 1.0:
                return Action.D
            return Action.C
        Window = min(self.WindowCap, roundsPlayed)
        start_idx = max(0, roundsPlayed - Window)
        if Window <= 0:
            coopRate = np.zeros((opp_count,), dtype=float)
            coopAfterMyC = np.zeros((opp_count,), dtype=float)
            coopAfterMyD = np.zeros((opp_count,), dtype=float)
        else:
            opp_slice = history.opponent_actions[start_idx:roundsPlayed, :]
            if opp_count > 0:
                coopRate = np.array(np.mean(opp_slice.astype(float), axis=0), dtype=float)
            else:
                coopRate = np.zeros((0,), dtype=float)
            if roundsPlayed - start_idx >= 2 and opp_count > 0:
                prev_my = history.my_actions[start_idx:roundsPlayed - 1]
                current_opp = history.opponent_actions[start_idx + 1:roundsPlayed, :]
                maskC = prev_my.astype(bool)
                maskD = ~maskC
                denomC = float(np.sum(maskC))
                denomD = float(np.sum(maskD))
                if denomC > 0.0:
                    coopAfterMyC = np.array(np.sum(current_opp[maskC, :].astype(float), axis=0) / denomC, dtype=float)
                else:
                    coopAfterMyC = np.zeros((opp_count,), dtype=float)
                if denomD > 0.0:
                    coopAfterMyD = np.array(np.sum(current_opp[maskD, :].astype(float), axis=0) / denomD, dtype=float)
                else:
                    coopAfterMyD = np.zeros((opp_count,), dtype=float)
            else:
                coopAfterMyC = np.zeros((opp_count,), dtype=float)
                coopAfterMyD = np.zeros((opp_count,), dtype=float)
        responsiveDrop = coopAfterMyC - coopAfterMyD
        isRetaliator = np.zeros_like(responsiveDrop, dtype=bool)
        isNaiveCooperator = np.zeros_like(responsiveDrop, dtype=bool)
        if opp_count > 0:
            isRetaliator = responsiveDrop >= self.PunishDelta
            isNaiveCooperator = (coopRate >= self.NaiveCoopThresh) & (responsiveDrop < self.PunishDelta / 2.0)
        groupCoopRate = float(np.mean(coopRate)) if opp_count > 0 else 0.0
        RetaliatorFraction = float(np.mean(isRetaliator.astype(float))) if opp_count > 0 else 0.0
        ImmediateGainIfIDefect = 1.0 - k / max(1.0, n)
        ApproxFutureLossIfCollapse = rem * max(0.0, k - 1.0)
        InstrumentalThreshold = RetaliatorFraction * ApproxFutureLossIfCollapse > ImmediateGainIfIDefect
        for i in range(len(self.punish_timers)):
            if self.punish_timers[i] > 0:
                self.punish_timers[i] = max(0, self.punish_timers[i] - 1)

        def start_punish(indices, punish_len):
            for idx in indices:
                if 0 <= idx < len(self.punish_timers):
                    self.punish_timers[idx] = max(self.punish_timers[idx], punish_len)
        naive_exists = bool(np.any(isNaiveCooperator)) if opp_count > 0 else False
        conservative_exploit_thresh = 1.0 / (2.0 * max(1, n))
        if naive_exists and RetaliatorFraction < conservative_exploit_thresh:
            if self.appease_timer > 0:
                action = Action.C if state.round_number % 2 == 0 else Action.D
                self.appease_timer = max(0, self.appease_timer - 1)
                return action
            new_retaliators = np.where(isRetaliator)[0] if opp_count > 0 else []
            if len(new_retaliators) > 0:
                self.appease_timer = max(1, min(rem, 3))
                return Action.C if state.round_number % 2 == 0 else Action.D
            return Action.D
        if InstrumentalThreshold:
            if any((timer > 0 for timer in self.punish_timers)):
                return Action.D
            last_round_index = roundsPlayed - 1
            opponents_last = history.opponent_actions[last_round_index, :] if opp_count > 0 else np.array([], dtype=bool)
            my_last = bool(history.my_actions[last_round_index])
            total_coop_last = int(my_last) + int(np.sum(opponents_last.astype(int))) if opp_count >= 0 else int(my_last)
            majority_cooperated_last = total_coop_last >= math.ceil(n / 2)
            if groupCoopRate > 0.5 or majority_cooperated_last:
                return Action.C
            defectors_last = []
            if opp_count > 0:
                for j in range(opp_count):
                    if not bool(opponents_last[j]):
                        defectors_last.append(j)
            if defectors_last:
                MaxPunish = min(3, max(0, r - t))
                PunishLength = min(self.ForgiveLength, MaxPunish) if MaxPunish > 0 else 0
                if PunishLength > 0:
                    start_punish(defectors_last, PunishLength)
                    return Action.D
                return Action.D
            return Action.C
        if any((timer > 0 for timer in self.punish_timers)):
            return Action.D
        if self.appease_timer > 0:
            self.appease_timer = max(0, self.appease_timer - 1)
            return Action.C if state.round_number % 2 == 0 else Action.D
        if opp_count > 0 and np.any(isRetaliator):
            self.appease_timer = max(1, min(rem, 3))
            return Action.C if state.round_number % 2 == 0 else Action.D
        return Action.D

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    Seeks to sustain high group payoffs by conditional, proportionate reciprocity:
    - Reward rounds with strong group cooperation by cooperating
    - Punish rounds with significant defection by withholding cooperation for a short, calibrated punishment phase
    - Forgive and return to cooperation if group behavior recovers
    - Use small randomized probes to detect changes
    - Be cautious in the final rounds to avoid endgame exploitation
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.epsilon: float = 0.03
        self.low_threshold: float = 0.35
        n = float(self.game_description.n_players)
        k = float(self.game_description.k)
        expr = 0.6 + 0.3 * (1.0 - k / n)
        self.high_threshold: float = max(0.6, min(expr, 0.9))
        self.w: int = min(6, self.game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        t = state.round_number + 1

        def _to_int(x):
            return int(x)
        if state.round_number == 0 or history is None:
            if n_rounds == 1:
                return Action.D
            if n_rounds == 2:
                return Action.C
            if n_rounds == 3:
                return Action.D
            return Action.C
        past_rounds = history.my_actions.shape[0]
        window_size = min(self.w, past_rounds)
        window_start = max(0, past_rounds - window_size)
        window_indices = range(window_start, past_rounds)

        def group_coop_fraction_for_round(idx: int) -> float:
            opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.shape[0] > idx else 0
            my_coop = _to_int(history.my_actions[idx])
            total_coops = opp_coops + my_coop
            return float(total_coops) / float(n_players)
        last_idx = past_rounds - 1
        p_last = group_coop_fraction_for_round(last_idx)
        if window_size > 0:
            fractions = [group_coop_fraction_for_round(i) for i in window_indices]
            p_window = float(np.mean(np.array(fractions, dtype=float)))
        else:
            p_window = p_last
        frac_defectors_last = max(0.0, 1.0 - p_last)
        num_opponents = n_players - 1
        personal_rates = np.zeros(num_opponents, dtype=float)
        if window_size > 0:
            slice_actions = history.opponent_actions[window_start:past_rounds, :]
            sums = np.sum(slice_actions, axis=0)
            personal_rates = np.array(sums, dtype=float) / float(window_size)
        else:
            personal_rates = np.zeros(num_opponents, dtype=float)
        num_persistent_defectors = int(np.sum(personal_rates <= 0.2))
        num_good_players = int(np.sum(personal_rates > 0.5))
        fraction_good = float(num_good_players) / float(max(1, num_opponents))
        if self.punishment_counter > 0 and p_window >= self.high_threshold:
            self.punishment_counter = 0
        if t == n_rounds:
            return Action.D
        if t == n_rounds - 1:
            threshold_for_second_last = max(0.95, self.high_threshold)
            if p_window >= threshold_for_second_last:
                if random.random() < self.epsilon:
                    return Action.D
                return Action.C
            else:
                return Action.D
        if self.punishment_counter > 0:
            if random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return action
        if p_window >= self.high_threshold:
            if random.random() < self.epsilon:
                return Action.D
            return Action.C
        if p_last >= self.low_threshold and p_last < self.high_threshold:
            coop_base = p_last + 0.05
            if p_last > p_window:
                coop_base = coop_base + 0.05
            coop_base = max(0.0, min(1.0, coop_base))
            if num_persistent_defectors >= 1:
                coop_prob = min(coop_base, fraction_good)
            else:
                coop_prob = coop_base
            if random.random() < self.epsilon:
                sampled = random.random() < coop_prob
                return Action.D if sampled else Action.C
            if random.random() < coop_prob:
                return Action.C
            else:
                return Action.D
        if p_last < self.low_threshold:
            if frac_defectors_last >= 0.3:
                proposed = 1 + round(4.0 * frac_defectors_last)
                remaining_rounds = n_rounds - t
                max_allowed = max(0, remaining_rounds)
                self.punishment_counter = min(max(1, int(proposed)), max_allowed)
            if random.random() < self.epsilon:
                if self.punishment_counter > 0:
                    self.punishment_counter = max(0, self.punishment_counter - 1)
                return Action.C
            if self.punishment_counter > 0:
                self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    Opportunistic Exploiter:
    - Probes by cooperating early to identify steady cooperators and punishers.
    - Exploits reliably cooperative opponents with probabilistic defections when retaliation risk is low.
    - Detects coordinated retaliation and enters a short repair mode (cooperate a few rounds).
    - Always defect in the final rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.prev_group_rate: float | None = None
        self.last_action: Action | None = None
        self.in_repair_mode: bool = False
        self.repair_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def clamp(x, a, b):
            return a if x < a else b if x > b else x
        idx = state.round_number
        t = idx + 1
        r = self.r
        n = self.n
        if idx == 0 or history is None:
            self.prev_group_rate = 0.0
            self.last_action = Action.C
            return Action.C
        w = min(10, max(3, math.floor(r / 10)))
        past_rounds = int(history.opponent_actions.shape[0]) if history.opponent_actions is not None else 0
        others = max(0, n - 1)
        last_k = min(w, past_rounds)
        coop_rates = [0.0] * others
        group_rate = 0.0
        if last_k > 0 and others > 0:
            recent = history.opponent_actions[-last_k:, :]
            col_sums = np.sum(recent.astype(np.float64), axis=0)
            coop_rates = [float(col_sums[j]) / last_k for j in range(others)]
            group_rate = float(np.mean(coop_rates)) if others > 0 else 0.0
        else:
            coop_rates = [0.0] * others
            group_rate = 0.0
        post_after_my_C_counts = [0] * others
        post_after_my_C_sums = [0.0] * others
        post_after_my_D_counts = [0] * others
        post_after_my_D_sums = [0.0] * others
        if past_rounds >= 2:
            start = max(0, past_rounds - w - 1)
            end = past_rounds - 2
            for r_idx in range(start, end + 1):
                my_act = bool(history.my_actions[r_idx])
                next_row = history.opponent_actions[r_idx + 1, :]
                for j in range(others):
                    val = bool(next_row[j])
                    if my_act:
                        post_after_my_C_counts[j] += 1
                        post_after_my_C_sums[j] += 1.0 if val else 0.0
                    else:
                        post_after_my_D_counts[j] += 1
                        post_after_my_D_sums[j] += 1.0 if val else 0.0
        retaliation_scores = [0.0] * others
        for j in range(others):
            c_cnt = post_after_my_C_counts[j]
            d_cnt = post_after_my_D_counts[j]
            postC = post_after_my_C_sums[j] / c_cnt if c_cnt > 0 else 0.0
            postD = post_after_my_D_sums[j] / d_cnt if d_cnt > 0 else 0.0
            retaliation_scores[j] = float(postC - postD)
        punisher_count = sum((1 for s in retaliation_scores if s >= 0.25))
        steady_cooperators = sum((1 for j in range(others) if coop_rates[j] >= 0.95 and retaliation_scores[j] < 0.1))
        detected_retaliation = False
        if self.last_action is not None and self.last_action == Action.D and (self.prev_group_rate is not None):
            if self.prev_group_rate - group_rate >= 0.25:
                detected_retaliation = True
                remaining_rounds = r - idx
                if remaining_rounds <= 1:
                    detected_retaliation = False
                else:
                    m = min(3, max(0, remaining_rounds - 1))
                    if m > 0:
                        self.in_repair_mode = True
                        self.repair_rounds_remaining = m
        if self.in_repair_mode and self.repair_rounds_remaining > 0:
            if r - idx <= 1:
                action = Action.D
                self.in_repair_mode = False
                self.repair_rounds_remaining = 0
            else:
                action = Action.C
                self.repair_rounds_remaining -= 1
                if self.repair_rounds_remaining <= 0:
                    self.in_repair_mode = False
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if t >= r - 2:
            action = Action.D
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if r <= 3 and idx >= 1:
            action = Action.D
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        remaining_rounds_for_window = r - idx
        short_game_flag = w >= remaining_rounds_for_window
        if short_game_flag and idx >= 1:
            if group_rate >= 0.7 and punisher_count == 0:
                p_exploit = clamp(0.4 + 0.8 * (group_rate - 0.7) / 0.3, 0.4, 0.9)
                if random.random() < p_exploit:
                    action = Action.D
                else:
                    action = Action.C
            else:
                action = Action.D
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if group_rate <= 0.2:
            action = Action.D
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if punisher_count >= math.ceil((n - 1) / 2):
            action = Action.C
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if steady_cooperators >= 1 and punisher_count == 0:
            p = min(0.7, 0.25 + 0.5 * group_rate)
            if random.random() < p:
                action = Action.D
            else:
                action = Action.C
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if group_rate >= 0.7 and punisher_count == 0:
            p_exploit = clamp(0.4 + 0.8 * (group_rate - 0.7) / 0.3, 0.4, 0.9)
            if random.random() < p_exploit:
                action = Action.D
            else:
                action = Action.C
            self.prev_group_rate = group_rate
            self.last_action = action
            return action
        if 0.2 < group_rate < 0.7:
            small_punisher_threshold = math.ceil((n - 1) / 4)
            if punisher_count < small_punisher_threshold:
                p = 0.3 + 0.4 * (group_rate - 0.2) / 0.5
                if random.random() < p:
                    action = Action.D
                else:
                    action = Action.C
                self.prev_group_rate = group_rate
                self.last_action = action
                return action
            else:
                recent_majority_defected = False
                if past_rounds >= 1 and others > 0:
                    last_round = history.opponent_actions[-1, :]
                    num_coops = int(np.sum(last_round.astype(np.int64)))
                    frac_coops = num_coops / others if others > 0 else 0.0
                    if frac_coops < 0.5:
                        recent_majority_defected = True
                if recent_majority_defected:
                    action = Action.D
                else:
                    action = Action.C
                self.prev_group_rate = group_rate
                self.last_action = action
                return action
        action = Action.C
        self.prev_group_rate = group_rate
        self.last_action = action
        return action

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    Collective-Conditional with Limited Punishment and Forgiveness (CC-LPF)

    - Starts cooperating.
    - Uses a short memory window to estimate recent cooperation among others.
    - If a strong majority cooperates, cooperate.
    - If cooperation collapses, apply a short, proportionate punishment (defection) phase.
    - If cooperation is ambiguous, probe with 50/50 cooperate/defect to rebuild cooperation.
    - Always defect in the final round (last-round safety) and if the game has only one round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        self.w = min(4, max(0, r - 1))
        self.T = 0.6
        self.T_low = 0.3
        self.P_max = min(4, max(1, math.floor(r / 10) + 1))
        self.pun_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if r == 1:
            return Action.D
        if t == r - 1:
            return Action.D
        if t == 0:
            return Action.C
        if self.pun_remaining > 0:
            self.pun_remaining = max(0, self.pun_remaining - 1)
            return Action.D
        p_recent = 1.0
        if history is None:
            p_recent = 1.0
        else:
            rounds_available = int(history.opponent_actions.shape[0])
            consider = min(self.w, rounds_available)
            if consider <= 0:
                p_recent = 1.0
            else:
                slice_actions = history.opponent_actions[-consider:, :]
                if slice_actions.size == 0:
                    p_recent = 1.0
                else:
                    try:
                        per_round_frac = np.mean(slice_actions.astype(float), axis=1)
                        p_recent = float(np.mean(per_round_frac))
                    except Exception:
                        p_recent = 1.0
        if p_recent >= self.T:
            return Action.C
        if p_recent < self.T_low:
            proportion = (self.T - p_recent) / self.T if self.T > 0 else 1.0
            L = int(min(self.P_max, 1 + round(proportion * self.P_max)))
            L = max(1, L)
            self.pun_remaining = max(0, L - 1)
            return Action.D
        if random.random() < 0.5:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Collective Conditional-Cooperation with Forgiving Punishment (CCC-F)

    Starts cooperative, monitors recent group cooperation, issues proportional group-level
    punishments when recent cooperation falls below a medium threshold, and forgives when
    cooperation recovers. Becomes cautious in the final few rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        self.F = self.W
        self.P0 = min(2, self.r)
        self.L = min(3, max(1, math.floor(0.1 * self.r)))
        self.Th_high = 0.9
        self.Th_med = 0.6
        self.g = 0.8
        self.p_forgive = 0.35
        self.in_punishment = False
        self.punishment_remaining = 0
        self.last_planned_P = 0
        self.just_finished_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.r == 1:
            return Action.C
        if history is None:
            return Action.C
        rounds_completed = int(state.round_number)
        r_total = self.r
        r_remaining = max(0, r_total - rounds_completed)

        def recent_fraction_over(window: int) -> float:
            if rounds_completed <= 0:
                return 0.0
            my_arr = np.asarray(history.my_actions, dtype=np.int64)
            opp_arr = np.asarray(history.opponent_actions, dtype=np.int64)
            if opp_arr.size == 0:
                total_per_round = my_arr
            else:
                total_per_round = my_arr + np.sum(opp_arr, axis=1)
            fractions = total_per_round / float(self.n)
            m = min(window, rounds_completed)
            if m <= 0:
                return 0.0
            return float(np.mean(fractions[-m:]))

        def long_run_fraction() -> float:
            if rounds_completed <= 0:
                return 0.0
            my_arr = np.asarray(history.my_actions, dtype=np.int64)
            opp_arr = np.asarray(history.opponent_actions, dtype=np.int64)
            if opp_arr.size == 0:
                total_per_round = my_arr
            else:
                total_per_round = my_arr + np.sum(opp_arr, axis=1)
            fractions = total_per_round / float(self.n)
            return float(np.mean(fractions))
        if self.in_punishment and self.punishment_remaining > 0:
            action = Action.D
            self.punishment_remaining -= 1
            if self.punishment_remaining == 0:
                self.in_punishment = False
                self.just_finished_punishment = True
            return action
        if self.just_finished_punishment:
            recent_frac = recent_fraction_over(self.F)
            if recent_frac >= self.Th_med:
                self.just_finished_punishment = False
                if random.random() < self.p_forgive:
                    return Action.C
                else:
                    return Action.D
            else:
                Pprime = max(1, math.floor(self.last_planned_P / 2)) if self.last_planned_P > 0 else 1
                Pprime = min(max(1, r_remaining), Pprime)
                self.in_punishment = True
                self.punishment_remaining = int(Pprime)
                self.last_planned_P = int(Pprime)
                self.just_finished_punishment = False
                self.punishment_remaining -= 1
                if self.punishment_remaining == 0:
                    self.in_punishment = False
                    self.just_finished_punishment = True
                return Action.D
        if r_remaining <= self.L:
            lr_frac = long_run_fraction()
            if lr_frac >= self.Th_high:
                return Action.C
            if rounds_completed >= 1:
                my_last = bool(history.my_actions[-1])
                opp_last = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
                if my_last and bool(np.all(opp_last)):
                    return Action.C
            return Action.D
        recent_frac = recent_fraction_over(self.W)
        if recent_frac >= self.Th_high:
            return Action.C
        if recent_frac >= self.Th_med:
            if random.random() < self.g:
                return Action.C
            return Action.D
        deficit = max(0.0, self.Th_med - recent_frac)
        extra_units = math.ceil(deficit / 0.1) if deficit > 0 else 0
        P = int(min(max(1, r_remaining), self.P0 + extra_units))
        self.in_punishment = True
        self.punishment_remaining = int(P)
        self.last_planned_P = int(P)
        self.punishment_remaining -= 1
        if self.punishment_remaining == 0:
            self.in_punishment = False
            self.just_finished_punishment = True
        return Action.D

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Reciprocal Collective (finite repeated Public Goods Game).
    Starts by cooperating (except one-shot), sustains cooperation when others reciprocate,
    punishes proportionally for weak cooperation, forgives isolated lapses with small
    probability, detects and isolates persistent exploiters, and is conservative in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = min(5, max(1, math.floor(r / 5) + 2))
        self.Pmax = min(3, max(0, r - 1))
        self.F = 0.05
        self.TH_high_base = 0.8
        self.TH_low = 0.5
        self._punish_until = -1
        self._exploiter_idx = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if r == 1 and t == 0:
            return Action.D
        if t == 0 or history is None:
            return Action.C if r > 1 else Action.D
        endgame_conservative = t >= max(0, r - 2)
        th_high = 0.95 if endgame_conservative else self.TH_high_base
        total_past_rounds = history.my_actions.shape[0]
        last_m = min(self.W, total_past_rounds)
        my_recent = history.my_actions[-last_m:]
        opp_recent = history.opponent_actions[-last_m:, :]
        opp_total_coops = float(opp_recent.sum())
        avg = opp_total_coops / (last_m * (n - 1))
        last_round_unanimous = bool(history.my_actions[-1]) and bool(opp_recent[-1, :].all())
        per_round_total_coops = opp_recent.sum(axis=1) + my_recent.astype(int)
        rounds_all_coop = int((per_round_total_coops == n).sum())
        rounds_one_defector = int((per_round_total_coops == n - 1).sum())
        single_lapse_noise = last_m >= 1 and rounds_one_defector == 1 and (rounds_all_coop >= last_m - 1)
        exploiter_candidate = None
        if last_m >= 1:
            opp_def_counts = (~opp_recent).sum(axis=0).astype(int)
            for j in range(n - 1):
                def_count = int(opp_def_counts[j])
                repeated_threshold = max(2, math.ceil(last_m / 2))
                if def_count >= repeated_threshold:
                    others_idx = [idx for idx in range(n - 1) if idx != j]
                    if others_idx:
                        others_coop = float(opp_recent[:, others_idx].sum()) / (last_m * len(others_idx))
                    else:
                        others_coop = 1.0
                    if others_coop >= th_high:
                        exploiter_candidate = j
                        break
        if self._exploiter_idx is None and exploiter_candidate is not None:
            self._exploiter_idx = int(exploiter_candidate)
        if self._exploiter_idx is not None:
            exploiter_recent_coop = bool(history.opponent_actions[-1, self._exploiter_idx])
            if exploiter_recent_coop:
                self._exploiter_idx = None
            else:
                return Action.D
        if self._punish_until > t and avg >= self.TH_low:
            self._punish_until = -1
        if not self._punish_until > t and avg < self.TH_low:
            consecutive = 0
            for idx in range(total_past_rounds - 1, -1, -1):
                frac = float(history.opponent_actions[idx, :].sum()) / (n - 1)
                if frac < self.TH_low:
                    consecutive += 1
                else:
                    break
            P = min(self.Pmax, 1 + consecutive)
            self._punish_until = t + max(1, P)
            return Action.D
        if self._punish_until > t:
            return Action.D
        final_round_index = r - 1
        if t == final_round_index:
            lookback = min(3, max(0, r - 1))
            if lookback == 0:
                return Action.D
            if history.my_actions.shape[0] < 1:
                return Action.D
            recent_my = history.my_actions[-lookback:]
            recent_opp = history.opponent_actions[-lookback:, :]
            if bool(recent_my.all()) and bool(recent_opp.all()):
                if last_round_unanimous:
                    return Action.C
            return Action.D
        if single_lapse_noise:
            if random.random() < 1.0 - self.F:
                return Action.C
            else:
                return Action.D
        if avg >= th_high:
            return Action.C
        if self.TH_low <= avg < th_high:
            if random.random() < 0.8:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    Adaptive Exploiter for the N-player Public Goods Game.

    - Probes early to estimate opponents' cooperativeness and responsiveness.
    - Labels opponents as Naive (safe to exploit), Retaliator (punishes defections), or Mixed.
    - Softly exploits Naive opponents while avoiding/retreating from retaliation or group collapse.
    - Defects in the final E_end rounds (endgame unraveling).
    - Uses deterministic exploitation scheduling (no RNG) to be tournament-friendly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(3, self.r // 5))
        self.P_probe = min(5, max(3, self.r // 6))
        self.E_end = min(3, max(1, self.r // 10))
        self.R_recover = min(5, max(2, self.r // 10))
        self.HIGH_COOP = 0.8
        self.LOW_RESP = 0.1
        self.RETALIATE_DROP = 0.2
        self.SIGNIFICANT_GROUP_DROP = 0.15
        self.retaliators = set()
        self.checked_exploit_rounds = set()
        self.recovery_until_round = -1
        if self.P_probe >= 3:
            self.probe_defect_round = 2
        else:
            self.probe_defect_round = max(0, self.P_probe - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        m = max(0, self.n - 1)
        if t >= max(0, self.r - self.E_end):
            return Action.D
        if t == 0 or history is None:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_so_far = my_actions.shape[0]
        if rounds_so_far == 0:
            return Action.C
        for t0 in range(rounds_so_far):
            if t0 in self.checked_exploit_rounds:
                continue
            if not my_actions[t0]:
                after_start = t0 + 1
                after_end = t0 + 1 + self.W
                before_start = max(0, t0 - self.W)
                before_end = t0
                if after_end <= rounds_so_far:
                    before_slice = opp_actions[before_start:before_end, :] if before_end > before_start else np.empty((0, m), dtype=bool)
                    after_slice = opp_actions[after_start:after_end, :]
                    if before_slice.size == 0:
                        self.checked_exploit_rounds.add(t0)
                        continue
                    avg_before = float(np.mean(before_slice)) if before_slice.size > 0 else 0.0
                    avg_after = float(np.mean(after_slice)) if after_slice.size > 0 else 0.0
                    coop_at_defect = float(np.mean(opp_actions[t0, :])) if m > 0 else 0.0
                    if coop_at_defect >= 0.5:
                        drop = avg_before - avg_after
                        if drop >= self.SIGNIFICANT_GROUP_DROP:
                            self.recovery_until_round = t + self.R_recover
                    self.checked_exploit_rounds.add(t0)
                else:
                    continue
        if t < self.recovery_until_round:
            last_coops = int(np.sum(opp_actions[-1, :])) if rounds_so_far >= 1 else 0
            coop_threshold = math.ceil(m / 2)
            return Action.C if last_coops >= coop_threshold else Action.D
        if t < self.P_probe:
            if t <= 1:
                return Action.C
            if t == self.probe_defect_round:
                return Action.D
            return Action.C
        window_len = min(self.W, rounds_so_far)
        start_idx = max(0, rounds_so_far - window_len)
        recent_opp = opp_actions[start_idx:rounds_so_far, :]
        if recent_opp.size == 0:
            coop_rate_recent = np.zeros((m,), dtype=float)
        else:
            coop_rate_recent = np.array(np.mean(recent_opp, axis=0), dtype=float)
        indices_with_follow = [i for i in range(rounds_so_far - 1)]
        if not indices_with_follow:
            prob_after_def = np.zeros((m,), dtype=float)
            prob_after_coop = np.zeros((m,), dtype=float)
            count_after_def = np.zeros((m,), dtype=int)
            count_after_coop = np.zeros((m,), dtype=int)
        else:
            my_actions_bool = np.asarray(my_actions, dtype=bool)
            my_actions_truncated = my_actions_bool[0:rounds_so_far - 1]
            opp_next = opp_actions[1:rounds_so_far, :]
            coop_given_my_def_mask = ~my_actions_truncated
            coop_given_my_coop_mask = my_actions_truncated
            if coop_given_my_def_mask.any():
                after_def_counts = int(np.sum(coop_given_my_def_mask))
                sums_after_def = np.sum(opp_next[coop_given_my_def_mask, :], axis=0)
                prob_after_def = np.array(sums_after_def / max(1, after_def_counts), dtype=float)
                count_after_def = np.array([after_def_counts] * m, dtype=int)
            else:
                prob_after_def = np.zeros((m,), dtype=float)
                count_after_def = np.zeros((m,), dtype=int)
            if coop_given_my_coop_mask.any():
                after_coop_counts = int(np.sum(coop_given_my_coop_mask))
                sums_after_coop = np.sum(opp_next[coop_given_my_coop_mask, :], axis=0)
                prob_after_coop = np.array(sums_after_coop / max(1, after_coop_counts), dtype=float)
                count_after_coop = np.array([after_coop_counts] * m, dtype=int)
            else:
                prob_after_coop = np.zeros((m,), dtype=float)
                count_after_coop = np.zeros((m,), dtype=int)
        responsiveness_samples = count_after_def + count_after_coop
        responsiveness = prob_after_coop - prob_after_def
        naive_flags = np.zeros((m,), dtype=bool)
        retaliator_flags = np.zeros((m,), dtype=bool)
        mixed_flags = np.zeros((m,), dtype=bool)
        total_coop_counts = np.sum(opp_actions, axis=0) if opp_actions.size > 0 else np.zeros((m,), dtype=int)
        for j in range(m):
            samples = int(responsiveness_samples[j]) if j < len(responsiveness_samples) else 0
            if samples < 3:
                mixed_flags[j] = True
                continue
            if responsiveness[j] > self.RETALIATE_DROP:
                retaliator_flags[j] = True
                self.retaliators.add(j)
                continue
            if j in self.retaliators:
                retaliator_flags[j] = True
                continue
            if coop_rate_recent[j] > self.HIGH_COOP and responsiveness[j] < self.LOW_RESP and (int(total_coop_counts[j]) >= 3):
                naive_flags[j] = True
                continue
            mixed_flags[j] = True
        for j in self.retaliators:
            if 0 <= j < m:
                retaliator_flags[j] = True
                naive_flags[j] = False
                mixed_flags[j] = False
        num_naive = int(np.sum(naive_flags))
        num_retaliators = int(np.sum(retaliator_flags))
        frac_naive = num_naive / m if m > 0 else 0.0
        frac_retaliators = num_retaliators / m if m > 0 else 0.0
        avg_coop_last = float(np.mean(opp_actions[-1, :])) if opp_actions.size > 0 else 0.0
        last_cooperators = int(np.sum(opp_actions[-1, :])) if opp_actions.size > 0 else 0
        if frac_retaliators > m / 2.0:
            coop_threshold = math.ceil(m / 2)
            return Action.C if last_cooperators >= coop_threshold else Action.D
        if num_naive >= 1 and frac_retaliators <= frac_naive:
            p_exploit = 0.2 + 0.6 * frac_naive
            if p_exploit < 0.2:
                p_exploit = 0.2
            if p_exploit > 0.9:
                p_exploit = 0.9
            k = max(2, math.floor(1.0 / p_exploit)) if p_exploit > 0 else max(2, 1)
            eligible_idx = max(0, t - self.P_probe)
            if eligible_idx % k == 0:
                return Action.D
            else:
                return Action.C
        coop_threshold = math.ceil(m / 2)
        if last_cooperators >= coop_threshold:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    Collective Conditional-Cooperator with Forgiving, Proportional Punishment.

    - Starts by cooperating (unless single-shot game).
    - Uses a sliding window W to estimate recent group cooperation.
    - Punishes proportionally (length scales with number of recent defectors),
      but is forgiving for likely one-off defections by otherwise-cooperative players.
    - If a majority of opponents are persistent defectors, stops trying and defects.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        self.W = max(3, min(10, math.ceil(r / 10))) if r > 0 else 3
        self.P_max = max(1, min(5, math.ceil(r / 10))) if r > 0 else 1
        denom = max(1, n - 1)
        self.C_thresh = 0.5 + 0.2 * ((k - 1.0) / denom)
        self.D_thresh = 0.2
        self.PunishRemaining = 0
        self.PunishTriggerers = set()
        self._abandon = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if r <= 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        if history is None:
            return Action.D
        past_rounds = int(history.opponent_actions.shape[0])
        total_rounds = max(1, past_rounds)
        if history.opponent_actions.size == 0:
            opp_coop_counts = np.zeros((n - 1,), dtype=float)
        else:
            opp_coop_counts = np.sum(history.opponent_actions.astype(float), axis=0)
        total_coop_rate = opp_coop_counts / float(total_rounds)
        m = min(self.W, past_rounds)
        if m <= 0:
            recent_group_coop_rate = 0.0
        else:
            start = past_rounds - m
            opp_slice = history.opponent_actions[start:past_rounds, :]
            my_slice = history.my_actions[start:past_rounds]
            opp_per_round = np.sum(opp_slice.astype(float), axis=1) if opp_slice.size else np.zeros((m,))
            my_per_round = my_slice.astype(float)
            total_coops_per_round = opp_per_round + my_per_round
            per_round_prop = total_coops_per_round / float(n)
            recent_group_coop_rate = float(np.mean(per_round_prop))
        prev_idx = past_rounds - 1
        recent_defectors = set()
        if prev_idx >= 0:
            if history.opponent_actions.size:
                prev_opp_actions = history.opponent_actions[prev_idx, :]
                for j in range(prev_opp_actions.shape[0]):
                    if not bool(prev_opp_actions[j]):
                        recent_defectors.add(j)
        persistent_defectors = set((j for j, rate in enumerate(total_coop_rate) if float(rate) <= self.D_thresh))
        if self.PunishRemaining > 0:
            self.PunishRemaining -= 1
            if self.PunishRemaining <= 0:
                self.PunishRemaining = 0
                self.PunishTriggerers.clear()
            return Action.D
        if len(persistent_defectors) > (n - 1) / 2:
            self._abandon = True
        if self._abandon:
            return Action.D
        everyone_cooperated_last_round = False
        if prev_idx >= 0:
            last_my = bool(history.my_actions[prev_idx])
            last_opp_all = True
            if history.opponent_actions.size:
                last_opp_all = bool(np.all(history.opponent_actions[prev_idx, :]))
            everyone_cooperated_last_round = last_my and last_opp_all
        if recent_group_coop_rate >= self.C_thresh or everyone_cooperated_last_round:
            return Action.C
        d = len(recent_defectors)
        if d == 0:
            d = 1
        proposed_length = 1 + d
        punish_len = min(self.P_max, proposed_length)
        if len(recent_defectors) == 1:
            j = next(iter(recent_defectors))
            if float(total_coop_rate[j]) > 0.6:
                punish_len = 1
        self.PunishRemaining = int(max(1, punish_len))
        for j in recent_defectors:
            self.PunishTriggerers.add(j)
        return Action.D

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    Probe-and-Exploit with Conditional Maintenance (PECM)

    Exploitative public-goods strategy that:
    - probes early to classify opponents (unconditional cooperators, conditional cooperators, defectors, mixed)
    - exploits unconditional cooperators by defecting when safe
    - tries to sustain cooperation when a conditional-cooperator majority exists, cooperating most rounds but
      occasionally defecting to extract surplus and testing for punishment
    - reprobes occasionally to detect changes
    - punishes and forgives quickly (short vendettas) to maintain long-run extraction opportunities
    - always defects in the final round (and also in the last 2 rounds for short games)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.base_p_probe = 0.3
        self.TH_high = 0.75
        self.TH_low = 0.25
        self.TH_resp = 0.15
        self.p_reprobe = 0.05
        self.F = 2
        self.unpunished_probes = 0.0
        self.punished_probes = 0.0
        self.max_defect_rate_when_extracting = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n_players
        r = self.n_rounds
        t_idx = state.round_number
        if t_idx == r - 1:
            return Action.D
        if r <= 10 and t_idx >= r - 2:
            return Action.D
        if t_idx == 0 or history is None:
            if n <= 3:
                p_probe = max(0.05, self.base_p_probe * 0.5)
            elif n == 4:
                p_probe = max(0.1, self.base_p_probe * 0.75)
            else:
                p_probe = self.base_p_probe
            return Action.C if random.random() < p_probe else Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        rounds_so_far = opp_actions.shape[0]
        if r < 5:
            W = r
        else:
            W = min(10, r)
        window_start = max(0, rounds_so_far - W)
        window_slice = slice(window_start, rounds_so_far)
        if rounds_so_far - window_start <= 0:
            q_j = np.zeros(n - 1, dtype=float)
        else:
            q_j = np.mean(opp_actions[window_slice, :].astype(float), axis=0)
        defenders_after_defect = [[] for _ in range(n - 1)]
        defenders_after_coop = [[] for _ in range(n - 1)]
        for t in range(rounds_so_far - 1):
            my_action_t = bool(my_actions[t])
            next_actions = opp_actions[t + 1, :]
            if my_action_t:
                for j in range(n - 1):
                    defenders_after_coop[j].append(1.0 if next_actions[j] else 0.0)
            else:
                for j in range(n - 1):
                    defenders_after_defect[j].append(1.0 if next_actions[j] else 0.0)
        resp_j = np.zeros(n - 1, dtype=float)
        for j in range(n - 1):
            mean_after_def = float(np.mean(defenders_after_defect[j])) if len(defenders_after_defect[j]) > 0 else 0.0
            mean_after_coop = float(np.mean(defenders_after_coop[j])) if len(defenders_after_coop[j]) > 0 else 0.0
            resp_j[j] = mean_after_def - mean_after_coop
        n_UC = 0
        n_CC = 0
        n_D = 0
        mixed_indices = []
        for j in range(n - 1):
            q = q_j[j]
            rj = resp_j[j]
            if q >= self.TH_high and abs(rj) < self.TH_resp * 0.5:
                n_UC += 1
            elif q <= self.TH_low:
                n_D += 1
            elif q >= self.TH_low and rj <= -self.TH_resp:
                n_CC += 1
            else:
                mixed_indices.append(j)
        est_other_coop = float(np.sum(q_j))
        last_round_other_coop = int(np.sum(opp_actions[-1, :])) if rounds_so_far > 0 else 0
        coop_broad_threshold = int(math.ceil((n - 1) * 0.6))
        action = Action.D
        if n_UC >= 1 and n_CC + n_UC < n / 2.0:
            action = Action.D
        elif n_CC >= (n - 1) / 2.0 or est_other_coop >= (n - 1) * 0.6:
            if n_CC > 0:
                cc_indices = [j for j in range(n - 1) if q_j[j] >= self.TH_low and resp_j[j] <= -self.TH_resp]
                avg_sensitivity = float(np.mean(np.abs(resp_j[cc_indices]))) if len(cc_indices) > 0 else 0.0
            else:
                avg_sensitivity = float(np.mean(np.abs(resp_j))) if n - 1 > 0 else 0.0
            if avg_sensitivity < 0.0:
                avg_sensitivity = 0.0
            if avg_sensitivity > 1.0:
                avg_sensitivity = 1.0
            epsilon_exploit = max(0.05, 0.1 * (1.0 - avg_sensitivity))
            adapt_term = 0.02 * (self.unpunished_probes - self.punished_probes)
            defect_prob = epsilon_exploit + adapt_term
            if defect_prob < 0.0:
                defect_prob = 0.0
            if defect_prob > self.max_defect_rate_when_extracting:
                defect_prob = self.max_defect_rate_when_extracting
            cooperate_prob = 1.0 - defect_prob
            if last_round_other_coop >= coop_broad_threshold:
                action = Action.C if random.random() < cooperate_prob else Action.D
            else:
                F_check = min(self.F, rounds_so_far)
                if F_check <= 0:
                    action = Action.D
                else:
                    recent_block = opp_actions[-F_check:, :]
                    rounds_broad = [int(np.sum(recent_block[i, :]) >= coop_broad_threshold) for i in range(recent_block.shape[0])]
                    if all(rounds_broad):
                        action = Action.C if random.random() < cooperate_prob else Action.D
                    else:
                        action = Action.D
        else:
            action = Action.D
        if random.random() < self.p_reprobe:
            action = Action.C if action == Action.D else Action.D
        group_after_def = []
        group_after_coop = []
        t_start = max(0, rounds_so_far - W)
        for tt in range(t_start, rounds_so_far - 1):
            if not bool(my_actions[tt]):
                group_after_def.append(int(np.sum(opp_actions[tt + 1, :])))
            else:
                group_after_coop.append(int(np.sum(opp_actions[tt + 1, :])))
        mean_group_after_def = float(np.mean(group_after_def)) if len(group_after_def) > 0 else None
        mean_group_after_coop = float(np.mean(group_after_coop)) if len(group_after_coop) > 0 else None
        if mean_group_after_def is not None and mean_group_after_coop is not None:
            diff = mean_group_after_def - mean_group_after_coop
            frac_diff = diff / max(1.0, n - 1)
            if frac_diff <= -self.TH_resp:
                self.punished_probes += 1.0
                self.unpunished_probes = max(0.0, self.unpunished_probes - 0.5)
            else:
                self.unpunished_probes += 1.0
                self.punished_probes = max(0.0, self.punished_probes - 0.2)
        else:
            self.unpunished_probes = max(0.0, self.unpunished_probes - 0.01)
            self.punished_probes = max(0.0, self.punished_probes - 0.01)
        if self.unpunished_probes > 50.0:
            self.unpunished_probes = 50.0
        if self.punished_probes > 50.0:
            self.punished_probes = 50.0
        return action

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    Majority-Driven Forgiving Conditional Cooperator (MD-FCC)

    - Open with cooperation (unless the game has only one round).
    - In non-final rounds, compute recent group cooperation F over up to the last w rounds (w = min(3, r-1)).
    - If F >= 0.5 -> cooperate.
    - If F < 0.5 -> consider:
        * If persistent defectors form a majority -> defect.
        * Else:
            - If recently punished (within G rounds) and the group has largely returned (recent F >= 0.5) -> cooperate (probation generosity).
            - Otherwise defect now (short punishment). Also, if own payoff fell substantially (>= 0.5) relative to previous round, impose immediate punishment.
    - Always defect in the final round (and when r == 1).
    - Uses only observable history (actions and payoffs) and game parameters; stateless (infers probation/punishment from history).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(3, max(0, self.r - 1))
        self.T = 0.5
        self.P = 1
        self.G = 2
        self.s = min(3, max(0, self.r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def compute_F_up_to(end_index: int) -> float:
            if history is None or end_index < 0:
                return 0.0
            window_len = min(self.w, end_index + 1)
            if window_len <= 0:
                return 0.0
            start = end_index - window_len + 1
            opp_slice = history.opponent_actions[start:end_index + 1, :]
            opp_sums = np.sum(opp_slice, axis=1).astype(float)
            my_slice = history.my_actions[start:end_index + 1].astype(float)
            total_cooperators = opp_sums + my_slice
            frac_per_round = total_cooperators / float(self.n)
            return float(np.mean(frac_per_round)) if frac_per_round.size > 0 else 0.0
        t_index = int(state.round_number)
        t_number = t_index + 1
        if self.r <= 1:
            return Action.D
        if t_number == self.r:
            return Action.D
        if t_index == 0:
            return Action.C
        F_current = compute_F_up_to(t_index - 1)
        if F_current >= self.T:
            return Action.C
        persistent_count = 0
        if history is not None and self.s > 0:
            avail_rounds = min(self.s, t_index)
            if avail_rounds > 0:
                start = t_index - avail_rounds
                end = t_index - 1
                opp_recent = history.opponent_actions[start:end + 1, :]
                all_defected = np.all(opp_recent == False, axis=0)
                persistent_count = int(np.sum(all_defected))
        persistent_fraction = persistent_count / float(self.n)
        if persistent_fraction >= self.T:
            return Action.D
        if history is not None and history.my_payoffs.size >= 2:
            prev_payoff = float(history.my_payoffs[-1])
            prevprev_payoff = float(history.my_payoffs[-2])
            if prevprev_payoff - prev_payoff >= 0.5:
                return Action.D
        prev_index = t_index - 1
        last_punish_index = None
        if history is not None:
            search_start = prev_index
            search_end = max(prev_index - self.G + 1, 0)
            for i in range(search_start, search_end - 1, -1):
                if not bool(history.my_actions[i]) and i != self.r - 1:
                    F_at_i = compute_F_up_to(i - 1)
                    if F_at_i < self.T:
                        last_punish_index = i
                        break
        in_probation = False
        if last_punish_index is not None:
            rounds_since_punish = t_index - last_punish_index
            if 1 <= rounds_since_punish <= self.G:
                in_probation = True
        if in_probation:
            F_last = compute_F_up_to(t_index - 1)
            if F_last >= self.T:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    Selective Conditional Cooperator with Timed Exploitation (SCC-TE).

    - Starts cooperatively to attract conditional cooperators.
    - Monitors recent behavior of others (window W) and uses thresholds to decide:
      unreliable groups -> defect; reliable groups -> probabilistic, limited exploitation.
    - Limits consecutive exploits, forces recovery cooperations, and monitors for
      retaliation (punishment). If retaliation occurs, enter a conservative punish/recovery
      mode: defect until group cooperation recovers, then signal with forced cooperations.
    - Deterministic pseudo-random tie-breaking based on round number and recent observed sums.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(3, math.floor(self.r / 4))) if self.r >= 1 else 3
        self.R_high = 0.6
        self.R_low = 0.3
        self.T_high = 0.8
        self.Exploit_rate_base = 0.2
        self.Max_consecutive_exploits = 2
        self.Recovery_length = 2
        self.Punishment_threshold = 0.25
        self.Last_rounds_defect = max(1, math.floor(self.r / 10)) if self.r >= 1 else 1
        self.ce_count = 0
        self.recovery_counter = 0
        self.punish_mode = False
        self.last_exploit_round = None
        self.last_exploit_pre_R = None
        self.consecutive_recovery_high_R = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        n = self.n
        r = self.r
        nb_opponents = max(0, n - 1)
        if state.round_number == 0 or history is None:
            self.ce_count = 0
            self.recovery_counter = 0
            self.punish_mode = False
            self.last_exploit_round = None
            self.last_exploit_pre_R = None
            self.consecutive_recovery_high_R = 0
            return Action.C
        if t > r:
            return Action.D
        if t > r - self.Last_rounds_defect:
            self.ce_count = 0
            self.recovery_counter = 0
            return Action.D
        rounds_played = t - 1
        w = min(self.W, rounds_played)
        opp_actions = history.opponent_actions
        if nb_opponents == 0 or rounds_played == 0:
            r_js = np.zeros(nb_opponents, dtype=float)
            R = 0.0
            m_high = 0
            recent_coop_count = 0
        else:
            if w == 0:
                r_js = np.zeros(nb_opponents, dtype=float)
            else:
                window = opp_actions[-w:, :]
                r_js = np.sum(window, axis=0) / float(w)
            R = float(np.mean(r_js)) if r_js.size > 0 else 0.0
            m_high = int(np.sum(r_js >= self.T_high))
            recent_coop_count = int(np.sum(opp_actions[-1, :])) if rounds_played >= 1 else 0
        num_very_low = int(np.sum(r_js <= 0.1)) if r_js.size > 0 else 0
        if nb_opponents > 0 and num_very_low > nb_opponents / 2:
            self.punish_mode = True
            self.ce_count = 0
            self.recovery_counter = 0
            return Action.D
        if self.last_exploit_round is not None:
            rounds_since_exploit = t - self.last_exploit_round
            if 1 <= rounds_since_exploit <= self.W:
                preR = self.last_exploit_pre_R if self.last_exploit_pre_R is not None else R
                if preR - R >= self.Punishment_threshold:
                    self.punish_mode = True
                    self.ce_count = 0
                    self.recovery_counter = 0
                    self.last_exploit_round = None
                    self.last_exploit_pre_R = None
        if self.punish_mode:
            if R >= self.R_high and w > 0:
                self.consecutive_recovery_high_R += 1
            else:
                self.consecutive_recovery_high_R = 0
            if self.consecutive_recovery_high_R >= self.W and self.W > 0:
                self.punish_mode = False
                self.consecutive_recovery_high_R = 0
                self.recovery_counter = self.Recovery_length
            else:
                return Action.D
        if self.recovery_counter > 0:
            self.recovery_counter -= 1
            self.ce_count = 0
            return Action.C
        if R <= self.R_low:
            self.ce_count = 0
            return Action.D
        threshold_m_high = max(1, math.floor(n / 4))
        if R >= self.R_high and m_high >= threshold_m_high:
            if r > 0:
                factor = 1.0 + (r - t) / float(r) * 2.0
            else:
                factor = 1.0
            exploit_probability = self.Exploit_rate_base * factor
            if exploit_probability < 0.0:
                exploit_probability = 0.0
            if exploit_probability > 1.0:
                exploit_probability = 1.0
            if self.ce_count >= self.Max_consecutive_exploits:
                self.recovery_counter = self.Recovery_length
                self.ce_count = 0
                self.recovery_counter -= 1
                return Action.C
            recent_window_sum = 0
            if w > 0 and nb_opponents > 0:
                recent_window_sum = int(np.sum(opp_actions[-w:, :]))
            seed_val = t * 1009 + recent_coop_count * 9176 + recent_window_sum * 3011
            pseudo = int(abs(seed_val)) % 10000
            threshold = int(round(exploit_probability * 10000))
            will_exploit = pseudo < threshold
            if will_exploit and exploit_probability > 0.0:
                self.ce_count += 1
                self.last_exploit_round = t
                self.last_exploit_pre_R = R
                return Action.D
            else:
                self.ce_count = 0
                return Action.C
        if R > self.R_low and R < self.R_high:
            if rounds_played >= 1:
                majority_needed = math.ceil(nb_opponents / 2) if nb_opponents > 0 else 0
                if recent_coop_count >= majority_needed:
                    self.ce_count = 0
                    return Action.C
            self.ce_count = 0
            return Action.D
        self.ce_count = 0
        return Action.D

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Cooperates initially (unless single-shot game), cooperates when recent group cooperation
      fraction F >= theta.
    - Forgives a single isolated drop in F (one-round generosity).
    - If persistent low cooperators form a substantial core (>= ceil(n/2)) according to recent
      history, enters a short punishment phase of P rounds (defecting), then requires S
      consecutive cooperative moves from flagged players to forgive them.
    - Always defects in the final round and in single-shot games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.theta = 0.6
        self.S = 2
        self.generosity_G = 1
        if self.r <= 1:
            self.L_default = 0
        else:
            self.L_default = min(5, max(0, self.r - 1))
        if self.r <= 3:
            self.P_default = 1
        else:
            self.P_default = min(3, max(1, math.floor(self.r / 10)))
        self.punishment_counter = 0
        self.punished_players = set()
        self.unforgiven_counters = dict()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r == 1:
            return Action.D
        if t == self.r - 1:
            return Action.D
        if t == 0 or history is None:
            return Action.C
        if self.r <= 3:
            L = max(0, self.r - 1)
            P = 1
        else:
            L = min(5, max(0, self.r - 1))
            P = min(3, max(1, math.floor(self.r / 10)))
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        rounds_played = int(my_actions.shape[0])

        def _get_action(player_idx: int, round_idx: int) -> bool:
            if player_idx == self.n - 1:
                return bool(my_actions[round_idx])
            else:
                return bool(opp_actions[round_idx, player_idx])
        H_len = min(L, rounds_played)
        if H_len == 0:
            F = 1.0
            personal_rates = [1.0] * self.n
        else:
            start = rounds_played - H_len
            my_sum = int(np.sum(my_actions[start:rounds_played]))
            opp_sum = int(np.sum(opp_actions[start:rounds_played, :]))
            total_contribs = my_sum + opp_sum
            denom = float(self.n * H_len) if self.n * H_len > 0 else 1.0
            F = total_contribs / denom
            personal_rates = []
            for col in range(self.n - 1):
                col_sum = int(np.sum(opp_actions[start:rounds_played, col]))
                personal_rates.append(col_sum / float(H_len))
            personal_rates.append(int(np.sum(my_actions[start:rounds_played])) / float(H_len))
        if rounds_played >= 1:
            H_prev_len = min(L, rounds_played - 1)
            if H_prev_len == 0:
                F_prev = 1.0
            else:
                start_prev = rounds_played - 1 - H_prev_len
                my_sum_prev = int(np.sum(my_actions[start_prev:rounds_played - 1]))
                opp_sum_prev = int(np.sum(opp_actions[start_prev:rounds_played - 1, :]))
                denom_prev = float(self.n * H_prev_len) if self.n * H_prev_len > 0 else 1.0
                F_prev = (my_sum_prev + opp_sum_prev) / denom_prev
        else:
            F_prev = 1.0
        if self.unforgiven_counters and rounds_played >= 1:
            last_round_idx = rounds_played - 1
            to_remove = []
            for p in list(self.unforgiven_counters.keys()):
                acted_coop = False
                if 0 <= p < self.n:
                    acted_coop = _get_action(p, last_round_idx)
                if acted_coop:
                    self.unforgiven_counters[p] += 1
                else:
                    self.unforgiven_counters[p] = 0
                if self.unforgiven_counters[p] >= self.S:
                    to_remove.append(p)
            for p in to_remove:
                self.unforgiven_counters.pop(p, None)
                self.punished_players.discard(p)
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter -= 1
            if self.punishment_counter == 0 and self.punished_players:
                self.unforgiven_counters = {p: 0 for p in self.punished_players}
            return action
        if F >= self.theta:
            return Action.C
        isolated_drop = F < self.theta and F_prev >= self.theta
        if isolated_drop:
            return Action.C
        count_low = 0
        low_players = set()
        if H_len == 0:
            count_low = 0
            low_players = set()
        else:
            for idx, rate in enumerate(personal_rates):
                if rate < 0.3:
                    low_players.add(idx)
            count_low = len(low_players)
        threshold_core = math.ceil(self.n / 2)
        if count_low >= threshold_core:
            self.punished_players = set(self.punished_players).union(low_players)
            self.punishment_counter = P
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    Exploitative Conditional Prober (ECP)

    - Probe early with alternating C/D to classify opponents as exploitable, retaliatory, or defect-leaning.
    - Exploit reliably exploitable opponents by defecting, with occasional maintenance cooperations if their unconditional cooperation rate drifts downward.
    - If no exploitable opponents are found, be cautious: cooperate only when a strong recent majority signal and high recent average cooperation among others exist; punish quickly if cooperation collapses.
    - Always defect in the final E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        base = max(2, math.floor(self.r / 6))
        self.W = min(6, base)
        self.P = min(6, base)
        self.TE = 0.8
        self.TR = 0.4
        self.M = max(6, math.floor(self.r / 10))
        self.E = min(2, self.r)
        self.banned_exploitable = set()
        self.last_maintenance_avg = None
        self.last_maintenance_round = None
        self.maintenance_watch = None
        self.punishment_end_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        if round_idx == 0 or history is None:
            if round_idx < self.P:
                return Action.C if round_idx % 2 == 0 else Action.D
            return Action.D
        if round_idx >= self.r - self.E:
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        total_rounds = my_actions.shape[0]
        n_opponents = self.n - 1
        if total_rounds >= 2:
            if my_actions[-2]:
                prev_count = int(np.sum(opp_actions[-2, :]))
                last_count = int(np.sum(opp_actions[-1, :]))
                denom = max(1, prev_count)
                drop = (prev_count - last_count) / denom
                if drop > 0.3:
                    self.punishment_end_round = round_idx + self.P - 1
        if round_idx <= self.punishment_end_round:
            return Action.D
        if round_idx < self.P:
            return Action.C if round_idx % 2 == 0 else Action.D
        W = min(self.W, total_rounds)
        start = max(0, total_rounds - W)
        end = total_rounds
        my_window = my_actions[start:end]
        opp_window = opp_actions[start:end, :]
        DefRounds = int(np.sum(~my_window))
        if DefRounds > 0:
            mask_def = ~my_window
            coop_when_i_def = np.sum(opp_window[mask_def, :], axis=0)
            UncondRate = coop_when_i_def / float(max(1, DefRounds))
        else:
            UncondRate = np.zeros(n_opponents, dtype=float)
        if W >= 2:
            my_window_prefix = my_window[:-1]
            opp_next = opp_window[1:, :]
            mask_afterC = my_window_prefix
            mask_afterD = ~my_window_prefix
            denom_afterC = int(np.sum(mask_afterC))
            denom_afterD = int(np.sum(mask_afterD))
            if denom_afterC > 0:
                P_afterC = np.sum(opp_next[mask_afterC, :], axis=0) / float(denom_afterC)
            else:
                P_afterC = np.zeros(n_opponents, dtype=float)
            if denom_afterD > 0:
                P_afterD = np.sum(opp_next[mask_afterD, :], axis=0) / float(denom_afterD)
            else:
                P_afterD = np.zeros(n_opponents, dtype=float)
        else:
            P_afterC = np.zeros(n_opponents, dtype=float)
            P_afterD = np.zeros(n_opponents, dtype=float)
        RetaliationDrop = np.maximum(0.0, P_afterC - P_afterD)
        exploitable = []
        retaliator = []
        uncertain = []
        for j in range(n_opponents):
            if UncondRate[j] >= self.TE and RetaliationDrop[j] <= self.TR and (j not in self.banned_exploitable):
                exploitable.append(j)
            elif RetaliationDrop[j] > self.TR:
                retaliator.append(j)
            else:
                uncertain.append(j)
        if exploitable:
            avg_uncond = float(np.mean([UncondRate[j] for j in exploitable])) if exploitable else 0.0
        else:
            avg_uncond = 0.0
        if self.last_maintenance_avg is None and round_idx >= self.P:
            self.last_maintenance_avg = avg_uncond
            self.last_maintenance_round = round_idx
        if self.maintenance_watch is not None:
            watch_start, baseline_avg, watch_targets = self.maintenance_watch
            if round_idx <= watch_start + max(0, self.W - 1):
                if watch_targets:
                    current_avg = float(np.mean([UncondRate[j] for j in watch_targets]))
                else:
                    current_avg = 0.0
                if baseline_avg - current_avg > 0.1:
                    for j in watch_targets:
                        self.banned_exploitable.add(j)
                    self.maintenance_watch = None
                    self.last_maintenance_avg = current_avg
                    self.last_maintenance_round = round_idx
            else:
                self.maintenance_watch = None
                self.last_maintenance_avg = avg_uncond
                self.last_maintenance_round = round_idx
        if exploitable:
            rounds_since_probe_end = round_idx - (self.P - 1)
            do_maintenance_pulse = False
            if rounds_since_probe_end > 0 and rounds_since_probe_end % self.M == 0:
                if self.last_maintenance_avg is None:
                    self.last_maintenance_avg = avg_uncond
                    self.last_maintenance_round = round_idx
                    do_maintenance_pulse = False
                elif self.last_maintenance_avg - avg_uncond > 0.05:
                    do_maintenance_pulse = True
            if do_maintenance_pulse:
                self.maintenance_watch = (round_idx, avg_uncond, list(exploitable))
                self.last_maintenance_avg = avg_uncond
                self.last_maintenance_round = round_idx
                return Action.C
            else:
                if rounds_since_probe_end > 0 and rounds_since_probe_end % self.M == 0:
                    self.last_maintenance_avg = avg_uncond
                    self.last_maintenance_round = round_idx
                return Action.D
        T = math.ceil(0.6 * max(1, n_opponents))
        others_prev = 0
        if total_rounds >= 1:
            others_prev = int(np.sum(opp_actions[-1, :]))
        if W > 0:
            avg_contrib_others = float(np.mean(opp_window))
        else:
            avg_contrib_others = 0.0
        strong_signal = others_prev >= T and avg_contrib_others >= 0.6
        if strong_signal:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    Adaptive ProberExploiter (APE)

    - Starts with short cooperative probes to identify cooperators.
    - If population seems highly cooperative, exploit by defecting most rounds,
      with occasional small-probability cooperation to "refuel" cooperation.
    - If exploitation triggers a sharp drop in cooperation, punish briefly then
      switch to reciprocal behavior to recover cooperation.
    - Always defect in the final round. In the last W_end rounds before the
      final round, bias toward defection by disabling voluntary epsilon-cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.R_probe = min(3, self.r)
        self.L = min(10, self.r)
        self.T_high = 0.6
        self.T_low = 0.4
        self.eps = 0.15
        self.P_len = 2
        self.W_end = max(1, math.floor(self.r / 10))
        self.delta = self.T_high - self.T_low
        self.punish_until_exclusive = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if t == self.r - 1:
            return Action.D
        in_end_window = t >= max(0, self.r - self.W_end)
        if history is None:
            if t < self.R_probe and (not in_end_window):
                return Action.C
            return Action.D
        opp_actions = history.opponent_actions
        rounds_played = 0 if opp_actions is None else opp_actions.shape[0]
        n_opponents = self.n - 1
        if rounds_played == 0:
            if t < self.R_probe and (not in_end_window):
                return Action.C
            return Action.D
        last_window = min(self.L, rounds_played)
        if last_window <= 0:
            p_j = np.zeros(n_opponents, dtype=float)
        else:
            recent_block = opp_actions[-last_window:, :]
            if n_opponents > 0:
                p_j = np.array(np.mean(recent_block.astype(float), axis=0), dtype=float)
            else:
                p_j = np.zeros(0, dtype=float)
        P_recent = float(np.mean(p_j)) if p_j.size > 0 else 0.0
        last_round_majority_C = False
        if rounds_played >= 1:
            last_round_actions = opp_actions[-1, :]
            coop_count_last = int(np.sum(last_round_actions))
            if coop_count_last > n_opponents / 2.0:
                last_round_majority_C = True
        if self.punish_until_exclusive is not None and t < self.punish_until_exclusive:
            return Action.D
        effective_eps = 0.0 if in_end_window else self.eps
        if in_end_window and t < self.R_probe:
            pass
        elif t < self.R_probe:
            return Action.C
        triggered_punish = False
        if rounds_played >= 2 and last_window >= 2:
            prev_block = opp_actions[-last_window:-1, :]
            prev_avg = float(np.mean(prev_block.astype(float)))
            last_round_frac = float(np.mean(opp_actions[-1, :].astype(float)))
            if prev_avg - last_round_frac > self.delta:
                triggered_punish = True
        if triggered_punish:
            self.punish_until_exclusive = t + self.P_len
            return Action.D
        if P_recent >= self.T_high:
            if effective_eps > 0.0:
                if random.random() < effective_eps:
                    return Action.C
            return Action.D
        else:
            if P_recent <= 0.2:
                return Action.D
            if last_round_majority_C:
                return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    - Probes early to learn whether others respond to my cooperation.
    - Computes avg contributions of others after I cooperated vs defected (rolling window).
    - Uses an economic threshold Theta tied to n, k and remaining horizon to decide
      whether cooperating is an investment that will pay off.
    - Free-rides when others are reliably contributing; defects in endgame.
    - Small persistent exploration (eps) except in the final K rounds.
    - Conservative (temporary) defection mode if a sudden drop in group contributions is detected.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players: int = int(game_description.n_players)
        self.r: int = int(game_description.n_rounds)
        self.k: float = float(game_description.k)
        self.P: int = min(6, max(1, math.floor(self.r / 10)))
        self.K: int = min(3, self.r)
        self.W: int = min(40, self.r)
        self.eps: float = 0.03
        self.conservative_until_round: int = -1
        self.responsive_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t: int = int(state.round_number)
        if t == 0 or t < self.P:
            return Action.C
        if t >= self.r - self.K:
            return Action.D
        if getattr(self, 'conservative_until_round', -1) >= 0 and t <= self.conservative_until_round:
            return Action.D
        R_C = []
        R_D = []
        if history is None:
            return Action.C if random.random() < self.eps else Action.D
        played_rounds = history.my_actions.shape[0]
        if history.opponent_actions.size == 0:
            return Action.C if random.random() < self.eps else Action.D
        for round_idx in range(1, played_rounds):
            prev_my_action = bool(history.my_actions[round_idx - 1])
            S = int(np.sum(history.opponent_actions[round_idx, :]))
            if prev_my_action:
                R_C.append(float(S))
            else:
                R_D.append(float(S))

        def recent(lst):
            if not lst:
                return []
            return lst[-self.W:]
        R_Cw = recent(R_C)
        R_Dw = recent(R_D)
        avg_after_C = float(np.mean(R_Cw)) if len(R_Cw) > 0 else None
        avg_after_D = float(np.mean(R_Dw)) if len(R_Dw) > 0 else None

        def detect_drop(lst: list[float]) -> bool:
            L = len(lst)
            if L < 4:
                return False
            arr = np.array(lst[-min(self.W, L):], dtype=float)
            Wl = arr.size
            half = max(1, Wl // 2)
            early = arr[:max(1, Wl - half)]
            late = arr[-half:]
            early_mean = float(np.mean(early)) if early.size > 0 else 0.0
            late_mean = float(np.mean(late)) if late.size > 0 else 0.0
            if early_mean > 0 and late_mean < early_mean * 0.7:
                return True
            return False
        if detect_drop(R_Cw) or detect_drop(R_Dw):
            M = max(0, self.r - (t + 1))
            hold = min(3, M)
            self.conservative_until_round = t + hold
            return Action.D
        if avg_after_C is None or avg_after_D is None:
            if random.random() < self.eps:
                return Action.C
            return Action.D
        delta = avg_after_C - avg_after_D
        M = max(0, self.r - (t + 1))
        denom = self.k * (M + 1) if self.k * (M + 1) != 0 else float('inf')
        Theta = self.n_players / denom
        if delta >= Theta:
            self.responsive_streak += 1
            return Action.C
        self.responsive_streak = 0
        if avg_after_D >= (self.n_players - 1) * 0.6:
            return Action.D
        if random.random() < self.eps:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    Collective Conditional Reciprocity (CCR)

    - Signals cooperation early, sustains cooperation when the group is cooperative.
    - Targets measured, time-limited punishment at repeat defectors using only public history.
    - Forgives occasionally to avoid retaliation cycles and includes probabilistic tests for recovery.
    - Defects in the last round (endgame awareness).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = int(min(10, max(1, math.floor(self.r / 5))))
        raw_leniency = 0.6 - 0.3 * (self.k / self.n - 0.5)
        self.leniency = max(0.4, min(0.8, raw_leniency))
        self.individual_threshold = max(0.25, self.leniency - 0.15)
        self.group_threshold = self.leniency
        self.B = int(min(3, max(1, math.floor(self.r / 6))))
        self.max_punish = int(min(4, max(1, math.floor(self.r / 8))))
        self.p_forgive_small = 0.05
        self.R_recovery_default = int(min(3, self.W))
        self.S_return = int(min(3, self.max_punish))
        self.active_punishments = {}
        self.defensive_mode = False
        self.last_rebuild_round = -9999

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_index = int(state.round_number)
        R_rem = self.r - round_index
        if round_index == 0 or history is None:
            if self.r == 1:
                return Action.D
            return Action.C
        if R_rem == 1:
            return Action.D
        to_remove = []
        for j, remaining in list(self.active_punishments.items()):
            new_remaining = remaining - 1
            if new_remaining <= 0:
                to_remove.append(j)
            else:
                self.active_punishments[j] = new_remaining
        for j in to_remove:
            self.active_punishments.pop(j, None)
        rounds_so_far = history.my_actions.shape[0]
        W = min(self.W, rounds_so_far)
        start_idx = max(0, rounds_so_far - W)
        n_opponents = self.n - 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if opp_actions.size == 0:
            opp_total_coop = np.zeros(n_opponents, dtype=int)
            opp_recent_coop = np.zeros(n_opponents, dtype=int)
            opp_last_round = np.zeros(n_opponents, dtype=bool)
        else:
            opp_total_coop = np.sum(opp_actions[:rounds_so_far, :], axis=0).astype(int)
            opp_recent_coop = np.sum(opp_actions[start_idx:rounds_so_far, :], axis=0).astype(int)
            opp_last_round = opp_actions[rounds_so_far - 1, :]
        my_total_coop = int(np.sum(my_actions[:rounds_so_far]))
        my_recent_coop = int(np.sum(my_actions[start_idx:rounds_so_far]))
        my_last_action = bool(my_actions[rounds_so_far - 1])
        coop_rates_opponents = opp_total_coop / float(rounds_so_far)
        my_coop_rate = my_total_coop / float(rounds_so_far)
        total_recent_coops = my_recent_coop + int(np.sum(opp_recent_coop))
        group_recent_coop_rate = total_recent_coops / float(self.n * W) if W > 0 else 0.0
        persistent_defectors = []
        for j in range(n_opponents):
            if coop_rates_opponents[j] < self.individual_threshold:
                persistent_defectors.append(j)
        last_round_defectors = [j for j in range(n_opponents) if not bool(opp_last_round[j])]
        i_defected_last = not my_last_action
        if history.opponent_payoffs.size == 0:
            opp_mean_payoffs = np.zeros(n_opponents, dtype=float)
        else:
            opp_mean_payoffs = np.mean(history.opponent_payoffs[start_idx:rounds_so_far, :], axis=0)
        my_mean_payoff = float(np.mean(history.my_payoffs[start_idx:rounds_so_far])) if W > 0 else 0.0
        all_player_means = np.concatenate([opp_mean_payoffs, np.array([my_mean_payoff])])
        group_mean_payoff = float(np.mean(all_player_means)) if all_player_means.size > 0 else 0.0
        group_std_payoff = float(np.std(all_player_means)) if all_player_means.size > 0 else 0.0
        exploit_detected = my_mean_payoff + 1e-12 < group_mean_payoff - group_std_payoff
        R_recovery = self.R_recovery_default
        if exploit_detected:
            R_recovery = max(1, R_recovery - 1)
        if group_recent_coop_rate < self.group_threshold - 0.15:
            self.defensive_mode = True
        elif group_recent_coop_rate >= self.group_threshold:
            self.defensive_mode = False
        start_new_punish = []
        for j in persistent_defectors:
            recent_coop_j = int(opp_recent_coop[j])
            recent_defections_j = W - recent_coop_j
            last_two_defected = False
            if rounds_so_far >= 2:
                last_two = opp_actions[max(0, rounds_so_far - 2):rounds_so_far, j]
                if last_two.shape[0] == 2 and (not bool(last_two[0])) and (not bool(last_two[1])):
                    last_two_defected = True
            if recent_defections_j >= 2 or last_two_defected:
                recent_rate = recent_coop_j / float(max(1, W))
                if recent_rate < self.individual_threshold or (rounds_so_far >= 1 and (not bool(opp_last_round[j]))):
                    start_new_punish.append((j, recent_defections_j))
        for j, recent_defections_j in start_new_punish:
            P = int(min(max(1, math.ceil(2 * float(recent_defections_j))), self.max_punish))
            if exploit_detected:
                P = int(min(self.max_punish, P + 1))
            existing = self.active_punishments.get(j, 0)
            self.active_punishments[j] = max(existing, P)
        to_remove = []
        for j, remaining in list(self.active_punishments.items()):
            coop_rate_j = coop_rates_opponents[j] if rounds_so_far > 0 else 0.0
            if coop_rate_j >= self.individual_threshold:
                to_remove.append(j)
                continue
            if R_recovery > 0 and rounds_so_far >= R_recovery:
                last_r = opp_actions[rounds_so_far - R_recovery:rounds_so_far, j]
                if bool(np.all(last_r)):
                    to_remove.append(j)
        for j in to_remove:
            self.active_punishments.pop(j, None)
        in_targeted_punishment = len(self.active_punishments) > 0
        S = self.S_return
        my_consec_defections = 0
        for a in reversed(my_actions):
            if not bool(a):
                my_consec_defections += 1
            else:
                break
        players_recent_rates = []
        for j in range(n_opponents):
            players_recent_rates.append(float(opp_recent_coop[j]) / float(max(1, W)))
        players_recent_rates.append(float(my_recent_coop) / float(max(1, W)))
        num_good_recent = sum((1 for rate in players_recent_rates if rate >= self.group_threshold))
        half_needed = (self.n + 1) // 2
        attempt_rebuild = False
        if my_consec_defections > S and num_good_recent >= half_needed and (round_index - self.last_rebuild_round > 1):
            attempt_rebuild = True
            self.last_rebuild_round = round_index
        if in_targeted_punishment:
            if random.random() < self.p_forgive_small:
                chosen = Action.C
            else:
                chosen = Action.D
            return chosen
        if self.defensive_mode:
            if random.random() < self.p_forgive_small:
                return Action.C
            return Action.D
        if attempt_rebuild:
            return Action.C
        if group_recent_coop_rate >= self.group_threshold:
            return Action.C
        lower = self.group_threshold - 0.15
        if group_recent_coop_rate >= lower:
            closeness = max(0.0, min(1.0, (group_recent_coop_rate - lower) / 0.15))
            prob_coop = closeness * 0.5
            if random.random() < prob_coop:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    Collective Conditional Cooperation with Graduated Punishment and Forgiveness.

    - Starts by signalling cooperation (C) except in a single-round game (last round -> D).
    - Cooperates when the fraction of other players who cooperated in the previous round
      meets an effective threshold (majority normally, near-unanimous in the last 3 rounds).
    - If the fraction falls short, enters a graduated temporary punishment (1-4 rounds),
      proportional to the shortfall. The first punishment round is the same round the
      insufficiency is observed.
    - Shows contrition: if it defected in the previous round but others mostly cooperated,
      it cooperates once to restore trust (unless it's the final round).
    - Has a Resign mode: if over the last M = min(5, r) completed rounds others cooperated
      on average <= 0.20, it switches to persistent defection until recovery (average > 0.50).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.punish_remaining: int = 0
        self.last_action_cooperated: bool = True
        self.resign: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        t_idx = int(state.round_number)
        rem = r - t_idx
        TH = 0.5
        TH_CLOSE = 0.9
        M = min(5, r)
        if t_idx == 0:
            if r == 1:
                action = Action.D
                self.last_action_cooperated = False
                return action
            action = Action.C
            self.last_action_cooperated = True
            return action
        prev_round_index = t_idx - 1
        f_prev = 1.0
        if history is not None and history.opponent_actions.size > 0:
            if prev_round_index >= 0 and prev_round_index < history.opponent_actions.shape[0]:
                others_coop = int(np.sum(history.opponent_actions[prev_round_index, :]))
                denom = max(1, n - 1)
                f_prev = others_coop / denom
            else:
                f_prev = 1.0
        avg_frac = None
        if history is not None and history.opponent_actions.size > 0:
            start = max(0, t_idx - M)
            end = t_idx
            if start < end:
                slice_arr = history.opponent_actions[start:end, :]
                coop_counts = np.sum(slice_arr, axis=1)
                denom = max(1, n - 1)
                fractions = coop_counts / denom
                avg_frac = float(np.mean(fractions))
        if self.resign:
            if avg_frac is not None and avg_frac > 0.5:
                self.resign = False
            else:
                self.last_action_cooperated = False
                return Action.D
        if rem == 1:
            self.last_action_cooperated = False
            return Action.D
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            self.last_action_cooperated = False
            if avg_frac is not None and avg_frac <= 0.2:
                self.resign = True
            return action
        effective_threshold = TH_CLOSE if rem <= 3 else TH
        if not self.last_action_cooperated and f_prev >= TH and (rem > 1) and (not self.resign):
            action = Action.C
            self.last_action_cooperated = True
            if avg_frac is not None and avg_frac <= 0.2:
                self.resign = True
            return action
        f = f_prev
        if f >= effective_threshold:
            action = Action.C
            self.last_action_cooperated = True
            if avg_frac is not None and avg_frac > 0.5:
                self.resign = False
            self.punish_remaining = 0
            return action
        else:
            s = effective_threshold - f
            severity = 1 + math.floor(4 * s)
            punish = min(4, int(severity))
            self.punish_remaining = punish
            self.punish_remaining = max(0, self.punish_remaining - 1)
            action = Action.D
            self.last_action_cooperated = False
            if avg_frac is not None and avg_frac <= 0.2:
                self.resign = True
            return action

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    Opportunistic Conditional Exploiter (OCE)

    Exploitative, adaptive public-goods strategy. Starts cooperatively, learns who
    reliably cooperates, then defects to exploit them while monitoring for and
    briefly punishing retaliators. Uses short punishments and a forgiveness schedule,
    occasional stochastic cooperation to avoid deterministic coordination on mutual
    defection, and defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.H_high = 0.8
        self.H_mid = 0.5
        self.punish_len = 3
        self.forgive_len = 5
        self.eps_coop = 0.05
        self.punish_timer = 0
        self.forgive_timer = 0
        self.exploit_burst_timer = 0
        self.collapse_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        remaining_rounds = max(0, self.r - state.round_number)
        endgame_margin = min(3, math.floor(self.r / 10))
        if remaining_rounds <= endgame_margin:
            return Action.D
        opp_actions = np.array(history.opponent_actions, dtype=np.bool_)
        my_actions = np.array(history.my_actions, dtype=np.bool_)
        rounds_played = my_actions.shape[0]
        w = min(20, remaining_rounds)
        effective_window = min(w, rounds_played)
        if rounds_played == 0:
            mean_other_fraction = 0.0
            coop_rate_i = np.zeros((self.n - 1,), dtype=float)
            unconditional_cooperators = 0
        else:
            others_sum = np.sum(opp_actions.astype(float), axis=1)
            if effective_window > 0:
                mean_other_fraction = float(np.mean(others_sum[-effective_window:]) / max(1, self.n - 1))
            else:
                mean_other_fraction = float(np.mean(others_sum) / max(1, self.n - 1))
            coop_rate_i = np.sum(opp_actions.astype(float), axis=0) / float(rounds_played)
            unconditional_cooperators = int(np.sum(coop_rate_i >= 0.95))
        if effective_window >= 1 and mean_other_fraction < 0.05 and (effective_window == w):
            self.collapse_flag = True
        elif mean_other_fraction >= self.H_mid:
            self.collapse_flag = False
        responsive_drop_detected = False
        for t0 in range(rounds_played):
            if not my_actions[t0]:
                frac_others_then = float(np.mean(opp_actions[t0, :].astype(float)))
                if frac_others_then >= self.H_high:
                    start_after = t0 + 1
                    if start_after >= rounds_played:
                        continue
                    window_after = min(w, rounds_played - start_after)
                    if window_after <= 0:
                        continue
                    before_counts = np.sum(opp_actions[:t0 + 1, :].astype(float), axis=0)
                    before_denom = float(t0 + 1)
                    after_counts = np.sum(opp_actions[start_after:start_after + window_after, :].astype(float), axis=0)
                    after_denom = float(window_after)
                    before_rates = before_counts / max(1.0, before_denom)
                    after_rates = after_counts / max(1.0, after_denom)
                    drops = before_rates - after_rates
                    if np.any(drops > 0.3):
                        responsive_drop_detected = True
                        break
        if rounds_played >= 2:
            last_idx = rounds_played - 1
            prev_idx = last_idx - 1
            prev_frac_others = float(np.mean(opp_actions[prev_idx, :].astype(float)))
            if not my_actions[prev_idx] and prev_frac_others >= self.H_mid:
                frac_now = float(np.mean(opp_actions[last_idx, :].astype(float)))
                if prev_frac_others - frac_now >= 0.3:
                    switched = np.logical_and(opp_actions[prev_idx, :], np.logical_not(opp_actions[last_idx, :]))
                    if np.any(switched):
                        if self.punish_timer == 0 and self.forgive_timer == 0:
                            self.punish_timer = self.punish_len
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.forgive_timer = self.forgive_len
            return action
        if self.forgive_timer > 0:
            prob_coop = 0.7
            self.forgive_timer -= 1
            if random.random() < prob_coop:
                return Action.C
            else:
                return Action.D
        if self.collapse_flag:
            return Action.D
        half_opponents = math.ceil((self.n - 1) / 2)
        if unconditional_cooperators >= half_opponents and mean_other_fraction >= self.H_mid:
            if self.exploit_burst_timer == 0:
                self.exploit_burst_timer = 2
            if self.exploit_burst_timer > 0:
                self.exploit_burst_timer -= 1
                return Action.D
        if mean_other_fraction >= self.H_high:
            if not responsive_drop_detected:
                if random.random() < self.eps_coop:
                    return Action.C
                return Action.D
            else:
                if self.punish_timer == 0 and self.forgive_timer == 0:
                    self.punish_timer = self.punish_len
                self.punish_timer -= 1
                if self.punish_timer == 0:
                    self.forgive_timer = self.forgive_len
                return Action.D
        elif mean_other_fraction >= self.H_mid:
            if unconditional_cooperators >= 2 and random.random() < 0.25:
                return Action.D
            return Action.C
        else:
            if random.random() < 0.1:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    Reciprocal Collective (RC)
    - Cooperate first, punish briefly, forgive quickly, defect in the end.
    - Uses only public history and game parameters to decide deterministically.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.phase = 'Cooperate'
        self.punish_remaining = 0
        self.safe_default_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r_total = int(self.game_description.n_rounds)

        def T_coop():
            return math.ceil((n - 1) / 2)

        def P_short_default():
            return min(3, max(1, math.floor(r_total / 10)))

        def P_long_default():
            return min(5, max(1, r_total - 1))

        def L_window():
            return min(5, r_total)
        rounds_played = state.round_number
        t = rounds_played + 1
        R_remaining = r_total - t + 1
        if rounds_played == 0 or history is None:
            self.phase = 'Cooperate'
            self.punish_remaining = 0
            self.safe_default_active = False
            return Action.C
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            self.phase = 'Cooperate'
            self.punish_remaining = 0
            self.safe_default_active = False
            return Action.C

        def compute_persistent_defectors(upto_rounds_played):
            L = L_window()
            available = min(L, max(0, upto_rounds_played))
            num_persistent = 0
            if available == 0:
                return 0
            start = max(0, upto_rounds_played - available)
            slice_actions = opp_actions[start:upto_rounds_played, :]
            coop_counts = np.sum(slice_actions, axis=0)
            defect_rates = 1.0 - coop_counts / float(available)
            num_persistent = int(np.sum(defect_rates >= 0.8))
            return num_persistent

        def replay_to_round(upto_rounds_played):
            local_phase = 'Cooperate'
            local_punish = 0
            local_safe = False
            for sim_i in range(1, upto_rounds_played + 1):
                sim_round_index = sim_i - 1
                R_sim = r_total - sim_i + 1
                num_persistent = compute_persistent_defectors(upto_rounds_played=sim_i - 1)
                majority_threshold = math.ceil((n - 1) / 2)
                if sim_i == r_total:
                    action_choice = Action.D
                    if num_persistent >= majority_threshold:
                        local_safe = True
                    continue
                if num_persistent >= majority_threshold:
                    action_choice = Action.D
                    local_safe = True
                    continue
                if local_phase == 'Punish':
                    action_choice = Action.D
                    local_punish = max(0, local_punish - 1)
                    if local_punish == 0:
                        local_phase = 'Probation'
                    continue
                elif local_phase == 'Probation':
                    action_choice = Action.C
                    coop_count = int(np.sum(opp_actions[sim_round_index, :]))
                    if coop_count >= T_coop():
                        local_phase = 'Cooperate'
                        local_punish = 0
                    else:
                        frac = coop_count / float(max(1, n - 1))
                        if R_sim == 3:
                            chosen_p = 1
                        else:
                            chosen_p = P_short_default() if frac >= 0.2 else P_long_default()
                        local_phase = 'Punish'
                        local_punish = chosen_p
                    continue
                else:
                    if sim_i == 1:
                        action_choice = Action.C
                        continue
                    last_round_others = int(np.sum(opp_actions[sim_round_index - 1, :]))
                    if last_round_others >= T_coop():
                        action_choice = Action.C
                        continue
                    else:
                        frac = last_round_others / float(max(1, n - 1))
                        if R_sim == 3:
                            chosen_p = 1
                        elif last_round_others >= 1 and frac >= 0.2:
                            chosen_p = P_short_default()
                        else:
                            chosen_p = P_long_default()
                        local_phase = 'Punish'
                        action_choice = Action.D
                        local_punish = max(0, chosen_p - 1)
                        if local_punish == 0:
                            local_phase = 'Probation'
                        continue
            return (local_phase, int(local_punish), bool(local_safe))
        sim_phase, sim_punish, sim_safe = replay_to_round(rounds_played)
        self.phase = sim_phase
        self.punish_remaining = sim_punish
        self.safe_default_active = sim_safe
        if t == r_total:
            self.phase = 'Cooperate'
            self.punish_remaining = 0
            self.safe_default_active = False
            return Action.D
        num_persistent_now = compute_persistent_defectors(upto_rounds_played=rounds_played)
        majority_threshold = math.ceil((n - 1) / 2)
        if num_persistent_now >= majority_threshold:
            self.safe_default_active = True
            return Action.D
        if self.safe_default_active and num_persistent_now < majority_threshold:
            self.safe_default_active = False
        P_short = P_short_default()
        if R_remaining == 3:
            P_short = 1
        P_long = P_long_default()
        if self.phase == 'Punish':
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.phase = 'Probation'
            return action
        if self.phase == 'Probation':
            return Action.C
        last_round_others = int(np.sum(opp_actions[-1, :]))
        if R_remaining <= 2:
            if R_remaining == 2:
                if last_round_others == n - 1:
                    return Action.C
                return Action.D
            if R_remaining == 0:
                return Action.D
        if last_round_others >= T_coop():
            return Action.C
        else:
            frac = last_round_others / float(max(1, n - 1))
            if last_round_others >= 1 and frac >= 0.2:
                chosen_p = P_short
            else:
                chosen_p = P_long
            post_punish = max(0, chosen_p - 1)
            if post_punish == 0:
                self.phase = 'Probation'
                self.punish_remaining = 0
            else:
                self.phase = 'Punish'
                self.punish_remaining = int(post_punish)
            return Action.D

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    Exploit-and-Repair (E&R)

    - Seed cooperation for s rounds to probe conditional cooperators.
    - Mostly defect thereafter, exploiting unconditional cooperators when group
      cooperation is high enough.
    - Against many conditional cooperators, use a lenient stochastic exploitation
      (occasionally defect) and perform short repairs (T rounds of cooperation)
      if defection causes a sustained collapse in cooperation.
    - Always defect in the final e rounds.
    - Track and avoid provoking permanent punishers where possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.k = float(game_description.k)
        self.r = int(game_description.n_rounds)
        base_w = math.floor(self.r / 10)
        self.w = min(5, max(3, base_w))
        base_s = math.floor(self.r / 10)
        self.s = min(3, max(1, base_s))
        if self.r <= 6:
            self.s = 1
        base_e = math.floor(self.r / 10)
        self.e = max(1, min(2, base_e))
        ratio = self.k / max(1.0, float(self.n))
        self.H = 0.75
        if ratio > 0.7:
            self.H = 0.85
        elif ratio < 0.4:
            self.H = 0.65
        self.M = 0.45
        self.T = 2
        if self.r <= 6:
            self.T = 1
        self.leniency_rounds = 2
        self.resp_delta = 0.15
        self.low_resp_tol = 0.1
        self.repair_mode = False
        self.repair_rounds_left = 0
        self.permanent_defect = False
        self.punishers = None
        self.last_window_avg = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        if self.punishers is None and history is not None:
            opp_count = max(0, history.opponent_actions.shape[1])
            self.punishers = np.zeros(opp_count, dtype=bool)
        if t == 0:
            return Action.C if self.s >= 1 else Action.D
        if t < self.s:
            return Action.C
        if t >= self.r - self.e:
            return Action.D
        if history is None:
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        rounds_so_far = my_actions.shape[0]
        opp_count = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        if opp_count == 0:
            return Action.D
        window_start = max(0, t - self.w)
        window_end = t
        window_len = window_end - window_start
        if window_len <= 0:
            p_js = np.zeros(opp_count, dtype=float)
            window_per_round_totals = np.array([], dtype=float)
            window_avg_total = 0.0
        else:
            slice_ops = opp_actions[window_start:window_end, :]
            p_js = slice_ops.mean(axis=0).astype(float)
            window_per_round_totals = slice_ops.sum(axis=1).astype(float)
            window_avg_total = float(window_per_round_totals.mean()) if window_per_round_totals.size > 0 else 0.0
        G = float(p_js.sum())
        p_after_defect = np.zeros(opp_count, dtype=float)
        p_after_coop = np.zeros(opp_count, dtype=float)
        count_after_defect = np.zeros(opp_count, dtype=int)
        count_after_coop = np.zeros(opp_count, dtype=int)
        for idx in range(window_start, window_end - 1):
            my_act = bool(my_actions[idx])
            next_idx = idx + 1
            opp_next = opp_actions[next_idx, :]
            if not my_act:
                p_after_defect += opp_next.astype(float)
                count_after_defect += 1
            else:
                p_after_coop += opp_next.astype(float)
                count_after_coop += 1
        for j in range(opp_count):
            if count_after_defect[j] > 0:
                p_after_defect[j] = p_after_defect[j] / float(count_after_defect[j])
            else:
                p_after_defect[j] = p_js[j]
            if count_after_coop[j] > 0:
                p_after_coop[j] = p_after_coop[j] / float(count_after_coop[j])
            else:
                p_after_coop[j] = p_js[j]
        responsiveness = p_after_coop - p_after_defect > self.resp_delta
        low_responsiveness = p_after_coop - p_after_defect <= self.low_resp_tol
        if window_len >= 3:
            for j in range(opp_count):
                if self.punishers is not None and self.punishers[j]:
                    continue
                punished = False
                for idx in range(window_start, window_end - 2):
                    if not my_actions[idx]:
                        if not opp_actions[idx + 1, j] and (not opp_actions[idx + 2, j]):
                            punished = True
                            break
                if punished:
                    self.punishers[j] = True
        punishers_count = int(self.punishers.sum()) if self.punishers is not None else 0
        uncond_mask = (p_js >= 0.9) & low_responsiveness
        cond_mask = (p_js >= 0.4) & responsiveness
        def_mask = p_js <= 0.2
        count_uncond = int(uncond_mask.sum())
        count_cond = int(cond_mask.sum())
        count_def = int(def_mask.sum())
        recent_len = min(2, window_len)
        if recent_len <= 0:
            recent_avg = window_avg_total
            recent_totals = np.array([], dtype=float)
        else:
            recent_slice = opp_actions[window_end - recent_len:window_end, :]
            recent_totals = recent_slice.sum(axis=1).astype(float)
            recent_avg = float(recent_totals.mean())
        collapse_detected = False
        if window_len >= 2:
            drop_threshold = 0.2 * float(n - 1)
            if recent_len >= 2:
                if recent_totals[0] < window_avg_total - drop_threshold and recent_totals[1] < window_avg_total - drop_threshold:
                    collapse_detected = True
            else:
                collapse_detected = False
        if self.repair_mode:
            action = Action.C
            self.repair_rounds_left = max(0, self.repair_rounds_left - 1)
            if self.repair_rounds_left == 0:
                ws = max(1, min(self.w, t))
                start = max(0, t - ws)
                slice_ops2 = opp_actions[start:t, :]
                per_round_totals2 = slice_ops2.sum(axis=1).astype(float) if slice_ops2.size > 0 else np.array([0.0])
                new_avg = float(per_round_totals2.mean()) if per_round_totals2.size > 0 else 0.0
                if new_avg >= self.M * float(n - 1):
                    self.repair_mode = False
                    self.permanent_defect = False
                else:
                    self.repair_mode = False
                    self.permanent_defect = True
            return action
        if self.permanent_defect:
            if count_uncond >= 1 and G >= self.H * float(n - 1):
                self.permanent_defect = False
            else:
                return Action.D
        if count_uncond >= 1 and G >= self.H * float(n - 1):
            return Action.D
        if count_cond >= count_uncond + count_def and G >= self.M * float(n - 1):
            last_action = bool(my_actions[-1])
            triggered_repair = False
            if not last_action:
                if window_len >= 1:
                    if window_avg_total - recent_avg > 0.2 * float(n - 1):
                        if recent_len >= 2:
                            triggered_repair = True
            if triggered_repair:
                self.repair_mode = True
                self.repair_rounds_left = self.T
                return Action.C
            if last_action:
                large_drop = False
                if window_len >= 1:
                    if window_avg_total - recent_avg > 0.2 * float(n - 1):
                        large_drop = True
                p_exploit = 0.5
                if punishers_count > 0:
                    p_exploit = 0.2
                if large_drop:
                    return Action.C
                if random.random() < p_exploit:
                    return Action.D
                else:
                    return Action.C
            else:
                p_defect = 0.6
                if punishers_count > 0:
                    p_defect = 0.45
                if random.random() < p_defect:
                    return Action.D
                else:
                    return Action.C
        return Action.D

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    Collective, Forgiving Conditional Cooperator (CFCC)

    - Opens with cooperation.
    - Uses a short memory window to estimate recent cooperation rates.
    - Punishes proportionally to number of recent defectors with short, capped punishments.
    - Forgives quickly after punishments (requires one good observed round to resume cooperation).
    - Detects persistent free-riders (consecutive poor recent rates) and defects until they recover.
    - In the final round, cooperates only if there is perfect recent cooperation in the endgame window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = float(game_description.n_players)
        k = float(game_description.k)
        r = int(game_description.n_rounds)
        base = 0.5 + 0.25 * (1.0 - k / n)
        self.base_threshold = max(0.5, min(0.75, base))
        self.L = min(5, r)
        self.P_max = min(4, r)
        self.T_end = min(3, r)
        self.noise_tolerance = 0.1
        self.remaining_punish_rounds = 0
        self.awaiting_reentry = False
        self._persistent_counters = [0] * (self.game_description.n_players - 1)
        self._persistent_targets = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        rounds_played = history.my_actions.shape[0]
        if rounds_played == 0:
            return Action.C
        window = min(self.L, rounds_played)
        start_idx = rounds_played - window
        opp_window = history.opponent_actions[start_idx:rounds_played, :]
        try:
            rates_opponents = np.mean(opp_window.astype(float), axis=0)
        except Exception:
            rates_opponents = np.array([np.mean(opp_window[:, j]) for j in range(opp_window.shape[1])])
        if rates_opponents.size > 0:
            R = float(np.mean(rates_opponents))
        else:
            R = 0.0
        twoL = 2 * self.L
        for j in range(len(rates_opponents)):
            rate_j = float(rates_opponents[j])
            if rate_j < 0.2:
                self._persistent_counters[j] += 1
            else:
                self._persistent_counters[j] = 0
            if self._persistent_counters[j] > twoL:
                self._persistent_targets.add(j)
            if j in self._persistent_targets and rate_j > 0.3:
                self._persistent_targets.remove(j)
                self._persistent_counters[j] = 0
        if len(self._persistent_targets) > 0:
            return Action.D
        if state.round_number == r - 1:
            if rounds_played >= self.T_end:
                start_e = rounds_played - self.T_end
                last_opp_window = history.opponent_actions[start_e:rounds_played, :]
                last_my_window = history.my_actions[start_e:rounds_played]
                all_opp_coop = bool(np.all(last_opp_window))
                all_my_coop = bool(np.all(last_my_window))
                if all_opp_coop and all_my_coop:
                    return Action.C
            return Action.D
        last_idx = rounds_played - 1
        coop_opp_last = int(np.sum(history.opponent_actions[last_idx, :]))
        my_last_coop = bool(history.my_actions[last_idx])
        coop_total_last = coop_opp_last + (1 if my_last_coop else 0)
        d = n - coop_total_last
        if self.remaining_punish_rounds > 0:
            self.remaining_punish_rounds -= 1
            if self.remaining_punish_rounds == 0:
                self.awaiting_reentry = True
            return Action.D
        if self.awaiting_reentry:
            if R >= self.base_threshold:
                self.awaiting_reentry = False
                return Action.C
            else:
                self.remaining_punish_rounds = min(1, self.P_max)
                return Action.D
        if R >= self.base_threshold:
            return Action.C
        if d == 0:
            self.remaining_punish_rounds = min(1, self.P_max)
            return Action.D
        if d == 1:
            if R >= self.base_threshold - self.noise_tolerance:
                self.remaining_punish_rounds = 1
                return Action.D
            else:
                self.remaining_punish_rounds = min(1 + d, self.P_max)
                return Action.D
        if d >= 2:
            self.remaining_punish_rounds = min(1 + d, self.P_max)
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    Adaptive Targeted-Exploiter (ATE)

    - Probes opponents for a short initial phase to estimate cooperation rates and responsiveness.
    - Classifies opponents into unconditional cooperators, reciprocators (conditional), defectors, or unknown/noisy.
    - Exploits unconditionals when it is safe (and soft-exploits sporadically if many reciprocators).
    - Forgives briefly if reciprocators punish, but does not engage in long costly punishments.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.UNCOND_COOP_THRESH = 0.9
        self.DEFECTOR_THRESH = 0.2
        self.RESPOND_THRESH = 0.2
        self.RESPOND_SMALL_ABS = 0.05
        base_probe = math.floor(max(1, self.r / 10))
        self.PROBE_ROUNDS = min(6, max(3, base_probe))
        if self.r > 1:
            self.PROBE_ROUNDS = min(self.PROBE_ROUNDS, max(1, self.r - 1))
        else:
            self.PROBE_ROUNDS = 0
        self.SOFT_EXPLOIT_FREQ = max(5, math.floor(max(1, self.r / 10)))
        self.FORGIVENESS_WINDOW = 2
        self.last_exploit_round = None
        self.forgiveness_remaining = 0
        self.last_round_was_probe = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number

        def A_C():
            return Action.C

        def A_D():
            return Action.D
        if t == 0 or (self.PROBE_ROUNDS > 0 and t < self.PROBE_ROUNDS):
            if t <= 1:
                self.last_round_was_probe = True
                return A_C()
            if t == 2 and self.PROBE_ROUNDS >= 3:
                self.last_round_was_probe = True
                return A_D()
            self.last_round_was_probe = True
            if random.random() < 0.6:
                return A_C()
            return A_D()
        if history is None:
            return A_D()
        if t == self.r - 1:
            return A_D()
        T = history.opponent_actions.shape[0]
        opponent_count = history.opponent_actions.shape[1] if history.opponent_actions.ndim >= 2 else 0
        if opponent_count == 0:
            return A_D()
        coop_counts = np.sum(history.opponent_actions, axis=0).astype(int)
        rounds_seen = T
        coop_after_myC = np.zeros(opponent_count, dtype=int)
        coop_after_myD = np.zeros(opponent_count, dtype=int)
        if T >= 2:
            my_prev_actions = history.my_actions[:-1]
            for idx in range(T - 1):
                was_C = bool(my_prev_actions[idx])
                opp_actions_next = history.opponent_actions[idx + 1, :]
                if was_C:
                    coop_after_myC += opp_actions_next.astype(int)
                else:
                    coop_after_myD += opp_actions_next.astype(int)
            times_I_was_C = int(np.sum(my_prev_actions))
            times_I_was_D = T - 1 - times_I_was_C
        else:
            times_I_was_C = 0
            times_I_was_D = 0
            coop_after_myC = coop_after_myC
            coop_after_myD = coop_after_myD
        coop_rates = (1.0 + coop_counts) / (2.0 + max(1, rounds_seen))
        denom_C = 2.0 + max(0, times_I_was_C)
        denom_D = 2.0 + max(0, times_I_was_D)
        p_after_C = (1.0 + coop_after_myC) / denom_C
        p_after_D = (1.0 + coop_after_myD) / denom_D
        responsivity = p_after_C - p_after_D
        unconditionals = []
        reciprocators = []
        defectors = []
        unknowns = []
        for j in range(opponent_count):
            cr = float(coop_rates[j])
            resp = float(responsivity[j])
            if cr >= self.UNCOND_COOP_THRESH and abs(resp) <= self.RESPOND_SMALL_ABS:
                unconditionals.append(j)
            elif resp >= self.RESPOND_THRESH:
                reciprocators.append(j)
            elif cr <= self.DEFECTOR_THRESH:
                defectors.append(j)
            else:
                unknowns.append(j)
        n_uncond = len(unconditionals)
        n_recip = len(reciprocators)
        n_def = len(defectors)
        n_unknown = len(unknowns)
        expected_cooperators = 0
        expected_cooperators = int(np.sum(coop_rates >= 0.5))
        expected_cooperators = max(expected_cooperators, n_recip + n_uncond)
        if expected_cooperators < 1:
            return A_D()
        if self.forgiveness_remaining > 0:
            self.forgiveness_remaining = max(0, self.forgiveness_remaining - 1)
            return A_C()
        remaining = self.r - t
        many_recip_threshold = math.ceil((self.n_players - 1) / 2)
        if n_uncond >= 1 and n_uncond + n_recip >= 2:
            if n_recip >= many_recip_threshold:
                allow_soft = False
                if self.last_exploit_round is None:
                    allow_soft = True
                elif t - self.last_exploit_round >= self.SOFT_EXPLOIT_FREQ:
                    allow_soft = True
                if allow_soft:
                    self.last_exploit_round = t
                    return A_D()
                else:
                    if remaining >= 3 and (n_recip >= 1 and expected_cooperators >= many_recip_threshold):
                        return A_C()
                    return A_D()
            else:
                return A_D()
        if n_uncond == 0 and n_recip >= 1 and (expected_cooperators >= many_recip_threshold):
            if remaining >= 3 or (remaining >= 1 and expected_cooperators == self.n_players - 1):
                return A_C()
            return A_D()
        if n_unknown >= math.ceil((self.n_players - 1) / 2):
            if remaining >= 4:
                if random.random() < 0.35:
                    return A_C()
            return A_D()
        if expected_cooperators >= math.ceil((self.n_players - 1) / 2) and remaining >= 2:
            return A_C()
        if n_recip > 0 and T >= 2:
            my_last_action = bool(history.my_actions[-1])
            if not my_last_action:
                if n_recip > 0:
                    recent_coop_frac = float(np.mean(history.opponent_actions[-1, reciprocators].astype(float)))
                    if T >= 2:
                        if T - 1 >= 1:
                            baseline_frac = float(np.mean(history.opponent_actions[:-1, reciprocators].astype(float)))
                        else:
                            baseline_frac = recent_coop_frac
                    else:
                        baseline_frac = recent_coop_frac
                    if baseline_frac - recent_coop_frac >= 0.3:
                        self.forgiveness_remaining = self.FORGIVENESS_WINDOW
                        return A_C()
        return A_D()

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    Collective Proportional Conditional Cooperator (CPCC)

    - Starts cooperative to seed contributions.
    - Uses a sliding memory window of recent rounds to compute the fraction P of cooperators.
    - If recent cooperation is strong (P >= T) it cooperates.
    - If cooperation falls below threshold, it issues short proportional punishments (defect for L rounds),
      where L is computed from the shortfall and capped by remaining rounds.
    - Punishments are temporary and forgiving: if group cooperation recovers (P >= T) the punishment ends early.
    - Special handling for single likely mistakes, slow erosion, persistent exploiters, and endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.M = min(5, self.r)
        self.T = 0.75
        self.punishment_remaining = 0
        self._persistent_exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            self.punishment_remaining = 0
            self._persistent_exploiters.clear()
            return Action.C
        n = self.n
        r = self.r
        remaining_rounds = r - t
        if t == r - 1:
            self.punishment_remaining = 0
            return Action.D
        if remaining_rounds <= 2:
            if history.opponent_actions.shape[0] >= 1:
                coop_last = int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))
                if coop_last == n:
                    self.punishment_remaining = 0
                    return Action.C
            self.punishment_remaining = 0
            return Action.D
        last_m = min(self.M, t)
        my_slice = history.my_actions[-last_m:]
        opp_slice = history.opponent_actions[-last_m:, :]
        total_coop_last_m = int(np.sum(my_slice)) + int(np.sum(opp_slice))
        denom = n * last_m if last_m > 0 else 1
        P = total_coop_last_m / denom
        coop_last_round = int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))
        D_recent = n - coop_last_round
        if self.punishment_remaining > 0:
            if P >= self.T:
                self.punishment_remaining = 0
                return Action.C
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return Action.D
        if P >= self.T:
            return Action.C
        shortfall = max(0.0, self.T - P)
        L_base = 1 + round(shortfall * self.M * 2)
        max_allowed = max(0, remaining_rounds - 1)
        L = min(L_base, max_allowed)
        if D_recent == 0 and P < self.T:
            L = min(1, max_allowed)
        defect_count_window = n * last_m - total_coop_last_m
        if P < self.T and P >= self.T - 0.15:
            prior_m = min(self.M, max(0, t - last_m))
            prior_good = False
            if prior_m >= 1:
                my_prior = history.my_actions[-last_m - prior_m:-last_m] if t - last_m >= prior_m else history.my_actions[:0]
                opp_prior = history.opponent_actions[-last_m - prior_m:-last_m, :] if t - last_m >= prior_m else history.opponent_actions[:0, :]
                if my_prior.size + opp_prior.size > 0:
                    prior_total = int(np.sum(my_prior)) + int(np.sum(opp_prior))
                    prior_denom = n * prior_m
                    prior_P = prior_total / prior_denom if prior_denom > 0 else 0.0
                    prior_good = prior_P >= self.T
            if prior_good and defect_count_window <= 1 or defect_count_window <= 1:
                L = min(1, max_allowed)
        last_k = min(10, t)
        if last_k >= 1:
            for idx in range(history.opponent_actions.shape[1]):
                opp_recent = history.opponent_actions[-last_k:, idx]
                opp_rate = float(np.mean(opp_recent)) if opp_recent.size > 0 else 1.0
                if opp_rate <= 0.2:
                    contrib_by_player = int(np.sum(history.opponent_actions[-last_m:, idx])) if last_m >= 1 else 0
                    total_excl = total_coop_last_m - contrib_by_player
                    denom_excl = (n - 1) * last_m if last_m > 0 else 1
                    group_excl_rate = total_excl / denom_excl if denom_excl > 0 else 0.0
                    if group_excl_rate >= 0.6:
                        self._persistent_exploiters.add(idx)
                        L = max(L, min(1, max_allowed))
        if L <= 0:
            return Action.D
        self.punishment_remaining = max(0, L - 1)
        return Action.D

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    Selective Free-Rider with Probing, Mild Punishment and Endgame Defection (SFR-PMP)
    Implements probe -> detect responsiveness -> exploit or cooperate -> short punishments -> endgame defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.PROBE_ROUNDS = min(5, max(2, math.floor(self.r / 6)))
        self.OBS_WINDOW = min(6, self.r)
        self.RECIPROCITY_THRESHOLD = 0.15
        self.EXPLOIT_TRIGGER = 0.5
        self.PUNISH_LENGTH = 2
        self.ENDGAME_ROUNDS = min(3, self.r)
        self.SUPPORT_PROB_MIN = 0.05
        self.SUPPORT_DECAY = 0.4
        self.current_state = 'PROBE'
        self.punish_counter = 0
        self.permanent_defect = False
        self.last_recent_avg_others = 1.0
        self.last_support_round = -999
        self._last_round_processed = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        rn = state.round_number
        TOTAL_OTHERS = self.n - 1
        if rn >= self.r - self.ENDGAME_ROUNDS:
            self.current_state = 'ENDGAME'
            self.permanent_defect = True
            return Action.D
        if rn == 0 or history is None:
            self.current_state = 'PROBE'
            self.punish_counter = 0
            self.permanent_defect = False
            self.last_support_round = -999
            self._last_round_processed = rn
            return Action.C
        m = history.my_actions.shape[0]
        if m != rn:
            m = history.my_actions.shape[0]
        if self.permanent_defect:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.current_state = 'PROBE'
            else:
                self.current_state = 'PUNISH'
            return Action.D
        window = min(self.OBS_WINDOW, m)
        start = max(0, m - window)
        if window <= 0 or TOTAL_OTHERS <= 0:
            recent_avg_others = 0.0
        else:
            recent_op_acts = history.opponent_actions[start:m, :]
            try:
                total_true = float(np.sum(recent_op_acts))
                recent_avg_others = total_true / (window * TOTAL_OTHERS)
            except Exception:
                recent_avg_others = 0.0
        A_vals = []
        B_vals = []
        for i in range(start, m - 1):
            my_act_i = bool(history.my_actions[i])
            opp_next = history.opponent_actions[i + 1, :]
            try:
                mean_next = float(np.mean(opp_next)) if opp_next.size > 0 else 0.0
            except Exception:
                mean_next = 0.0
            if my_act_i:
                A_vals.append(mean_next)
            else:
                B_vals.append(mean_next)
        A = float(np.mean(A_vals)) if len(A_vals) > 0 else 0.0
        B = float(np.mean(B_vals)) if len(B_vals) > 0 else 0.0
        R = A - B
        if rn < self.PROBE_ROUNDS:
            self.current_state = 'PROBE'
            self.last_recent_avg_others = recent_avg_others
            return Action.C
        if rn < self.PROBE_ROUNDS + 2 and m < max(3, self.OBS_WINDOW):
            self.current_state = 'PROBE'
            self.last_recent_avg_others = recent_avg_others
            return Action.C
        if m >= 1:
            w2 = min(self.OBS_WINDOW, m)
            try:
                my_payoff_avg = float(np.mean(history.my_payoffs[-w2:]))
            except Exception:
                my_payoff_avg = float(np.mean(history.my_payoffs)) if history.my_payoffs.size > 0 else 0.0
            if my_payoff_avg < 1.0 - 1e-08:
                if m >= self.OBS_WINDOW:
                    self.permanent_defect = True
                    self.current_state = 'EXPLOIT'
                    return Action.D
        if R >= self.RECIPROCITY_THRESHOLD and recent_avg_others >= self.EXPLOIT_TRIGGER:
            selected_state = 'EXPLOIT'
        elif recent_avg_others >= self.EXPLOIT_TRIGGER:
            selected_state = 'COOPERATE'
        else:
            rounds_remaining = max(0, self.r - (rn + 1))
            if rounds_remaining > self.PROBE_ROUNDS:
                selected_state = 'COOPERATE'
            else:
                selected_state = 'EXPLOIT'
        self.current_state = selected_state
        drop_since_last = self.last_recent_avg_others - recent_avg_others
        my_last_action = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
        recently_supported = rn - self.last_support_round <= 1
        if (my_last_action or recently_supported) and drop_since_last > 0.2:
            self.current_state = 'PUNISH'
            self.punish_counter = self.PUNISH_LENGTH
            self.last_recent_avg_others = recent_avg_others
            return Action.D
        if self.current_state == 'EXPLOIT':
            recent_total_contrib_rate = recent_avg_others
            p_support = max(self.SUPPORT_PROB_MIN, self.SUPPORT_DECAY * max(0.0, self.EXPLOIT_TRIGGER - recent_total_contrib_rate))
            if drop_since_last > 0.1:
                p_support = min(1.0, p_support + 0.5)
            if random.random() < p_support:
                self.last_support_round = rn
                self.last_recent_avg_others = recent_avg_others
                return Action.C
            else:
                self.last_recent_avg_others = recent_avg_others
                return Action.D
        elif self.current_state == 'COOPERATE':
            self.last_recent_avg_others = recent_avg_others
            return Action.C
        elif self.current_state == 'PROBE':
            self.last_recent_avg_others = recent_avg_others
            return Action.C
        elif self.current_state == 'PUNISH':
            self.last_recent_avg_others = recent_avg_others
            return Action.D
        self.last_recent_avg_others = recent_avg_others
        return Action.D

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    Collective Gradual Reciprocity with Proportional Punishment (CGR-PP)

    - Default: start cooperating and try to sustain high group cooperation.
    - If a recent round shows cooperation below threshold theta, trigger a
      proportional punishment of length P (includes current round).
    - After punishment, require short good-streaks (tracked by last_good_streak)
      to re-establish cooperation; punish again on breaches.
    - If cooperation is hopeless over a medium window, switch to permanent defection.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'Cooperate'
        self.punish_remaining = 0
        self.last_good_streak = 0
        self.theta = 0.6
        self.M = 2
        self.beta = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if state.round_number == r - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        if history is None:
            c_prev = 1.0
            recent_rounds = 0
        else:
            recent_rounds = int(history.my_actions.shape[0])
            if recent_rounds == 0:
                c_prev = 1.0
            else:
                opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=bool)
                my_last = bool(history.my_actions[-1]) if history.my_actions.size else False
                coop_count_last = int(np.sum(opp_last)) + (1 if my_last else 0)
                c_prev = coop_count_last / max(1, n)
        L = min(5, r)
        W = max(3, math.ceil(r / 10))
        F = 1.0
        if history is not None and recent_rounds > 0:
            start_L = max(0, recent_rounds - L)
            opp_slice = history.opponent_actions[start_L:recent_rounds, :] if history.opponent_actions.size else np.zeros((0, max(0, n - 1)), dtype=bool)
            my_slice = history.my_actions[start_L:recent_rounds] if history.my_actions.size else np.array([], dtype=bool)
            if opp_slice.shape[0] > 0:
                opp_counts = np.sum(opp_slice, axis=1).astype(int)
                my_counts = my_slice.astype(int)
                per_round_coop = opp_counts + my_counts
                F = float(np.mean(per_round_coop) / max(1, n))
            else:
                F = 1.0
        hopeless_avg = 1.0
        if history is not None and recent_rounds > 0:
            window = min(W, recent_rounds)
            start_W = recent_rounds - window
            opp_w = history.opponent_actions[start_W:recent_rounds, :] if history.opponent_actions.size else np.zeros((0, max(0, n - 1)), dtype=bool)
            my_w = history.my_actions[start_W:recent_rounds] if history.my_actions.size else np.array([], dtype=bool)
            if opp_w.shape[0] > 0:
                opp_counts_w = np.sum(opp_w, axis=1).astype(int)
                my_counts_w = my_w.astype(int)
                per_round_coop_w = opp_counts_w + my_counts_w
                hopeless_avg = float(np.mean(per_round_coop_w) / max(1, n))
            else:
                hopeless_avg = 1.0
        if hopeless_avg < 0.2:
            self.mode = 'DefectForever'
            self.punish_remaining = 0
            self.last_good_streak = 0
            return Action.D
        if self.mode == 'DefectForever':
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.mode = 'Cooperate'
                self.last_good_streak = 0
            return Action.D
        if c_prev >= self.theta:
            self.last_good_streak += 1
            return Action.C
        else:
            s = 1.0 - c_prev
            remaining_rounds = max(1, r - state.round_number)
            P = min(remaining_rounds, 1 + math.ceil(self.beta * s * n / 2.0))
            self.punish_remaining = max(0, int(P - 1))
            self.mode = 'Punish'
            self.last_good_streak = 0
            return Action.D

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    Adaptive ProberExploit (APE)

    Exploitative adaptive strategy for the N-player public goods game.
    - Probes for cooperators early.
    - Exploits when many others cooperate.
    - Uses short, limited retaliation and short repairs to avoid long punishment spirals.
    - Defaults to defect in low-cooperation environments, with occasional probes.
    - Endgame: always defect in the final EndgameWindow rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.phase = None
        self.phase_timer = 0
        self.last_probe_round = None
        self.promoted_to_conditional = False
        self.failed_last_repair = False
        self.last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        W = min(10, max(3, math.floor(r / 10)))
        ProbeRounds = min(3, max(1, math.floor(r / 10)))
        EndgameWindow = min(2, r)
        HighCoop = 0.75
        MidCoop = 0.4
        LowCoop = 0.2
        EpsilonProbe = min(0.1, 3 / max(1, r))
        RepairLength = 2
        RetaliateLength = min(3, max(1, math.floor(r / 10)))
        r_remaining = r - t + 1
        if t > r - EndgameWindow:
            self._advance_phase_timers()
            self.last_action = Action.D
            return Action.D
        if t <= ProbeRounds:
            self._advance_phase_timers()
            self.last_action = Action.C
            return Action.C
        if history is None:
            self._advance_phase_timers()
            self.last_action = Action.D
            return Action.D
        rounds_played = history.opponent_actions.shape[0]
        if rounds_played <= 0:
            self._advance_phase_timers()
            self.last_action = Action.D
            return Action.D
        window_len = min(W, rounds_played)
        recent_slice = slice(rounds_played - window_len, rounds_played)
        try:
            recent_op_actions = history.opponent_actions[recent_slice, :]
        except Exception:
            recent_op_actions = history.opponent_actions[-window_len:, :]
        if recent_op_actions.size == 0:
            coop_rates = np.zeros((history.opponent_actions.shape[1],), dtype=float)
        else:
            coop_rates = np.array(np.mean(recent_op_actions.astype(float), axis=0), dtype=float)
        groupCoopRate = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
        if self.phase is not None and self.phase_timer > 0:
            action = self._phase_action()
            self._advance_phase_timers()
            self.last_action = action
            return action
        my_actions = history.my_actions
        recent_my_slice = slice(max(0, rounds_played - RepairLength), rounds_played)
        my_recent = my_actions[recent_my_slice]
        my_recent_defected = np.any(my_recent == False)
        drop_after_last_my_defection = 0.0
        last_def_idx = None
        for idx in range(rounds_played - 1, -1, -1):
            if not bool(my_actions[idx]):
                last_def_idx = idx
                break
        if last_def_idx is not None:
            pre_end = last_def_idx - 1
            pre_start = max(0, pre_end - W + 1)
            post_start = last_def_idx + 1
            post_end = min(rounds_played - 1, post_start + W - 1)
            pre_len = pre_end - pre_start + 1
            post_len = post_end - post_start + 1
            if pre_len > 0 and post_len > 0:
                pre_slice = slice(pre_start, pre_end + 1)
                post_slice = slice(post_start, post_end + 1)
                pre_actions = history.opponent_actions[pre_slice, :]
                post_actions = history.opponent_actions[post_slice, :]
                pre_rate = float(np.mean(pre_actions.astype(float))) if pre_actions.size > 0 else 0.0
                post_rate = float(np.mean(post_actions.astype(float))) if post_actions.size > 0 else 0.0
                drop_after_last_my_defection = pre_rate - post_rate
        if self.last_probe_round is not None:
            if t - self.last_probe_round <= W:
                if groupCoopRate >= MidCoop:
                    self.promoted_to_conditional = True
                    self.last_probe_round = None
            else:
                self.last_probe_round = None
        if groupCoopRate >= HighCoop:
            if drop_after_last_my_defection >= 0.3 and last_def_idx is not None:
                if self.phase is None and self.phase_timer == 0:
                    self.phase = 'retaliate'
                    self.phase_timer = RetaliateLength
                    action = Action.D
                    self.last_action = action
                    return action
            probe_cond = r_remaining >= RepairLength + RetaliateLength and random.random() < 0.05
            if probe_cond:
                self.last_probe_round = t
                self.last_action = Action.C
                return Action.C
            self.last_action = Action.D
            return Action.D
        if MidCoop <= groupCoopRate < HighCoop or self.promoted_to_conditional:
            allow_coop = groupCoopRate >= MidCoop
            recent_my_actions = my_actions[recent_my_slice]
            majority_ok = True
            if recent_my_actions.size > 0 and np.any(recent_my_actions == True):
                coop_rounds_idx = []
                abs_start = max(0, rounds_played - RepairLength)
                for offset, val in enumerate(recent_my_actions):
                    if bool(val):
                        coop_rounds_idx.append(abs_start + offset)
                if len(coop_rounds_idx) > 0:
                    opp_actions_on_those = history.opponent_actions[coop_rounds_idx, :]
                    if opp_actions_on_those.size > 0:
                        per_op_rates = np.mean(opp_actions_on_those.astype(float), axis=0)
                        count_majority = np.sum(per_op_rates > 0.5)
                        majority_ok = count_majority >= per_op_rates.size / 2.0
                    else:
                        majority_ok = False
            if allow_coop and majority_ok and (not self.failed_last_repair):
                self.last_action = Action.C
                return Action.C
            else:
                if drop_after_last_my_defection >= 0.15 and last_def_idx is not None:
                    if groupCoopRate >= LowCoop:
                        self.phase = 'repair'
                        self.phase_timer = RepairLength
                        self.failed_last_repair = False
                        action = Action.C
                        self.last_action = action
                        return action
                    else:
                        self.failed_last_repair = True
                        self.last_action = Action.D
                        return Action.D
                self.last_action = Action.D
                return Action.D
        if LowCoop <= groupCoopRate < MidCoop:
            probe_prob = EpsilonProbe
            if random.random() < probe_prob:
                self.last_probe_round = t
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        probe_prob = EpsilonProbe / 2.0
        if random.random() < probe_prob:
            self.last_probe_round = t
            self.last_action = Action.C
            return Action.C
        self.last_action = Action.D
        return Action.D

    def _phase_action(self) -> Action:
        """
        Return the action associated with the current phase.
        Does NOT decrement timers; caller should call _advance_phase_timers afterwards.
        """
        if self.phase == 'retaliate':
            return Action.D
        if self.phase == 'repair':
            return Action.C
        return Action.D

    def _advance_phase_timers(self):
        """
        Advance (decrement) phase timers and handle transitions.
        To be called once per round after action selection (or before next decision).
        """
        if self.phase is None or self.phase_timer <= 0:
            return
        self.phase_timer -= 1
        if self.phase_timer > 0:
            return
        if self.phase == 'retaliate':
            self.phase = 'repair'
            self.phase_timer = 2
            return
        if self.phase == 'repair':
            self.phase = None
            self.phase_timer = 0
            return

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    Collective Conditional Cooperation with Proportional Punishment and Forgiveness (C3-P)

    Implements a forgiving but proportional punishment strategy for the repeated
    Public Goods Game. Starts by cooperating, monitors short-term and long-term
    cooperation rates, punishes proportionally when defection rises, and probes
    for restoration after punishment. Adapts thresholds for small groups (n=2)
    and very short games (r <= 3). Uses only observable history and game params.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.punishment_target: int = 0
        self.pending_probe: bool = False
        self.awaiting_probe_result: bool = False
        self.probe_round_index: int | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        current_round_index = int(state.round_number)
        t = current_round_index + 1
        if current_round_index == r - 1:
            self.pending_probe = False
            self.awaiting_probe_result = False
            return Action.D
        if current_round_index == 0 or history is None:
            return Action.C
        completed = current_round_index
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if completed <= 0:
            return Action.C
        num_opponents = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        group_size = 1 + num_opponents
        try:
            opp_counts = np.sum(opp_actions.astype(int), axis=1)
        except Exception:
            opp_counts = np.array([int(np.sum(row)) for row in opp_actions], dtype=int)
        my_counts = my_actions.astype(int)
        per_round_counts = opp_counts + my_counts
        per_round_fraction = per_round_counts.astype(float) / float(group_size)
        w_target = min(5, max(1, math.floor(r / 10)))
        recent_w = min(w_target, completed)
        recent_start = completed - recent_w
        recent_frac_values = per_round_fraction[recent_start:completed]
        recent_group_rate = float(np.mean(recent_frac_values)) if recent_frac_values.size > 0 else 0.0
        long_term_group_rate = float(np.mean(per_round_fraction)) if per_round_fraction.size > 0 else 0.0
        if num_opponents > 0:
            coop_rate_opponents = np.mean(opp_actions.astype(float), axis=0)
            coop_rate_opponents = coop_rate_opponents.astype(float)
        else:
            coop_rate_opponents = np.array([], dtype=float)
        coop_rate_self = float(np.mean(my_actions.astype(float)))
        coop_rates_all = np.concatenate(([coop_rate_self], coop_rate_opponents))
        core_threshold_count = math.ceil(0.5 * n)
        num_reliable = int(np.sum(coop_rates_all >= 0.5))
        num_high = int(np.sum(coop_rates_all >= 0.9))
        num_low = int(np.sum(coop_rates_all < 0.2))
        base_theta = 0.6 if n == 2 else 0.7
        endgame_increase = max(0.0, (r - t) / r * 0.2)
        theta = base_theta * (1.0 + endgame_increase)
        recovery_threshold = theta
        if self.awaiting_probe_result and self.probe_round_index is not None:
            if completed - 1 >= self.probe_round_index:
                probe_last_idx = self.probe_round_index
                probe_fraction = float(per_round_fraction[probe_last_idx])
                t_probe_1indexed = probe_last_idx + 1
                endgame_increase_probe = max(0.0, (r - t_probe_1indexed) / r * 0.2)
                theta_probe = base_theta * (1.0 + endgame_increase_probe)
                if probe_fraction >= theta_probe:
                    self.awaiting_probe_result = False
                    self.pending_probe = False
                    self.probe_round_index = None
                    self.punishment_counter = 0
                    self.punishment_target = 0
                else:
                    new_target = min(3, max(1, self.punishment_target + 1))
                    self.punishment_target = new_target
                    self.punishment_counter = new_target
                    self.punishment_counter = max(0, self.punishment_counter - 1)
                    self.awaiting_probe_result = False
                    self.pending_probe = False
                    self.probe_round_index = None
                    return Action.D
            else:
                return Action.D
        if self.pending_probe:
            self.pending_probe = False
            self.awaiting_probe_result = True
            self.probe_round_index = current_round_index
            return Action.C
        if long_term_group_rate < 0.3 and t > max(3, math.floor(r / 4)):
            return Action.D
        if self.punishment_counter > 0:
            if recent_group_rate >= recovery_threshold:
                self.punishment_counter = 0
                self.punishment_target = 0
                self.pending_probe = True
                self.awaiting_probe_result = False
                self.probe_round_index = None
            else:
                self.punishment_counter = max(0, self.punishment_counter - 1)
                if self.punishment_counter == 0:
                    self.pending_probe = True
                return Action.D
        if r <= 3:
            if np.any(per_round_fraction < 1.0):
                return Action.D
            return Action.C
        cooperate_decision = False
        if recent_group_rate >= theta:
            cooperate_decision = True
        else:
            cooperate_decision = False
        if not cooperate_decision and num_reliable >= core_threshold_count:
            margin = 0.05
            if recent_group_rate >= theta - margin:
                cooperate_decision = True
        if not cooperate_decision and num_high >= core_threshold_count:
            margin_high = 0.08
            if recent_group_rate >= theta - margin_high:
                cooperate_decision = True
        if num_low >= core_threshold_count:
            stricter_margin = 0.05
            if recent_group_rate < theta + stricter_margin:
                cooperate_decision = False
        if cooperate_decision:
            return Action.C
        last_idx = completed - 1
        last_round_fraction = float(per_round_fraction[last_idx])
        f = 1.0 - last_round_fraction
        if f < 0.25:
            punishment_target = 1
        else:
            punishment_target = min(3, 1 + int(round(3.0 * f)))
        self.punishment_target = punishment_target
        self.punishment_counter = punishment_target
        self.punishment_counter = max(0, self.punishment_counter - 1)
        if self.punishment_counter == 0:
            self.pending_probe = True
        else:
            self.pending_probe = False
        return Action.D

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    Exploitative Conditional-Cooperator (ECC)

    - Probes early by cooperating for a short T_probe.
    - Tracks opponents' cooperation rates and responsiveness to my defections.
    - Marks opponents as exploitable if they cooperate a lot and don't punish my defections.
    - When enough exploitable opponents exist, defect probabilistically to harvest,
      with occasional forgiveness cooperations.
    - If exploitation causes a measurable collapse in group cooperation, enter a rebuild
      mode of unconditional cooperation for several rounds and raise standards if collapse
      recurs.
    - Always defect in the final Endgame_length rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r_total = game_description.n_rounds
        self.k = game_description.k
        self.T_probe = min(6, max(2, math.floor(self.r_total / 6)))
        self.W = min(50, self.r_total)
        self.M = 2
        self.Exploitable_coop_rate = 0.75
        self.Responsiveness_threshold = 0.25
        self.Exploitation_threshold = max(1, math.floor((self.n - 1) / 3))
        self.Exploit_prob = 0.9
        self.Forgiveness_prob = 0.05
        self.Rebuild_length_initial = min(4, max(1, math.floor(self.r_total / 20)))
        self.Rebuild_length = self.Rebuild_length_initial
        self.Endgame_length = min(3, max(1, math.floor(self.r_total / 10)))
        self.rebuild_remaining = 0
        self.collapse_count = 0
        self.recent_rebuilds = 0
        self.last_rebuild_round = -999
        self.dynamic_exploitation_threshold = self.Exploitation_threshold

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r_total = self.r_total
        if r_total <= 3:
            if t == 0:
                return Action.C
            return Action.D
        if t >= r_total - self.Endgame_length:
            return Action.D
        if t == 0 or history is None:
            return Action.C
        rounds_so_far = history.my_actions.shape[0]
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        num_opponents = 0
        if opp_actions is not None and opp_actions.size:
            num_opponents = opp_actions.shape[1]
        else:
            num_opponents = n - 1
        group_coop_per_round = []
        for rr in range(rounds_so_far):
            opp_coops = 0
            if opp_actions.size:
                opp_coops = int(opp_actions[rr].sum())
            my_coop = 1 if bool(my_actions[rr]) else 0
            total_coops = opp_coops + my_coop
            group_coop_per_round.append(total_coops / max(1, n))
        group_coop_per_round = np.array(group_coop_per_round, dtype=float)
        if self.rebuild_remaining > 0:
            self.rebuild_remaining -= 1
            return Action.C
        if t < self.T_probe:
            return Action.C
        window_start = max(0, rounds_so_far - self.W)
        window_len = rounds_so_far - window_start if rounds_so_far > window_start else 0
        coop_rates = np.zeros(num_opponents, dtype=float)
        for j in range(num_opponents):
            if window_len <= 0:
                coop_rates[j] = 0.0
            else:
                slice_arr = opp_actions[window_start:rounds_so_far, j]
                coop_rates[j] = float(slice_arr.sum()) / float(window_len)
        responsiveness = np.zeros(num_opponents, dtype=float)
        my_defections = [idx for idx in range(rounds_so_far) if not bool(my_actions[idx])]
        for j in range(num_opponents):
            post_rates = []
            for def_idx in my_defections:
                post_start = def_idx + 1
                if post_start >= rounds_so_far:
                    continue
                post_end = min(rounds_so_far, post_start + self.M)
                post_slice = opp_actions[post_start:post_end, j]
                if post_slice.size > 0:
                    post_rates.append(float(post_slice.sum()) / float(post_slice.size))
            if len(post_rates) == 0:
                responsiveness[j] = 0.0
            else:
                avg_post = float(np.mean(post_rates))
                responsiveness[j] = max(0.0, coop_rates[j] - avg_post)
        exploitable_mask = (coop_rates >= self.Exploitable_coop_rate) & (responsiveness <= self.Responsiveness_threshold)
        num_exploitable = int(exploitable_mask.sum())
        collapse_detected = False
        for def_idx in my_defections:
            post_start = def_idx + 1
            if post_start >= rounds_so_far:
                continue
            post_end = min(rounds_so_far, post_start + self.M)
            post_slice = group_coop_per_round[post_start:post_end]
            if post_slice.size == 0:
                continue
            post_avg = float(post_slice.mean())
            pre_start = max(0, def_idx - 2)
            pre_end = def_idx
            if pre_end > pre_start:
                baseline_slice = group_coop_per_round[pre_start:pre_end]
                baseline = float(baseline_slice.mean())
            else:
                baseline = float(group_coop_per_round[def_idx])
            if baseline - post_avg > 0.3:
                if rounds_so_far - def_idx >= 0 and rounds_so_far - def_idx <= self.M + 2:
                    collapse_detected = True
                    break
        if collapse_detected:
            self.collapse_count += 1
            self.recent_rebuilds += 1
            if self.collapse_count > 1:
                self.dynamic_exploitation_threshold = min(n - 1, self.dynamic_exploitation_threshold + 1)
                self.Rebuild_length = min(6, self.Rebuild_length + 1)
            else:
                self.Rebuild_length = max(self.Rebuild_length_initial, self.Rebuild_length)
            self.rebuild_remaining = self.Rebuild_length
            self.last_rebuild_round = rounds_so_far
            self.rebuild_remaining -= 1
            return Action.C
        if rounds_so_far >= 2:
            recent_window = min(4, rounds_so_far - 1)
            baseline_idx_start = max(0, rounds_so_far - 1 - recent_window)
            baseline_idx_end = rounds_so_far - 1
            if baseline_idx_end > baseline_idx_start:
                baseline_group_mean = float(group_coop_per_round[baseline_idx_start:baseline_idx_end].mean())
            else:
                baseline_group_mean = float(group_coop_per_round[baseline_idx_end])
            last_group = float(group_coop_per_round[rounds_so_far - 1])
            group_drop = max(0.0, baseline_group_mean - last_group)
        else:
            baseline_group_mean = float(group_coop_per_round[-1])
            last_group = baseline_group_mean
            group_drop = 0.0
        threshold_to_use = max(1, self.dynamic_exploitation_threshold)
        if num_exploitable >= threshold_to_use:
            if group_drop <= 0.25:
                defect_prob = self.Exploit_prob
            else:
                defect_prob = max(0.5, self.Exploit_prob * 0.5)
            if random.random() < self.Forgiveness_prob:
                return Action.C
            if random.random() < defect_prob:
                return Action.D
            else:
                return Action.C
        else:
            if random.random() < self.Forgiveness_prob:
                return Action.D
            return Action.C

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    Selective Exploiter (SE)

    Adaptive public-goods strategy that:
    - Signals cooperation in round 1 to lure unconditional cooperators.
    - Defects in the last round.
    - Detects punishers (players who retaliate when I defect) and preserves cooperation
      if a threshold block of punishers exists.
    - Exploits when recent group cooperation is high and punishers are not numerous.
    - Cooperates to build/repair cooperation when recent cooperation is low.
    - Defects trivially if almost all opponents are chronic defectors.
    - Uses a small randomness epsilon to avoid deterministic cycles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punisher_threshold = 0.6
        self.sucker_threshold = 0.7
        self.epsilon = 0.03

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = state.round_number + 1
        if state.round_number == 0 or history is None:
            return Action.C
        my_actions = np.array(history.my_actions, dtype=bool)
        opp_actions = np.array(history.opponent_actions, dtype=bool)
        rounds_played = my_actions.shape[0]
        denom_for_coop = max(1, rounds_played)
        opp_coop_counts = np.sum(opp_actions[:rounds_played, :], axis=0).astype(float)
        coop_rate = opp_coop_counts / float(denom_for_coop)
        my_defect_flags = ~my_actions
        my_defect_count = int(np.sum(my_defect_flags))
        punish_counts = np.zeros(n - 1, dtype=float)
        if my_defect_count > 0 and rounds_played >= 1:
            if rounds_played >= 2:
                s_idx = np.arange(1, rounds_played, dtype=int)
                my_prev_defects = ~my_actions[s_idx - 1]
                opp_defects_at_s = ~opp_actions[s_idx, :]
                masked = opp_defects_at_s & my_prev_defects[:, None]
                punish_counts = np.sum(masked, axis=0).astype(float)
            else:
                punish_counts = np.zeros(n - 1, dtype=float)
            punish_rate = punish_counts / float(my_defect_count)
        else:
            punish_rate = np.zeros(n - 1, dtype=float)
        L = min(5, max(1, t - 1))
        recent_slice = opp_actions[-L:, :] if L <= opp_actions.shape[0] else opp_actions
        recent_group_coop = float(np.sum(recent_slice)) / float((n - 1) * L)
        P_thresh = max(1, math.floor((n - 1) / 4.0))
        punishers_mask = (punish_rate >= self.punisher_threshold) & (coop_rate >= 0.2)
        num_punishers = int(np.sum(punishers_mask))
        stage_coop_threshold = 0.6 if t <= r / 2.0 else 0.5
        total_opp_coop_rate_sum = float(np.sum(coop_rate))
        chronic_defector_threshold = 0.15 * (n - 1)
        if total_opp_coop_rate_sum <= chronic_defector_threshold:
            chosen = Action.D
        elif t == r:
            chosen = Action.D
        elif t == 1:
            chosen = Action.C
        elif num_punishers >= P_thresh:
            chosen = Action.C
        elif recent_group_coop >= stage_coop_threshold:
            if random.random() < 1.0 - self.epsilon:
                chosen = Action.D
            else:
                chosen = Action.C
        else:
            chosen = Action.C
        if random.random() < self.epsilon:
            chosen = Action.C if chosen == Action.D else Action.D
        return chosen

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Collective-Conditional Cooperate-with-Forgiveness (CCF)

    - Starts cooperative, defects in final round.
    - Uses a short sliding window to estimate recent cooperation.
    - Enters short proportional punishments when few cooperated recently.
    - Marks persistent defectors and can contain them (play D) when they form a majority.
    - Probes occasionally to re-establish cooperation after sustained defection.
    - Scales back cooperation in the small endgame window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(10, self.r)
        self.alpha = 0.5
        self.beta = 0.3
        self.m_max = 3
        self.gamma = 0.2
        self.L = min(3, max(1, math.floor(self.r / 10)))
        self.punish_remaining = 0
        self.punish_start_round = None
        self.containment_active = False
        self.long_defect_start = None
        self.probe_base_round = None
        self.last_probe_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        round_index = state.round_number
        rounds_observed = history.opponent_actions.shape[0]
        n_others = max(1, self.n - 1)
        window_size = min(self.w, rounds_observed)
        if rounds_observed >= 1:
            last_round_actions = history.opponent_actions[-1, :]
            if last_round_actions.size != n_others and last_round_actions.size > 0:
                f_recent = float(np.mean(history.opponent_actions[-1:, :]))
            else:
                f_recent = float(np.mean(last_round_actions)) if last_round_actions.size > 0 else 0.0
        else:
            f_recent = 0.0
        if window_size >= 1:
            recent_block = history.opponent_actions[-window_size:, :]
            F_window = float(np.mean(recent_block)) if recent_block.size > 0 else 0.0
        else:
            F_window = 0.0
        if window_size >= 1:
            per_player_rates = np.mean(history.opponent_actions[-window_size:, :], axis=0)
            persistent_mask = per_player_rates <= 0.2
            persistent_count = int(np.sum(persistent_mask))
        else:
            persistent_mask = np.array([False] * n_others)
            persistent_count = 0
        containment_threshold = math.ceil(self.n / 2)
        if persistent_count >= containment_threshold:
            self.containment_active = True
        else:
            pass
        if self.containment_active:
            non_persistent = (n_others - persistent_count) / float(n_others) if n_others > 0 else 0.0
            if non_persistent > 0.5 or F_window >= self.alpha:
                self.containment_active = False
            else:
                return Action.D
        if self.punish_remaining > 0:
            if F_window >= self.alpha:
                self.punish_remaining = 0
                self.punish_start_round = None
            else:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                if self.punish_remaining == 0:
                    self.punish_start_round = None
                return Action.D
        my_actions = history.my_actions
        my_consec_defects = 0
        if my_actions.size > 0:
            for val in my_actions[::-1]:
                if not bool(val):
                    my_consec_defects += 1
                else:
                    break
        if my_consec_defects >= self.m_max and F_window < self.alpha:
            if self.long_defect_start is None:
                self.long_defect_start = round_index
            if self.probe_base_round is None:
                self.probe_base_round = self.long_defect_start
        else:
            self.long_defect_start = None
            self.probe_base_round = None
        chosen_C = False
        if f_recent >= self.alpha:
            chosen_C = True
        elif f_recent >= self.beta and f_recent < self.alpha:
            chosen_C = True
        elif f_recent < self.beta:
            punished_rounds_so_far = 0
            if self.punish_start_round is not None:
                punished_rounds_so_far = max(0, round_index - self.punish_start_round)
            remaining_inclusive = max(1, self.r - round_index)
            m = min(self.m_max, remaining_inclusive, 1 + punished_rounds_so_far)
            self.punish_start_round = round_index if self.punish_start_round is None else self.punish_start_round
            self.punish_remaining = int(m)
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.punish_start_round = None
            return Action.D
        if not chosen_C:
            if self.long_defect_start is not None and self.probe_base_round is not None and (F_window < self.alpha):
                if (round_index - self.probe_base_round) % 3 == 0:
                    if random.random() < self.gamma:
                        chosen_C = True
                        self.last_probe_round = round_index
                    else:
                        return Action.D
                else:
                    return Action.D
            else:
                return Action.D
        endgame_remaining = self.r - round_index - 1
        in_endgame_scaleback = endgame_remaining < self.L and round_index < self.r - 1
        if chosen_C:
            if in_endgame_scaleback:
                if random.random() < 0.5:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    Conditional, forgiving reciprocal strategy for N-player Public Goods Game.

    - Starts by cooperating.
    - Defects in the final round.
    - In intermediate rounds, looks at up to the last W = min(5, r-1) completed rounds.
      A past round is "good" if a majority of the other players cooperated in that round.
      If a majority of those recent rounds are "good", cooperate.
      Otherwise, forgive a single lapse in the immediately preceding round (i.e. allow
      up to one other defector) and cooperate; if not, defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(0, self.n_rounds - 1))
        self.M = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == max(0, self.n_rounds - 1):
            return Action.D
        completed_rounds = state.round_number
        window_actual = min(self.W, completed_rounds)
        if window_actual <= 0:
            if completed_rounds >= 1:
                last_idx = completed_rounds - 1
                other_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
                if other_coops_last >= max(0, self.n_players - 2):
                    return Action.C
                return Action.D
            return Action.C
        start_idx = completed_rounds - window_actual
        end_idx = completed_rounds
        window_actions = history.opponent_actions[start_idx:end_idx, :]
        window_actions = np.atleast_2d(window_actions)
        other_coop_counts = np.sum(window_actions, axis=1)
        good_mask = other_coop_counts >= self.M
        G = int(np.sum(good_mask))
        needed_good = math.ceil(window_actual / 2)
        if G >= needed_good:
            return Action.C
        last_idx = completed_rounds - 1
        other_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
        if other_coops_last >= max(0, self.n_players - 2):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    Adaptive Probe-and-Exploit (APE) for the Public Goods Game.

    - Probes opponents early to estimate sensitivity to defections.
    - Classifies opponents and the group, and adapts:
      * Full exploitation if group does not punish.
      * Conditional cooperation with occasional opportunistic defections when safe.
      * Tit-for-tat style cautious play if punishers are present.
      * Conservative/noisy exploitation otherwise.
    - Always defects in the final round.
    - Uses only game parameters and observed history; adapts every round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.exploit_mode = False
        self.repair_counter = 0
        self.q_current = None
        self.punish_counter = 0
        self.last_q_chosen = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = state.round_number
        n = self.n
        r = self.r
        n_opponents = max(0, n - 1)
        if r <= 1:
            return Action.D
        if round_idx == 0 or history is None:
            return Action.C
        if round_idx == r - 1:
            return Action.D
        hist_rounds = len(history.my_actions)
        hist_rounds = max(0, min(hist_rounds, round_idx))
        probe_len = min(6, r)
        if round_idx < probe_len:
            if round_idx in (0, 1):
                return Action.C
            if round_idx == 2 and r >= 3:
                return Action.D
            return Action.C
        L = min(20, max(3, round_idx))
        trans_start = max(1, round_idx - L)
        trans_end = round_idx - 1
        trans_count = max(0, trans_end - trans_start + 1)
        overall_start = max(0, round_idx - L)
        overall_end = round_idx - 1
        overall_count = max(0, overall_end - overall_start + 1)
        coop_after_me1 = np.zeros(n_opponents, dtype=float)
        coop_after_me0 = np.zeros(n_opponents, dtype=float)
        overall_coop = np.zeros(n_opponents, dtype=float)
        if n_opponents == 0:
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        for j in range(n_opponents):
            if overall_count > 0:
                segment = opp_actions[overall_start:overall_end + 1, j]
                overall_coop[j] = float(np.mean(segment)) if segment.size > 0 else 0.0
            else:
                overall_coop[j] = 0.5
            denom1 = 0
            num1 = 0
            denom0 = 0
            num0 = 0
            if trans_count > 0:
                prev_actions = my_actions[trans_start - 1:trans_end]
                opp_segment = opp_actions[trans_start:trans_end + 1, j]
                for prev_a, opp_a in zip(prev_actions, opp_segment):
                    if bool(prev_a):
                        denom1 += 1
                        if bool(opp_a):
                            num1 += 1
                    else:
                        denom0 += 1
                        if bool(opp_a):
                            num0 += 1
            coop_after_me1[j] = float(num1) / denom1 if denom1 > 0 else float(overall_coop[j])
            coop_after_me0[j] = float(num0) / denom0 if denom0 > 0 else float(overall_coop[j])
        group_pC = float(np.mean(coop_after_me1))
        group_pD = float(np.mean(coop_after_me0))
        group_overall = float(np.mean(overall_coop))
        unconditional_mask = np.zeros(n_opponents, dtype=bool)
        conditional_mask = np.zeros(n_opponents, dtype=bool)
        punisher_mask = np.zeros(n_opponents, dtype=bool)
        indifferent_mask = np.zeros(n_opponents, dtype=bool)
        for j in range(n_opponents):
            oc = overall_coop[j]
            ca1 = coop_after_me1[j]
            ca0 = coop_after_me0[j]
            diff = ca1 - ca0
            if oc >= 0.9 and abs(ca1 - oc) <= 0.05 and (abs(ca0 - oc) <= 0.05):
                unconditional_mask[j] = True
            if diff >= 0.15:
                conditional_mask[j] = True
            if ca0 <= ca1 - 0.25:
                punisher_mask[j] = True
            if abs(diff) < 0.05 and abs(oc - 0.5) <= 0.15:
                indifferent_mask[j] = True
        num_unconditional = int(np.sum(unconditional_mask))
        num_conditional = int(np.sum(conditional_mask))
        num_punishers = int(np.sum(punisher_mask))
        if self.exploit_mode:
            return Action.D
        if group_pD >= group_pC - 0.05:
            self.exploit_mode = True
            return Action.D
        if round_idx == r - 2:
            many_unconditional = num_unconditional >= max(1, n_opponents // 2)
            if group_pD >= group_pC - 0.05 or many_unconditional:
                return Action.D
            if num_punishers >= 1:
                if r <= 3:
                    if group_pD >= group_pC - 0.05:
                        return Action.D
                    return Action.C
                return Action.C
        if group_overall >= 0.8 and group_pC - group_pD >= 0.15:
            if self.repair_counter > 0:
                self.repair_counter -= 1
                return Action.C
            gap = group_pC - group_pD
            q_base = min(0.2, gap / 2.0)
            if num_unconditional >= 1:
                q_base = min(0.2, q_base + 0.05 * num_unconditional)
                q_base = min(0.2, q_base)
            if self.q_current is None:
                self.q_current = q_base
            else:
                self.q_current = 0.6 * self.q_current + 0.4 * q_base
                self.q_current = min(0.2, max(0.0, self.q_current))
            if num_punishers >= 1:
                safe_to_defect = True
                if hist_rounds >= 2:
                    last_two = opp_actions[-2:, :]
                    for j in range(n_opponents):
                        if punisher_mask[j]:
                            if not (bool(last_two[0, j]) and bool(last_two[1, j])):
                                safe_to_defect = False
                                break
                else:
                    safe_to_defect = False
                if not safe_to_defect:
                    return Action.C
            if hist_rounds >= 2 and (not bool(my_actions[-1])):
                last_coop = float(np.mean(opp_actions[-1, :]))
                prev_coop = float(np.mean(opp_actions[-2, :]))
                if prev_coop - last_coop >= 0.25:
                    self.repair_counter = 3
                    if self.q_current is None:
                        self.q_current = q_base / 2.0
                    else:
                        self.q_current = max(0.0, self.q_current / 2.0)
                    return Action.C
            q_try = float(self.q_current)
            if num_unconditional >= 1:
                q_try = min(0.2, q_try + 0.05)
            self.last_q_chosen = q_try
            if random.random() < q_try:
                return Action.D
            return Action.C
        if num_punishers >= 1:
            if self.punish_counter > 0:
                self.punish_counter -= 1
                return Action.D
            if hist_rounds >= 1:
                last_round_ops = opp_actions[-1, :]
                all_coop_last = bool(np.all(last_round_ops))
                if all_coop_last:
                    return Action.C
                num_defectors_last = int(np.sum(~last_round_ops))
                P = min(3, num_defectors_last) if num_defectors_last > 0 else 0
                if P > 0:
                    self.punish_counter = P - 1
                    return Action.D
            return Action.C
        q0 = 0.05
        if group_pC - group_pD > 0.05:
            q0 *= 0.5
        if abs(group_pC - group_pD) <= 0.05:
            q0 = min(0.2, q0 * 1.5)
        if num_unconditional >= 1:
            q0 = min(0.2, q0 + 0.05)
        if num_punishers >= 1:
            q0 *= 0.7
        self.last_q_chosen = 0.6 * self.last_q_chosen + 0.4 * q0 if self.last_q_chosen is not None else q0
        q_try = self.last_q_chosen
        if num_punishers >= 1:
            safe_to_defect = True
            if hist_rounds >= 2:
                last_two = opp_actions[-2:, :]
                for j in range(n_opponents):
                    if punisher_mask[j]:
                        if not (bool(last_two[0, j]) and bool(last_two[1, j])):
                            safe_to_defect = False
                            break
            else:
                safe_to_defect = False
            if not safe_to_defect:
                return Action.C
        if random.random() < q_try:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    Opportunistic Probe-and-Exploit (OPE)

    Adaptive public-goods strategy that probes opponents to detect tolerance,
    exploits tolerant populations by defecting while occasionally cooperating
    (forgiveness) and probing, and switches to a reciprocal mode if defections
    are punished (population labeled "retaliatory"). Always defects in a short
    endgame window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.E = min(2, self.r)
        self.T_probe = 5
        self.delta = 0.2
        self.p_coop = 0.6
        self.q = 0.1
        self.targeted_info: dict[int, tuple[float, int]] = {}
        self.fallback_to_reciprocity_until: int = -1
        self._EPS = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t0 = state.round_number
        t = t0 + 1
        if self.r == 1:
            return Action.D
        if self.r <= self.E:
            return Action.D
        if t > self.r - self.E:
            return Action.D
        if t0 == 0:
            return Action.C
        if history is None:
            return Action.C
        rounds_played = history.my_actions.shape[0]
        T = min(self.W, rounds_played)
        if T > 0:
            opp_window = history.opponent_actions[-T:, :]
            my_window = history.my_actions[-T:]
            pay_window = history.my_payoffs[-T:]
        else:
            opp_window = np.zeros((0, self.n - 1), dtype=np.bool_)
            my_window = np.zeros((0,), dtype=np.bool_)
            pay_window = np.zeros((0,), dtype=np.float64)
        if T > 0:
            c_rate_j = np.array(opp_window, dtype=float).mean(axis=0)
        else:
            c_rate_j = np.zeros((self.n - 1,), dtype=float)
        if c_rate_j.size > 0:
            g_rate = float(np.mean(c_rate_j))
        else:
            g_rate = 0.0
        if rounds_played >= 1:
            last_opp_actions = history.opponent_actions[-1, :]
            others_contrib_last_round = int(np.sum(last_opp_actions))
        else:
            others_contrib_last_round = 0
        population_is_retaliatory = False
        if T > 0:
            others_coop_each_round = np.array(opp_window, dtype=float).mean(axis=1)
            cooperated_mask = np.array(my_window, dtype=bool)
            defected_mask = ~cooperated_mask
            if cooperated_mask.any() and defected_mask.any():
                mean_others_when_i_coop = float(np.mean(others_coop_each_round[cooperated_mask]))
                mean_others_when_i_defect = float(np.mean(others_coop_each_round[defected_mask]))
                if mean_others_when_i_coop - mean_others_when_i_defect > self.delta + self._EPS:
                    population_is_retaliatory = True
        if self.fallback_to_reciprocity_until > t0:
            population_is_retaliatory = True
        if T > 0:
            my_mean_payoff = float(np.mean(pay_window))
            others_counts = np.array(opp_window, dtype=int).sum(axis=1) if T > 0 else np.zeros((0,), dtype=int)
            mean_others_count = float(np.mean(others_counts)) if others_counts.size > 0 else 0.0
            est_defection_payoff = 1.0 + self.k / self.n * mean_others_count
            if my_mean_payoff + self._EPS < est_defection_payoff:
                self.fallback_to_reciprocity_until = max(self.fallback_to_reciprocity_until, t0 + self.W)
                population_is_retaliatory = True
        if len(self.targeted_info) > 0:
            for opp_idx, (prev_rate, targeted_round) in list(self.targeted_info.items()):
                curr_rate = float(c_rate_j[opp_idx]) if 0 <= opp_idx < c_rate_j.size else 0.0
                if prev_rate - curr_rate > self.delta + self._EPS:
                    self.fallback_to_reciprocity_until = max(self.fallback_to_reciprocity_until, t0 + self.W)
                    population_is_retaliatory = True
                    self.targeted_info.clear()
                    break
                if t0 - targeted_round > self.W:
                    self.targeted_info.pop(opp_idx, None)
        chosen_action = Action.D
        if population_is_retaliatory:
            threshold = math.ceil((self.n - 1) / 2)
            if others_contrib_last_round >= threshold:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
            return chosen_action
        unconditional_idxs = [i for i, rate in enumerate(c_rate_j) if rate >= 1.0 - 1e-08]
        will_target_idx = None
        for idx in unconditional_idxs:
            if idx not in self.targeted_info and t0 >= 1:
                will_target_idx = idx
                break
        do_probe_round = t % self.T_probe == 0
        if others_contrib_last_round >= 1:
            chosen_action = Action.D
        else:
            collapse_threshold = 1.0 / max(1, self.n - 1)
            if g_rate < collapse_threshold - self._EPS or others_contrib_last_round == 0:
                if random.random() < self.q:
                    chosen_action = Action.C
                else:
                    chosen_action = Action.D
            else:
                chosen_action = Action.D
        if do_probe_round and chosen_action == Action.C:
            chosen_action = Action.D
        if chosen_action == Action.D and random.random() < self.q:
            chosen_action = Action.C
        if will_target_idx is not None:
            chosen_action = Action.D
            prev_rate = float(c_rate_j[will_target_idx]) if 0 <= will_target_idx < c_rate_j.size else 0.0
            self.targeted_info[will_target_idx] = (prev_rate, t0)
        return chosen_action

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    Collective Gradual-Reciprocity (CGR) for the N-player Public Goods Game.

    - Starts cooperatively (except in very short games or one-shot).
    - Uses a sliding window to detect persistent defectors among opponents.
    - Applies short, proportional collective punishments when persistent defection is detected.
    - Forgives and tests after punishment, with a small epsilon chance to cooperate during punishment
      to help escape mutual-defection traps.
    - Handles finite horizon: defects in the final round and becomes cautious in the
      last few rounds before the final round according to the specification.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W = min(10, self.r)
        self.T_persist = math.ceil(0.6 * self.W)
        self.P_base = max(1, math.ceil(self.r / 10))
        if self.P_base > 3:
            self.P_base = 3
        self.P_max = 3
        self.Q = 2
        self.s = math.ceil(0.2 * self.n)
        self.epsilon = 0.05
        self.H = min(2, math.floor(self.r / 10))
        self.punishment_end_round = -1
        self.test_end_round = -1
        self.retaliate_until_round = -1
        self.last_S_size = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        idx = state.round_number
        n = self.n
        r = self.r

        def explore_cooperate() -> bool:
            return random.random() < self.epsilon
        if idx == r - 1:
            return Action.D
        in_cautious_phase = False
        if self.H > 0 and idx >= r - self.H and (idx <= r - 2):
            in_cautious_phase = True
        if idx == 0:
            if r == 1:
                return Action.D
            if r <= 3:
                return Action.D
            return Action.C
        if history is None:
            return Action.D
        last_idx = idx - 1
        if last_idx < 0:
            window_start = 0
        else:
            window_start = max(0, last_idx - self.W + 1)
        window_end_inclusive = last_idx
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        try:
            my_last_action = bool(my_actions[last_idx])
        except Exception:
            my_last_action = False
        try:
            opp_last = opp_actions[last_idx, :]
        except Exception:
            opp_last = np.array([False] * max(0, n - 1), dtype=np.bool_)
        D_last = (0 if my_last_action else 1) + int(np.sum(np.logical_not(opp_last)))
        S_indices = set()
        if window_end_inclusive >= window_start:
            if opp_actions.shape[0] > 0:
                window_opp = opp_actions[window_start:window_end_inclusive + 1, :]
                defect_counts = np.sum(np.logical_not(window_opp), axis=0)
                for opp_idx, cnt in enumerate(defect_counts):
                    if int(cnt) >= self.T_persist:
                        S_indices.add(opp_idx)
        else:
            S_indices = set()
        S_size = len(S_indices)
        if in_cautious_phase:
            no_defections = True
            every_other_perfect = True
            if window_end_inclusive < window_start:
                no_defections = True
                every_other_perfect = True
            else:
                my_window = my_actions[window_start:window_end_inclusive + 1]
                if my_window.size > 0 and np.any(np.logical_not(my_window)):
                    no_defections = False
                if opp_actions.shape[0] > 0:
                    opp_window = opp_actions[window_start:window_end_inclusive + 1, :]
                    if np.any(np.logical_not(opp_window)):
                        no_defections = False
                    if not np.all(np.all(opp_window, axis=0)):
                        every_other_perfect = False
                else:
                    every_other_perfect = False
            if no_defections or every_other_perfect:
                if idx < self.punishment_end_round:
                    if explore_cooperate():
                        return Action.C
                    return Action.D
                if idx < self.retaliate_until_round:
                    return Action.D
                if idx < self.test_end_round:
                    return Action.C
                return Action.C
            else:
                return Action.D
        if S_size == 0:
            if idx < self.punishment_end_round:
                if explore_cooperate():
                    return Action.C
                return Action.D
            if idx < self.test_end_round:
                return Action.C
            if idx < self.retaliate_until_round:
                return Action.D
            if D_last == 0:
                return Action.C
            if 0 < D_last <= self.s:
                return Action.C
            if idx >= self.retaliate_until_round:
                self.retaliate_until_round = idx + 1
            return Action.D
        if idx < self.punishment_end_round:
            if explore_cooperate():
                return Action.C
            return Action.D
        if idx < self.test_end_round:
            return Action.C
        begin_punishment = False
        if self.last_S_size == 0 and S_size > 0:
            begin_punishment = True
        elif self.test_end_round != -1 and idx >= self.test_end_round and (S_size > 0):
            begin_punishment = True
        elif self.punishment_end_round <= idx and self.test_end_round <= idx:
            begin_punishment = True
        if begin_punishment:
            P = min(self.P_base + max(0, S_size - 1), self.P_max)
            self.punishment_end_round = idx + P
            self.test_end_round = self.punishment_end_round + self.Q
            if explore_cooperate():
                return Action.C
            return Action.D
        return Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        idx = state.round_number
        n = self.n
        r = self.r

        def explore_cooperate() -> bool:
            return random.random() < self.epsilon
        if idx == r - 1:
            return Action.D
        in_cautious_phase = False
        if self.H > 0 and idx >= r - self.H and (idx <= r - 2):
            in_cautious_phase = True
        if idx == 0:
            if r == 1:
                return Action.D
            if r <= 3:
                return Action.D
            return Action.C
        if history is None:
            return Action.D
        last_idx = idx - 1
        if last_idx < 0:
            window_start = 0
        else:
            window_start = max(0, last_idx - self.W + 1)
        window_end_inclusive = last_idx
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        try:
            my_last_action = bool(my_actions[last_idx])
        except Exception:
            my_last_action = False
        try:
            opp_last = opp_actions[last_idx, :]
        except Exception:
            opp_last = np.array([False] * max(0, n - 1), dtype=np.bool_)
        D_last = (0 if my_last_action else 1) + int(np.sum(np.logical_not(opp_last)))
        S_indices = set()
        if window_end_inclusive >= window_start:
            if opp_actions.shape[0] > 0:
                window_opp = opp_actions[window_start:window_end_inclusive + 1, :]
                defect_counts = np.sum(np.logical_not(window_opp), axis=0)
                for opp_idx, cnt in enumerate(defect_counts):
                    if int(cnt) >= self.T_persist:
                        S_indices.add(opp_idx)
        S_size = len(S_indices)
        if in_cautious_phase:
            no_defections = True
            every_other_perfect = True
            if window_end_inclusive < window_start:
                no_defections = True
                every_other_perfect = True
            else:
                my_window = my_actions[window_start:window_end_inclusive + 1]
                if my_window.size > 0 and np.any(np.logical_not(my_window)):
                    no_defections = False
                if opp_actions.shape[0] > 0:
                    opp_window = opp_actions[window_start:window_end_inclusive + 1, :]
                    if np.any(np.logical_not(opp_window)):
                        no_defections = False
                    if not np.all(np.all(opp_window, axis=0)):
                        every_other_perfect = False
                else:
                    every_other_perfect = False
            self.last_S_size = S_size
            if no_defections or every_other_perfect:
                if idx < self.punishment_end_round:
                    if explore_cooperate():
                        return Action.C
                    return Action.D
                if idx < self.retaliate_until_round:
                    return Action.D
                if idx < self.test_end_round:
                    return Action.C
                return Action.C
            else:
                return Action.D
        if S_size == 0:
            if idx < self.punishment_end_round:
                self.last_S_size = S_size
                if explore_cooperate():
                    return Action.C
                return Action.D
            if idx < self.test_end_round:
                self.last_S_size = S_size
                return Action.C
            if idx < self.retaliate_until_round:
                self.last_S_size = S_size
                return Action.D
            self.last_S_size = S_size
            if D_last == 0:
                return Action.C
            if 0 < D_last <= self.s:
                return Action.C
            if idx >= self.retaliate_until_round:
                self.retaliate_until_round = idx + 1
            return Action.D
        if idx < self.punishment_end_round:
            self.last_S_size = S_size
            if explore_cooperate():
                return Action.C
            return Action.D
        if idx < self.test_end_round:
            self.last_S_size = S_size
            return Action.C
        begin_punishment = False
        if self.last_S_size == 0 and S_size > 0:
            begin_punishment = True
        elif self.test_end_round != -1 and idx >= self.test_end_round and (S_size > 0):
            begin_punishment = True
        elif self.punishment_end_round <= idx and self.test_end_round <= idx:
            begin_punishment = True
        if begin_punishment:
            P = min(self.P_base + max(0, S_size - 1), self.P_max)
            self.punishment_end_round = idx + P
            self.test_end_round = self.punishment_end_round + self.Q
            self.last_S_size = S_size
            if explore_cooperate():
                return Action.C
            return Action.D
        self.last_S_size = S_size
        return Action.C

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Forgiving proportional conditional cooperator for the N-player Public Goods Game.

    - Starts by cooperating.
    - Tracks a recent window of other-players' cooperation rates.
    - Rewards high cooperation (>= TH_H), uses a probabilistic soft zone between TH_L and TH_H,
      and issues temporary proportionate punishments when cooperation is clearly broken (< TH_L).
    - Forgives quickly when cooperation is restored for two rounds.
    - Tolerates isolated noise (requires >=2 bad rounds in the window or a large one-round drop to punish).
    - Has a simple targeted-defector handling: if a small set of persistent defectors exist,
      evaluate cooperation excluding them when deciding whether the rest of the group is cooperative.
    - Applies an endgame rule on the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        self.TH_H = 0.7
        self.TH_L = 0.4
        self.L_max = 4
        self.L_min = 1
        self.large_drop_threshold = 0.5
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opp_actions = history.opponent_actions
        num_past = int(opp_actions.shape[0])
        if num_past == 0:
            return Action.C
        window_size = min(self.W, num_past)
        window_rows = opp_actions[-window_size:, :]
        per_round_fracs = np.mean(window_rows.astype(float), axis=1) if window_size > 0 else np.array([], dtype=float)
        recent_coop_rate = float(np.mean(per_round_fracs)) if per_round_fracs.size > 0 else 0.0
        per_opponent_rates = np.mean(window_rows.astype(float), axis=0) if window_size > 0 else np.array([], dtype=float)
        persistent_mask = per_opponent_rates < 0.2
        m = int(np.sum(persistent_mask))
        if m > 0:
            remaining_mask = ~persistent_mask
            if remaining_mask.any():
                remaining_mean = float(np.mean(per_opponent_rates[remaining_mask]))
            else:
                remaining_mean = 0.0
            if remaining_mean >= self.TH_H:
                recent_coop_rate = remaining_mean
        if state.round_number == self.r - 1:
            last_frac = float(per_round_fracs[-1]) if per_round_fracs.size > 0 else 0.0
            if last_frac >= self.TH_H:
                return Action.C
            return Action.D
        if self.punish_remaining > 0:
            if per_round_fracs.size >= 2:
                if float(per_round_fracs[-1]) >= self.TH_H and float(per_round_fracs[-2]) >= self.TH_H:
                    self.punish_remaining = 0
                    return Action.C
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return action
        if num_past < 2:
            return Action.C
        if recent_coop_rate >= self.TH_H:
            return Action.C
        if recent_coop_rate >= self.TH_L:
            try:
                prob = float(recent_coop_rate)
            except Exception:
                prob = 0.0
            if random.random() < prob:
                return Action.C
            return Action.D
        bad_count = int(np.sum(per_round_fracs < self.TH_L)) if per_round_fracs.size > 0 else 0
        large_drop = False
        if per_round_fracs.size >= 2:
            prev_frac = float(per_round_fracs[-2])
            last_frac = float(per_round_fracs[-1])
            if prev_frac - last_frac >= self.large_drop_threshold:
                large_drop = True
        if bad_count >= 2 or large_drop:
            raw_severity = (self.TH_L - recent_coop_rate) / 0.1
            s = int(math.ceil(raw_severity)) if raw_severity > 0 else self.L_min
            s = max(self.L_min, min(self.L_max, s))
            self.punish_remaining = s
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    Exploitative adaptive strategy for the N-player Public Goods Game.

    - Probes early with an alternating C/D pattern to estimate opponents' conditional
      cooperation probabilities.
    - Classifies opponents as Unconditional / Conditional / Defector / Uncertain.
    - If any Unconditional cooperators exist, exploits them by defecting.
    - Otherwise decides to cooperate only when immediate gain + simple future inducement
      estimate is positive. Always defects in final round.
    - Occasionally probes or forgives with small probability to avoid permanent isolation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = self.k / max(1, self.n)
        self.probe_rounds = int(min(4, max(2, math.floor(self.r / 10))))
        self.th_uncond_coop = 0.85
        self.th_resp_low = 0.05
        self.th_resp_cond = 0.15
        self.th_defector_coop = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = int(state.round_number)
        if t_idx == self.r - 1:
            return Action.D
        if self.r <= 2:
            return Action.D
        if history is None:
            return Action.C if t_idx % 2 == 0 and t_idx < self.probe_rounds or (t_idx % 2 == 0 and self.probe_rounds <= 1) else Action.D
        num_rounds = int(history.opponent_actions.shape[0]) if history.opponent_actions is not None else 0
        n_others = max(0, self.n - 1)
        if t_idx < self.probe_rounds:
            return Action.C if t_idx % 2 == 0 else Action.D
        if n_others == 0:
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if num_rounds <= 0:
            coop_rates = np.zeros(n_others, dtype=float)
        else:
            coop_rates = np.array(np.mean(opp_actions.astype(float), axis=0), dtype=float)
        p_c_given_i_c = np.zeros(n_others, dtype=float)
        p_c_given_i_d = np.zeros(n_others, dtype=float)
        if num_rounds >= 2:
            prev_my = np.array(my_actions, dtype=bool)
            counts_if_ic = np.zeros(n_others, dtype=float)
            denom_if_ic = 0.0
            counts_if_id = np.zeros(n_others, dtype=float)
            denom_if_id = 0.0
            for t in range(1, num_rounds):
                if prev_my[t - 1]:
                    denom_if_ic += 1.0
                    counts_if_ic += opp_actions[t, :].astype(float)
                else:
                    denom_if_id += 1.0
                    counts_if_id += opp_actions[t, :].astype(float)
            if denom_if_ic > 0:
                p_c_given_i_c = counts_if_ic / denom_if_ic
            else:
                p_c_given_i_c = np.array(coop_rates, dtype=float)
            if denom_if_id > 0:
                p_c_given_i_d = counts_if_id / denom_if_id
            else:
                p_c_given_i_d = np.array(coop_rates, dtype=float)
        else:
            p_c_given_i_c = np.array(coop_rates, dtype=float)
            p_c_given_i_d = np.array(coop_rates, dtype=float)
        responsiveness = p_c_given_i_c - p_c_given_i_d
        avg_other_coop = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
        avg_resp = float(np.mean(responsiveness)) if responsiveness.size > 0 else 0.0
        estimated_E_others_if_I_C = float(np.sum(p_c_given_i_c))
        estimated_E_others_if_I_D = float(np.sum(p_c_given_i_d))
        uncond_mask = (coop_rates >= self.th_uncond_coop) & (responsiveness <= self.th_resp_low)
        cond_mask = (responsiveness >= self.th_resp_cond) & (coop_rates >= 0.25)
        def_mask = (coop_rates <= self.th_defector_coop) & (responsiveness <= self.th_resp_low)
        n_uncond = int(np.sum(uncond_mask))
        n_cond = int(np.sum(cond_mask))
        n_def = int(np.sum(def_mask))
        if n_uncond >= 1:
            return Action.D
        if n_def > n_others / 2.0 and n_cond == 0:
            return Action.D
        payoff_C = 0.0 + self.alpha * (estimated_E_others_if_I_C + 1.0)
        payoff_D = 1.0 + self.alpha * estimated_E_others_if_I_D
        immediate_diff = payoff_C - payoff_D
        delta = estimated_E_others_if_I_C - estimated_E_others_if_I_D
        remaining = float(self.r - t_idx)
        future_benefit = delta * self.alpha * max(0.0, remaining - 1.0)
        cooperate_decision = immediate_diff + future_benefit > 0.0
        probe_play = False
        recent_window = min(3, num_rounds) if num_rounds > 0 else 0
        if recent_window >= 1:
            recent_start = max(0, num_rounds - recent_window)
            recent_opp = opp_actions[recent_start:num_rounds, :] if num_rounds > 0 else np.zeros((0, n_others))
            recent_avg = float(np.mean(recent_opp.astype(float))) if recent_opp.size > 0 else 0.0
            if recent_avg < 0.1:
                p_probe = min(0.1, 2.0 / max(1.0, remaining))
                if random.random() < p_probe:
                    probe_play = True
        forgive_probe = False
        if num_rounds >= 2:
            my_last = bool(my_actions[-1])
            my_prev = bool(my_actions[-2])
            if not my_last and (not my_prev):
                last_window = int(min(2, num_rounds))
                prior_end = max(0, num_rounds - last_window)
                prior_start = max(0, prior_end - 4)
                last_avg = float(np.mean(opp_actions[num_rounds - last_window:num_rounds, :].astype(float)))
                prior_slice = opp_actions[prior_start:prior_end, :] if prior_end - prior_start > 0 else np.zeros((0, n_others))
                prior_avg = float(np.mean(prior_slice.astype(float))) if prior_slice.size > 0 else last_avg
                if prior_avg - last_avg > 0.08:
                    if random.random() < 0.25:
                        forgive_probe = True
        insufficient_data = num_rounds < max(2, self.probe_rounds)
        if probe_play or forgive_probe:
            return Action.C
        if insufficient_data:
            if immediate_diff + future_benefit > 0.05:
                return Action.C
            return Action.D
        return Action.C if cooperate_decision else Action.D

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Starts cooperative to signal willingness.
    - Cooperates when a majority cooperated in the previous round or recent
      average cooperation among others is high.
    - If the player cooperated but a substantial fraction of others defected,
      issues a short, proportional punishment (defect for punish_length rounds).
    - Forgives quickly if recent cooperation rate is high.
    - While defecting, occasionally (with probe_prob) probe by cooperating to test recovery.
    - Always defects in the final round (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(0, self.r))
        self.majority_fraction = 0.5
        self.punish_length = 1
        if self.r <= 3:
            self.punish_length = 0
        self.punish_remaining = 0
        self.probe_prob = min(0.3, 0.05 + 0.3 * (self.k / max(1.0, float(self.n))))
        self.forgiveness_threshold = self.majority_fraction
        self.final_rounds_to_guard = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if t == self.r - 1:
            return Action.D
        if t == 0 or history is None or history.my_actions.size == 0:
            return Action.C
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        rounds_done = int(history.my_actions.shape[0])
        w_eff = min(self.w, rounds_done)
        n_opponents = max(0, self.n - 1)
        if w_eff <= 0 or n_opponents == 0:
            other_coop_rate = 0.0
        else:
            start = rounds_done - w_eff
            window_opponent_slice = history.opponent_actions[start:rounds_done, :]
            total_other_coops = int(np.sum(window_opponent_slice))
            other_coop_rate = total_other_coops / (w_eff * n_opponents)
        last_idx = rounds_done - 1
        my_prev = bool(history.my_actions[last_idx])
        last_round_other_coops = int(np.sum(history.opponent_actions[last_idx, :]))
        last_round_S = last_round_other_coops + (1 if my_prev else 0)
        majority_threshold = math.ceil(self.n * self.majority_fraction)
        if last_round_S >= majority_threshold:
            return Action.C
        if n_opponents > 0:
            last_round_other_defectors = n_opponents - last_round_other_coops
            defectors_threshold = math.ceil(n_opponents * (1.0 / 3.0))
        else:
            last_round_other_defectors = 0
            defectors_threshold = 0
        if self.punish_length > 0 and my_prev and (last_round_other_defectors >= defectors_threshold) and (last_round_S < majority_threshold):
            if self.punish_length <= 1:
                self.punish_remaining = 0
            else:
                self.punish_remaining = self.punish_length - 1
            return Action.D
        if other_coop_rate >= self.forgiveness_threshold:
            return Action.C
        if random.random() < float(self.probe_prob):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    - Probes early to estimate opponents' responsiveness to my cooperation.
    - Computes smoothed P_j(coop | I defect) and P_j(coop | I coop).
    - Cooperates only when induced increase (Delta) makes cooperation personally profitable,
      with safety rules to free-ride on persistent cooperators and to avoid endgame traps.
    - Occasional low-probability re-probing if group cooperation has collapsed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        if self.r < 4:
            self.m = 1
        else:
            self.m = min(6, max(2, math.floor(self.r / 10)))
        self.endgame_len = min(3, max(1, math.floor(self.r / 10)))
        self.smoothing_num = 1.0
        self.smoothing_den = 2.0
        self.gamma_endgame = 1.5
        self.L_collapsed = 2
        self.K_defect_streak = 3
        self.p_probe = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        n = self.n
        r = self.r
        k = self.k
        if state.round_number == 0 or history is None:
            return Action.D
        T = int(history.my_actions.shape[0])
        if t <= self.m:
            if self.m == 1 or t % 2 == 1:
                return Action.D
            else:
                return Action.C
        if t == r:
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opponents = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        if n_opponents == 0:
            n_opponents = max(0, n - 1)
        A_j_CD = [0] * n_opponents
        N_j_CD = [0] * n_opponents
        A_j_CC = [0] * n_opponents
        N_j_CC = [0] * n_opponents
        for round_idx in range(T):
            my_coop = bool(my_actions[round_idx])
            for j in range(n_opponents):
                try:
                    opp_coop = bool(opp_actions[round_idx, j])
                except Exception:
                    opp_coop = False
                if my_coop:
                    N_j_CC[j] += 1
                    if opp_coop:
                        A_j_CC[j] += 1
                else:
                    N_j_CD[j] += 1
                    if opp_coop:
                        A_j_CD[j] += 1
        P_cd = [0.0] * n_opponents
        P_cc = [0.0] * n_opponents
        for j in range(n_opponents):
            denom_cd = N_j_CD[j] + self.smoothing_den
            P_cd[j] = (A_j_CD[j] + self.smoothing_num) / denom_cd
            denom_cc = N_j_CC[j] + self.smoothing_den
            P_cc[j] = (A_j_CC[j] + self.smoothing_num) / denom_cc
        E_others_if_I_defect = float(sum(P_cd))
        Delta = float(sum((P_cc[j] - P_cd[j] for j in range(n_opponents))))
        target_others = max(1, round((n - 1) * 0.6))
        if E_others_if_I_defect >= float(target_others):
            return Action.D
        unresponsive_detected = False
        if T >= 6 and n_opponents > 0:
            third = max(1, T // 3)
            early_slice = slice(0, third)
            late_slice = slice(max(third, T - third), T)
            for j in range(n_opponents):
                early_A_CC = 0
                early_N_CC = 0
                for idx in range(early_slice.start, early_slice.stop):
                    if bool(my_actions[idx]):
                        early_N_CC += 1
                        if bool(opp_actions[idx, j]):
                            early_A_CC += 1
                late_A_CC = 0
                late_N_CC = 0
                for idx in range(late_slice.start, late_slice.stop):
                    if bool(my_actions[idx]):
                        late_N_CC += 1
                        if bool(opp_actions[idx, j]):
                            late_A_CC += 1
                P_early = (early_A_CC + self.smoothing_num) / (early_N_CC + self.smoothing_den)
                P_late = (late_A_CC + self.smoothing_num) / (late_N_CC + self.smoothing_den)
                if P_early - P_late >= 0.5:
                    unresponsive_detected = True
                    break
        if unresponsive_detected:
            return Action.D
        pi_def = 1.0 + k / n * E_others_if_I_defect
        pi_coop = 0.0 + k / n * (E_others_if_I_defect + Delta + 1.0)
        base_threshold = n / k - 1.0
        threshold = base_threshold
        if t > r - self.endgame_len:
            threshold = threshold * self.gamma_endgame
        cooperate_decision = Delta > threshold
        probing_override = False
        consec_my_defections = 0
        for idx in range(T - 1, -1, -1):
            if not bool(my_actions[idx]):
                consec_my_defections += 1
            else:
                break
        L = self.L_collapsed
        if T >= L:
            total_contribs = 0.0
            for idx in range(max(0, T - L), T):
                row_sum = 0
                try:
                    row_sum = int(bool(my_actions[idx])) + int(np.sum(opp_actions[idx, :]))
                except Exception:
                    row_sum = int(bool(my_actions[idx]))
                total_contribs += row_sum
            avg_contribs = total_contribs / float(L)
        else:
            avg_contribs = float('inf')
        if consec_my_defections >= self.K_defect_streak and avg_contribs < 0.1 * n:
            if random.random() < self.p_probe:
                probing_override = True
        if cooperate_decision:
            return Action.C
        else:
            if probing_override:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Collective Conditional Cooperator with Proportional Punishment and Forgiveness (C3P)

    - Starts by cooperating and defects in the final (last) round.
    - Rewards full or majority cooperation by cooperating and incrementing a recent cooperation run counter.
    - If cooperation falls below majority, enters a proportional punishment for a number of rounds equal
      to the number of defectors (at least 1). During punishment it defects.
    - After punishment ends it probes with a cooperative move, then evaluates the group's response; if the
      group reciprocates (majority cooperated during the probe), it forgives and resumes cooperation,
      otherwise it re-enters punishment.
    - If there has been a long recent run of cooperation (recent_coop_run >= 4), single defections receive
      only 1-round punishment to avoid destabilizing cooperation.
    - Near the end of the game (last two rounds) it is conservative and defects in the final round and
      may prefer defect in round r-1 if a sudden severe collapse in cooperation is detected.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.recent_coop_run: int = 0
        self.probe_stage: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r_total = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        if t == r_total - 1:
            return Action.D
        prev_my_coop = bool(history.my_actions[-1])
        prev_opponent_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        C_prev = (1 if prev_my_coop else 0) + prev_opponent_coops
        defectors_prev = max(0, n - C_prev)
        majority_prev = C_prev >= math.ceil(n / 2)
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter -= 1
            self.recent_coop_run = 0
            if self.punishment_counter == 0:
                self.probe_stage = 1
            return action
        if self.probe_stage == 1:
            self.probe_stage = 2
            return Action.C
        if self.probe_stage == 2:
            if majority_prev:
                self.recent_coop_run = 1
                self.probe_stage = 0
                return Action.C
            else:
                self.punishment_counter = max(1, defectors_prev)
                self.recent_coop_run = 0
                self.probe_stage = 0
                self.punishment_counter -= 1
                if self.punishment_counter == 0:
                    self.probe_stage = 1
                return Action.D
        rounds_left = r_total - t
        if t >= r_total - 2:
            if t == r_total - 2:
                start_idx = max(0, t - 1 - 3)
                end_idx = t - 1
                prior_counts = []
                for idx in range(start_idx, end_idx):
                    my_c = bool(history.my_actions[idx])
                    opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
                    prior_counts.append((1 if my_c else 0) + opp_c)
                if prior_counts:
                    prior_avg = float(np.mean(prior_counts))
                    drop_threshold = max(1, math.ceil(n / 4))
                    if prior_avg - C_prev >= drop_threshold:
                        self.recent_coop_run = 0
                        return Action.D
        if C_prev == n:
            self.recent_coop_run += 1
            return Action.C
        if majority_prev:
            self.recent_coop_run += 1
            return Action.C
        if self.recent_coop_run >= 4 and defectors_prev == 1:
            self.punishment_counter = 1
        else:
            self.punishment_counter = max(1, defectors_prev)
        self.recent_coop_run = 0
        self.punishment_counter -= 1
        if self.punishment_counter == 0:
            self.probe_stage = 1
        return Action.D

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    Calculated ProberPunisher (CPP)

    - Cooperate for a short burn-in to attract reciprocators.
    - Track recent others' cooperation over a sliding window W.
    - When cooperation is stably high, perform single-round probes (one-shot defections).
      After a probe, observe the next W rounds; if others' cooperation drops by more than delta,
      enter a deterministic punishment phase of length P (defect-only) to discourage exploitation.
    - Play conditional cooperation when cooperation is moderate.
    - Defect when cooperation is low.
    - Always defect in the last F rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = max(1, math.floor(math.sqrt(r)))
        self.B = min(3, r)
        self.F = min(3, r)
        self.theta_high = 0.9
        self.theta_low = 0.5
        self.delta = 0.15
        self.P = self.W
        self.punishment_remaining = 0
        self.last_probe_round = None
        self.awaiting_probe_outcome = False
        self.pre_probe_others_rate = 0.0
        self.probe_cooldown = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        W = self.W
        if t == 0 or history is None:
            return Action.C
        history_len = history.my_actions.shape[0]

        def others_coop_rate_at_decision(end_t: int) -> float:
            if end_t <= 0:
                return 0.0
            start = max(0, end_t - W)
            window = history.opponent_actions[start:end_t, :]
            window_len = end_t - start
            if window_len <= 0:
                return 0.0
            total_coop = float(np.sum(window))
            denom = float((n - 1) * window_len)
            if denom <= 0.0:
                return 0.0
            return total_coop / denom
        if self.awaiting_probe_outcome and self.last_probe_round is not None:
            p = int(self.last_probe_round)
            if history_len >= p + W + 1:
                pre_rate = float(self.pre_probe_others_rate)
                start = p + 1
                end = start + W
                if start < 0:
                    start = 0
                if end > history_len:
                    end = history_len
                window_len = max(0, end - start)
                if window_len == 0:
                    post_rate = 0.0
                else:
                    window = history.opponent_actions[start:end, :]
                    total_coop = float(np.sum(window))
                    denom = float((n - 1) * window_len)
                    post_rate = total_coop / denom if denom > 0.0 else 0.0
                drop = pre_rate - post_rate
                self.awaiting_probe_outcome = False
                if drop > self.delta:
                    self.punishment_remaining = int(self.P)
                    self.probe_cooldown = True
                else:
                    self.probe_cooldown = True
        if t >= r - self.F:
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        if t < self.B:
            return Action.C
        others_rate_now = others_coop_rate_at_decision(t)
        stable_for = 0
        s = t
        while s > 0:
            rate_s = others_coop_rate_at_decision(s)
            if rate_s >= self.theta_high:
                stable_for += 1
                s -= 1
            else:
                break
        if self.probe_cooldown and self.last_probe_round is not None:
            stable_start_round = t - stable_for
            if stable_for >= self.W and self.last_probe_round < stable_start_round:
                self.probe_cooldown = False
        if others_rate_now >= self.theta_high:
            if stable_for >= self.W:
                can_probe = not self.probe_cooldown and (not self.awaiting_probe_outcome)
                if can_probe:
                    self.pre_probe_others_rate = float(others_rate_now)
                    self.last_probe_round = t
                    self.awaiting_probe_outcome = True
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.C
        elif others_rate_now >= self.theta_low:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR)

    - Starts by cooperating.
    - Uses a sliding window of recent rounds to estimate per-player and group cooperation rates.
    - Cooperates when recent group cooperation is high.
    - In low-group-cooperation periods, applies short, proportional punishments whose length depends
      on how many players are persistently defecting (n_bad). Punishments are cancellable if players
      recover within the window.
    - In ambiguous cases, mixes cooperation/defection probabilistically and occasionally probes with
      a small probability eps to test for recovery.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = min(10, r)
        self.theta_high = 0.8
        self.theta_low = 0.5
        self.s_bad = 0.5
        self.Punish_max = min(3, max(1, math.floor(r / 10))) if r > 0 else 1
        self.eps = 0.05
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        last_round_index = self.game_description.n_rounds - 1
        if state.round_number == last_round_index:
            self.punish_remaining = 0
            return Action.D
        n = int(self.game_description.n_players)
        past_rounds = int(history.my_actions.shape[0])
        window_len = min(self.W, max(1, past_rounds))
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        opp_window = opp_actions[-window_len:, :]
        if opp_window.size == 0:
            opponent_rates = np.array([])
        else:
            opponent_rates = np.mean(opp_window.astype(float), axis=0)
        my_rate = float(np.mean(my_actions[-window_len:].astype(float)))
        group_rate_window = float(np.mean(opponent_rates)) if opponent_rates.size > 0 else 0.0
        recent_row = opp_actions[-1, :]
        group_rate_recent = float(np.mean(recent_row.astype(float))) if recent_row.size > 0 else 0.0
        n_bad = int(np.sum(opponent_rates < self.s_bad)) if opponent_rates.size > 0 else 0
        n_total = n
        thresh_one_third = math.floor(n_total / 3)
        thresh_two_third = math.floor(2 * n_total / 3)
        if self.punish_remaining > 0:
            if n_bad == 0:
                self.punish_remaining = 0
            else:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                return Action.D
        if random.random() < self.eps:
            return Action.C
        if group_rate_window >= self.theta_high:
            return Action.C
        if group_rate_window <= self.theta_low:
            if n_bad == 0:
                P = 1
            elif 1 <= n_bad <= thresh_one_third:
                P = 1
            elif thresh_one_third < n_bad <= thresh_two_third:
                P = min(2, self.Punish_max)
            else:
                P = self.Punish_max
            self.punish_remaining = max(0, P - 1)
            return Action.D
        denom = self.theta_high - self.theta_low
        if denom <= 0:
            p_coop = 0.5
        else:
            p_coop = (group_rate_window - self.theta_low) / denom
            if p_coop < 0.0:
                p_coop = 0.0
            elif p_coop > 1.0:
                p_coop = 1.0
        if random.random() < p_coop:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    Collective Conditional Cooperation with Adaptive Forgiveness (CCC-AF).

    - Optimistic start.
    - Tracks recent cooperation scores S_j over a window W = min(20, max(3, r)).
    - Uses a cooperation threshold p_thresh that adapts with k and n.
    - If a sufficient fraction F of others are recent cooperators, cooperate (with occasional probing).
    - If cooperation drops, give a one-round grace for a single isolated non-persistent deviator, otherwise trigger
      a finite collective punishment of P rounds. Watch mechanism: one-round test before punishing isolated mistakes.
    - Finite punishment length P = 3. Probe probability = 0.10.
    - Forgives quickly when scores recover.
    - Defects on the final round; second-to-last round is cautious.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = float(game_description.n_players)
        self.n_int = game_description.n_players
        self.r = game_description.n_rounds
        self.k = float(game_description.k)
        self.W = min(20, max(3, self.r))
        if self.n_int - 1 > 0:
            self.p_thresh = 0.75 - 0.15 * ((self.k - 1.0) / float(self.n_int - 1))
        else:
            self.p_thresh = 0.75
        self.F_thresh = 0.6
        self.low_score = 0.25
        self.P = 3
        self.probe = 0.1
        self.endgame_large_F = 0.9
        self.P_remain = 0
        self.watch_active = False
        self.watch_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        n_players = self.n_int
        n_others = n_players - 1
        if state.round_number == 0 or history is None:
            self.P_remain = 0
            self.watch_active = False
            self.watch_target = None
            return Action.C
        if t == r:
            self.P_remain = 0
            self.watch_active = False
            self.watch_target = None
            return Action.D
        available_rounds = state.round_number
        lookback = min(self.W, available_rounds)
        if lookback <= 0:
            return Action.C
        recent_my = history.my_actions[-lookback:]
        try:
            my_coop_count = int(np.sum(recent_my))
        except Exception:
            my_coop_count = sum((1 if bool(x) else 0 for x in recent_my))
        S_self = float(my_coop_count) / float(lookback)
        opp_recent = history.opponent_actions[-lookback:, :]
        S_others = []
        persistent_flags = []
        for j in range(n_others):
            col = opp_recent[:, j]
            try:
                coop_count = int(np.sum(col))
            except Exception:
                coop_count = sum((1 if bool(x) else 0 for x in col))
            S_j = float(coop_count) / float(lookback)
            S_others.append(S_j)
            persistent = False
            if lookback >= 2:
                try:
                    cons_def = np.logical_and(~col[:-1], ~col[1:])
                    if bool(np.any(cons_def)):
                        persistent = True
                except Exception:
                    for idx in range(len(col) - 1):
                        if not bool(col[idx]) and (not bool(col[idx + 1])):
                            persistent = True
                            break
            if S_j <= self.low_score:
                persistent = True
            persistent_flags.append(persistent)
        coop_count_others = sum((1 for sj in S_others if sj >= self.p_thresh))
        F = float(coop_count_others) / float(n_others) if n_others > 0 else 1.0
        if t == r - 1:
            if F >= self.endgame_large_F and S_self >= self.p_thresh:
                return Action.C
            else:
                return Action.D
        if self.P_remain > 0:
            self.P_remain -= 1
            return Action.D
        if self.watch_active:
            target = self.watch_target
            if target is None or target < 0 or target >= n_others:
                self.watch_active = False
                self.watch_target = None
            else:
                try:
                    S_target = S_others[target]
                    target_persistent = persistent_flags[target]
                except Exception:
                    S_target = 0.0
                    target_persistent = False
                if S_target >= self.p_thresh and (not target_persistent):
                    self.watch_active = False
                    self.watch_target = None
                else:
                    self.P_remain = self.P
                    self.P_remain -= 1
                    self.watch_active = False
                    self.watch_target = None
                    return Action.D
        if F >= self.F_thresh:
            if F >= 0.6 and F < 0.75:
                if random.random() < self.probe:
                    return Action.D
            return Action.C
        low_indices = [idx for idx, sj in enumerate(S_others) if sj < self.p_thresh]
        num_low = len(low_indices)
        if num_low == 1 and (not persistent_flags[low_indices[0]]):
            self.watch_active = True
            self.watch_target = low_indices[0]
            return Action.C
        if random.random() < self.probe:
            return Action.C
        self.P_remain = self.P
        self.P_remain -= 1
        self.watch_active = False
        self.watch_target = None
        return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    Conditional Exploiter (CE)

    Adaptive public-goods strategy that defects when the immediate one-shot gain
    exceeds the empirically estimated future loss from reduced contributions by others.
    Uses short probes to exploit stable cooperators and cooperates to repair trust
    after defections or punishments. Learns opponent-specific punishment sensitivity
    from observed conditional cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_probe_round = -10000
        self.recovery_until_round = -1
        self.strong_punishers = set()
        self.eps = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        total_rounds = int(self.game_description.n_rounds)
        current_round = int(state.round_number)
        r_rem = total_rounds - current_round
        if r_rem == 1:
            return Action.D
        if current_round == 0 or history is None:
            return Action.C
        my_actions = np.array(history.my_actions.astype(np.bool_))
        opp_actions = np.array(history.opponent_actions.astype(np.bool_))
        rounds_seen = my_actions.shape[0]
        n_opponents = opp_actions.shape[1]
        W_param = min(20, max(3, r_rem))
        window_len = min(W_param, rounds_seen)
        start_idx = max(0, rounds_seen - window_len)
        end_idx = rounds_seen - 1
        N_afterC = np.zeros(n_opponents, dtype=float)
        T_afterC = np.zeros(n_opponents, dtype=float)
        N_afterD = np.zeros(n_opponents, dtype=float)
        T_afterD = np.zeros(n_opponents, dtype=float)
        for t in range(max(1, start_idx), end_idx + 1):
            prev_my_c = bool(my_actions[t - 1])
            opps_at_t = opp_actions[t, :].astype(float)
            if prev_my_c:
                T_afterC += 1.0
                N_afterC += opps_at_t
            else:
                T_afterD += 1.0
                N_afterD += opps_at_t
        p_afterC = N_afterC / np.maximum(1.0, T_afterC)
        p_afterD = N_afterD / np.maximum(1.0, T_afterD)
        delta_j = p_afterC - p_afterD
        Delta_total = float(np.sum(np.maximum(0.0, delta_j)))
        if window_len > 0:
            p_group = float(np.mean(opp_actions[start_idx:end_idx + 1, :].astype(float)))
        else:
            p_group = 0.0
        tau_cap = min(5, r_rem)
        defection_indices = [t for t in range(rounds_seen) if not bool(my_actions[t])]
        persistence_lengths = []
        per_opp_persistence = np.zeros(n_opponents, dtype=float)
        per_opp_counts = np.zeros(n_opponents, dtype=float)
        for t in defection_indices:
            if t + 1 > end_idx:
                continue
            pre_start = max(0, t - W_param)
            baseline_rounds = [u for u in range(pre_start, t) if bool(my_actions[u])]
            if len(baseline_rounds) == 0:
                baseline_rounds = list(range(pre_start, t))
            if len(baseline_rounds) == 0:
                baseline_group = float(np.mean(opp_actions[:t, :].astype(float))) if t > 0 else 0.5
                baseline_per_opp = np.mean(opp_actions[:t, :].astype(float), axis=0) if t > 0 else np.full(n_opponents, 0.5)
            else:
                baseline_group = float(np.mean(opp_actions[baseline_rounds, :].astype(float)))
                baseline_per_opp = np.mean(opp_actions[baseline_rounds, :].astype(float), axis=0)
            count_consec = 0
            for s in range(t + 1, min(t + 1 + W_param, rounds_seen)):
                group_coop_s = float(np.mean(opp_actions[s, :].astype(float)))
                if group_coop_s < baseline_group - 0.08:
                    count_consec += 1
                else:
                    break
            persistence_lengths.append(count_consec)
            for j in range(n_opponents):
                cnt_j = 0
                for s in range(t + 1, min(t + 1 + W_param, rounds_seen)):
                    if float(opp_actions[s, j]) < baseline_per_opp[j] - 0.08:
                        cnt_j += 1
                    else:
                        break
                per_opp_persistence[j] += cnt_j
                per_opp_counts[j] += 1.0
        if len(persistence_lengths) == 0:
            tau_raw = 1.0
        else:
            tau_raw = float(np.mean(persistence_lengths))
        avg_per_opp_persistence = np.zeros(n_opponents, dtype=float)
        for j in range(n_opponents):
            if per_opp_counts[j] > 0:
                avg_per_opp_persistence[j] = per_opp_persistence[j] / per_opp_counts[j]
            else:
                avg_per_opp_persistence[j] = 0.0
        tau = float(min(tau_raw, tau_cap, r_rem))
        if tau_cap > 0 and r_rem < tau_cap:
            tau = max(1.0, tau * (float(r_rem) / float(tau_cap)))
        G = max(0.0, 1.0 - k / float(n))
        L = k / float(n) * Delta_total * tau
        delta_thresh = 0.08
        strong_list = []
        for j in range(n_opponents):
            if float(delta_j[j]) >= 2.0 * delta_thresh and avg_per_opp_persistence[j] >= 2.0:
                strong_list.append(j)
        self.strong_punishers = set(strong_list)
        if len(self.strong_punishers) > n / 2.0:
            return Action.C
        if current_round <= self.recovery_until_round and current_round > self.last_probe_round:
            return Action.C
        last_round_index = rounds_seen - 1
        if last_round_index >= 0:
            last_round_group_coop = float(np.mean(opp_actions[last_round_index, :].astype(float)))
            if last_round_group_coop < 0.2:
                return Action.D
        theta_high = 0.75
        no_detected_punishers = len(self.strong_punishers) == 0 and Delta_total < delta_thresh / 2.0
        probe_spacing_ok = current_round - self.last_probe_round >= max(1, W_param)
        if p_group >= theta_high and no_detected_punishers and (r_rem > 3) and probe_spacing_ok and (self.recovery_until_round < current_round):
            self.last_probe_round = current_round
            self.recovery_until_round = current_round + 2
            return Action.D
        if G > L + self.eps:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    Probe-then-exploit public goods strategy.

    - Brief probe phase to estimate opponents' cooperativeness and forgiveness.
    - Classifies opponents as naive, reciprocators, or defectors using simple statistics.
    - After probing:
        * If many naive forgiving cooperators: probabilistic exploitation (randomized defection).
        * If many reciprocators: conditional cooperation (majority-rule reciprocity).
        * Otherwise: defensive/opportunistic play (avoid being the sole cooperator; opportunistic grabs).
    - Always defect in the final T_end rounds (endgame).
    - Monitors performance (whether cooperating improves future payoffs) and adapts.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        r = self.n_rounds
        self.T_probe = min(4, max(1, math.floor(r / 10) + 1))
        self.T_end = min(3, r)
        self.naive_coop_cutoff = 0.75
        self.forgiving_cutoff = 0.6
        self.reciprocal_cutoff = 0.4
        self.defector_cutoff = 0.2
        self.responsiveness_threshold = 0.15
        self.eps = 1e-08
        self.recent_punishment_round = None
        self.rebuild_trust_run_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n_players
        opponents = n - 1
        if self.n_rounds <= 1:
            return Action.D
        if t == 0 or history is None:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        my_payoffs = np.asarray(history.my_payoffs, dtype=float)
        opp_payoffs = np.asarray(history.opponent_payoffs, dtype=float)
        rounds_so_far = my_actions.shape[0]
        if t >= self.n_rounds - self.T_end:
            return Action.D
        last_my_action = bool(my_actions[-1])
        last_opp_coop_counts = int(np.sum(opp_actions[-1, :])) if rounds_so_far >= 1 else 0
        last_total_coop = last_opp_coop_counts + (1 if last_my_action else 0)
        if t < self.T_probe:
            if rounds_so_far >= 1:
                others_defected = (opponents - last_opp_coop_counts) / max(1, opponents)
                if others_defected >= 0.8:
                    return Action.D
            return Action.C
        coop_rates = np.zeros(opponents, dtype=float)
        after_my_defect_rates = np.zeros(opponents, dtype=float)
        responsiveness = np.zeros(opponents, dtype=float)
        if rounds_so_far > 0:
            coop_rates = np.asarray(np.mean(opp_actions, axis=0), dtype=float)
        after_counts = np.zeros(opponents, dtype=float)
        after_denom = np.zeros(opponents, dtype=float)
        if rounds_so_far >= 2:
            for s in range(1, rounds_so_far):
                if not my_actions[s]:
                    prev_coop = opp_actions[s - 1, :]
                    still_coop = opp_actions[s, :]
                    mask = prev_coop
                    after_denom += mask.astype(float)
                    after_counts += (still_coop & mask).astype(float)
        for j in range(opponents):
            if after_denom[j] > 0:
                after_my_defect_rates[j] = after_counts[j] / after_denom[j]
            else:
                after_my_defect_rates[j] = coop_rates[j]
        rate_after_my_coop = np.zeros(opponents, dtype=float)
        rate_after_my_defect = np.zeros(opponents, dtype=float)
        denom_coop = 0
        denom_def = 0
        if rounds_so_far >= 2:
            for s in range(0, rounds_so_far - 1):
                next_coop = opp_actions[s + 1, :]
                if my_actions[s]:
                    rate_after_my_coop += next_coop.astype(float)
                    denom_coop += 1
                else:
                    rate_after_my_defect += next_coop.astype(float)
                    denom_def += 1
            if denom_coop > 0:
                rate_after_my_coop = rate_after_my_coop / denom_coop
            else:
                rate_after_my_coop = coop_rates
            if denom_def > 0:
                rate_after_my_defect = rate_after_my_defect / denom_def
            else:
                rate_after_my_defect = coop_rates
            responsiveness = rate_after_my_coop - rate_after_my_defect
        else:
            responsiveness = np.zeros(opponents, dtype=float)
        naive_mask = (coop_rates >= self.naive_coop_cutoff) & (after_my_defect_rates >= self.forgiving_cutoff)
        naive_count = int(np.sum(naive_mask))
        F_naive = naive_count / max(1, opponents)
        recip_mask = (coop_rates >= self.reciprocal_cutoff) & (responsiveness >= self.responsiveness_threshold)
        recip_count = int(np.sum(recip_mask))
        F_recip = recip_count / max(1, opponents)
        defect_mask = coop_rates <= self.defector_cutoff
        defect_count = int(np.sum(defect_mask))
        F_defect = defect_count / max(1, opponents)
        perf_after_coop = None
        perf_after_def = None
        vals_after_coop = []
        vals_after_def = []
        if rounds_so_far >= 2:
            for s in range(0, rounds_so_far - 1):
                if my_actions[s]:
                    vals_after_coop.append(float(my_payoffs[s + 1]))
                else:
                    vals_after_def.append(float(my_payoffs[s + 1]))
        if len(vals_after_coop) > 0:
            perf_after_coop = float(np.mean(vals_after_coop))
        if len(vals_after_def) > 0:
            perf_after_def = float(np.mean(vals_after_def))
        coop_better_for_payoff = False
        if perf_after_coop is not None and perf_after_def is not None:
            if perf_after_coop > perf_after_def + 1e-06:
                coop_better_for_payoff = True
        if self.rebuild_trust_run_remaining > 0:
            self.rebuild_trust_run_remaining -= 1
            if coop_better_for_payoff:
                return Action.C
            return Action.D
        if F_defect >= 0.5:
            return Action.D
        if F_naive >= 0.5:
            p_exploit = min(0.9, 0.2 + 1.2 * F_naive)
            if last_total_coop < n / 2:
                p_exploit = p_exploit * 0.5
            if naive_count > 0:
                surprising_retaliators = 0
                for j in range(opponents):
                    if naive_mask[j] and responsiveness[j] > max(self.responsiveness_threshold, 0.15):
                        surprising_retaliators += 1
                if surprising_retaliators > 0:
                    p_exploit = max(0.1, p_exploit * 0.5)
                    if surprising_retaliators / max(1, naive_count) > 0.4:
                        self.recent_punishment_round = t
                        if coop_better_for_payoff:
                            self.rebuild_trust_run_remaining = 2
            naive_cooperated_last = False
            if rounds_so_far >= 1 and naive_count > 0:
                naive_cooperated_last = np.any(opp_actions[-1, :] & naive_mask)
            effective_p = p_exploit
            if not naive_cooperated_last:
                effective_p = effective_p * 0.7
            if random.random() < effective_p:
                return Action.D
            else:
                return Action.C
        if F_recip >= 0.5:
            if rounds_so_far >= 1:
                if last_opp_coop_counts >= math.ceil(opponents / 2):
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        if rounds_so_far >= 1:
            plurality_cooperated = last_opp_coop_counts > opponents / 2
            if plurality_cooperated and coop_better_for_payoff:
                return Action.C
            small_naive_coalition = naive_count >= 2
            if small_naive_coalition:
                coalition_fraction = naive_count / max(1, opponents)
                if random.random() < coalition_fraction:
                    return Action.D
                else:
                    return Action.C
        if coop_better_for_payoff:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    Graduated Collective Conditional Cooperator (GCCC)

    - Signals cooperation on first move, defects on the final round.
    - Uses a sliding window of recent rounds to compute reciprocity (f).
    - Rewards clear collective recoveries immediately (cooperate if any full-cooperation round observed in window).
    - If the group cooperation rate is sufficiently high (f >= theta) it cooperates.
    - If f is slightly below theta (within a soft band) it forgives with 50% probability.
    - If f is well below theta it defects and begins a short, proportional punishment phase (capped).
    - While punishing it defects; a full-cooperation round in the window causes immediate forgiveness (cooperate this round).
      If f >= theta for two consecutive punishment rounds the strategy stops punishing (but still defects in the current punishment
      round and will cooperate starting next round).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(0, self.r - 1))
        self.theta = 0.5 + 0.1 * (1.0 - self.k / max(1, self.n))
        self.delta = 0.15
        self.P_max = 3
        self.in_punishment = False
        self.punishment_remaining = 0
        self.consecutive_recovery_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            self.in_punishment = False
            self.punishment_remaining = 0
            self.consecutive_recovery_count = 0
            return Action.C
        if t == self.r - 1:
            return Action.D
        available_rounds = t
        w_eff = min(self.w, available_rounds) if self.w > 0 else 0
        start_idx = max(0, available_rounds - w_eff)
        end_idx = available_rounds
        if w_eff > 0:
            opp_window = history.opponent_actions[start_idx:end_idx, :]
            my_window = history.my_actions[start_idx:end_idx]
        else:
            opp_window = np.empty((0, max(0, self.n - 1)), dtype=bool)
            my_window = np.empty((0,), dtype=bool)
        full_coop_in_window = False
        if w_eff > 0:
            if opp_window.size > 0:
                opponents_all = np.all(opp_window, axis=1)
            else:
                opponents_all = np.ones((w_eff,), dtype=bool)
            rounds_all_coop = np.logical_and(opponents_all, my_window)
            full_coop_in_window = bool(np.any(rounds_all_coop))
        else:
            full_coop_in_window = False
        if w_eff <= 0 or self.n - 1 <= 0:
            f = 0.0
        else:
            total_coops = float(np.sum(opp_window))
            denom = float(w_eff * (self.n - 1))
            f = total_coops / denom if denom > 0.0 else 0.0
        if full_coop_in_window:
            self.in_punishment = False
            self.punishment_remaining = 0
            self.consecutive_recovery_count = 0
            return Action.C
        if self.in_punishment:
            if f >= self.theta:
                self.consecutive_recovery_count += 1
            else:
                self.consecutive_recovery_count = 0
            if self.consecutive_recovery_count >= 2:
                self.in_punishment = False
                self.punishment_remaining = 0
                self.consecutive_recovery_count = 0
                return Action.D
            action = Action.D
            if self.punishment_remaining > 0:
                self.punishment_remaining -= 1
                if self.punishment_remaining <= 0:
                    self.in_punishment = False
                    self.punishment_remaining = 0
            else:
                self.in_punishment = False
                self.punishment_remaining = 0
            return action
        self.consecutive_recovery_count = 0
        if f >= self.theta:
            return Action.C
        if self.theta - self.delta <= f < self.theta:
            if random.random() < 0.5:
                return Action.C
            else:
                return Action.D
        if f < self.theta - self.delta:
            raw = 1.0 + round((self.theta - f) / max(1e-12, self.delta))
            P = int(min(self.P_max, max(1, int(raw))))
            self.in_punishment = True
            self.punishment_remaining = max(0, P - 1)
            self.consecutive_recovery_count = 0
            if self.punishment_remaining == 0:
                self.in_punishment = False
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    Opportunistic, learning public-goods strategy.

    - Probes early with an alternating C/D schedule to estimate how each opponent
      responds to my previous action.
    - Learns per-opponent conditional cooperation rates p_afterC and p_afterD,
      estimates punishment deltas, and sums them to an estimated_group_delta.
    - Defects when the immediate per-round gain from defecting outweighs the
      expected cumulative future loss due to opponents' reduced cooperation.
    - Uses optimistic defaults when data are scarce, increases caution if many
      opponents are unknown, and includes occasional re-probing to adapt.
    - Always defects in the final round. If most opponents are near-unconditional
      cooperators, exploits them but occasionally probes to detect punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.min_samples = 3
        self.optimistic_default_delta = 0.02
        self.optimistic_default_delta_cautious = 0.05
        self.epsilon_reprobe = 0.03
        self.unconditional_probe_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number) + 1
        L = r - t
        T_probe = min(8, max(2, math.floor(r / 4)))
        if state.round_number == 0 or history is None:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        num_past_rounds = my_actions.shape[0]
        num_opponents = max(0, opp_actions.shape[1])
        if t <= T_probe:
            return Action.C if t % 2 == 1 else Action.D
        total_coop = np.zeros(num_opponents, dtype=float)
        base_rate = np.zeros(num_opponents, dtype=float)
        if num_past_rounds > 0 and num_opponents > 0:
            total_coop = np.sum(opp_actions.astype(float), axis=0)
            denom = max(1, num_past_rounds)
            base_rate = total_coop / float(denom)
        else:
            total_coop = np.zeros(num_opponents, dtype=float)
            base_rate = np.zeros(num_opponents, dtype=float)
        count_afterC = np.zeros(num_opponents, dtype=int)
        coop_afterC = np.zeros(num_opponents, dtype=int)
        count_afterD = np.zeros(num_opponents, dtype=int)
        coop_afterD = np.zeros(num_opponents, dtype=int)
        if num_past_rounds >= 2 and num_opponents > 0:
            for r_idx in range(1, num_past_rounds):
                prev_my = bool(my_actions[r_idx - 1])
                row = opp_actions[r_idx, :].astype(int)
                if prev_my:
                    count_afterC += 1
                    coop_afterC += row
                else:
                    count_afterD += 1
                    coop_afterD += row
        p_afterC = np.zeros(num_opponents, dtype=float)
        p_afterD = np.zeros(num_opponents, dtype=float)
        for j in range(num_opponents):
            p_afterC[j] = float(coop_afterC[j]) / float(max(1, count_afterC[j]))
            p_afterD[j] = float(coop_afterD[j]) / float(max(1, count_afterD[j]))
        lacks_samples = np.array([count_afterC[j] < self.min_samples or count_afterD[j] < self.min_samples for j in range(num_opponents)], dtype=bool)
        many_unknowns = np.sum(lacks_samples) > n / 2
        optimistic_delta = self.optimistic_default_delta_cautious if many_unknowns else self.optimistic_default_delta
        estimated_punish_delta = np.zeros(num_opponents, dtype=float)
        for j in range(num_opponents):
            if lacks_samples[j]:
                estimated_punish_delta[j] = optimistic_delta
            else:
                delta = float(p_afterC[j] - p_afterD[j])
                if delta <= 0.0:
                    estimated_punish_delta[j] = 0.0
                else:
                    estimated_punish_delta[j] = delta
        estimated_group_delta = float(np.sum(estimated_punish_delta))
        immediate_gain = 1.0 - k / float(n)
        per_round_future_loss = k / float(n) * estimated_group_delta
        avg_base_rate = float(np.mean(base_rate) if num_opponents > 0 else 0.0)
        if num_opponents > 0:
            near_unconditional = np.array([base_rate[j] > 0.95 and estimated_punish_delta[j] < 0.01 for j in range(num_opponents)], dtype=bool)
            fraction_unconditional = float(np.sum(near_unconditional) / float(num_opponents))
        else:
            fraction_unconditional = 0.0
        if L == 0:
            return Action.D
        if avg_base_rate < 0.15:
            if random.random() < self.epsilon_reprobe:
                return Action.C
            return Action.D
        if fraction_unconditional > 0.5:
            if random.random() < self.unconditional_probe_prob:
                return Action.C
            if random.random() < self.epsilon_reprobe:
                return Action.C
            return Action.D
        threshold = L * per_round_future_loss
        if immediate_gain >= threshold:
            decision = Action.D
        else:
            decision = Action.C
        if random.random() < self.epsilon_reprobe:
            decision = Action.C if decision == Action.D else Action.D
        return decision

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    Collective Tit-for-Tat with Proportional Punishment and Forgiveness (CTP-PF)

    - Starts cooperative.
    - Tracks recent opponent behavior in a window W.
    - Identifies persistent defectors and punishes proportionally for a short number of rounds (P).
    - Forgives isolated one-off defections and requires F consecutive cooperative moves for rehabilitation.
    - Uses conservative behavior in the final L rounds and always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = max(1, min(4, math.floor(self.r / 6) + 1))
        self.q = 0.5 + 0.25 * (1.0 - self.k / float(self.n))
        self.T_base = min(3, max(1, math.ceil((1.0 - self.k / float(self.n)) * 4.0)))
        self.F = 1
        self.L = min(2, max(0, self.r - 1))
        self.P = 0
        self.persistent_set = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        opp_actions = history.opponent_actions
        played_rounds = int(opp_actions.shape[0])
        n_opponents = self.n - 1
        last_idx = played_rounds - 1
        if last_idx >= 0:
            last_round_ops = opp_actions[last_idx, :]
            last_round_coop_count = int(np.sum(last_round_ops))
        else:
            last_round_ops = np.array([], dtype=bool)
            last_round_coop_count = 0
        other_coop_fraction = 0.0
        if n_opponents > 0:
            other_coop_fraction = float(last_round_coop_count) / float(n_opponents)
        if t == self.r - 1:
            return Action.D
        if self.L > 0 and t >= self.r - self.L and (t < self.r - 1):
            if played_rounds >= 1 and other_coop_fraction == 1.0:
                return Action.C
            return Action.D
        if self.P > 0:
            self.P = max(0, self.P - 1)
            return Action.D
        current_persistent = set()
        consider_R = min(self.W, played_rounds)
        if consider_R > 0:
            window = opp_actions[-consider_R:, :]
            for j in range(n_opponents):
                col = window[:, j]
                defect_count = int(np.sum(~col))
                if self.W == 1:
                    if defect_count >= 1:
                        current_persistent.add(j)
                elif defect_count >= 2:
                    current_persistent.add(j)
        new_persistent = set(self.persistent_set)
        for j in current_persistent:
            new_persistent.add(j)
        if self.F > 0:
            for j in list(new_persistent):
                if j in current_persistent:
                    continue
                if played_rounds >= self.F:
                    last_F = opp_actions[-self.F:, j]
                    if bool(np.all(last_F)):
                        new_persistent.discard(j)
        else:
            for j in list(new_persistent):
                if j not in current_persistent:
                    new_persistent.discard(j)
        self.persistent_set = new_persistent
        severe = len(self.persistent_set) >= 1
        crowd_defect = other_coop_fraction < self.q
        num_defectors_last = n_opponents - last_round_coop_count
        if not severe and (not crowd_defect):
            return Action.C
        if num_defectors_last == 1 and other_coop_fraction >= 0.5:
            if played_rounds >= 1:
                defector_indices = [int(idx) for idx, val in enumerate(last_round_ops) if not bool(val)]
                if len(defector_indices) == 1:
                    j = defector_indices[0]
                    if j not in self.persistent_set:
                        return Action.C
        remaining = self.r - 1 - t
        if remaining < 0:
            remaining = 0
        scale = 1 + len(self.persistent_set)
        P_new = int(min(remaining, int(self.T_base * scale)))
        self.P = max(0, P_new)
        if self.P > 0:
            self.P = max(0, self.P - 1)
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    - Probes cooperativeness in an initial test phase by cooperating.
    - Maintains sliding-window statistics per opponent (cooperation rates and responsiveness).
    - Labels opponents as UNCONDITIONAL_COOPERATOR, CONDITIONAL, DEFECTOR, or MIXED.
    - Exploits unconditional cooperators by defecting, defects when defectors are a majority,
      and otherwise tries to sustain cooperation with conditional players while performing
      deterministic, limited exploitation on a fixed schedule.
    - Uses brief one-round punishments and swift forgiveness to avoid long vendettas.
    - Always defects in the final round. Deterministic (no randomness in decisions).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = game_description.n_rounds
        self.test_rounds = min(4, max(1, math.floor(r / 5)))
        self.w = min(8, r)
        self.coop_uncond = 0.9
        self.coop_defect = 0.2
        self.resp_threshold = 0.25
        self.exploitation_rate = 0.25
        self.last_round_defect = 1
        self.K = max(1, math.ceil(1.0 / self.exploitation_rate))
        self.paused_until = 0
        self.last_one_round_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        t_one_based = state.round_number + 1
        if r_total <= 1:
            return Action.D
        if t_one_based == r_total:
            return Action.D
        if history is None or history.my_actions.size == 0:
            if t_one_based <= self.test_rounds:
                return Action.C
            return Action.C
        hist_len = history.my_actions.shape[0]
        n_opponents = n - 1
        last_m = min(self.w, hist_len)
        if hist_len == 0:
            if t_one_based <= self.test_rounds:
                return Action.C
            return Action.C
        if t_one_based <= self.test_rounds:
            return Action.C
        start_idx = hist_len - last_m
        opp_window = history.opponent_actions[start_idx:hist_len, :]
        my_window = history.my_actions[start_idx:hist_len]
        coop_counts = np.sum(opp_window, axis=0).astype(float)
        coop_rate = coop_counts / float(last_m)
        if last_m >= 2:
            pairs_len = last_m - 1
            my_earlier = history.my_actions[start_idx:hist_len - 1]
            opp_later = history.opponent_actions[start_idx + 1:hist_len, :]
            myC_mask = my_earlier.astype(bool)
            myD_mask = ~myC_mask
            denomC = float(np.sum(myC_mask))
            denomD = float(np.sum(myD_mask))
            if denomC > 0.0:
                coop_after_myC = np.sum(opp_later[myC_mask, :], axis=0).astype(float) / denomC
            else:
                coop_after_myC = coop_rate.copy()
            if denomD > 0.0:
                coop_after_myD = np.sum(opp_later[myD_mask, :], axis=0).astype(float) / denomD
            else:
                coop_after_myD = coop_rate.copy()
        else:
            coop_after_myC = coop_rate.copy()
            coop_after_myD = coop_rate.copy()
        responsiveness = coop_after_myC - coop_after_myD
        labels = np.full(n_opponents, 3, dtype=int)
        uncond_mask = (coop_rate >= self.coop_uncond) & (responsiveness <= 0.1)
        labels[uncond_mask] = 0
        def_mask = coop_rate <= self.coop_defect
        labels[def_mask] = 1
        cond_mask = (responsiveness >= self.resp_threshold) & ~uncond_mask & ~def_mask
        labels[cond_mask] = 2
        f_uncond = float(np.sum(labels == 0)) / float(n_opponents)
        f_cond = float(np.sum(labels == 2)) / float(n_opponents)
        f_def = float(np.sum(labels == 1)) / float(n_opponents)
        g = float(np.mean(coop_rate)) if n_opponents > 0 else 0.0
        punish_now = False
        if hist_len >= 2:
            my_prev_prev = history.my_actions[hist_len - 2]
            opp_prev = history.opponent_actions[hist_len - 1, :]
            if not my_prev_prev:
                cond_indices = np.where(labels == 2)[0]
                if cond_indices.size > 0:
                    cond_defected = np.any(~opp_prev[cond_indices])
                    if cond_defected:
                        punish_now = True
                        self.last_one_round_punish = t_one_based
        if not punish_now and hist_len >= 2:
            my_prev = history.my_actions[hist_len - 2]
            if my_prev:
                cond_indices = np.where(labels == 2)[0]
                if cond_indices.size > 0:
                    opp_prev = history.opponent_actions[hist_len - 1, :]
                    cond_defected = np.any(~opp_prev[cond_indices])
                    if cond_defected:
                        punish_now = True
                        self.last_one_round_punish = t_one_based
        if self.last_one_round_punish == t_one_based:
            return Action.D
        if f_uncond > 0.0:
            return Action.D
        if f_def >= 0.5:
            return Action.D
        if f_cond < 0.3 and g < 0.4:
            return Action.D
        if g >= 0.6 and f_cond >= 0.3:
            if t_one_based <= self.paused_until:
                return Action.C
            cond_indices = np.where(labels == 2)[0]
            n_cond = cond_indices.size
            if n_cond > 0:
                cond_responsive_count = int(np.sum(responsiveness[cond_indices] >= self.resp_threshold))
                if cond_responsive_count > 0 and float(cond_responsive_count) / float(n_cond) >= 0.3:
                    self.paused_until = t_one_based + 1
                    return Action.C
            if (t_one_based - self.test_rounds) % self.K == 0:
                if n_cond > 0 and cond_responsive_count > 0 and (float(cond_responsive_count) / float(n_cond) >= 0.5):
                    self.paused_until = t_one_based + 1
                    return Action.C
                return Action.D
            else:
                return Action.C
        if 0.4 <= g < 0.6 and f_cond >= 0.15:
            if hist_len >= 1:
                last_idx = hist_len - 1
                others_coops_last = int(np.sum(history.opponent_actions[last_idx, :]))
                my_coop_last = bool(history.my_actions[last_idx])
                k = self.game_description.k
                payoff_if_D = 1.0 + k / float(n) * float(others_coops_last)
                actual_payoff = float(history.my_payoffs[last_idx])
                if payoff_if_D - actual_payoff > 1e-09 and f_cond < 0.3:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    Majority-Forgiving Reciprocity (MFR) for the N-player Public Goods Game.

    - Starts cooperatively (except in one-shot).
    - Uses a short recent window W (<=5) to measure others' cooperation.
    - If recent cooperation by others is high (>= f_high) cooperate.
    - If recent cooperation by others is low (<= f_low) start a bounded punishment block of length P.
    - In ambiguous region (f_low < G < f_high) follow a one-round majority tie-breaker.
    - Detects persistent exploiters (<=10% cooperation while the rest are cooperative) and defects
      until the exploiter's recent cooperation exceeds 10%.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.W = min(5, self.n_rounds)
        self.f_high = 0.6
        self.f_low = 0.4
        self.P = min(3, max(1, math.ceil(self.W / 2)))
        self.punishment_timer = 0
        self.flagged_exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.n_rounds == 1:
            return Action.D
        if t == self.n_rounds - 1:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D
        available_rounds = int(history.opponent_actions.shape[0])
        window_size = min(self.W, available_rounds)
        if window_size <= 0:
            return Action.D
        opp_window = history.opponent_actions[-window_size:, :]
        my_window = history.my_actions[-window_size:]
        coop_counts = np.sum(opp_window, axis=0)
        coop_fracs = np.array(coop_counts, dtype=float) / float(window_size)
        my_frac = float(np.sum(my_window)) / float(window_size)
        if coop_fracs.size > 0:
            G = float(np.mean(coop_fracs))
        else:
            G = 0.0
        n_opponents = coop_fracs.size
        fractions_with_self = np.concatenate(([my_frac], coop_fracs))
        for j in range(n_opponents):
            j_frac = coop_fracs[j]
            rest_fracs = np.delete(fractions_with_self, j + 1)
            rest_mean = float(np.mean(rest_fracs)) if rest_fracs.size > 0 else 0.0
            if j_frac <= 0.1 and rest_mean >= self.f_high:
                self.flagged_exploiters.add(j)
            elif j in self.flagged_exploiters and j_frac > 0.1:
                self.flagged_exploiters.discard(j)
        if len(self.flagged_exploiters) > 0:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if G >= self.f_high:
            return Action.C
        if G <= self.f_low:
            self.punishment_timer = self.P
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        last_round_opp_coops = int(np.sum(history.opponent_actions[-1, :]))
        last_round_my_coop = int(bool(history.my_actions[-1]))
        last_round_total_coops = last_round_opp_coops + last_round_my_coop
        majority_threshold = math.ceil(self.n_players / 2)
        if last_round_total_coops >= majority_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    Exploit-and-forgive public goods strategy.

    - Starts cooperating in round 0 to signal willingness.
    - Tracks recent cooperation rates (q_j) for each opponent over a sliding window L.
    - Identifies persistent defectors and places them on a targeted blacklist.
    - Triggers short, firm punishments (P rounds) after opportunistic exploitation or
      coordinated low cooperation.
    - Opportunistically exploits reliably cooperative groups (probabilistic defection capped by exploit_cap),
      while ensuring occasional cooperations to avoid permanent collapse.
    - Biases exploitation toward single very-reliable cooperators.
    - Forgives blacklist members after verified sustained cooperation.
    - Defects in the terminal T_end rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.L = max(3, min(10, math.floor(self.r / 10)))
        self.T_end = max(1, math.floor(0.05 * self.r))
        self.P = max(1, math.floor(self.r / 20))
        self.low_thresh = 0.4
        self.high_thresh = 0.8
        denom = max(1, self.n - 1)
        self.exploit_cap = min(0.5, max(0.0, (self.n - self.k) / denom))
        self.punish_timer = 0
        self.targeted_blacklist = set()
        self.blacklist_entry_round = dict()
        self.last_coop_round = -10000

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            self.last_coop_round = 0
            return Action.C
        if t >= self.r - self.T_end:
            return Action.D
        m = max(0, self.n - 1)
        window = min(self.L, t)
        q = np.zeros(m, dtype=float)
        if window > 0 and m > 0:
            recent = history.opponent_actions[-window:, :]
            if recent.size > 0:
                q = np.mean(recent.astype(float), axis=0)
            else:
                q = np.zeros(m, dtype=float)
        if m > 0 and t >= self.L:
            for j in range(m):
                if j in self.targeted_blacklist:
                    continue
                if q[j] <= 0.05 and t >= self.L:
                    self.targeted_blacklist.add(j)
                    self.blacklist_entry_round[j] = t
        remove_candidates = []
        for j in list(self.targeted_blacklist):
            entry = self.blacklist_entry_round.get(j, 0)
            rounds_since = t - entry
            if rounds_since >= self.L and m > 0:
                if t >= self.L:
                    recent_actions = history.opponent_actions[-self.L:, j]
                    coop_frac = float(np.mean(recent_actions.astype(float))) if recent_actions.size > 0 else 0.0
                    if coop_frac >= 0.8:
                        remove_candidates.append(j)
        for j in remove_candidates:
            self.targeted_blacklist.discard(j)
            self.blacklist_entry_round.pop(j, None)
        non_black_indices = [j for j in range(m) if j not in self.targeted_blacklist]
        non_black_len = len(non_black_indices)
        last_round_opponent_actions = None
        if non_black_len > 0 and t >= 1:
            last_round_opponent_actions = history.opponent_actions[-1, non_black_indices]
        my_last_action = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
        trigger_punish = False
        if non_black_len > 0 and last_round_opponent_actions is not None:
            coop_count_last = int(np.sum(last_round_opponent_actions.astype(int)))
            defect_count_last = non_black_len - coop_count_last
            if my_last_action and defect_count_last >= 1:
                if coop_count_last >= math.ceil(0.5 * non_black_len):
                    trigger_punish = True
        if non_black_len > 0:
            low_q_count = sum((1 for j in non_black_indices if q[j] < self.low_thresh))
            if low_q_count >= 2:
                trigger_punish = True
        if trigger_punish:
            self.punish_timer = self.P
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if non_black_len == 0:
            return Action.D
        group_recent = float(np.mean([q[j] for j in non_black_indices])) if non_black_len > 0 else 0.0
        high_cooperators = [j for j in non_black_indices if q[j] >= 0.9]
        single_high = None
        delta = 0.0
        if len(high_cooperators) == 1:
            single_high = high_cooperators[0]
            delta = min(0.2, max(0.0, q[single_high] - 0.9))
        else:
            delta = 0.0
        chosen_action = Action.D
        if group_recent >= self.high_thresh:
            denom = max(1e-09, 1.0 - self.high_thresh)
            base_p_exploit = self.exploit_cap * (group_recent - self.high_thresh) / denom
            p_exploit = min(self.exploit_cap, max(0.0, base_p_exploit))
            p_total = min(self.exploit_cap, p_exploit + delta)
            min_p = max(p_total, 0.05)
            coop_interval = math.ceil(1.0 / min_p) if min_p > 0.0 else math.ceil(1.0 / 0.05)
            if t - self.last_coop_round >= coop_interval:
                chosen_action = Action.C
                self.last_coop_round = t
                return chosen_action
            if random.random() < p_total:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
                self.last_coop_round = t
            return chosen_action
        if group_recent >= self.low_thresh:
            coop_last = int(np.sum(history.opponent_actions[-1, non_black_indices].astype(int))) if t >= 1 and non_black_len > 0 else 0
            if coop_last > non_black_len / 2.0:
                if delta > 0.0 and random.random() < delta:
                    chosen_action = Action.D
                else:
                    chosen_action = Action.C
                    self.last_coop_round = t
            else:
                chosen_action = Action.D
            return chosen_action
        chosen_action = Action.D
        return chosen_action

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Punishments (4G).

    - Opens with cooperation to seed cooperation.
    - Uses a short lookback to estimate others' cooperation (threshold T = 0.5).
    - If others defect while we cooperated, registers a strike and applies graduated punishment.
    - If group cooperation appears to collapse, enters collapsed-probe mode: defect but cooperate once every 5 rounds to test recovery.
    - Defects in the final L rounds to avoid endgame exploitation and does not update strike/punishment state based on endgame rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.L = min(3, max(1, math.ceil(self.r / 10)))
        self.w = min(5, max(1, math.floor(self.r / 10)))
        self.T = 0.5
        self.P_max = 5
        self.grace = 1
        self.strike_count = 0
        self.punishment_counter = 0
        self.mode = 'normal'
        self.collapsed_start_round = None
        self.collapsed_has_probed = False
        self.in_punishment_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        n = self.n
        if state.round_number == 0 or history is None:
            self.in_punishment_last_round = False
            self.mode = 'normal'
            self.collapsed_start_round = None
            self.collapsed_has_probed = False
            return Action.C

        def is_endgame_round_index(idx: int) -> bool:
            return idx >= r - self.L
        prev_idx = state.round_number - 1
        if prev_idx >= 0 and (not is_endgame_round_index(prev_idx)):
            try:
                prev_my_action = bool(history.my_actions[prev_idx])
                prev_opponent_actions = history.opponent_actions[prev_idx, :]
            except Exception:
                prev_my_action = False
                prev_opponent_actions = np.array([], dtype=np.bool_)
            if prev_my_action:
                if prev_opponent_actions.size > 0 and (not np.all(prev_opponent_actions)):
                    self.strike_count += 1
                    self.punishment_counter = min(self.P_max, 1 + self.strike_count)
        if self.in_punishment_last_round and self.punishment_counter == 0:
            num_past_rounds = state.round_number
            num_window = min(self.w, num_past_rounds)
            if num_window > 0:
                start_idx = state.round_number - num_window
                try:
                    slice_actions = history.opponent_actions[start_idx:state.round_number, :]
                    p_hat = float(np.mean(slice_actions)) if slice_actions.size > 0 else 0.0
                except Exception:
                    p_hat = 0.0
            else:
                p_hat = 0.0
            if p_hat >= self.T:
                self.strike_count = 0
                self.mode = 'normal'
                self.collapsed_start_round = None
                self.collapsed_has_probed = False
        recent_group_rate = 0.0
        recent_rounds = min(10, state.round_number)
        if recent_rounds > 0:
            try:
                recent_slice = history.opponent_actions[state.round_number - recent_rounds:state.round_number, :]
                recent_group_rate = float(np.mean(recent_slice)) if recent_slice.size > 0 else 0.0
            except Exception:
                recent_group_rate = 0.0
        if recent_group_rate <= 0.1 and t <= r - self.L:
            if self.mode != 'collapsed':
                self.mode = 'collapsed'
                self.collapsed_start_round = t
                self.collapsed_has_probed = False
        if is_endgame_round_index(state.round_number):
            self.in_punishment_last_round = False
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            self.in_punishment_last_round = True
            return Action.D
        self.in_punishment_last_round = False
        if self.mode == 'collapsed':
            if self.collapsed_start_round is None:
                self.mode = 'normal'
            else:
                offset = t - self.collapsed_start_round
                if offset % 5 == 0:
                    self.collapsed_has_probed = True
                    return Action.C
                else:
                    return Action.D
        num_past_rounds = state.round_number
        num_window = min(self.w, num_past_rounds)
        if num_window > 0:
            start_idx = state.round_number - num_window
            try:
                window_slice = history.opponent_actions[start_idx:state.round_number, :]
                p_hat = float(np.mean(window_slice)) if window_slice.size > 0 else 0.0
            except Exception:
                p_hat = 0.0
        else:
            p_hat = 0.0
        if self.mode == 'collapsed' and self.collapsed_has_probed and (p_hat >= self.T):
            self.mode = 'normal'
            self.collapsed_start_round = None
            self.collapsed_has_probed = False
            self.strike_count = 0
        if p_hat >= self.T:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    Exploitative, adaptive public-goods strategy.

    - Probe by cooperating in the first round.
    - Maintain a short moving estimate of others' cooperation (p) over up to L recent rounds.
    - Estimate responsiveness R of others to my actions.
    - Free-ride when safe (high p and others insensitive); preserve cooperation when others punish.
    - Use short, controlled punishments (P rounds) on sudden drops and follow with a single probe.
    - Always defect in the final few rounds (last_phase).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(10, max(1, self.r - 1))
        self.last_phase = min(3, self.r)
        self.T_low = 0.25
        self.T_high = 0.6
        self.resp_notice = 0.05
        self.resp_strong = 0.1
        self.q_rebuild = 0.25
        self.P = 2
        self.punish_remaining = 0
        self.probe_pending = False
        self.last_punish_trigger_round = -999

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t0 = state.round_number
        if t0 >= self.r - self.last_phase:
            return Action.D
        num_past = t0
        opp_actions = history.opponent_actions[:num_past, :]
        if opp_actions.size == 0:
            others_contribs = np.array([], dtype=int)
        else:
            others_contribs = np.sum(opp_actions.astype(int), axis=1)
        L_eff = min(self.L, num_past) if num_past > 0 else 0
        if L_eff > 0:
            window_start = num_past - L_eff
            window_vals = others_contribs[window_start:num_past]
            denom = L_eff * (self.n - 1)
            p = float(np.sum(window_vals)) / float(denom)
        else:
            p = 0.0
        last_others = int(others_contribs[-1]) if num_past > 0 else 0
        last_frac = last_others / float(self.n - 1)
        my_actions = history.my_actions[:num_past].astype(bool) if history.my_actions.size >= num_past else history.my_actions.astype(bool)
        exclude_first_probe = my_actions.size > 0 and my_actions[0] is True
        coop_indices = []
        defect_indices = []
        for s in range(num_past):
            if exclude_first_probe and s == 0:
                continue
            if my_actions[s]:
                coop_indices.append(s)
            else:
                defect_indices.append(s)
        R = 0.0
        try:
            if len(coop_indices) > 0 and len(defect_indices) > 0:
                avg_coop = float(np.mean(others_contribs[coop_indices])) / float(self.n - 1)
                avg_def = float(np.mean(others_contribs[defect_indices])) / float(self.n - 1)
                R = avg_coop - avg_def
            else:
                R = 0.0
        except Exception:
            R = 0.0
        baseline_frac = p
        sudden_drop = False
        if L_eff > 0:
            if baseline_frac - last_frac > 0.3:
                sudden_drop = True
        if sudden_drop and self.punish_remaining == 0 and (not self.probe_pending) and (self.last_punish_trigger_round != t0):
            self.punish_remaining = self.P
            self.probe_pending = True
            self.last_punish_trigger_round = t0
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        if self.probe_pending:
            self.probe_pending = False
            return Action.C
        if p <= self.T_low:
            return Action.D
        if p >= self.T_high:
            if R >= self.resp_strong:
                threshold = math.floor((self.n - 1) * 0.5)
                if last_others >= threshold:
                    return Action.C
                return Action.D
            else:
                return Action.D
        if R >= self.resp_notice:
            threshold = math.floor((self.n - 1) * 0.5)
            if last_others >= threshold:
                return Action.C
            else:
                return Action.D
        elif random.random() < self.q_rebuild:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC)

    A firm-but-fair public-goods strategy:
    - Lead with cooperation.
    - Track recent cooperation over a short lookback L = min(4, r).
    - If the group is cooperatively behaving (f >= T_high) cooperate.
    - If clearly non-cooperative (f <= T_low) defect and impose a short, bounded punishment.
    - If recent behavior is ambiguous (T_low < f < T_high) be lenient and cooperate.
    - If the previous round had a large cooperative majority but at least one defection,
      send a single-round punitive signal (punishment_counter = 1).
    - If nobody cooperated for L consecutive rounds, defect forever.
    - In the final round, only cooperate if recent cooperation is reliably high (f >= T_high),
      unless an existing punishment forces defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.defect_forever: bool = False
        self.T_high: float = 0.8
        self.T_low: float = 0.5
        self.L: int = min(4, int(self.game_description.n_rounds))
        ratio = float(self.game_description.k) / float(self.game_description.n_players)
        self.P_base: int = 5 if ratio >= 0.6 else 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        if self.defect_forever:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        L_effective = min(self.L, t)
        start_idx = t - L_effective
        end_idx = t
        if L_effective == 0:
            return Action.D
        opp_actions_slice = history.opponent_actions[start_idx:t, :]
        my_actions_slice = history.my_actions[start_idx:t]
        opp_coop_count = int(np.sum(opp_actions_slice)) if opp_actions_slice.size > 0 else 0
        my_coop_count = int(np.sum(my_actions_slice)) if my_actions_slice.size > 0 else 0
        denom = (n - 1) * L_effective
        f = float(opp_coop_count) / float(denom) if denom > 0 else 0.0
        total_coop_last_L = opp_coop_count + my_coop_count
        if L_effective == self.L and total_coop_last_L == 0:
            self.defect_forever = True
            return Action.D
        if t == r - 1:
            return Action.C if f >= self.T_high else Action.D
        prev_round_idx = t - 1
        prev_my = bool(history.my_actions[prev_round_idx])
        prev_opp_row = history.opponent_actions[prev_round_idx, :]
        prev_opp_coops = int(np.sum(prev_opp_row)) if prev_opp_row.size > 0 else 0
        prev_total_coops = (1 if prev_my else 0) + prev_opp_coops
        prev_fraction = float(prev_total_coops) / float(n) if n > 0 else 0.0
        if prev_fraction >= self.T_high and prev_total_coops < n:
            self.punishment_counter = 1
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        if f >= self.T_high:
            return Action.C
        if f <= self.T_low:
            r_rem = r - t
            remaining_for_punish = max(0, r_rem - 1)
            P_max = min(self.P_base, remaining_for_punish)
            P = min(P_max, max(1, round(self.P_base)))
            self.punishment_counter = int(P)
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    ProbeExploitAdapt (PEA) strategy for N-player public goods game.

    - Probes initially to elicit cooperation.
    - Exploits persistent/unconditional cooperators with controlled defect streaks,
      inserting occasional maintenance cooperations to avoid permanent ostracism.
    - Detects and responds to punishment (retaliation) by switching to cautious
      conditional cooperation.
    - In low-cooperation environments, defects but periodically probes for recovery.
    - Always defects in the final two rounds (backward-induction robustness).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.b = self.k / self.n
        self.w = min(10, max(3, math.floor(self.r / 10)))
        self.m = min(3, max(1, math.floor(self.r / 20)))
        self.T_high = 0.7
        self.T_low = 0.3
        self.punish_threshold = 0.25
        self.K_exploit_max = 5
        self.M = 7
        self.random_probe_prob = 0.08
        self.last_probe_round = -self.M
        self.punishers_were_present = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        opponents_count = max(0, self.n - 1)
        if self.r <= 2:
            return Action.D
        if t >= self.r - 2:
            return Action.D
        if t < self.m:
            return Action.C
        if history is None:
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        rounds_played = my_actions.shape[0]
        if opp_actions.ndim == 1:
            opp_actions = opp_actions.reshape(-1, 1)
        window_start = max(0, rounds_played - self.w)
        window_end = rounds_played
        window_len = max(0, window_end - window_start)
        if window_len > 0 and opponents_count > 0:
            recent_opp = opp_actions[window_start:window_end, :]
            p_i = np.array(np.mean(recent_opp.astype(float), axis=0))
        else:
            p_i = np.zeros((opponents_count,), dtype=float)
        g = float(np.mean(p_i) if p_i.size > 0 else 0.0)
        many_unconditional = int(np.sum(p_i >= 0.9)) >= 1
        any_extremely_unconditional = np.any(p_i >= 0.95) if p_i.size > 0 else False
        punishers_present = False
        for idx in range(window_start, window_end):
            if idx >= rounds_played:
                continue
            if my_actions[idx]:
                continue
            prev_start = max(window_start, idx - 2)
            prev_end = idx
            next_start = idx + 1
            next_end = min(rounds_played, idx + 3)
            prev_len = max(0, prev_end - prev_start)
            next_len = max(0, next_end - next_start)
            if prev_len == 0 or next_len == 0:
                continue
            prev_avg = float(np.mean(opp_actions[prev_start:prev_end, :].astype(float)))
            next_avg = float(np.mean(opp_actions[next_start:next_end, :].astype(float)))
            if prev_avg - next_avg > self.punish_threshold:
                punishers_present = True
                break
        self.punishers_were_present = punishers_present
        cons_defects = 0
        for a in my_actions[::-1]:
            if not bool(a):
                cons_defects += 1
            else:
                break
        raw = 1 + math.floor(10 * (g - self.T_high))
        K_exploit = min(self.K_exploit_max, max(1, int(raw)))
        if punishers_present:
            K_exploit_effective = max(1, math.ceil(self.K_exploit_max / 2))
        elif self.punishers_were_present:
            K_exploit_effective = max(1, math.ceil(K_exploit / 2))
        else:
            K_exploit_effective = K_exploit
        others_cooperated_last = 0
        if rounds_played >= 1 and opp_actions.shape[0] >= 1:
            others_cooperated_last = int(np.sum(opp_actions[-1, :]))
        if self.r <= 3 and t >= 1 and (g < self.T_high):
            return Action.D
        if any_extremely_unconditional and g < self.T_high:
            return Action.D
        if punishers_present:
            if opponents_count > 0:
                threshold_count = math.ceil(0.6 * opponents_count)
                if others_cooperated_last >= threshold_count:
                    return Action.C
            return Action.D
        if many_unconditional and (not punishers_present):
            if cons_defects < K_exploit_effective:
                if random.random() < self.random_probe_prob:
                    return Action.C
                return Action.D
            else:
                return Action.C
        if g >= self.T_high and (not punishers_present):
            if cons_defects < K_exploit_effective:
                if random.random() < self.random_probe_prob:
                    return Action.C
                return Action.D
            else:
                return Action.C
        if g > self.T_low and g < self.T_high:
            if opponents_count > 0:
                if others_cooperated_last > opponents_count / 2:
                    return Action.C
                if np.mean(p_i) >= 0.5:
                    return Action.C
            return Action.D
        if g <= self.T_low:
            if t - self.last_probe_round >= self.M:
                self.last_probe_round = t
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    Adaptive Exploiter for N-Player Public Goods Game.

    - Probes early (round 1) to identify cooperators.
    - Estimates conditional cooperation probabilities p_j(C|I_C) and p_j(C|I_D) from history.
    - Computes immediate expected net gain from cooperating (Delta_immediate).
    - Cooperates if immediately profitable, or if aggregate responsiveness is sufficiently large
      and there are enough rounds remaining to cultivate cooperation.
    - Otherwise defects, but performs rare random probes (epsilon) to detect recoveries.
    - Always defects in the final round. For very short games (r <= 3) is aggressive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_index = state.round_number
        t = current_index + 1
        rem = r - current_index
        epsilon = max(0.02, min(0.08, 4.0 / float(r))) if r > 0 else 0.05
        if current_index == 0:
            if r == 1:
                return Action.D
            return Action.C
        if history is None:
            return Action.D
        if rem == 1:
            return Action.D
        if r <= 3:
            if rem == 2:
                pass
            else:
                return Action.D
        past_rounds = history.opponent_actions.shape[0]
        opponents_count = n - 1
        if past_rounds == 0:
            if random.random() < epsilon and rem > 1:
                return Action.C
            return Action.D
        window_len_for_transition = max(0, past_rounds - 1)
        coop_after_myC = [0] * opponents_count
        coop_after_myD = [0] * opponents_count
        times_I_cooperated_before = [0] * opponents_count
        times_I_defected_before = [0] * opponents_count
        overall_coop_counts = [0] * opponents_count
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        try:
            overall_coop_counts_np = np.sum(opp_actions, axis=0)
            for j in range(opponents_count):
                overall_coop_counts[j] = int(overall_coop_counts_np[j])
        except Exception:
            for j in range(opponents_count):
                s = 0
                for rr in range(past_rounds):
                    if bool(opp_actions[rr, j]):
                        s += 1
                overall_coop_counts[j] = s
        for r_idx in range(past_rounds - 1):
            my_prev = bool(my_actions[r_idx])
            for j in range(opponents_count):
                opp_next = bool(opp_actions[r_idx + 1, j])
                if my_prev:
                    times_I_cooperated_before[j] += 1
                    if opp_next:
                        coop_after_myC[j] += 1
                else:
                    times_I_defected_before[j] += 1
                    if opp_next:
                        coop_after_myD[j] += 1
        p_if_I_C = [0.5] * opponents_count
        p_if_I_D = [0.5] * opponents_count
        R_list = [0.0] * opponents_count
        for j in range(opponents_count):
            if times_I_cooperated_before[j] > 0:
                p = coop_after_myC[j] / times_I_cooperated_before[j]
                p_if_I_C[j] = float(p)
            else:
                p_if_I_C[j] = 0.5
            if times_I_defected_before[j] > 0:
                p = coop_after_myD[j] / times_I_defected_before[j]
                p_if_I_D[j] = float(p)
            else:
                p_if_I_D[j] = 0.5
            R_list[j] = p_if_I_C[j] - p_if_I_D[j]
        expected_others_if_I_C = sum(p_if_I_C)
        expected_others_if_I_D = sum(p_if_I_D)
        R_sum = sum(R_list)
        Delta_immediate = k / float(n) * (1.0 + expected_others_if_I_C - expected_others_if_I_D) - 1.0
        R_threshold = 0.35 * float(max(0, n - 1))
        window_rounds = int(max(4, math.floor(r / 5))) if r > 0 else 4
        start_idx = max(0, past_rounds - window_rounds)
        group_coop_rates = []
        for rr in range(start_idx, past_rounds):
            opp_sum_rr = 0
            for j in range(opponents_count):
                if bool(opp_actions[rr, j]):
                    opp_sum_rr += 1
            self_coop_rr = 1 if bool(my_actions[rr]) else 0
            total_coop = opp_sum_rr + self_coop_rr
            group_coop_rates.append(total_coop / float(n))
        avg_group_coop = float(np.mean(group_coop_rates)) if len(group_coop_rates) > 0 else 1.0
        low_coop_mode = False
        if avg_group_coop < 0.2:
            low_coop_mode = True
        decision = None
        if Delta_immediate >= 0.0:
            decision = Action.C
        elif R_sum >= R_threshold and rem >= 3:
            decision = Action.C
        else:
            decision = Action.D
        if rem == 2:
            if Delta_immediate >= 0.0:
                decision = Action.C
            else:
                decision = Action.D
        if decision == Action.D and rem > 1:
            probe_prob = epsilon / 2.0 if low_coop_mode else epsilon
            if random.random() < probe_prob:
                return Action.C
        return decision

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    Collective-minded public goods strategy.

    - Leads with cooperation on the first round.
    - Defaults to cooperation when recent opponent cooperation is above a computed threshold.
    - Uses a one-round mild test (single defection) when cooperation is borderline.
    - Applies proportional, limited punishments when exploitation is clear.
    - Forgives after F consecutive sufficiently cooperative rounds.
    - If the group collapses to (near) zero cooperation, performs occasional deterministic probes to try to re-start cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_remaining: int = 0
        self.last_punish_start: int | None = None
        self.last_test_round: int = -10000
        self.last_probe_round: int = -10000
        self.probes_since_reset: int = 0
        self.collapsed_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if state.round_number == r - 1:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        W = max(3, round(0.15 * r))
        W = min(W, r)
        denom = max(1, n - 1)
        R_coop = 0.6 + 0.2 * ((n - k) / max(1.0, n - 1))
        F = 2
        M = max(3, round(r / 10))
        rounds_played = int(history.my_actions.shape[0])
        available_W = min(W, rounds_played)
        if available_W <= 0:
            recent_coop_rate = 1.0
        else:
            opp_slice = history.opponent_actions[-available_W:, :]
            opp_coop_count = int(np.sum(opp_slice))
            recent_coop_rate = opp_coop_count / (available_W * (n - 1))
        last_opp_coops = int(np.sum(history.opponent_actions[-1, :]))
        last_round_defectors = n - 1 - last_opp_coops
        S_defect = 0
        for idx in range(rounds_played - 1, -1, -1):
            my_act = bool(history.my_actions[idx])
            opp_count = int(np.sum(history.opponent_actions[idx, :]))
            total_contrib = (1 if my_act else 0) + opp_count
            if total_contrib < 1:
                S_defect += 1
            else:
                break
        if S_defect >= W:
            if recent_coop_rate >= R_coop:
                self.probes_since_reset = 0
                self.collapsed_mode = False
            elif state.round_number - self.last_probe_round >= M:
                self.last_probe_round = state.round_number
                self.probes_since_reset += 1
                if self.probes_since_reset >= 2:
                    self.collapsed_mode = True
                return Action.C
            else:
                if self.collapsed_mode:
                    return Action.D
                return Action.D
        if self.punish_remaining > 0:
            forgiving = False
            if rounds_played >= F:
                sufficiently_coop = True
                for j in range(1, F + 1):
                    opp_coops = int(np.sum(history.opponent_actions[-j, :]))
                    per_round_rate = opp_coops / max(1, n - 1)
                    if per_round_rate < R_coop:
                        sufficiently_coop = False
                        break
                if sufficiently_coop:
                    forgiving = True
            if forgiving:
                self.punish_remaining = 0
                self.last_punish_start = None
                self.probes_since_reset = 0
                self.collapsed_mode = False
                return Action.C
            else:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                return Action.D
        if recent_coop_rate >= R_coop:
            self.probes_since_reset = 0
            self.collapsed_mode = False
            return Action.C
        borderline_floor = R_coop - 0.15
        if recent_coop_rate < R_coop and recent_coop_rate >= borderline_floor:
            if self.last_test_round != state.round_number - 1:
                self.last_test_round = state.round_number
                return Action.D
            else:
                small_threshold = math.ceil(0.2 * (n - 1))
                if last_round_defectors <= small_threshold:
                    return Action.C
        many_defectors_threshold = math.ceil(0.2 * (n - 1))
        sustained_failure = recent_coop_rate < R_coop - 0.15
        if recent_coop_rate < R_coop and (last_round_defectors >= many_defectors_threshold or sustained_failure):
            raw_fraction = last_round_defectors / (n - 1) if n - 1 > 0 else 1.0
            P = max(1, min(W, math.ceil(raw_fraction * W)))
            self.punish_remaining = P
            self.last_punish_start = state.round_number
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR)

    Adaptive conditional-cooperation strategy for the N-player public goods game.
    - Signals cooperation in the opening rounds.
    - Cooperates when most others have cooperated recently.
    - Initiates short, proportional punishments (defection) when substantial defection is observed.
    - Escalates punishment length if defection persists, but with a capped maximum.
    - Forgives when recent cooperation recovers above the recovery requirement.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_cooperate_rounds = min(3, self.r)
        self.lookback_window = min(3, max(0, self.r - 1))
        self.tau = 0.25
        self.base_punish = 1
        self.max_punish = min(5, max(1, math.floor(self.r / 4)))
        self.recovery_requirement = 0.75
        self.last_punish_length = 0
        self.punish_ends_at = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        last_round_index = max(0, self.r - 1)
        if current_round == last_round_index:
            return Action.D
        past_rounds = 0 if history is None else int(history.opponent_actions.shape[0])

        def compute_recent_avg() -> float:
            if past_rounds <= 0 or self.lookback_window <= 0:
                return 1.0
            w = min(self.lookback_window, past_rounds)
            rows = history.opponent_actions[-w:, :]
            per_round_rates = np.sum(rows.astype(np.int64), axis=1) / float(max(1, self.n - 1))
            return float(np.mean(per_round_rates))

        def per_opponent_rates() -> list:
            if past_rounds <= 0 or self.lookback_window <= 0:
                return [1.0] * (self.n - 1)
            w = min(self.lookback_window, past_rounds)
            rows = history.opponent_actions[-w:, :]
            sums = np.sum(rows.astype(np.int64), axis=0)
            rates = [float(s) / float(w) for s in sums]
            return rates
        if current_round < self.initial_cooperate_rounds:
            return Action.C
        if self.punish_ends_at is not None and current_round <= self.punish_ends_at:
            return Action.D
        if self.punish_ends_at is not None and current_round > self.punish_ends_at:
            recent_avg = compute_recent_avg()
            if recent_avg >= self.recovery_requirement:
                self.punish_ends_at = None
                self.last_punish_length = 0
            else:
                if self.last_punish_length <= 0:
                    self.last_punish_length = self.base_punish
                else:
                    self.last_punish_length = min(self.max_punish, 2 * self.last_punish_length)
                self.punish_ends_at = current_round + self.last_punish_length - 1
                return Action.D
        recent_avg = compute_recent_avg()
        if recent_avg >= 1.0 - self.tau:
            return Action.C
        opp_rates = per_opponent_rates()
        low_rate_count = sum((1 for r in opp_rates if r < 1.0 - self.tau))
        low_rate_fraction = float(low_rate_count) / float(max(1, self.n - 1))
        if self.punish_ends_at is None:
            if low_rate_fraction <= self.tau:
                self.last_punish_length = self.base_punish
            else:
                self.last_punish_length = self.base_punish
        elif self.last_punish_length <= 0:
            self.last_punish_length = self.base_punish
        else:
            self.last_punish_length = min(self.max_punish, 2 * self.last_punish_length)
        self.punish_ends_at = current_round + self.last_punish_length - 1
        return Action.D

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    Opportunistic Extractor for N-player Public Goods Game.

    - Short deterministic probes at start to detect reciprocators.
    - Exploits a detected reciprocator core by mostly defecting and sparse maintenance cooperation.
    - Performs cautious rebuild attempts when no reciprocators found.
    - Detects punitive responses and goes into a short defect cooldown, then re-probes.
    - Always defects in the final three rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.w = min(10, max(3, math.floor(self.r / 10)))
        self.m_probe = min(4, max(2, math.floor(self.r / 10)))
        self.S = max(3, math.floor(self.r / 20))
        self.max_rebuild_attempts = math.ceil(self.r / 10)
        self.lastCoopRound = -10000
        self.punishUntil = -1
        self.reciprocator_set = set()
        self.gave_up = False
        self.rebuild_attempts = 0
        self.post_coop_total = 0
        self.post_coop_counts = np.zeros(self.n - 1, dtype=int)
        self.processed_my_coop_indices = set()
        self.short_probe_remaining = 0
        self.short_probe_next_coop = True

    def _process_post_coop_followups(self, history: PlayerHistory):
        """
        Scan past my cooperation rounds in history and update post_coop counts
        for those cooperation rounds where the immediate next round is available.
        """
        if history is None:
            return
        total_rounds = history.my_actions.shape[0]
        for i in range(total_rounds):
            if not history.my_actions[i]:
                continue
            if i in self.processed_my_coop_indices:
                continue
            if i + 1 >= total_rounds:
                continue
            follow_actions = history.opponent_actions[i + 1, :]
            self.post_coop_counts += follow_actions.astype(int)
            self.post_coop_total += 1
            self.processed_my_coop_indices.add(i)
        if self.post_coop_total > 0:
            fractions = self.post_coop_counts / float(self.post_coop_total)
            new_set = set((j for j in range(self.n - 1) if fractions[j] >= 0.6))
            self.reciprocator_set.update(new_set)

    def _compute_others_coop_rate(self, history: PlayerHistory, start: int, end: int) -> float:
        """
        Compute OthersCoopRate over rounds [start, end) (0-based, end exclusive).
        Returns fraction in [0,1]. If no rounds in interval, returns 0.0.
        """
        if history is None:
            return 0.0
        if start >= end:
            return 0.0
        slice_rows = history.opponent_actions[start:end, :]
        rows = slice_rows.shape[0]
        if rows == 0:
            return 0.0
        total_coops = int(np.sum(slice_rows.astype(int)))
        denom = rows * (self.n - 1)
        if denom == 0:
            return 0.0
        return total_coops / denom

    def _compute_reciprocators_rate(self, history: PlayerHistory, start: int, end: int) -> float:
        """
        Compute cooperation rate of currently flagged reciprocators over rounds [start, end).
        If no reciprocators or no rounds, return 0.0.
        """
        if history is None:
            return 0.0
        R = len(self.reciprocator_set)
        if R == 0:
            return 0.0
        if start >= end:
            return 0.0
        cols = sorted(self.reciprocator_set)
        if len(cols) == 0:
            return 0.0
        slice_rows = history.opponent_actions[start:end, :][:, cols]
        rows = slice_rows.shape[0]
        if rows == 0:
            return 0.0
        total_coops = int(np.sum(slice_rows.astype(int)))
        denom = rows * R
        if denom == 0:
            return 0.0
        return total_coops / denom

    def _detect_and_handle_punishment_from_coop(self, coop_round_index: int, history: PlayerHistory):
        """
        Given that I cooperated in coop_round_index and we have subsequent data,
        detect if the group's cooperation dropped sharply (punishment).
        If detected, set punishUntil and remove punitive players from reciprocator set.
        """
        if history is None:
            return
        total_rounds = history.my_actions.shape[0]
        w = self.w
        b_end = coop_round_index
        b_start = max(0, b_end - w)
        a_start = coop_round_index + 1
        a_end = min(total_rounds, a_start + w)
        if b_start >= b_end or a_start >= a_end:
            return
        before_rate = self._compute_others_coop_rate(history, b_start, b_end)
        after_rate = self._compute_others_coop_rate(history, a_start, a_end)
        if before_rate - after_rate >= 0.4:
            cols = list(range(self.n - 1))
            punitive_players = set()
            for j in cols:
                before_slice = history.opponent_actions[b_start:b_end, j]
                after_slice = history.opponent_actions[a_start:a_end, j]
                before_rows = before_slice.shape[0]
                after_rows = after_slice.shape[0]
                before_rate_j = float(np.sum(before_slice.astype(int))) / before_rows if before_rows > 0 else 0.0
                after_rate_j = float(np.sum(after_slice.astype(int))) / after_rows if after_rows > 0 else 0.0
                if before_rate_j - after_rate_j >= 0.4:
                    punitive_players.add(j)
            self.reciprocator_set.difference_update(punitive_players)
            last_observed_round = history.my_actions.shape[0] - 1
            cooldown = math.ceil(self.r / 20)
            self.punishUntil = last_observed_round + cooldown
            self.short_probe_remaining = 2
            self.short_probe_next_coop = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        idx = state.round_number
        if idx == 0:
            self.lastCoopRound = -10000
            return Action.C
        if history is None:
            return Action.D
        self._process_post_coop_followups(history)
        for coop_i in list(self.processed_my_coop_indices):
            if coop_i + 1 < history.my_actions.shape[0]:
                self._detect_and_handle_punishment_from_coop(coop_i, history)
        if idx >= self.r - 3:
            return Action.D
        if self.punishUntil >= 0 and idx <= self.punishUntil:
            return Action.D
        if self.short_probe_remaining > 0:
            action = Action.C if self.short_probe_next_coop else Action.D
            if action == Action.C:
                self.lastCoopRound = idx
            self.short_probe_remaining -= 1
            self.short_probe_next_coop = not self.short_probe_next_coop
            return action
        if idx < self.m_probe:
            probe_idx = idx
            if probe_idx % 2 == 0:
                self.lastCoopRound = idx
                return Action.C
            else:
                return Action.D
        if self.gave_up:
            return Action.D
        prev_rounds = idx
        W = min(self.w, prev_rounds)
        if W <= 0:
            q = 0.0
        else:
            start_idx = idx - W
            q = self._compute_others_coop_rate(history, start_idx, idx)
        R = len(self.reciprocator_set)
        recips_rate = self._compute_reciprocators_rate(history, max(0, idx - W), idx) if W > 0 else 0.0
        if R >= math.ceil(self.n / 3) and recips_rate >= 0.75:
            if q >= 0.6:
                return Action.D
            if idx - self.lastCoopRound >= self.S:
                last_my_action = history.my_actions[-1] if history.my_actions.shape[0] > 0 else False
                if last_my_action:
                    return Action.D
                self.lastCoopRound = idx
                return Action.C
            else:
                return Action.D
        if q <= 0.25:
            if idx - self.lastCoopRound >= self.S:
                if self.rebuild_attempts >= self.max_rebuild_attempts and len(self.reciprocator_set) == 0:
                    self.gave_up = True
                    return Action.D
                last_my_action = history.my_actions[-1] if history.my_actions.shape[0] > 0 else False
                if last_my_action and len(self.reciprocator_set) == 0:
                    return Action.D
                self.rebuild_attempts += 1
                self.lastCoopRound = idx
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    Adaptive Collective Conditional Contribution (ACCC)

    Starts by cooperating. Stays cooperative while a clear majority of others
    have cooperated recently. If cooperation falls below a threshold, enters a
    proportional-but-brief punishment phase, occasionally testing forgiveness.
    If punishments accumulate or long-run cooperation is very low, gives up and
    defects permanently. Uses a stricter threshold near the end of the horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.n_others = max(0, self.n_players - 1)
        self.tau = 0.6
        self.tau_end = 0.85
        self.P_max = min(4, self.n_rounds)
        self.p_test = 0.1
        self.mode = 'C'
        self.remaining_punish_rounds = 0
        self.total_punish_rounds = 0
        self.permanent_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.n_rounds
        n_others = self.n_others
        if n_others <= 0:
            return Action.D
        if state.round_number == 0:
            self.mode = 'C'
            self.remaining_punish_rounds = 0
            self.total_punish_rounds = 0
            self.permanent_defect = False
            return Action.C
        if history is None:
            rounds_played = 0
        else:
            rounds_played = history.opponent_actions.shape[0]
        w = min(5, t - 1)
        if w == 0 or rounds_played == 0:
            recent_coop_rate = 1.0
        else:
            window = history.opponent_actions[-w:, :]
            per_round_fracs = np.sum(window, axis=1) / float(n_others)
            recent_coop_rate = float(np.mean(per_round_fracs))
        if rounds_played == 0:
            overall_coop_rate = 1.0
        else:
            per_round_fracs_all = np.sum(history.opponent_actions, axis=1) / float(n_others)
            overall_coop_rate = float(np.mean(per_round_fracs_all))
        remaining_rounds = r - t + 1
        tau_eff = self.tau_end if remaining_rounds <= 2 else self.tau
        if self.total_punish_rounds > math.floor(r / 3) or overall_coop_rate < 0.2:
            self.permanent_defect = True
        if self.permanent_defect:
            return Action.D
        if t == r:
            if recent_coop_rate >= self.tau_end:
                self.mode = 'C'
                self.remaining_punish_rounds = 0
                return Action.C
            else:
                return Action.D

        def compute_punishment_length(recent_rate: float) -> int:
            avg_others = recent_rate * n_others
            target = tau_eff * n_others
            delta = target - avg_others
            c = math.ceil(delta)
            p_len = 1 + c
            if p_len < 1:
                p_len = 1
            p_len = min(self.P_max, p_len)
            return int(p_len)
        if self.mode != 'P' and recent_coop_rate >= tau_eff:
            self.mode = 'C'
            self.remaining_punish_rounds = 0
            return Action.C
        if self.mode != 'P' and recent_coop_rate < tau_eff:
            p_len = compute_punishment_length(recent_coop_rate)
            self.mode = 'P'
            self.remaining_punish_rounds = max(0, p_len - 1)
            self.total_punish_rounds += 1
            return Action.D
        if self.mode == 'P':
            if self.remaining_punish_rounds > 0:
                if random.random() < self.p_test:
                    action = Action.C
                else:
                    action = Action.D
                self.remaining_punish_rounds -= 1
                self.total_punish_rounds += 1
                return action
            else:
                if w == 0 or rounds_played == 0:
                    recent_coop_rate_post = 1.0
                else:
                    window = history.opponent_actions[-w:, :]
                    per_round_fracs = np.sum(window, axis=1) / float(n_others)
                    recent_coop_rate_post = float(np.mean(per_round_fracs))
                if self.total_punish_rounds > math.floor(r / 3) or overall_coop_rate < 0.2:
                    self.permanent_defect = True
                    return Action.D
                if recent_coop_rate_post >= tau_eff:
                    self.mode = 'C'
                    self.remaining_punish_rounds = 0
                    return Action.C
                else:
                    p_len = compute_punishment_length(recent_coop_rate_post)
                    self.mode = 'P'
                    self.remaining_punish_rounds = max(0, p_len - 1)
                    self.total_punish_rounds += 1
                    return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    Opportunistic Minimal-Provider (OMP)

    Exploitative, minimally cooperative strategy for repeated public goods games.
    Maintains per-opponent statistics from observed history only, probes occasionally,
    free-rides on near-unconditionals, makes tiny targeted investments to sustain
    reciprocators when a conservative payoff test is satisfied, performs one-shot
    repairs when group cooperation dips and reciprocators exist, avoids courting
    punishers, and always defects in the final E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.E = min(3, self.n_rounds)
        self.W = min(10, self.n_rounds)
        self.uncond_threshold = 0.95
        self.delta_threshold = 0.18
        self.group_target = 0.6
        self.cost_fraction = 0.6
        self.p_probe = min(0.2, 4.0 / max(1, self.n_rounds))
        self.p_rand = min(0.05, 1.0 / max(10, self.n_rounds))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        m = max(0, self.n_players - 1)
        if history is None:
            T = 0
            if t > self.n_rounds - self.E:
                return Action.D
            if t <= 2:
                if random.random() < self.p_probe:
                    return Action.C
                return Action.D
            if random.random() < self.p_rand:
                return Action.C
            return Action.D
        T = int(history.my_actions.shape[0])
        if t > self.n_rounds - self.E:
            return Action.D
        if m == 0:
            return Action.D
        opp_act = history.opponent_actions.astype(np.bool_) if T > 0 else np.zeros((0, m), dtype=np.bool_)
        my_act = history.my_actions.astype(np.bool_) if T > 0 else np.zeros((0,), dtype=np.bool_)
        if T > 0:
            R_j = np.full((m,), T, dtype=float)
        else:
            R_j = np.zeros((m,), dtype=float)
        if T > 0:
            C_j = opp_act.astype(np.int32).sum(axis=0).astype(float)
        else:
            C_j = np.zeros((m,), dtype=float)
        if T >= 2:
            prev_my = my_act[:-1]
            next_opp = opp_act[1:, :]
            mask_afterC = prev_my
            denom_afterC = int(mask_afterC.sum())
            if denom_afterC > 0:
                C_j_afterC = next_opp[mask_afterC, :].astype(np.int32).sum(axis=0).astype(float)
                p_afterC = C_j_afterC / float(denom_afterC)
            else:
                C_j_afterC = np.zeros((m,), dtype=float)
                p_afterC = np.zeros((m,), dtype=float)
            mask_afterD = ~prev_my
            denom_afterD = int(mask_afterD.sum())
            if denom_afterD > 0:
                C_j_afterD = next_opp[mask_afterD, :].astype(np.int32).sum(axis=0).astype(float)
                p_afterD = C_j_afterD / float(denom_afterD)
            else:
                C_j_afterD = np.zeros((m,), dtype=float)
                p_afterD = np.zeros((m,), dtype=float)
        else:
            denom_afterC = 0
            denom_afterD = 0
            C_j_afterC = np.zeros((m,), dtype=float)
            C_j_afterD = np.zeros((m,), dtype=float)
            p_afterC = np.zeros((m,), dtype=float)
            p_afterD = np.zeros((m,), dtype=float)
        p_j = np.zeros((m,), dtype=float)
        if T > 0:
            p_j = C_j / R_j
        delta_j = p_afterC - p_afterD
        n_rec = int((delta_j >= self.delta_threshold).sum())
        n_uncond = int((p_j >= self.uncond_threshold).sum())
        punisher_mask = (delta_j <= -self.delta_threshold) & (p_afterC <= 0.1)
        n_punishers = int(punisher_mask.sum())
        last_W = min(self.W, T)
        if last_W > 0:
            recent_block = opp_act[-last_W:, :]
            G = float(recent_block.astype(np.int32).sum()) / float(last_W * m)
        else:
            G = 0.0
        if n_uncond >= 1:
            return Action.D
        if T >= 3 and float((p_j <= 0.05).all()):
            return Action.D
        avg_R = float(R_j.mean()) if R_j.size > 0 else 0.0
        if t <= 2 or avg_R < 3.0:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        effective_delta = delta_j.copy()
        if n_punishers > 0:
            effective_delta[punisher_mask] = 0.0
        S = float(effective_delta.sum())
        marginal_next_round_benefit = self.k / float(self.n_players) * S
        if marginal_next_round_benefit >= self.cost_fraction:
            return Action.C
        reciprocator_mask = (delta_j >= self.delta_threshold) & ~punisher_mask
        n_rec_effective = int(reciprocator_mask.sum())
        if G < self.group_target and n_rec_effective >= 1:
            return Action.C
        if n_punishers >= 1 and n_rec_effective == 0:
            return Action.D
        if random.random() < self.p_rand:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC)

    Aims to sustain group cooperation while avoiding long-term exploitation.
    - Uses a short recent window L = min(5, total_rounds) to evaluate group behavior.
    - Tracks a short Punish counter (instance attribute) for proportional, brief punishments.
    - Cooperates by default, defects in the final round, and uses thresholds on recent
      group cooperation fraction to decide when to punish or forgive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.L = min(5, self.r)
        self.punish = 0
        self._low_coop_standard = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        R_rem = self.r - state.round_number
        if R_rem <= 1:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        M = min(self.L, state.round_number)
        start_idx = state.round_number - M
        my_actions_window = np.asarray(history.my_actions[start_idx:state.round_number], dtype=np.bool_)
        opp_actions_window = np.asarray(history.opponent_actions[start_idx:state.round_number, :], dtype=np.bool_)
        total_cooperators_per_round = my_actions_window.astype(int) + np.sum(opp_actions_window.astype(int), axis=1)
        total_contribs = int(np.sum(total_cooperators_per_round))
        denom = self.n * M
        if denom == 0:
            f = 0.0
        else:
            f = float(total_contribs) / float(denom)
        exploited_flags = (my_actions_window.astype(int) == 1) & (total_cooperators_per_round.astype(float) / float(self.n) < float(self._low_coop_standard))
        exploitation_count = int(np.sum(exploited_flags))
        e = float(exploitation_count) / float(M) if M > 0 else 0.0
        if R_rem == 2:
            if f >= 0.7:
                return Action.C
            return Action.D
        if self.punish > 0:
            if f >= 0.6:
                self.punish = 0
                return Action.C
            self.punish = max(0, self.punish - 1)
            return Action.D
        if f >= 0.6:
            return Action.C
        if 0.4 <= f < 0.6:
            if e > 0.5:
                self.punish = 1
                return Action.D
            return Action.C
        self.punish = min(3, max(1, R_rem - 1))
        return Action.D

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    Collective-First Conditional Cooperation (CFCC)

    - Starts by cooperating.
    - Uses a sliding recent window (W = min(5, r)) over opponents' contributions to compute
      per-player recent cooperation rates and a group recent cooperation rate.
    - Issues short, proportional punishments against persistent under-contributors,
      uses single-round retaliation for isolated defections, rewards unanimous cooperation,
      probes with small probability when the group is mostly defecting, and avoids cooperating
      in the final round (and is cautious in the final two rounds unless the previous round
      was unanimous cooperation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_timer = 0
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r_total = int(self.game_description.n_rounds)
        t_index = state.round_number
        t = t_index + 1
        W = min(5, r_total)
        if t_index == 0 or history is None:
            return Action.C
        last_round_idx = t_index - 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        try:
            opponents_last = bool(opp_actions[last_round_idx, :].all())
        except Exception:
            opponents_last = False
        try:
            i_last_coop = bool(my_actions[last_round_idx])
        except Exception:
            i_last_coop = False
        unanimous_last = opponents_last and i_last_coop
        if t == r_total:
            return Action.D
        Rem = r_total - t + 1
        if Rem <= 2:
            if unanimous_last:
                return Action.C
            return Action.D
        num_past_rounds = last_round_idx + 1
        window_size = min(W, num_past_rounds)
        if window_size <= 0:
            return Action.C
        start_idx = last_round_idx - window_size + 1
        recent_opp_window = opp_actions[start_idx:last_round_idx + 1, :]
        coop_counts_per_opponent = np.sum(recent_opp_window.astype(np.int64), axis=0)
        recent_rates = coop_counts_per_opponent.astype(float) / float(window_size)
        if recent_rates.size > 0:
            group_recent_rate = float(np.mean(recent_rates))
        else:
            group_recent_rate = 0.0
        total_coop_recent = int(np.sum(coop_counts_per_opponent))
        recent_defections = window_size * (n - 1) - total_coop_recent
        if getattr(self, 'punish_timer', 0) > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        if unanimous_last:
            return Action.C
        if group_recent_rate >= 0.7:
            return Action.C
        persistent_mask = recent_rates <= 0.4
        if np.any(persistent_mask):
            L = min(5, 1 + int(recent_defections))
            self.punish_timer = max(0, L - 1)
            return Action.D
        try:
            any_defected_last = not bool(opp_actions[last_round_idx, :].all())
        except Exception:
            any_defected_last = False
        if any_defected_last:
            return Action.D
        if group_recent_rate < 0.4:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    Adaptive Collective Cooperation with Proportional Punishment (ACCPP)

    - Begins by cooperating.
    - Uses a recent-memory window W to measure each opponent's cooperation rate.
    - If the group's recent cooperation (excluding self) is high (>= ) it cooperates.
    - If cooperation falls below :
        * Forgives isolated single-round lapses.
        * If no clear suspects (everyone >= ) uses a lenient probabilistic cooperate.
        * If one or more suspected defectors (coop_rate < ) starts a targeted punishment
          phase: defect for L rounds (L scales with number of suspects, capped), with
          small-probability contrition cooperations.
    - Punishment can be terminated early if suspected defectors recover (coop_rate_i >= )
      and group cooperation >=  for S consecutive rounds.
    - Endgame: final round is defect unless near-perfect full-history cooperation; last two
      rounds apply a stricter _end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(8, max(3, math.floor(self.r_rounds / 5)))
        self.Theta = 0.6
        self.tau = 0.5
        self.S = 2
        self.L_max = min(4, max(1, self.r_rounds - 1))
        self.q = 0.2
        self.active_punishment = False
        self.punishment_remaining = 0
        self.punishment_total = 0
        self.punished_suspects = tuple()
        self.recovery_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        m = history.opponent_actions.shape[0]
        n_opponents = max(1, history.opponent_actions.shape[1])
        rounds_left = self.r_rounds - state.round_number
        if state.round_number >= max(0, self.r_rounds - 2):
            Theta_eff = min(0.95, self.Theta + 0.2)
        else:
            Theta_eff = self.Theta
        w = min(self.W, m)
        w_prev = min(self.W, max(0, m - 1))

        def coop_rates_over(slice_start: int, slice_end: int):
            if slice_end <= slice_start:
                return np.zeros((n_opponents,), dtype=float)
            sub = history.opponent_actions[slice_start:slice_end, :]
            return np.array(np.mean(sub.astype(float), axis=0))
        start_curr = max(0, m - w)
        coop_rate_curr = coop_rates_over(start_curr, m)
        group_rate_curr = float(np.mean(coop_rate_curr)) if coop_rate_curr.size > 0 else 0.0
        if m >= 2 and w_prev > 0:
            start_prev = max(0, m - 1 - w_prev)
            coop_rate_prev = coop_rates_over(start_prev, m - 1)
            group_rate_prev = float(np.mean(coop_rate_prev)) if coop_rate_prev.size > 0 else group_rate_curr
        else:
            coop_rate_prev = coop_rate_curr.copy()
            group_rate_prev = group_rate_curr
        suspects_mask = coop_rate_curr < self.tau
        suspected_indices = tuple((int(i) for i in np.nonzero(suspects_mask)[0]))
        d = len(suspected_indices)
        full_history_coop_rates = coop_rates_over(0, m)
        full_group_rate = float(np.mean(full_history_coop_rates)) if full_history_coop_rates.size > 0 else 0.0
        if state.round_number == self.r_rounds - 1:
            if full_group_rate >= 0.95 and d == 0:
                return Action.C
            return Action.D
        if self.active_punishment:
            suspects_recovered = True
            if len(self.punished_suspects) == 0:
                suspects_recovered = True
            else:
                for si in self.punished_suspects:
                    if si < 0 or si >= coop_rate_curr.size:
                        continue
                    if coop_rate_curr[si] < self.tau:
                        suspects_recovered = False
                        break
            if suspects_recovered and group_rate_curr >= Theta_eff:
                self.recovery_counter += 1
            else:
                self.recovery_counter = 0
            if self.recovery_counter >= self.S:
                self.active_punishment = False
                self.punishment_remaining = 0
                self.punished_suspects = tuple()
                self.recovery_counter = 0
                return Action.C
            choice = Action.C if random.random() < self.q else Action.D
            if self.punishment_remaining > 0:
                self.punishment_remaining -= 1
            if self.punishment_remaining <= 0:
                self.active_punishment = False
            return choice
        forgiving_one_off = False
        if m >= 1 and w_prev > 0:
            prev_counts = np.sum(history.opponent_actions[max(0, m - 1 - w_prev):m - 1, :], axis=1)
            avg_prev_cooperators = float(np.mean(prev_counts)) if prev_counts.size > 0 else 0.0
            last_round_cooperators = float(np.sum(history.opponent_actions[-1, :]))
            if avg_prev_cooperators / max(1, n_opponents) >= Theta_eff and group_rate_curr < Theta_eff:
                if avg_prev_cooperators - last_round_cooperators >= 0.9 and avg_prev_cooperators - last_round_cooperators <= 1.1:
                    forgiving_one_off = True
        if forgiving_one_off:
            return Action.C
        try:
            my_last_action_was_D = history.my_actions[-1] == False
        except Exception:
            my_last_action_was_D = False
        if not self.active_punishment and my_last_action_was_D and (d == 0) and (group_rate_prev >= Theta_eff):
            return Action.C
        if group_rate_curr >= Theta_eff:
            return Action.C
        if d == 0:
            if Theta_eff > 0:
                p = 1.0 - (Theta_eff - group_rate_curr) / max(1e-09, Theta_eff)
            else:
                p = 0.2
            p = max(0.2, min(0.9, p))
            return Action.C if random.random() < p else Action.D
        L = max(1, 1 + d)
        L = min(self.L_max, L)
        remaining_rounds = max(0, self.r_rounds - state.round_number - 1)
        if remaining_rounds <= 0:
            return Action.D
        L = min(L, remaining_rounds)
        self.active_punishment = True
        self.punishment_total = L
        self.punishment_remaining = L - 1
        self.punished_suspects = suspected_indices
        self.recovery_counter = 0
        if random.random() < self.q:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    Adaptive Opportunist for N-Player Public Goods Game.

    - Starts by signalling cooperation and probing opponents in early rounds.
    - Labels opponents as naive / retaliator / conditional based on recent behavior.
    - Exploits naive majorities when group cooperation is strong.
    - Otherwise uses calibrated reciprocity with bounded punishments and occasional stochastic probes.
    - Always defects in the final E rounds and includes safety fallback when group cooperation collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.w = min(20, max(3, self.n_rounds // 5))
        self.PR = min(4, max(2, self.n_rounds // 10))
        self.E = min(2, self.n_rounds)
        self.p_high = 0.8
        self.delta = 0.15
        self.L_punish_base = 1
        probe_indices = []
        if self.PR >= 2:
            probe_indices.append(1)
            mid = 1 + self.PR // 2
            if mid != 1 and mid < self.PR:
                probe_indices.append(mid)
        self.probe_defect_rounds = sorted({i for i in probe_indices if 0 <= i < self.PR})
        self.exploit_mode = False
        self.punish_until = -1
        self.last_punish_end = -1
        self.low_coop_windows = 0
        self.safety_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.n_rounds
        n = self.n_players
        m = max(1, n - 1)
        if t == 0 or history is None:
            return Action.C
        if t >= r - self.E:
            self.exploit_mode = False
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_so_far = my_actions.shape[0]
        start = max(0, rounds_so_far - self.w)
        recent_rounds = rounds_so_far - start
        if recent_rounds <= 0:
            p_js = np.zeros(m, dtype=float)
        else:
            opp_slice = opp_actions[start:rounds_so_far, :]
            p_js = np.array(np.mean(opp_slice.astype(float), axis=0), dtype=float)
        defect_indices = [i for i in range(rounds_so_far - 1) if not bool(my_actions[i])]
        q_js = [None] * m
        if len(defect_indices) > 0:
            next_actions = np.array([opp_actions[i + 1, :] for i in defect_indices], dtype=np.bool_)
            means = np.mean(next_actions.astype(float), axis=0)
            for j in range(m):
                q_js[j] = float(means[j])
        else:
            q_js = [None] * m
        labels = []
        for j in range(m):
            p_j = float(p_js[j])
            q_j = q_js[j]
            if p_j >= self.p_high and (q_j is None or q_j >= p_j - self.delta):
                labels.append('naive')
            elif q_j is not None and q_j <= p_j - self.delta:
                labels.append('retaliator')
            else:
                labels.append('conditional')
        naive_count = sum((1 for lbl in labels if lbl == 'naive'))
        retaliator_count = sum((1 for lbl in labels if lbl == 'retaliator'))
        conditional_count = sum((1 for lbl in labels if lbl == 'conditional'))
        naive_frac = naive_count / m
        retaliator_frac = retaliator_count / m
        conditional_frac = conditional_count / m
        prev_idx = rounds_so_far - 1
        if prev_idx >= 0:
            opp_prev = opp_actions[prev_idx, :].astype(float)
            my_prev = float(my_actions[prev_idx])
            G_prev = (np.sum(opp_prev) + my_prev) / float(n)
        else:
            G_prev = 0.0
        if recent_rounds <= 0:
            group_coop_rate = 0.0
        else:
            opp_block = opp_actions[start:rounds_so_far, :].astype(float)
            my_block = my_actions[start:rounds_so_far].astype(float)[:, None]
            combined = np.concatenate([my_block, opp_block], axis=1)
            group_coop_rate = float(np.mean(combined))
        if group_coop_rate < 0.2:
            self.low_coop_windows += 1
        else:
            self.low_coop_windows = 0
        if self.low_coop_windows >= 2:
            self.safety_defect = True
        if group_coop_rate >= 0.25:
            self.safety_defect = False
        if self.safety_defect:
            self.exploit_mode = False
            return Action.D
        if rounds_so_far < self.PR and t < r - self.E:
            if t in self.probe_defect_rounds:
                return Action.D
            else:
                return Action.C
        punish_decision_made = False
        if prev_idx >= 0:
            opp_defected_prev = (~opp_actions[prev_idx, :]).astype(float)
            num_defected_prev = int(np.sum(opp_defected_prev))
            fraction_defected_prev = num_defected_prev / float(m)
            if num_defected_prev > 0:
                punish_len = min(3, 1 + round(2 * fraction_defected_prev * max(1, self.w) / max(1, self.PR)))
                self.punish_until = t - 1 + punish_len
                self.last_punish_end = self.punish_until
                punish_decision_made = True
        if t <= self.punish_until:
            return Action.D
        if self.last_punish_end != -1 and t > self.last_punish_end:
            if prev_idx >= 0:
                opp_prev_coop = np.sum(opp_actions[prev_idx, :].astype(float))
                my_prev_coop = float(my_actions[prev_idx])
                total_prev = opp_prev_coop + my_prev_coop
                if total_prev / float(n) >= 0.5:
                    self.punish_until = -1
                    self.last_punish_end = -1
        classification_ready = rounds_so_far >= self.PR or recent_rounds >= self.w
        if classification_ready and naive_frac >= 0.5 and (G_prev >= 0.6):
            if retaliator_frac == 0:
                self.exploit_mode = True
            else:
                self.exploit_mode = False
        if self.exploit_mode:
            if conditional_frac >= 0.3:
                prob_coop_gesture = 0.1
                if random.random() < prob_coop_gesture:
                    return Action.C
            if naive_frac < 0.5 or retaliator_frac > 0.0:
                self.exploit_mode = False
            else:
                return Action.D
        if classification_ready and 0.2 <= naive_frac < 0.5 and (G_prev >= 0.6):
            if retaliator_frac == 0:
                return Action.D
            else:
                pass
        L = min(3, self.w)
        recent_start_L = max(0, rounds_so_far - L)
        if recent_start_L < rounds_so_far:
            recent_opp_block = opp_actions[recent_start_L:rounds_so_far, :].astype(float)
            any_recent_defection = np.any(recent_opp_block == 0.0)
        else:
            any_recent_defection = False
        if not any_recent_defection and random.random() < 0.05:
            return Action.D
        if not any_recent_defection:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Starts cooperative (unless single-shot).
    - Uses a sliding window of recent opponent contributions to compute p_hat.
    - Cooperates when recent cooperation >= alpha (max(0.5, k/n)).
    - Triggers bounded punishments when p_hat < alpha; punishment length scales with severity.
    - After a punishment block, does one cooperative "test" round to probe for recovery.
    - If recent cooperation is very high (>= 0.95) clears punishments and resumes cooperation.
    - Endgame-aware: defects in final round; in the penultimate round cooperates only if near-full cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        self.alpha = max(0.5, self.k / max(1, self.n))
        self.m_base = 1
        self.m_max = 3
        self.h_end = min(2, self.r)
        self.punish_counter = 0
        self.pending_test = False
        self.last_w_history = np.zeros((0, max(0, self.n - 1)), dtype=np.bool_)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        w = self.w
        alpha = self.alpha

        def compute_p_hat(opponent_slice):
            if opponent_slice is None:
                return 0.0
            if opponent_slice.size == 0:
                return 0.0
            return float(np.mean(opponent_slice.astype(np.float64)))
        if t == 0:
            self.last_w_history = np.zeros((0, max(0, n - 1)), dtype=np.bool_)
            if r <= 1:
                return Action.D
            return Action.C
        past_rounds_available = history.opponent_actions.shape[0] if history is not None else 0
        rounds_to_consider = min(w, past_rounds_available)
        if rounds_to_consider > 0:
            opponent_slice = history.opponent_actions[-rounds_to_consider:, :]
        else:
            opponent_slice = np.zeros((0, max(0, n - 1)), dtype=np.bool_)
        self.last_w_history = np.array(opponent_slice, copy=True)
        p_hat = compute_p_hat(opponent_slice)
        if t == r - 1:
            return Action.D
        if r >= 2 and t == r - 2:
            if p_hat >= 0.95:
                self.punish_counter = 0
                self.pending_test = False
                return Action.C
            return Action.D
        if p_hat >= 0.95:
            self.punish_counter = 0
            self.pending_test = False
            return Action.C
        if self.pending_test:
            self.pending_test = False
            return Action.C
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            if self.punish_counter == 0:
                self.pending_test = True
            return Action.D
        if p_hat >= alpha:
            return Action.C
        severity = alpha - p_hat
        if severity < 0:
            severity = 0.0
        punish_length = int(min(self.m_max, self.m_base + math.ceil(severity * 3.0)))
        if punish_length <= 1:
            self.punish_counter = 0
            self.pending_test = True
        else:
            self.punish_counter = punish_length - 1
        return Action.D

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    Exploitative Public Goods strategy.

    - Probes early for cooperators (1-2 rounds).
    - In later rounds, computes recent cooperation rates over a sliding window.
    - If group is highly cooperative, run short exploitation bursts (defect a few rounds).
    - If group is mixed, mirror the group's previous-round behavior (majority).
    - If group is uncooperative, mostly defect but occasionally 'lure' (cooperate with small probability)
      to find latent cooperators. When individuals are identified as reliably cooperative, exploit them
      in short targeted bursts.
    - Forgives after short punishments once cooperation recovers.
    - Always defect in the final few rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.probe_rounds = min(2, self.r)
        self.window_w = min(8, max(2, math.floor(self.r / 5))) if self.r >= 1 else 2
        self.high_coop = 0.6
        self.low_coop = 0.4
        self.exploit_burst_max = 3
        self.lure_prob = 0.2
        self.forgiveness_reset = 3
        self.final_defect_rounds = min(3, self.r)
        if self.n <= 2:
            self.lure_prob = 0.1
            self.exploit_burst_max = 1
        self._reset_dynamic_state()

    def _reset_dynamic_state(self):
        self.burst_active = False
        self.burst_remaining = 0
        self.targeted_exploitees = set()
        self.last_punish_round = None
        self.last_lure_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        n = self.n
        num_opponents = max(0, n - 1)
        if r == 1:
            if state.round_number == 0:
                self._reset_dynamic_state()
            return Action.D
        if state.round_number == 0:
            self._reset_dynamic_state()
            if self.probe_rounds >= 1:
                return Action.C
            else:
                return Action.D
        if history is None:
            return Action.D
        rounds_played = int(history.my_actions.shape[0])
        effective_w = min(self.window_w, rounds_played)
        if effective_w > 0 and history.opponent_actions.size > 0:
            window_slice = history.opponent_actions[-effective_w:, :]
            if num_opponents > 0:
                p_j = np.mean(window_slice.astype(float), axis=0)
            else:
                p_j = np.zeros(0, dtype=float)
        else:
            p_j = np.zeros(num_opponents, dtype=float)
        P = float(np.mean(p_j)) if p_j.size > 0 else 0.0
        if effective_w > 0 and history.my_actions.size > 0:
            my_window = history.my_actions[-effective_w:].astype(float)
            M = float(np.mean(my_window)) if my_window.size > 0 else 0.0
        else:
            M = 0.0
        if t > r - self.final_defect_rounds:
            self.burst_active = False
            self.burst_remaining = 0
            return Action.D
        last_index = rounds_played - 1
        if last_index >= 0:
            last_op_actions = history.opponent_actions[last_index, :] if num_opponents > 0 else np.array([], dtype=bool)
            opp_coops_last = int(np.sum(last_op_actions)) if last_op_actions.size > 0 else 0
            majority_threshold = num_opponents // 2 + (1 if num_opponents % 2 == 1 else 0)
            we_defected_last = not bool(history.my_actions[last_index])
            if we_defected_last and opp_coops_last >= majority_threshold:
                self.last_punish_round = t - 1
        if self.last_punish_round is not None:
            if P >= self.low_coop and t - self.last_punish_round >= self.forgiveness_reset:
                self.last_punish_round = None
                self.burst_active = False
                self.burst_remaining = 0
        newly_targeted = set()
        for j_idx, pj in enumerate(p_j):
            if pj >= self.high_coop:
                newly_targeted.add(j_idx)
            elif self.last_lure_round is not None and t - self.last_lure_round <= self.forgiveness_reset and (pj > self.low_coop):
                newly_targeted.add(j_idx)
        self.targeted_exploitees = {j for j in self.targeted_exploitees | newly_targeted if (p_j[j] >= self.low_coop if j < p_j.size else False)}
        if P >= self.high_coop:
            if not self.burst_active:
                self.burst_active = True
                self.burst_remaining = self.exploit_burst_max
            if self.burst_active and self.burst_remaining > 0 and (P >= self.low_coop):
                self.burst_remaining -= 1
                if self.burst_remaining <= 0:
                    self.burst_active = False
                return Action.D
            else:
                self.burst_active = False
                self.burst_remaining = 0
        elif self.burst_active and P < self.low_coop:
            self.burst_active = False
            self.burst_remaining = 0
        if P >= self.low_coop and P < self.high_coop:
            has_strong_individual = any((pj >= self.high_coop for pj in p_j)) if p_j.size > 0 else False
            if has_strong_individual:
                if not self.burst_active:
                    self.burst_active = True
                    self.burst_remaining = self.exploit_burst_max
                if self.burst_remaining > 0:
                    self.burst_remaining -= 1
                    if self.burst_remaining <= 0:
                        self.burst_active = False
                    return Action.D
            if history.opponent_actions.shape[0] >= 1 and num_opponents > 0:
                last_round_ops = history.opponent_actions[-1, :]
                coop_count_last = int(np.sum(last_round_ops))
                if coop_count_last > num_opponents / 2.0:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if P < self.low_coop:
            if len(self.targeted_exploitees) > 0:
                if not self.burst_active:
                    self.burst_active = True
                    self.burst_remaining = self.exploit_burst_max
                if self.burst_remaining > 0:
                    self.burst_remaining -= 1
                    if self.burst_remaining <= 0:
                        self.burst_active = False
                    return Action.D
            if random.random() < self.lure_prob:
                self.last_lure_round = t
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Punishment & Forgiveness (C3-GPF).
    Signals cooperation initially, sustains cooperation when group cooperation rate is high,
    punishes proportionately with escalating but limited punishments, probes to discover
    restoration, forgives on successful restoration, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        if self.r <= 4:
            self.m = 1
            self.B = 1
        else:
            self.m = min(5, max(1, self.r // 4))
            self.B = 2
        self.T = 0.85
        self.p_probe = 0.2
        self.punishment_counter = 0
        self.escalation_level = 0
        self.probe_scheduled = False
        self.last_probe_round = -1
        self._last_probe_evaluated_for_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        remaining = max(0, self.r - t)
        if t == 0 or history is None:
            return Action.C
        if t == self.r - 1:
            return Action.D
        if remaining <= 3:
            past_rounds = t
            if past_rounds == 0:
                return Action.D
            all_my = bool(np.all(history.my_actions[:past_rounds]))
            all_opp = bool(np.all(history.opponent_actions[:past_rounds, :]))
            if all_my and all_opp:
                return Action.C
            else:
                return Action.D

        def compute_recent_stats():
            end = t
            start = max(0, end - self.m)
            available = end - start
            if available <= 0:
                return (1.0, 0)
            my_sum = int(np.sum(history.my_actions[start:end]))
            opp_sum = int(np.sum(history.opponent_actions[start:end, :]))
            total_contrib = my_sum + opp_sum
            observed_group_rate = total_contrib / (self.n * available)
            self_defected = bool(np.any(~history.my_actions[start:end]))
            if history.opponent_actions.shape[1] > 0:
                opp_defected_flags = np.any(~history.opponent_actions[start:end, :], axis=0)
                opp_defectors_count = int(np.sum(opp_defected_flags))
            else:
                opp_defectors_count = 0
            recent_defectors = int(self_defected) + opp_defectors_count
            return (observed_group_rate, recent_defectors)
        if self.last_probe_round >= 0 and self._last_probe_evaluated_for_round != self.last_probe_round and (t >= self.last_probe_round + self.m):
            observed_group_rate_after_probe, _ = compute_recent_stats()
            self._last_probe_evaluated_for_round = self.last_probe_round
            if observed_group_rate_after_probe >= self.T:
                self.escalation_level = 0
                self.last_probe_round = -1
            else:
                self.escalation_level += 1
                max_punishable = max(0, remaining - 1)
                proposed = int(math.ceil(self.B * (1 + self.escalation_level)))
                punishment_length = min(max_punishable, max(1, proposed))
                if punishment_length <= 0:
                    return Action.D
                self.punishment_counter = max(0, punishment_length - 1)
                self.last_probe_round = -1
                self._last_probe_evaluated_for_round = -1
                return Action.D
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                next_remaining = self.r - (t + 1)
                if next_remaining >= 1:
                    self.probe_scheduled = True
            return action
        elif self.punishment_counter == 0 and self.probe_scheduled:
            self.probe_scheduled = False
            self.last_probe_round = t
            self._last_probe_evaluated_for_round = -1
            return Action.C
        observed_group_rate, recent_defectors = compute_recent_stats()
        default_action = None
        if observed_group_rate >= self.T:
            default_action = Action.C
        elif recent_defectors == 1 and observed_group_rate >= self.T - 0.1:
            default_action = Action.C
            self.escalation_level = max(0, self.escalation_level - 1)
        else:
            max_punishable = max(0, remaining - 1)
            proposed = int(math.ceil(self.B * (1 + self.escalation_level)))
            punishment_length = min(max_punishable, max(1, proposed))
            if punishment_length <= 0:
                default_action = Action.D
            else:
                self.escalation_level += 1
                self.punishment_counter = max(0, punishment_length - 1)
                if self.punishment_counter == 0:
                    next_remaining = self.r - (t + 1)
                    if next_remaining >= 1:
                        self.probe_scheduled = True
                default_action = Action.D
        do_spontaneous_probe = not (self.punishment_counter > 0 or self.probe_scheduled) and remaining > 3 and (random.random() < self.p_probe)
        if do_spontaneous_probe:
            flipped = Action.C if default_action == Action.D else Action.D
            self.last_probe_round = t
            self._last_probe_evaluated_for_round = -1
            return flipped
        return default_action if default_action is not None else Action.D

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    Adaptive Conditional Exploiter (ACE)

    - Starts with a short cooperative probing phase (unless single-round).
    - Keeps smoothed per-opponent estimates of cooperation overall and conditionally
      on whether I cooperated or defected last round.
    - Chooses the action that maximizes expected immediate payoff using those
      conditional estimates, with a small defect bias when payoffs are close.
    - Occasionally probes tolerant groups with defections to detect exploitable opponents.
    - Forgives brief punishments to avoid long mutual-defection cycles.
    - Defects deterministically in the last H rounds (endgame).
    - Switches to persistent defection if group-wide cooperation collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = 0.2
        self.S = 0.25
        self.epsilon = 0.02
        self.p_probe = 0.1
        self.F = 2
        self.m = min(4, max(1, math.floor(self.r / 5))) if self.r > 1 else 0
        self.H = min(2, max(0, self.r - 1))
        self.n_opponents = max(0, self.n - 1)
        self.p = np.full(self.n_opponents, 0.5, dtype=float)
        self.p_given_C = np.full(self.n_opponents, 0.5, dtype=float)
        self.p_given_D = np.full(self.n_opponents, 0.5, dtype=float)
        self.count_overall = np.zeros(self.n_opponents, dtype=int)
        self.count_after_C = np.zeros(self.n_opponents, dtype=int)
        self.count_after_D = np.zeros(self.n_opponents, dtype=int)
        self.probe_rounds = set()
        self.forgive_until_round = -1
        self.collapse_mode = False
        self.last_processed_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if t == 0:
            return Action.C if self.m > 0 else Action.D
        if history is not None and self.last_processed_round < t - 1:
            self._update_from_history(history, t)
            self.last_processed_round = t - 1
        if self.H > 0 and t >= self.r - self.H:
            return Action.D
        if t < self.forgive_until_round:
            return Action.C
        if self.collapse_mode:
            return Action.D
        if t < self.m:
            return Action.C
        reliable_threshold = 2
        p_if_C = np.array(self.p_given_C, copy=True)
        p_if_D = np.array(self.p_given_D, copy=True)
        maskC = self.count_after_C < reliable_threshold
        maskD = self.count_after_D < reliable_threshold
        if self.n_opponents > 0:
            p_if_C[maskC] = self.p[maskC]
            p_if_D[maskD] = self.p[maskD]
        else:
            p_if_C = np.array([], dtype=float)
            p_if_D = np.array([], dtype=float)
        s = p_if_C - p_if_D
        E_others_if_C = float(np.sum(p_if_C)) if self.n_opponents > 0 else 0.0
        E_others_if_D = float(np.sum(p_if_D)) if self.n_opponents > 0 else 0.0
        factor = self.k / float(self.n) if self.n > 0 else 0.0
        U_C = factor * (1.0 + E_others_if_C)
        U_D = 1.0 + factor * E_others_if_D
        if U_D >= U_C + self.epsilon:
            chosen = Action.D
        elif U_C > U_D + self.epsilon:
            chosen = Action.C
        else:
            R = int(np.sum(s >= self.S))
            T = int(np.sum(s <= 0.05))
            majority_threshold = math.ceil(self.n / 2)
            cluster = int(np.sum((self.p >= 0.95) & (s <= 0.05)))
            if cluster >= majority_threshold:
                chosen = Action.D
            elif T >= majority_threshold:
                if random.random() < self.p_probe:
                    chosen = Action.C
                else:
                    chosen = Action.D
            elif R >= majority_threshold:
                chosen = Action.C
            else:
                chosen = Action.D
        if chosen == Action.C and random.random() < self.p_probe:
            chosen = Action.D
            self.probe_rounds.add(t)
        return chosen

    def _update_from_history(self, history: PlayerHistory, current_round: int) -> None:
        """
        Update smoothed statistics from history.
        - current_round is the index of the round we're about to play (so last observed is current_round-1).
        This method also detects punishment responses to recent probes and toggles forgiveness and collapse modes.
        """
        last = current_round - 1
        if last < 0 or history is None:
            return
        opp_actions = history.opponent_actions
        if opp_actions.ndim < 2:
            return
        max_cols = min(self.n_opponents, opp_actions.shape[1])
        if max_cols <= 0:
            return
        obs_row = opp_actions[last, :max_cols].astype(float)
        for j in range(max_cols):
            obs = float(obs_row[j])
            self.p[j] = (1.0 - self.alpha) * self.p[j] + self.alpha * obs
            self.count_overall[j] += 1
        my_last_action = bool(history.my_actions[last])
        if my_last_action:
            for j in range(max_cols):
                obs = float(obs_row[j])
                self.p_given_C[j] = (1.0 - self.alpha) * self.p_given_C[j] + self.alpha * obs
                self.count_after_C[j] += 1
        else:
            for j in range(max_cols):
                obs = float(obs_row[j])
                self.p_given_D[j] = (1.0 - self.alpha) * self.p_given_D[j] + self.alpha * obs
                self.count_after_D[j] += 1
        probe_trigger_round = last - 1
        if probe_trigger_round in self.probe_rounds:
            if probe_trigger_round >= 0 and opp_actions.shape[0] > probe_trigger_round:
                resp_mean = float(np.mean(opp_actions[last, :max_cols].astype(float)))
                probe_mean = float(np.mean(opp_actions[probe_trigger_round, :max_cols].astype(float)))
                if probe_mean - resp_mean >= self.S:
                    self.forgive_until_round = current_round + self.F
            self.probe_rounds.discard(probe_trigger_round)
        if self.n_opponents > 0:
            mean_p = float(np.mean(self.p[:max_cols]))
            sens = np.zeros(max_cols, dtype=float)
            for j in range(max_cols):
                pgC = self.p_given_C[j] if self.count_after_C[j] >= 1 else self.p[j]
                pgD = self.p_given_D[j] if self.count_after_D[j] >= 1 else self.p[j]
                sens[j] = pgC - pgD
            mean_sens = float(np.mean(sens))
            if mean_p < 0.2 and mean_sens <= 0.05:
                self.collapse_mode = True
            elif mean_p > 0.3:
                self.collapse_mode = False
        stale_threshold = current_round - 5
        self.probe_rounds = {pr for pr in self.probe_rounds if pr >= stale_threshold}

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC+)

    Adaptive, forgiving strategy for the N-player public goods game.
    States: Normal, Punish, Test.
    Uses a short memory window M, individual forgiveness threshold F,
    group threshold T (sensitive to k/n), finite punishments, and a one-round
    reconciliation test after punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = min(5, self.r)
        base_T = 0.6
        reduction = 0.0
        kn_ratio = self.k / float(self.n) if self.n > 0 else 0.0
        if kn_ratio > 0.5:
            reduction = 0.2 * min(kn_ratio - 0.5, 0.5)
        T_raw = base_T - reduction
        self.T = max(0.4, min(0.8, T_raw))
        self.F = 0.5
        self.L0 = 2
        self.Lmax = min(6, self.r)
        self.mode = 'Normal'
        self.punish_timer = 0
        self.awaiting_test_evaluation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        opp_actions = history.opponent_actions
        rounds_played = int(opp_actions.shape[0]) if opp_actions is not None else 0
        n_opponents = int(opp_actions.shape[1]) if opp_actions is not None else max(0, self.n - 1)
        window = min(self.M, rounds_played)
        if window <= 0 or n_opponents == 0:
            CR = np.zeros((max(0, n_opponents),), dtype=float)
        else:
            recent = opp_actions[-window:, :]
            CR = np.array(np.sum(recent.astype(float), axis=0) / float(window), dtype=float)
        if rounds_played >= 1 and n_opponents > 0:
            last_round = opp_actions[-1, :].astype(float)
            last_round_rate = float(np.sum(last_round) / float(n_opponents))
        else:
            last_round_rate = 0.0
        if n_opponents == 0:
            GCR = 0.0
        elif window <= 1:
            GCR = last_round_rate
        else:
            hist_mean = float(np.mean(CR)) if CR.size > 0 else 0.0
            weight_hist = float(window - 1)
            weight_last = 2.0
            GCR = (hist_mean * weight_hist + last_round_rate * weight_last) / (weight_hist + weight_last)
        if self.awaiting_test_evaluation:
            if GCR >= self.T:
                self.mode = 'Normal'
                self.punish_timer = 0
            else:
                low_cooperators = np.where(CR < self.F)[0] if CR.size > 0 else np.array([], dtype=int)
                if low_cooperators.size > 0:
                    escalation = int(min(self.Lmax, low_cooperators.size))
                    self.punish_timer = min(self.Lmax, self.L0 + escalation)
                    self.mode = 'Punish'
                else:
                    self.mode = 'Normal'
                    self.punish_timer = 0
            self.awaiting_test_evaluation = False
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = int(max(0, self.punish_timer - 1))
            if self.punish_timer == 0:
                self.mode = 'Test'
                self.awaiting_test_evaluation = False
            else:
                self.mode = 'Punish'
            return action
        if self.mode == 'Test' and (not self.awaiting_test_evaluation):
            self.awaiting_test_evaluation = True
            return Action.C
        final_round_index = max(0, self.r - 1)
        if t >= final_round_index:
            if rounds_played >= 1 and n_opponents > 0:
                all_cooperated_last = bool(np.all(opp_actions[-1, :]))
            else:
                all_cooperated_last = False
            if all_cooperated_last or GCR >= 0.9:
                return Action.C
            return Action.D
        if GCR >= self.T:
            return Action.C
        candidates = []
        if CR.size > 0 and rounds_played >= 1:
            for j in range(CR.size):
                if CR[j] < self.F:
                    if rounds_played >= 1 and (not bool(opp_actions[-1, j])):
                        candidates.append(j)
        if len(candidates) > 0:
            cr_values = [(j, float(CR[j])) for j in candidates]
            target_j, target_cr = min(cr_values, key=lambda x: x[1])
            diff = float(max(0.0, self.F - target_cr))
            escalation_length = int(math.ceil(diff * float(self.Lmax)))
            timer = int(min(self.Lmax, self.L0 + escalation_length))
            self.mode = 'Punish'
            self.punish_timer = timer
            action = Action.D
            self.punish_timer = int(max(0, self.punish_timer - 1))
            if self.punish_timer == 0:
                self.mode = 'Test'
                self.awaiting_test_evaluation = False
            return action
        return Action.D

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    Exploitative adaptive public-goods strategy.

    - Seeds cooperation for a short S rounds to elicit conditional cooperators.
    - Performs a deliberate probe-defection at round S+1 to identify unconditional cooperators
      and conditional/punisher types.
    - Exploits detected unconditional cooperators by defecting.
    - In mostly-cooperative groups, defects probabilistically (p_exploit) to harvest free-rides,
      while occasionally cooperating to avoid collapse.
    - Detects collective punishment (sharp drop in recent cooperation after our defections)
      and retreats with temporary forgiveness (cooperate for F rounds).
    - Always defects in a short endgame E and defects throughout very short games (r <= 3).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.S = min(4, max(1, math.floor(self.r * 0.1)))
        self.L = min(10, max(3, math.floor(self.r * 0.2)))
        self.H_high = 0.7
        self.H_low = 0.3
        self.P_uncond = 0.9
        self.P_cond = 0.6
        self.delta = 0.3
        self.p_exploit = 0.8
        self.F = 3
        self.E = min(max(1, math.ceil(self.r * 0.05)), 3)
        self.n_opponents = max(0, self.n - 1)
        self.uncond = [False] * self.n_opponents
        self.defector_flag = [False] * self.n_opponents
        self.conditional_flag = [False] * self.n_opponents
        self.punisher_flag = [False] * self.n_opponents
        self.forgiveness_until = -1
        self.last_probe_round = -9999
        self.last_maintenance_coop = -9999
        self.probe_interval = max(1, math.ceil(self.r / 10))
        self.prev_recent_rates = None

    def _safe_mean(self, arr):
        if arr is None:
            return 0.0
        try:
            if np.size(arr) == 0:
                return 0.0
            return float(np.mean(arr))
        except Exception:
            s = 0.0
            cnt = 0
            for v in arr:
                s += 1.0 if v else 0.0
                cnt += 1
            return s / cnt if cnt > 0 else 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        r = self.r
        S = self.S
        L = self.L
        E = self.E
        if r <= 3:
            return Action.D
        if round_idx >= max(0, r - E):
            return Action.D
        if round_idx == 0 or history is None:
            return Action.C
        num_past_rounds = int(history.opponent_actions.shape[0])
        if num_past_rounds > round_idx:
            num_past_rounds = round_idx
        if round_idx < S:
            return Action.C
        if round_idx == S:
            self.last_probe_round = round_idx
            return Action.D
        opp_actions = history.opponent_actions
        actual_n_opponents = opp_actions.shape[1] if opp_actions.ndim >= 2 else 0
        if actual_n_opponents != self.n_opponents:
            self.n_opponents = actual_n_opponents
            self.uncond = [False] * self.n_opponents
            self.defector_flag = [False] * self.n_opponents
            self.conditional_flag = [False] * self.n_opponents
            self.punisher_flag = [False] * self.n_opponents
            self.prev_recent_rates = None
        overall_rates = []
        recent_rates = []
        for j in range(self.n_opponents):
            col = opp_actions[:, j]
            R_overall = self._safe_mean(col)
            overall_rates.append(R_overall)
            window = L
            start = max(0, num_past_rounds - window)
            if start < num_past_rounds:
                R_recent = self._safe_mean(opp_actions[start:num_past_rounds, j])
            else:
                R_recent = 0.0
            recent_rates.append(R_recent)
        if round_idx == S + 1:
            probe_idx = S
            for j in range(self.n_opponents):
                R_overall = overall_rates[j]
                cooperated_in_probe = False
                try:
                    cooperated_in_probe = bool(opp_actions[probe_idx, j])
                except Exception:
                    cooperated_in_probe = False
                if R_overall >= self.P_uncond and cooperated_in_probe:
                    self.uncond[j] = True
                pre_rate = 0.0
                if S > 0:
                    pre_rate = self._safe_mean(opp_actions[0:S, j])
                post_rate = self._safe_mean(opp_actions[S:num_past_rounds, j]) if num_past_rounds > S else 0.0
                if R_overall >= self.P_cond and pre_rate - post_rate > self.delta / 4:
                    self.conditional_flag[j] = True
                if R_overall < self.H_low:
                    self.defector_flag[j] = True
            self.prev_recent_rates = list(recent_rates)
        punished_indices = []
        if self.prev_recent_rates is None:
            self.prev_recent_rates = list(recent_rates)
        else:
            if num_past_rounds >= 1:
                try:
                    my_last_action = bool(history.my_actions[num_past_rounds - 1])
                except Exception:
                    my_last_action = True
                if not my_last_action:
                    for j in range(self.n_opponents):
                        prev = self.prev_recent_rates[j] if j < len(self.prev_recent_rates) else 0.0
                        curr = recent_rates[j]
                        if prev - curr > self.delta:
                            self.punisher_flag[j] = True
                            punished_indices.append(j)
            self.prev_recent_rates = list(recent_rates)
        num_punishers = sum((1 for v in self.punisher_flag if v))
        collective_punish_fraction = num_punishers / self.n_opponents if self.n_opponents > 0 else 0.0
        collective_punish = collective_punish_fraction >= 0.4
        if collective_punish and self.forgiveness_until <= round_idx:
            self.forgiveness_until = round_idx + self.F
        maintenance_interval = max(1, self.probe_interval)
        my_recent_coop = 1.0
        try:
            start_my = max(0, num_past_rounds - L)
            if start_my < num_past_rounds:
                my_recent_coop = self._safe_mean(history.my_actions[start_my:num_past_rounds])
            else:
                my_recent_coop = 1.0
        except Exception:
            my_recent_coop = 1.0
        if my_recent_coop < 0.2 and round_idx - self.last_maintenance_coop >= maintenance_interval and (round_idx < r - E):
            self.last_maintenance_coop = round_idx
            return Action.C
        if round_idx < self.forgiveness_until:
            return Action.C
        if any(self.uncond):
            return Action.D
        G_recent = float(np.mean(np.array(recent_rates))) if len(recent_rates) > 0 else 0.0
        if G_recent >= self.H_high:
            if random.random() < self.p_exploit:
                return Action.D
            else:
                return Action.C
        if self.H_low <= G_recent < self.H_high:
            if round_idx < self.forgiveness_until:
                return Action.C
            if (round_idx - S) % self.probe_interval == 0 and round_idx > S + 1 and (round_idx - self.last_probe_round > 0) and (round_idx < r - E):
                self.last_probe_round = round_idx
                return Action.D
            return Action.C
        if G_recent < self.H_low:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    Proportional Reciprocity with Short Punishment and Periodic Probing (PR-SPP)

    - Starts by cooperating to signal willingness.
    - Uses majority threshold to sustain cooperation.
    - Short, finite punishments for single defectors, followed by a probation cooperative test.
    - If cooperation falls below majority, enters Safe-Mode: mostly defects but probes periodically.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = math.ceil(self.n / 2)
        self.P = min(3, max(1, math.ceil(self.r / 10)))
        self.Q = 1
        self.T_test = max(3, math.ceil(self.r / 5))
        self.current_mode = 'Cooperate-Mode'
        self.punish_rounds_remaining = 0
        self.rounds_since_last_probe = 0
        self.last_probe_round = None
        self.probation_active = False
        self.contributions_history = []
        self.defect_counts = np.zeros(max(0, self.n - 1), dtype=int)
        self.persistent_defectors = np.zeros(max(0, self.n - 1), dtype=bool)
        self.persistent_threshold = max(2, math.ceil(self.r / 4)) if self.r > 0 else 2
        self.unanimity_streak = 0
        self.last_actions_recorded = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            self.current_mode = 'Cooperate-Mode'
            self.punish_rounds_remaining = 0
            self.rounds_since_last_probe = 0
            self.last_probe_round = None
            self.probation_active = False
            self.contributions_history = []
            self.defect_counts = np.zeros(max(0, self.n - 1), dtype=int)
            self.persistent_defectors = np.zeros(max(0, self.n - 1), dtype=bool)
            self.unanimity_streak = 0
            self.last_actions_recorded = -1
            return Action.C

        def contributors_in_round(round_idx: int):
            my_act = bool(history.my_actions[round_idx])
            opps = history.opponent_actions[round_idx, :] if self.n > 1 else np.array([], dtype=bool)
            total = int(my_act) + int(opps.sum())
            return (total, bool(my_act), opps)
        prev_idx = t - 1
        if prev_idx > self.last_actions_recorded:
            prev_total, prev_my_coop, prev_opps = contributors_in_round(prev_idx)
            self.contributions_history.append(int(prev_total))
            if self.n > 1:
                defected = ~prev_opps
                self.defect_counts += defected.astype(int)
                self.persistent_defectors = self.defect_counts >= self.persistent_threshold
            if prev_total == self.n:
                self.unanimity_streak += 1
            else:
                self.unanimity_streak = 0
            if self.current_mode == 'Probation' or self.probation_active:
                effective_opps = prev_opps.copy() if self.n > 1 else np.array([], dtype=bool)
                if self.n > 1 and effective_opps.size > 0:
                    effective_opps = effective_opps & ~self.persistent_defectors
                    effective_total = int(prev_my_coop) + int(effective_opps.sum())
                else:
                    effective_total = int(prev_my_coop)
                if effective_total >= self.M:
                    self.current_mode = 'Cooperate-Mode'
                    self.rounds_since_last_probe = 0
                    self.last_probe_round = None
                    self.probation_active = False
                else:
                    self.current_mode = 'Safe-Mode'
                    self.rounds_since_last_probe = 0
                    self.last_probe_round = None
                    self.probation_active = False
            if self.current_mode == 'Safe-Mode' and self.last_probe_round is not None:
                if prev_idx == self.last_probe_round:
                    effective_opps = prev_opps.copy() if self.n > 1 else np.array([], dtype=bool)
                    if self.n > 1 and effective_opps.size > 0:
                        effective_opps = effective_opps & ~self.persistent_defectors
                        effective_total = int(prev_my_coop) + int(effective_opps.sum())
                    else:
                        effective_total = int(prev_my_coop)
                    if effective_total >= self.M:
                        self.current_mode = 'Cooperate-Mode'
                        self.rounds_since_last_probe = 0
                        self.last_probe_round = None
                    else:
                        self.rounds_since_last_probe = 0
                        self.last_probe_round = None
            self.last_actions_recorded = prev_idx
        if t == self.r - 1:
            return Action.D
        prev_total = int(self.contributions_history[-1]) if len(self.contributions_history) > 0 else 0

        def effective_contributors_in_round(round_idx: int):
            total, my_coop, opps = contributors_in_round(round_idx)
            if self.n > 1 and opps.size > 0:
                effective_opps = opps & ~self.persistent_defectors
                eff_total = int(my_coop) + int(effective_opps.sum())
            else:
                eff_total = int(my_coop)
            return eff_total
        if self.current_mode == 'Cooperate-Mode':
            if self.unanimity_streak >= 3:
                return Action.C
            if prev_total == self.n:
                return Action.C
            if prev_total >= self.M:
                return Action.C
            if prev_total == self.n - 1:
                remaining_rounds = max(0, self.r - t)
                self.punish_rounds_remaining = min(self.P, remaining_rounds)
                self.current_mode = 'Punish-Mode'
                if self.punish_rounds_remaining > 0:
                    self.punish_rounds_remaining -= 1
                    if self.punish_rounds_remaining == 0:
                        self.current_mode = 'Probation'
                        self.probation_active = False
                    return Action.D
                else:
                    self.current_mode = 'Probation'
                    self.probation_active = False
                    self.probation_active = True
                    return Action.C
            if prev_total < self.M:
                self.current_mode = 'Safe-Mode'
                self.rounds_since_last_probe = 0
                self.last_probe_round = None
                return Action.D
            return Action.C
        if self.current_mode == 'Punish-Mode':
            if self.punish_rounds_remaining > 0:
                self.punish_rounds_remaining -= 1
                if self.punish_rounds_remaining == 0:
                    self.current_mode = 'Probation'
                    self.probation_active = False
                return Action.D
            else:
                self.current_mode = 'Probation'
                self.probation_active = False
                self.probation_active = True
                return Action.C
        if self.current_mode == 'Probation':
            self.probation_active = True
            return Action.C
        if self.current_mode == 'Safe-Mode':
            rounds_remaining = max(0, self.r - t)
            if rounds_remaining < 2:
                self.rounds_since_last_probe += 1
                return Action.D
            if self.rounds_since_last_probe >= self.T_test:
                self.last_probe_round = t
                self.rounds_since_last_probe = 0
                return Action.C
            self.rounds_since_last_probe += 1
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    Calculated Opportunist (CO)

    Exploitative, adaptive public-goods strategy that:
    - Builds initial reputation (cooperates for first S rounds)
    - Exploits unconditional and generally-cooperative groups by defecting
    - Maintains conditional cooperators via occasional scheduled cooperation
    - Detects short-term punishment (drop in group cooperation after my defections)
      and performs a short recovery (cooperate for recovery_len rounds)
    - Probes occasionally while defecting to discover exploitable players
    - Always defects in the final endgame_horizon rounds
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(10, max(3, self.r // 5))
        if self.r <= 10:
            self.S = min(3, self.r)
        else:
            self.S = min(5, max(1, self.r // 10))
        self.alpha_uncond = 0.9
        self.delta_resp = 0.25
        self.drop_threshold = 0.3
        self.recovery_len = min(5, max(1, self.r // 10))
        self.probe_rate = 0.05
        self.probe_period = max(1, max(20, self.r // 10))
        self.endgame_horizon = min(2, self.r)
        self.recovery_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t0 = int(state.round_number)
        if t0 >= self.r - self.endgame_horizon:
            self.recovery_remaining = 0
            return Action.D
        if t0 == 0 or history is None:
            return Action.C
        if t0 < self.S:
            return Action.C
        hist_len = int(len(history.my_actions))
        last_w = min(self.w, hist_len)
        if last_w <= 0:
            if random.random() < min(0.5, self.probe_rate * 4) or t0 % self.probe_period == 0:
                return Action.C
            return Action.D
        start = hist_len - last_w
        my_window = np.array(history.my_actions[start:hist_len], dtype=bool)
        opp_window = np.array(history.opponent_actions[start:hist_len, :], dtype=bool)
        try:
            G = float(np.mean(opp_window)) if opp_window.size > 0 else 0.0
        except Exception:
            G = 0.0
        if opp_window.size == 0:
            p_j = np.zeros(self.n - 1, dtype=float)
        else:
            p_j = np.array(np.mean(opp_window, axis=0), dtype=float)
        maskC = my_window
        maskD = ~my_window
        if maskC.sum() > 0:
            p_j_given_C = np.array(np.mean(opp_window[maskC, :], axis=0), dtype=float)
        else:
            p_j_given_C = p_j.copy()
        if maskD.sum() > 0:
            p_j_given_D = np.array(np.mean(opp_window[maskD, :], axis=0), dtype=float)
        else:
            p_j_given_D = p_j.copy()
        responsiveness = p_j_given_C - p_j_given_D
        uncond_idx = np.where(p_j >= self.alpha_uncond)[0]
        cond_idx = np.where((responsiveness >= self.delta_resp) & (p_j_given_C >= 0.6))[0]
        after_coop_vals = []
        full_my = np.array(history.my_actions, dtype=bool)
        full_opp = np.array(history.opponent_actions, dtype=bool)
        for i in range(start, hist_len - 1):
            if not full_my[i]:
                after_coop_vals.append(float(np.mean(full_opp[i + 1, :])))
        if len(after_coop_vals) > 0:
            avg_G_after_my_D = sum(after_coop_vals) / len(after_coop_vals)
            punishment_detected = G - avg_G_after_my_D >= self.drop_threshold
        else:
            avg_G_after_my_D = None
            punishment_detected = False
        if self.recovery_remaining > 0:
            self.recovery_remaining -= 1
            return Action.C
        if punishment_detected:
            self.recovery_remaining = self.recovery_len - 1
            return Action.C
        action_C = False
        if uncond_idx.size > 0:
            action_C = False
        elif G >= 0.6:
            action_C = False
        elif 0.3 <= G < 0.6:
            if cond_idx.size > 0:
                mean_resp = float(np.mean(responsiveness[cond_idx]))
                if mean_resp <= 0:
                    action_C = False
                else:
                    L = math.ceil(1.0 / mean_resp) if mean_resp > 0 else self.r + 1
                    L = max(1, min(L, max(1, self.r)))
                    if L <= 1:
                        action_C = True
                    elif t0 % L == 0:
                        action_C = True
                    else:
                        action_C = False
            else:
                action_C = False
        elif self.recovery_len > 0 and t0 % self.recovery_len == 0:
            action_C = True
        else:
            action_C = False
        if not action_C:
            probe_rate_local = self.probe_rate
            if hist_len < self.w:
                probe_rate_local = min(0.25, self.probe_rate * 4)
            if self.probe_period > 0 and t0 % self.probe_period == 0 or random.random() < probe_rate_local:
                action_C = True
        return Action.C if action_C else Action.D

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    Proportional Collective Reciprocity (PCR)

    - Starts cooperative.
    - Tracks recent cooperation rates over a lookback window W = min(5, r).
    - In normal play cooperates when group statistics indicate recent strong cooperation,
      otherwise initiates a proportional, temporary punishment (group-wide defection).
    - Forgives quickly when cooperation returns (requires two consecutive rounds with s_avg >= 0.6
      to fully resume normal cooperation).
    - Becomes cautious in the final L_end = max(1, ceil(r/10)) rounds, cooperating there only when
      recent cooperation is very strong and the last round was unanimous cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r = game_description.n_rounds
        self.W = min(5, self.r)
        self.L_end = max(1, math.ceil(self.r / 10))
        self.punish_active = False
        self.punish_until = -1
        self.recovery_mode = False
        self.recovery_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_index = state.round_number
        if current_index == 0 or history is None:
            return Action.C
        rounds_so_far = history.my_actions.shape[0]
        if rounds_so_far == 0:
            return Action.C
        try:
            my_col = history.my_actions.reshape(rounds_so_far, 1)
            actions = np.concatenate([my_col, history.opponent_actions], axis=1)
        except Exception:
            return Action.C
        window_len = min(self.W, rounds_so_far)
        window_actions = actions[-window_len:, :]
        s_j = np.sum(window_actions, axis=0) / float(window_len)
        s_avg = float(np.mean(s_j)) if s_j.size > 0 else 0.0
        s_min = float(np.min(s_j)) if s_j.size > 0 else 0.0
        last_round_actions = actions[-1, :]
        last_round_all_coop = bool(np.all(last_round_actions))
        count_defectors_prev = int(np.sum(~last_round_actions))
        remaining_rounds = max(0, self.r - current_index)
        if self.punish_active and current_index < self.punish_until:
            early_end = False
            if s_avg >= 0.65 and rounds_so_far >= 2:
                last_two = actions[-2:, :]
                consecutive_zero = np.all(~last_two, axis=0)
                if not np.any(consecutive_zero):
                    early_end = True
            if early_end:
                self.punish_active = False
                self.punish_until = -1
                self.recovery_mode = True
                self.recovery_counter = 1 if s_avg >= 0.6 else 0
            else:
                return Action.D
        if self.punish_active and current_index >= self.punish_until:
            self.punish_active = False
            self.punish_until = -1
            self.recovery_mode = True
            self.recovery_counter = 0
        if self.recovery_mode:
            if s_avg >= 0.6:
                self.recovery_counter += 1
            else:
                self.recovery_counter = 0
            if self.recovery_counter >= 2:
                self.recovery_mode = False
                self.recovery_counter = 0
        if current_index >= self.r - self.L_end:
            if s_avg >= 0.75 and last_round_all_coop:
                return Action.C
            return Action.D
        if s_min >= 0.7:
            return Action.C
        if s_avg >= 0.6 and count_defectors_prev < 2:
            return Action.C
        severity = (0.5 - s_min) / 0.5 if s_min < 0.5 else 0.0
        if severity < 0.0:
            severity = 0.0
        elif severity > 1.0:
            severity = 1.0
        P = int(math.ceil(2.0 + 4.0 * severity))
        P = max(1, P)
        P = min(P, max(1, remaining_rounds))
        self.punish_active = True
        self.punish_until = current_index + P
        self.recovery_mode = False
        self.recovery_counter = 0
        return Action.D

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    Exploitative Adaptive Conditional Cooperator (EACC)

    - Probes briefly with cooperation.
    - Keeps EWMA estimates p_j of each opponent's cooperation prob.
    - Exploits if a sufficient fraction of opponents are reliable cooperators.
    - Otherwise uses myopic break-even threshold to decide to cooperate.
    - If exploitation provokes a sharp drop in group cooperation, enters a restore
      phase (cooperate for a few rounds).
    - Always defects in the final few rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game = game_description
        self.n = int(self.game.n_players)
        self.r = int(self.game.n_rounds)
        self.k = float(self.game.k)
        self.probe_rounds = min(3, max(1, math.floor(self.r / 10)))
        self.final_exploit = min(3, max(1, math.floor(self.r / 20)))
        self.alpha = 0.4
        self.p_high = 0.8
        self.p_low = 0.2
        self.restore_rounds = 3
        self.drop_detect = 0.25
        self.exploit_threshold_frac = 0.25
        self.safety_margin = 0.0
        self.unpredictable_coop_prob_when_exploiting = 0.05
        self.num_opponents = max(0, self.n - 1)
        self.p_j = [0.5 for _ in range(self.num_opponents)]
        self.p_avg = 0.5 if self.num_opponents > 0 else 0.0
        self.group_coop_history = []
        self.p_avg_history = []
        self.last_updated_round = -1
        self.restore_remaining = 0
        self.last_was_exploit = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None:
            observed_rounds = history.my_actions.shape[0]
            last_observed_round = observed_rounds - 1
            for r_idx in range(self.last_updated_round + 1, last_observed_round + 1):
                if self.num_opponents > 0:
                    opp_actions_row = history.opponent_actions[r_idx, :]
                    for j in range(self.num_opponents):
                        c_j = 1.0 if bool(opp_actions_row[j]) else 0.0
                        self.p_j[j] = (1.0 - self.alpha) * self.p_j[j] + self.alpha * c_j
                my_c = 1.0 if bool(history.my_actions[r_idx]) else 0.0
                opp_sum = float(np.sum(history.opponent_actions[r_idx, :])) if self.num_opponents > 0 else 0.0
                group_coop = (opp_sum + my_c) / float(self.n) if self.n > 0 else 0.0
                self.group_coop_history.append(group_coop)
                if self.num_opponents > 0:
                    self.p_avg = float(np.mean(self.p_j))
                else:
                    self.p_avg = 0.0
                self.p_avg_history.append(self.p_avg)
                self.last_updated_round = r_idx
        if t >= max(0, self.r - self.final_exploit):
            self.last_was_exploit = False
            return Action.D
        if self.restore_remaining > 0:
            self.restore_remaining -= 1
            self.last_was_exploit = False
            return Action.C
        if t < self.probe_rounds:
            self.last_was_exploit = False
            return Action.C
        E_S = float(sum(self.p_j)) if self.num_opponents > 0 else 0.0
        if self.k != 0.0:
            threshold = self.n / self.k - 1.0 + self.safety_margin
        else:
            threshold = float('inf')
        punishment_detected = False
        if len(self.group_coop_history) >= 2:
            prev = self.group_coop_history[-2]
            last = self.group_coop_history[-1]
            if prev > 0 and (prev - last) / prev > self.drop_detect:
                punishment_detected = True
        if not punishment_detected and len(self.p_avg_history) >= 2:
            prev_p = self.p_avg_history[-2]
            last_p = self.p_avg_history[-1]
            if prev_p > 0 and (prev_p - last_p) / max(prev_p, 1e-09) > self.drop_detect:
                punishment_detected = True
        if punishment_detected and self.last_was_exploit:
            self.restore_remaining = self.restore_rounds - 1
            self.last_was_exploit = False
            return Action.C
        fraction_high = 0.0
        if self.num_opponents > 0:
            high_count = sum((1 for p in self.p_j if p >= self.p_high))
            fraction_high = high_count / float(self.num_opponents)
        exploit_condition = fraction_high >= self.exploit_threshold_frac and self.p_avg >= 0.5
        if exploit_condition:
            if random.random() < self.unpredictable_coop_prob_when_exploiting:
                self.last_was_exploit = False
                return Action.C
            else:
                self.last_was_exploit = True
                return Action.D
        if E_S >= threshold:
            self.last_was_exploit = False
            return Action.C
        if 0.35 <= self.p_avg < 0.5 and (not exploit_condition):
            if E_S >= threshold - 0.5:
                if random.random() < float(self.p_avg):
                    self.last_was_exploit = False
                    return Action.C
        self.last_was_exploit = False
        return Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Adaptive Majority with Limited Punishment (collective-first, forgiving reciprocity).

    - Start by cooperating to signal willingness.
    - Continue cooperating when a clear majority of others cooperated in the previous round.
    - If cooperation falls below a majority threshold, impose a short P-round punishment (defection),
      but forgive quickly so cooperation can be restored.
    - If cooperation has effectively collapsed over a recent window, defect for all remaining rounds.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        self.n = n
        self.r = r
        M_frac = 0.6
        self.M_others = max(1, math.ceil(M_frac * (n - 1)))
        self.P = 2
        self.L = min(5, r)
        self.collapse_frac = 0.2
        self.punishment_counter = 0
        self.collapsed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        rem = self.r - state.round_number
        if rem == 1:
            return Action.D
        past_rounds = history.opponent_actions.shape[0]
        window = min(self.L, past_rounds) if past_rounds > 0 else 0
        if window > 0 and (not self.collapsed):
            recent = history.opponent_actions[-window:, :]
            sums_per_round = np.sum(recent, axis=1).astype(float)
            avg_other_contribs = float(np.mean(sums_per_round))
            if avg_other_contribs < self.collapse_frac * (self.n - 1):
                self.collapsed = True
                return Action.D
        if self.collapsed:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        last_round_others = int(np.sum(history.opponent_actions[-1, :]))
        if last_round_others >= self.M_others:
            return Action.C
        p_len = min(self.P, max(0, rem - 1))
        if p_len <= 0:
            return Action.D
        self.punishment_counter = p_len
        self.punishment_counter -= 1
        return Action.D

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    Exploitative-but-robust Public Goods strategy.

    - Builds a brief cooperative reputation (T_build rounds).
    - Tracks per-opponent cooperation and responsiveness with Laplace+1 smoothing.
    - Maintains a small supporter coalition (Supporters) of likely conditional cooperators.
    - Free-rides when many others are cooperating.
    - Performs infrequent maintenance cooperations to keep responsive supporters.
    - Attempts short repairs if supporters falter and overall cooperation looks promising.
    - Detects and stops rewarding exploitative opponents and switches to persistent defection
      if cooperation collapses or exploiters are a majority.
    - Occasional random probes to detect resumption of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_build = max(1, min(5, math.floor(0.1 * self.r)))
        self.T_end = min(2, max(1, math.floor(0.05 * self.r)))
        self.Cooperators_threshold_frac = 0.6
        self.p_probe = min(0.2, 0.05 + 0.5 / (self.r + 1))
        self.L_recency = 3
        self.K = max(3, math.ceil(0.1 * self.r))
        self.M_repair = 2
        self.supporter_fraction_for_m = 0.25
        if self.n <= 3:
            self.p_probe = max(0.01, self.p_probe * 0.5)
            self.T_build = max(1, min(3, math.floor(0.05 * self.r)))
        self.perm_defect = False
        self.in_repair = False
        self.repair_end_round = None
        self.seed_attempt_remaining = 0
        self.exploitative_set = set()
        self.last_maintenance_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 3:
            if t == 0:
                return Action.C
            return Action.D
        if t == 0:
            self.perm_defect = False
            self.in_repair = False
            self.repair_end_round = None
            self.seed_attempt_remaining = 1
            self.exploitative_set = set()
            self.last_maintenance_round = None
            return Action.C
        if history is None:
            return Action.D
        opponents = self.n - 1
        rounds_seen = int(history.opponent_actions.shape[0])
        rounds_seen = min(rounds_seen, t)
        if t >= self.r - self.T_end:
            return Action.D
        if self.perm_defect:
            return Action.D
        if rounds_seen == 0:
            opp_by_round = np.array([], dtype=np.int64)
        else:
            opp_by_round = np.sum(history.opponent_actions[:rounds_seen, :], axis=1).astype(np.int64)
        if rounds_seen >= self.L_recency:
            threshold_low = math.ceil(opponents * 0.15)
            last_L = opp_by_round[-self.L_recency:]
            if np.all(last_L <= threshold_low):
                self.perm_defect = True
                return Action.D
        if rounds_seen == 0:
            C_counts = np.zeros(opponents, dtype=np.int64)
        else:
            C_counts = np.sum(history.opponent_actions[:rounds_seen, :], axis=0).astype(np.int64)
        if rounds_seen <= 1:
            CO_given_myC = np.zeros(opponents, dtype=np.int64)
            CO_given_myD = np.zeros(opponents, dtype=np.int64)
            myC_occ = 0
            myD_occ = 0
        else:
            my_prev = history.my_actions[:rounds_seen - 1]
            opp_curr = history.opponent_actions[1:rounds_seen, :]
            mask_myC = my_prev.astype(np.bool_)
            mask_myD = ~mask_myC
            if mask_myC.ndim == 1:
                mask_myC = mask_myC[:, None]
                mask_myD = mask_myD[:, None]
            CO_given_myC = np.sum(opp_curr & mask_myC, axis=0).astype(np.int64)
            CO_given_myD = np.sum(opp_curr & mask_myD, axis=0).astype(np.int64)
            myC_occ = int(np.sum(mask_myC))
            myD_occ = int(np.sum(mask_myD))
        t_seen_plus2 = rounds_seen + 2
        P_i = (C_counts.astype(np.float64) + 1.0) / float(t_seen_plus2)
        denom_myC = myC_occ + 2 if myC_occ + 2 > 0 else 2
        denom_myD = myD_occ + 2 if myD_occ + 2 > 0 else 2
        if rounds_seen <= 1:
            P_i_given_myC = np.full(opponents, 0.5, dtype=np.float64)
            P_i_given_myD = np.full(opponents, 0.5, dtype=np.float64)
        else:
            P_i_given_myC = (CO_given_myC.astype(np.float64) + 1.0) / float(denom_myC)
            P_i_given_myD = (CO_given_myD.astype(np.float64) + 1.0) / float(denom_myD)
        R_i = P_i_given_myC - P_i_given_myD
        S_i = 0.6 * P_i + 0.4 * np.maximum(0.0, R_i)
        newly_exploitative = set((int(idx) for idx in np.where(R_i <= -0.2)[0]))
        self.exploitative_set.update(newly_exploitative)
        if len(self.exploitative_set) > opponents / 2.0:
            self.perm_defect = True
            return Action.D
        m = max(1, math.floor(opponents * self.supporter_fraction_for_m))
        high_supporters = set((int(i) for i in np.where(S_i >= 0.5)[0]))
        sorted_idx = np.argsort(-S_i)
        top_m = set((int(i) for i in sorted_idx[:m]))
        raw_supporters = high_supporters.union(top_m)
        supporters = [i for i in sorted(raw_supporters) if i not in self.exploitative_set]
        num_supporters = len(supporters)
        Others_last = int(opp_by_round[-1]) if rounds_seen >= 1 else 0
        coop_threshold = math.ceil(opponents * self.Cooperators_threshold_frac)
        if Others_last >= coop_threshold:
            return Action.D
        P_avg = float(np.mean(P_i)) if P_i.size > 0 else 0.0
        if num_supporters == 0 and P_avg > 0.4:
            if self.seed_attempt_remaining > 0:
                self.seed_attempt_remaining -= 1
                return Action.C
        if num_supporters > 0 and rounds_seen >= 1:
            supporters_last_coop = int(np.sum(history.opponent_actions[-1, supporters]))
            fraction_supporters_last = supporters_last_coop / float(num_supporters)
        else:
            fraction_supporters_last = 0.0
        any_supporter_responsive = False
        if num_supporters > 0:
            for idx in supporters:
                if R_i[int(idx)] > 0.05:
                    any_supporter_responsive = True
                    break
        if num_supporters > 0 and fraction_supporters_last >= 0.5 and any_supporter_responsive:
            offset = t - self.T_build
            if offset >= 0 and offset % self.K == 0:
                self.last_maintenance_round = t
                return Action.C
            else:
                return Action.D
        elif not self.in_repair and P_avg > 0.3 and (num_supporters > 0):
            self.in_repair = True
            self.repair_end_round = t + self.M_repair
            return Action.C
        elif self.in_repair:
            if t < self.repair_end_round:
                return Action.C
            else:
                if rounds_seen >= 1 and num_supporters > 0:
                    supporters_last_coop = int(np.sum(history.opponent_actions[-1, supporters]))
                    fraction_supporters_last = supporters_last_coop / float(num_supporters)
                else:
                    fraction_supporters_last = 0.0
                self.in_repair = False
                self.repair_end_round = None
                if fraction_supporters_last < 0.5:
                    self.perm_defect = True
                    return Action.D
                else:
                    offset = t - self.T_build
                    if offset >= 0 and offset % self.K == 0:
                        self.last_maintenance_round = t
                        return Action.C
                    return Action.D
        if random.random() < self.p_probe:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Proportional Punishment (CCC-GPP)

    - Signals cooperation in the first round.
    - Uses a short window of recent rounds to estimate the fraction of cooperators.
    - Forgives minor lapses, applies proportional temporary punishments for significant shortfalls,
      and uses a short probation window after punishments. Escalates punishments (doubling)
      if small punishments fail to restore cooperation. Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, math.floor(self.r / 10)))
        raw_theta = (self.n - self.k + 1.0) / float(self.n)
        self.theta = min(1.0, max(0.5, raw_theta))
        self.m = 0.2
        self.S = 3
        self.prob = min(2, max(1, math.floor(self.r / 20)))
        self.P = 0
        self.L_prev = 0
        self.probation = 0
        self.just_ended = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0:
            return Action.C
        if t == self.r - 1:
            return Action.D
        if history is None:
            return Action.D
        completed_rounds = t

        def compute_f_recent() -> float:
            last_count = min(self.w, completed_rounds)
            if last_count <= 0:
                return 0.0
            start = completed_rounds - last_count
            end = completed_rounds
            opp_slice = history.opponent_actions[start:end, :]
            my_slice = history.my_actions[start:end]
            opp_counts = np.sum(opp_slice.astype(np.int64), axis=1)
            my_counts = my_slice.astype(np.int64)
            total_coop = opp_counts + my_counts
            fractions = total_coop.astype(np.float64) / float(self.n)
            return float(np.mean(fractions))
        if t == self.r - 2:
            f_recent = compute_f_recent()
            if f_recent >= self.theta:
                return Action.C
            else:
                return Action.D
        if self.P > 0:
            self.P -= 1
            if self.P == 0:
                self.probation = self.prob
                self.just_ended = True
            return Action.D
        if self.probation > 0:
            f_recent = compute_f_recent()
            shortfall_threshold = self.theta - self.m
            if f_recent < shortfall_threshold:
                remaining = self.r - t
                max_p = remaining - 1
                if max_p < 1:
                    self.probation = 0
                    self.just_ended = False
                    return Action.D
                if self.just_ended and self.L_prev > 0:
                    p = min(max_p, max(1, 2 * int(self.L_prev)))
                else:
                    p_base = max(1, math.ceil((self.theta - f_recent) * self.S))
                    p = min(max_p, p_base)
                self.P = int(p)
                self.L_prev = int(p)
                self.probation = 0
                self.just_ended = False
                return Action.D
            else:
                self.probation -= 1
                if self.probation == 0:
                    self.just_ended = False
                return Action.C
        f_recent = compute_f_recent()
        if f_recent >= self.theta:
            return Action.C
        if f_recent >= self.theta - self.m:
            return Action.C
        remaining = self.r - t
        max_p = remaining - 1
        if max_p < 1:
            return Action.D
        s = max(0.0, self.theta - f_recent)
        p_base = max(1, math.ceil(s * self.S))
        p = min(max_p, p_base)
        self.P = int(p)
        self.L_prev = int(p)
        return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Proportional Forgiving Conditional Cooperator (PFCC)

    - Starts by cooperating (round 0).
    - In non-final rounds, if recent group cooperation is at least (n - permitted_defections) cooperators,
      cooperate. Otherwise start a proportional, finite punishment (defect) whose length is
      min(remaining_nonfinal_rounds, 1 + number_of_defectors_last_round).
    - Punishments never include the final round. After a finite punishment, the strategy forgives (returns to
      cooperation) unless further defections occur.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(self.game_description.n_players)
        self.n_rounds = int(self.game_description.n_rounds)
        self.permitted_defections = max(0, math.floor(0.1 * self.n_players))
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = int(state.round_number)
        if t_idx >= max(0, self.n_rounds - 1):
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if t_idx == 0 or history is None:
            return Action.C
        try:
            my_last = int(bool(history.my_actions[-1]))
            opponents_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        except Exception:
            return Action.C
        C_last = my_last + opponents_last
        if C_last >= self.n_players - self.permitted_defections:
            return Action.C
        defections_last = self.n_players - C_last
        max_punishable = max(0, self.n_rounds - t_idx - 1)
        punishment_length = min(max_punishable, 1 + int(defections_last))
        if punishment_length <= 0:
            return Action.D
        self.punishment_counter = max(0, int(punishment_length) - 1)
        return Action.D

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    - Probes early to identify cooperators.
    - Uses short-window empirical estimates of opponents' cooperation rates (p_j).
    - If group cooperation (GC) is high, mostly cooperates but opportunistically defects
      against near-unconditional cooperators with some probability (p_exploit).
    - If cooperation drops after we cooperated, issues short targeted punishment, then probes
      for forgiveness.
    - Reduces exploitation frequency when opponents retaliate or when exploitation
      converts unconditional cooperators into conditional ones.
    - Always defects in the final H rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.theta_high = 0.92
        self.base_p_exploit = 0.5
        self.current_p_exploit = float(self.base_p_exploit)
        self.punishment_remaining = 0
        self.post_punish_probe = False
        self.retreat_remaining = 0
        self._saved_p_exploit_for_retreat = None
        self.exploit_reduction_active = False
        self.recovery_counter = 0
        self.last_pj = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        rn = int(state.round_number)
        n = self.n
        r = self.r
        k = self.k
        if r <= 1:
            return Action.D
        W = min(5, max(1, math.floor(r / 10)))
        Init = min(3, r - 1)
        H = min(3, r - 1)
        alpha = max(0.25, min(0.75, k / float(n) + 0.15))
        if rn >= r - H:
            return Action.D
        if history is None or history.my_actions.size == 0:
            if Init >= 1:
                return Action.C
            return Action.D
        rounds_available = int(history.opponent_actions.shape[0])
        window = min(W, rounds_available)
        if window <= 0:
            p_j_current = np.zeros(n - 1, dtype=float)
        else:
            recent = history.opponent_actions[-window:, :]
            p_j_current = np.mean(recent.astype(float), axis=0)
        if rounds_available >= 2:
            prev_window = min(W, rounds_available - 1)
            if rounds_available - 1 >= prev_window:
                prev_slice = history.opponent_actions[-(prev_window + 1):-1, :]
            else:
                prev_slice = history.opponent_actions[:-1, :]
            if prev_slice.size == 0:
                p_j_prev = p_j_current.copy()
            else:
                p_j_prev = np.mean(prev_slice.astype(float), axis=0)
        else:
            p_j_prev = p_j_current.copy()
        if p_j_current.size > 0:
            GC = float(np.mean(p_j_current))
        else:
            GC = 0.0
        C_prev_others = int(np.sum(history.opponent_actions[-1, :])) if rounds_available >= 1 else 0
        my_last_action = bool(history.my_actions[-1])
        expected_others = GC * (n - 1)
        drop_threshold = math.floor(0.5 * expected_others)
        if my_last_action and rounds_available >= 1:
            if C_prev_others <= drop_threshold and self.punishment_remaining <= 0:
                max_punishable = max(0, r - H - (rn + 1))
                P = min(3, max_punishable)
                if P > 0:
                    self.punishment_remaining = P
                    self.post_punish_probe = True
                    self.current_p_exploit = max(0.01, self.current_p_exploit * 0.9)
                    self.last_pj = p_j_current.copy()
                    return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            self.last_pj = p_j_current.copy()
            return Action.D
        if self.post_punish_probe:
            self.post_punish_probe = False
            self.last_pj = p_j_current.copy()
            return Action.C
        if not my_last_action and rounds_available >= 2:
            deltas = p_j_prev - p_j_current
            if np.any(deltas >= 0.3):
                T_retreat = 3
                if self.retreat_remaining <= 0:
                    self._saved_p_exploit_for_retreat = float(self.current_p_exploit)
                    self.current_p_exploit = max(0.01, self.current_p_exploit * 0.5)
                self.retreat_remaining = max(self.retreat_remaining, T_retreat)
        if self.retreat_remaining > 0:
            self.retreat_remaining -= 1
            if self.retreat_remaining == 0:
                if self._saved_p_exploit_for_retreat is not None:
                    if not self.exploit_reduction_active:
                        self.current_p_exploit = float(self._saved_p_exploit_for_retreat)
                    else:
                        self.current_p_exploit = max(0.01, float(self.base_p_exploit) * 0.5)
                self._saved_p_exploit_for_retreat = None
        S_high_prev = set(np.where(p_j_prev >= self.theta_high)[0]) if p_j_prev.size > 0 else set()
        S_high_now = set(np.where(p_j_current >= self.theta_high)[0]) if p_j_current.size > 0 else set()
        dropped_from_high = [j for j in S_high_prev if j not in S_high_now]
        recent_check = min(3, rounds_available)
        recent_my_actions = history.my_actions[-recent_check:].astype(bool)
        defected_recently = np.any(recent_my_actions == False)
        if dropped_from_high and defected_recently:
            if not self.exploit_reduction_active:
                self.exploit_reduction_active = True
                self.current_p_exploit = max(0.01, float(self.current_p_exploit) * 0.5)
                self.recovery_counter = 0
        if self.exploit_reduction_active:
            if GC > alpha:
                self.recovery_counter += 1
            else:
                self.recovery_counter = 0
            if self.recovery_counter >= 2:
                self.exploit_reduction_active = False
                self.current_p_exploit = float(self.base_p_exploit)
                self.recovery_counter = 0
        if rn < Init:
            self.last_pj = p_j_current.copy()
            return Action.C
        S_high_indices = np.where(p_j_current >= self.theta_high)[0] if p_j_current.size > 0 else np.array([], dtype=int)
        has_high = S_high_indices.size > 0
        if GC >= alpha:
            if has_high:
                prob = float(self.current_p_exploit)
                draw = random.random()
                choice_defect = draw < prob
                if C_prev_others >= math.ceil((n - 1) / 2):
                    draw2 = random.random()
                    choice_defect = choice_defect or draw2 < 0.1
                self.last_pj = p_j_current.copy()
                return Action.D if choice_defect else Action.C
            else:
                self.last_pj = p_j_current.copy()
                return Action.C
        else:
            self.last_pj = p_j_current.copy()
            return Action.D

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    Collective Conditional Cooperator with Forgiving Graduated Punishments (CCC-FGP)

    - Starts cooperative.
    - Uses a sliding window W to estimate opponents' cooperation rates.
    - If group cooperation is high (>= THIGH) cooperate unconditionally.
    - If group cooperation is low (< TLOW) enter a short, finite punishment cycle (never punish into final round),
      then perform a one-round cooperative probe. If the probe is followed by a majority returning to cooperation,
      resume cooperation; otherwise resume punishment cycles.
    - If group cooperation is moderate (TLOW <= group < THIGH) use a mixed regime:
      cooperate if a majority cooperated last round; otherwise defect but attempt a quick recovery the next round.
    - Detects persistent defectors (coop_rate <= PDEF) and requires stronger evidence before returning to
      unconditional cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.THIGH = 0.8
        self.TLOW = 0.5
        self.PDEF = 0.2
        self.in_punishment = False
        self.punish_remaining = 0
        self.probed_in_cycle = False
        self.awaiting_probe_response = False
        self.probe_round_index = None
        self.probe_phase = 0
        self.mixed_defect_round = None
        self.mixed_prev_group_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = state.round_number
        r = self.r
        n = self.n
        remaining_rounds = r - t_idx
        W = min(10, max(1, math.floor(r / 5)))
        if t_idx == 0 or history is None:
            self.in_punishment = False
            self.punish_remaining = 0
            self.probed_in_cycle = False
            self.awaiting_probe_response = False
            self.probe_round_index = None
            self.probe_phase = 0
            self.mixed_defect_round = None
            self.mixed_prev_group_rate = None
            return Action.C
        if t_idx == r - 1:
            self.in_punishment = False
            self.punish_remaining = 0
            self.awaiting_probe_response = False
            self.probe_phase = 0
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        rounds_played = t_idx
        denom = min(W, rounds_played)
        if denom <= 0:
            denom = 0
        if denom > 0:
            recent_slice = opp_actions[-denom:, :]
            recent_counts = np.sum(recent_slice.astype(np.int64), axis=0)
            coop_rates = recent_counts / float(denom)
        else:
            coop_rates = np.full((opp_actions.shape[1],), 0.5, dtype=float)
        if coop_rates.size == 0:
            coop_rates = np.array([0.5])
        group_coop_rate = float(np.mean(coop_rates))
        persistent_mask = coop_rates <= self.PDEF
        persistent_count = int(np.sum(persistent_mask))
        THIGH_effective = self.THIGH
        if persistent_count > 0:
            THIGH_effective = min(0.95, self.THIGH + 0.05 * persistent_count)
        L_base = min(3, max(1, math.floor(r / 10), math.floor(remaining_rounds / 4)))
        if remaining_rounds <= max(2, math.floor(r / 10)):
            L = max(1, math.floor(remaining_rounds / 4))
            L = min(L_base, L)
        else:
            L = L_base
        majority_needed = math.ceil(n / 2)
        last_round_index = t_idx - 1
        if last_round_index >= 0:
            try:
                last_round_opp_coop = int(np.sum(opp_actions[last_round_index, :].astype(np.int64)))
                my_last_action = int(bool(my_actions[-1]))
                cooperators_last_round = last_round_opp_coop + my_last_action
            except Exception:
                cooperators_last_round = 0
        else:
            cooperators_last_round = 0
        if self.awaiting_probe_response:
            if self.probe_phase == 1:
                self.probe_phase = 2
                return Action.D
            elif self.probe_phase == 2:
                response_round_index = self.probe_round_index + 1
                if response_round_index <= last_round_index:
                    resp_opp_coop = int(np.sum(opp_actions[response_round_index, :].astype(np.int64)))
                    my_action_in_response = 0
                    if response_round_index < len(my_actions):
                        my_action_in_response = int(bool(my_actions[response_round_index]))
                    resp_total_coop = resp_opp_coop + my_action_in_response
                    if resp_total_coop >= majority_needed:
                        self.awaiting_probe_response = False
                        self.probe_phase = 0
                        self.probe_round_index = None
                        self.in_punishment = False
                        self.punish_remaining = 0
                        self.probed_in_cycle = False
                        return Action.C
                    else:
                        self.awaiting_probe_response = False
                        self.probe_phase = 0
                        self.probe_round_index = None
                        punish_len = min(L, max(0, remaining_rounds - 1))
                        if punish_len > 0:
                            self.in_punishment = True
                            self.punish_remaining = punish_len
                            self.probed_in_cycle = False
                            self.punish_remaining = max(0, self.punish_remaining - 1)
                            return Action.D
                        else:
                            self.in_punishment = False
                            self.punish_remaining = 0
                            return Action.D
                else:
                    self.awaiting_probe_response = False
                    self.probe_phase = 0
                    self.probe_round_index = None
                    return Action.D
        if self.in_punishment and self.punish_remaining > 0:
            if group_coop_rate >= THIGH_effective:
                self.in_punishment = False
                self.punish_remaining = 0
                self.probed_in_cycle = False
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.in_punishment = False
                self.probed_in_cycle = False
                self.awaiting_probe_response = True
                self.probe_round_index = t_idx
                self.probe_phase = 1
                self.probed_in_cycle = True
                return Action.C
            else:
                return Action.D
        if group_coop_rate >= THIGH_effective:
            self.in_punishment = False
            self.punish_remaining = 0
            self.awaiting_probe_response = False
            self.probe_phase = 0
            self.probe_round_index = None
            self.probed_in_cycle = False
            self.mixed_defect_round = None
            self.mixed_prev_group_rate = None
            return Action.C
        if group_coop_rate < self.TLOW:
            if not self.in_punishment and (not self.probed_in_cycle) and (not self.awaiting_probe_response):
                P = min(L, max(0, remaining_rounds - 1))
                if P > 0:
                    self.in_punishment = True
                    self.punish_remaining = max(0, P - 1)
                    self.probed_in_cycle = False
                    return Action.D
                else:
                    self.awaiting_probe_response = True
                    self.probe_round_index = t_idx
                    self.probe_phase = 1
                    self.probed_in_cycle = True
                    return Action.C
            if not self.in_punishment and self.probed_in_cycle and (not self.awaiting_probe_response):
                return Action.D
            return Action.D
        if group_coop_rate >= self.TLOW and group_coop_rate < THIGH_effective:
            if cooperators_last_round >= majority_needed:
                self.mixed_defect_round = None
                self.mixed_prev_group_rate = None
                return Action.C
            elif self.mixed_defect_round is None or self.mixed_defect_round < last_round_index:
                self.mixed_defect_round = last_round_index + 1
                self.mixed_prev_group_rate = group_coop_rate
                return Action.D
            else:
                increased = False
                if self.mixed_prev_group_rate is not None and group_coop_rate > self.mixed_prev_group_rate + 0.03:
                    increased = True
                half_group = math.ceil(n / 2)
                if cooperators_last_round >= half_group or increased:
                    self.mixed_defect_round = None
                    self.mixed_prev_group_rate = None
                    return Action.C
                else:
                    self.mixed_defect_round = last_round_index + 1
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    Adaptive Probe-and-Exploit for N-Player Public Goods Game.

    Exploitative strategy that probes for conditional cooperators, free-rides
    when the group is sufficiently cooperative, sends sparse cooperation
    signals to maintain responsiveness, punishes sustained defection, and
    switches to guaranteed defection in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.b = self.k / float(self.n) if self.n > 0 else 0.0
        self.W = min(10, self.r)
        self.T_end = min(3, max(1, math.floor(self.r / 10)))
        self.theta_resp = 0.2
        self.S = max(3, math.ceil(5 * (1 - self.b)))
        raw_p = 0.6 + 0.4 * (1 - self.b)
        self.p_exploit = max(0.6, min(0.95, raw_p))
        self.p_mod = 0.7
        self.F = 2
        self.repair_timer = 0
        self.safety_timer = 0
        self.opponents = max(0, self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 3:
            if t == 0:
                return Action.C
            return Action.D
        if t == 0 or history is None:
            return Action.C
        if t in (0, 1):
            return Action.C
        L = len(history.my_actions)
        my_actions = np.array(history.my_actions, dtype=bool)
        opp_actions = np.array(history.opponent_actions, dtype=bool)
        if opp_actions.size == 0:
            return Action.D
        start_recent = max(0, L - self.W)
        recent_slice = opp_actions[start_recent:L, :]
        if recent_slice.size == 0:
            RecentGroupCoop = 0.0
        else:
            RecentGroupCoop = float(np.mean(recent_slice))
        if L >= 1:
            last_round_opps = opp_actions[L - 1, :]
            LastRoundMajorityCoop = float(np.sum(last_round_opps)) > self.opponents / 2.0
        else:
            LastRoundMajorityCoop = False
        RecentDrop = False
        if L >= 2 * self.W:
            start_prev = max(0, L - 2 * self.W)
            prev_slice = opp_actions[start_prev:L - self.W, :]
            if prev_slice.size != 0:
                prev_mean = float(np.mean(prev_slice))
                if prev_mean - RecentGroupCoop > 0.3:
                    RecentDrop = True
        coop_after_C = np.zeros(self.opponents, dtype=float)
        coop_after_D = np.zeros(self.opponents, dtype=float)
        denom_after_C = np.zeros(self.opponents, dtype=int)
        denom_after_D = np.zeros(self.opponents, dtype=int)
        pre_start = max(0, L - self.W - 1)
        pre_end = L - 2
        if pre_end >= pre_start:
            for idx in range(pre_start, pre_end + 1):
                my_act = bool(my_actions[idx])
                next_idx = idx + 1
                opp_next = opp_actions[next_idx, :]
                for j in range(self.opponents):
                    if my_act:
                        denom_after_C[j] += 1
                        if opp_next[j]:
                            coop_after_C[j] += 1
                    else:
                        denom_after_D[j] += 1
                        if opp_next[j]:
                            coop_after_D[j] += 1
        Responsive_j = np.zeros(self.opponents, dtype=float)
        for j in range(self.opponents):
            rateC = coop_after_C[j] / denom_after_C[j] if denom_after_C[j] > 0 else 0.0
            rateD = coop_after_D[j] / denom_after_D[j] if denom_after_D[j] > 0 else 0.0
            Responsive_j[j] = rateC - rateD
        if self.opponents > 0:
            ResponsiveFraction = float(np.sum(Responsive_j >= self.theta_resp) / float(self.opponents))
        else:
            ResponsiveFraction = 0.0
        preceding_count = max(0, pre_end - pre_start + 1) if pre_end >= pre_start else 0
        insufficient_samples = preceding_count < max(2, min(self.W, 4))
        if insufficient_samples and L < self.W:
            return Action.C
        if RecentDrop and L >= 1 and (not my_actions[L - 1]) and (self.repair_timer == 0):
            self.repair_timer = self.F
        own_recent_slice = my_actions[start_recent:L] if L > start_recent else np.array([], dtype=bool)
        own_recent_coop_rate = float(np.mean(own_recent_slice)) if own_recent_slice.size > 0 else 0.0
        if own_recent_coop_rate > 0.6 and RecentDrop and (self.safety_timer == 0):
            self.safety_timer = self.F
        if self.repair_timer > 0:
            self.repair_timer -= 1
            return Action.C
        if self.safety_timer > 0:
            self.safety_timer -= 1
            if LastRoundMajorityCoop:
                return Action.C
            return Action.D
        if t >= self.r - self.T_end:
            return Action.D
        if RecentGroupCoop >= 0.9 and ResponsiveFraction < 0.2:
            draw = random.random()
            if draw < 0.05:
                return Action.C
            return Action.D
        if ResponsiveFraction >= 0.5 and RecentGroupCoop >= 0.5:
            coop_indices = np.where(my_actions == True)[0]
            if coop_indices.size == 0:
                gap = L
            else:
                last_coop_index = int(coop_indices[-1])
                gap = L - last_coop_index
            if gap >= self.S:
                return Action.C
            if random.random() < self.p_exploit:
                return Action.D
            return Action.C
        if RecentGroupCoop >= 0.75 and ResponsiveFraction < 0.5:
            coop_indices = np.where(my_actions == True)[0]
            if coop_indices.size == 0:
                gap = L
            else:
                last_coop_index = int(coop_indices[-1])
                gap = L - last_coop_index
            if gap >= self.S:
                return Action.C
            if random.random() < self.p_mod:
                return Action.D
            return Action.C
        if RecentGroupCoop < 0.5:
            if LastRoundMajorityCoop:
                return Action.C
            return Action.D
        if RecentGroupCoop >= 0.5:
            if random.random() < self.p_mod:
                return Action.D
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Collective-minded conditional-cooperation with short proportional punishment and forgiveness.

    - Starts cooperative (unless one-shot).
    - Monitors recent cooperation in a moving window M.
    - If recent cooperation fraction f >= tau: cooperate.
    - If f < tau: trigger a short proportional punishment of length P, increasing punishment_severity modestly.
    - After a punishment ends, perform a one-round cooperative test.
    - If a long-run collapse is detected (many players almost never cooperated), switch to permanent defection
      but perform occasional one-round cooperation tests every ceil(r/5) rounds to check for recovery.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = min(5, max(0, self.r - 1))
        raw_tau = 0.5 - 0.3 * (self.k - 1.0) / max(1, self.n - 1)
        self.tau = max(0.1, min(0.9, raw_tau))
        self.Pmax = min(4, max(1, math.floor(self.r / 10) + 1))
        self.L = 0.6
        self.test_interval = max(1, math.ceil(self.r / 5))
        self.punishment_counter = 0
        self.punishment_severity = 1
        self.cooperate_test_pending = False
        self.permanent_defecting = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.r == 1:
                return Action.D
            return Action.C
        if state.round_number == max(0, self.r - 1):
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                self.cooperate_test_pending = True
            return Action.D
        if self.cooperate_test_pending:
            self.cooperate_test_pending = False
            return Action.C
        if self.permanent_defecting:
            if state.round_number % self.test_interval == 0:
                return Action.C
            return Action.D
        rounds_played = state.round_number
        if history is None:
            return Action.C
        m = min(self.M, rounds_played)
        if m <= 0:
            return Action.C
        try:
            last_opp = history.opponent_actions[-m:, :]
        except Exception:
            return Action.C
        denom = (self.n - 1) * m
        coop_count = int(np.sum(last_opp)) if denom > 0 else 0
        f = coop_count / denom if denom > 0 else 0.0
        rounds_total = rounds_played
        if rounds_total > 0:
            if hasattr(history.opponent_actions, 'size') and history.opponent_actions.size > 0:
                opp_counts_all = np.sum(history.opponent_actions, axis=0)
                opp_counts_list = [int(x) for x in opp_counts_all.tolist()]
            else:
                opp_counts_list = [0] * (self.n - 1)
            my_count = int(np.sum(history.my_actions)) if hasattr(history, 'my_actions') else 0
            coop_counts = [my_count] + opp_counts_list
            low_cooperators = 0
            for c in coop_counts:
                rate = c / rounds_total if rounds_total > 0 else 0.0
                if rate <= 0.1 + 1e-12:
                    low_cooperators += 1
            frac_low = low_cooperators / max(1, self.n)
            if frac_low >= self.L:
                self.permanent_defecting = True
                if state.round_number % self.test_interval == 0:
                    return Action.C
                return Action.D
        if f >= self.tau:
            return Action.C
        deficit = max(0.0, self.tau - f)
        inc = math.ceil(deficit * 4.0)
        P = min(self.Pmax, inc + self.punishment_severity)
        self.punishment_severity = min(self.Pmax, self.punishment_severity + 1)
        self.punishment_counter = P
        self.punishment_counter -= 1
        if self.punishment_counter == 0:
            self.cooperate_test_pending = True
        return Action.D

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    Greedy Conditional Cooperator with Occasional Forgiveness (GCC-OF)

    - Signal cooperation in the first round (except the degenerate single-round game).
    - In later rounds, exploit cooperative environments by free-riding most of the time,
      with a low-probability cooperative move to maintain reputation.
    - In intermediate environments, exploit a clear majority of cooperators, otherwise try to
      encourage cooperation.
    - In hostile environments, primarily defect but occasionally probe to test recovery.
    - Short, targeted punishment phases when many opponents defect repeatedly.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.g = 1.0 - self.k / float(self.n) if self.n > 0 else 0.0
        self.w = int(min(4, max(0, self.r - 1)))
        self.H = 0.6
        self.L = 0.1
        frac = self.k / float(self.n) if self.n > 0 else 0.0
        self.p_coop = min(0.2, 0.1 + 0.5 * frac)
        denom = self.k / float(self.n) if self.n > 0 else 1.0
        probe_calc = 8.0 / denom if denom > 0 else 8.0
        self.P_probe = int(max(4, math.ceil(probe_calc)))
        self.P_punish = 2
        self.S_forgive = 2
        self.last_probe_round = -10000
        self.punish_until_round = -1
        self.hostile_enter_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        if n < 2 or r < 1:
            return Action.D
        if t == 0:
            if r == 1:
                return Action.D
            return Action.C
        if history is None:
            return Action.D
        if t == r - 1:
            return Action.D
        lookback_start = max(0, t - self.w)
        lookback_end = t
        num_lookback_rounds = max(0, lookback_end - lookback_start)
        opp_actions = history.opponent_actions
        rounds_available = 0
        try:
            rounds_available = opp_actions.shape[0]
        except Exception:
            rounds_available = 0
        effective_start = min(lookback_start, rounds_available)
        effective_end = min(lookback_end, rounds_available)
        if effective_end <= effective_start:
            recent_opp_slice = None
            denom = 0
            total_others_coops = 0
        else:
            recent_opp_slice = opp_actions[effective_start:effective_end, :]
            total_others_coops = int(np.sum(recent_opp_slice))
            num_rounds_used = recent_opp_slice.shape[0]
            num_opponents = recent_opp_slice.shape[1] if recent_opp_slice.ndim > 1 else 0
            denom = num_rounds_used * num_opponents if num_opponents > 0 else 0
        others_rate = float(total_others_coops) / float(denom) if denom > 0 else 0.0
        last_round_others = 0
        if rounds_available >= 1 and t - 1 < rounds_available:
            last_round_others = int(np.sum(opp_actions[t - 1, :]))
        else:
            last_round_others = 0
        if rounds_available >= 2 and t - 1 < rounds_available:
            idx_a = t - 2
            idx_b = t - 1
            if idx_a >= 0 and idx_b < rounds_available:
                a = opp_actions[idx_a, :]
                b = opp_actions[idx_b, :]
                defected_both = np.logical_and(~a, ~b)
                defects_count = int(np.sum(defected_both))
                if defects_count > (n - 1) / 2.0:
                    self.punish_until_round = max(self.punish_until_round, t + (self.P_punish - 1))
        if t <= self.punish_until_round:
            return Action.D
        if t == r - 2:
            prev_start = max(0, t - self.w - self.w)
            prev_end = max(0, t - self.w)
            prev_denom = 0
            prev_total = 0
            if prev_end > prev_start and opp_actions.shape[0] >= prev_end:
                prev_slice = opp_actions[prev_start:prev_end, :]
                prev_total = int(np.sum(prev_slice))
                prev_num_rounds = prev_slice.shape[0]
                prev_num_opponents = prev_slice.shape[1] if prev_slice.ndim > 1 else 0
                prev_denom = prev_num_rounds * prev_num_opponents if prev_num_opponents > 0 else 0
            prev_rate = float(prev_total) / float(prev_denom) if prev_denom > 0 else others_rate
            if others_rate < prev_rate - 0.05:
                return Action.D
        if others_rate <= self.L:
            if self.hostile_enter_round is None:
                self.hostile_enter_round = t
        else:
            forgive_ok = False
            if t >= 1:
                check_start = max(0, t - self.S_forgive)
                check_end = t
                check_start_eff = min(check_start, rounds_available)
                check_end_eff = min(check_end, rounds_available)
                if check_end_eff > check_start_eff:
                    slice_check = opp_actions[check_start_eff:check_end_eff, :]
                    denom_check = slice_check.shape[0] * slice_check.shape[1] if slice_check.ndim > 1 else 0
                    total_check = int(np.sum(slice_check)) if denom_check > 0 else 0
                    rate_check = float(total_check) / float(denom_check) if denom_check > 0 else 0.0
                    if rate_check > self.L:
                        forgive_ok = True
            if forgive_ok:
                self.hostile_enter_round = None
        if others_rate >= self.H:
            if random.random() < self.p_coop:
                if t < r - 1:
                    return Action.C
                else:
                    return Action.D
            return Action.D
        if others_rate > self.L and others_rate < self.H:
            majority_needed = math.ceil((n - 1) / 2.0)
            if last_round_others >= majority_needed:
                return Action.D
            return Action.C
        rounds_since_probe = t - self.last_probe_round
        if rounds_since_probe >= self.P_probe and t < r - 1:
            self.last_probe_round = t
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    ProbeExploitReciprocate (PER)
    Deterministic adaptive strategy for repeated public goods games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        self._last_round_evaluated = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        t = state.round_number + 1
        T_probe = min(6, max(3, math.floor(r / 5)))
        T_tail = min(3, r)
        base_punish_length = 3
        if r <= 4:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.D
        if t > r - T_tail or t == r:
            return Action.D
        rounds_played = state.round_number
        if rounds_played <= 0:
            return Action.D
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        m = n - 1
        total_coop = [0] * m
        afterMyC_coop = [0] * m
        afterMyD_coop = [0] * m
        afterMyC_count = [0] * m
        afterMyD_count = [0] * m
        for j in range(m):
            total_coop[j] = int(np.sum(opp_actions[:, j].astype(np.int64)))
        for idx in range(1, rounds_played):
            my_prev = bool(my_actions[idx - 1])
            for j in range(m):
                opp_at_idx = bool(opp_actions[idx, j])
                if my_prev:
                    afterMyC_count[j] += 1
                    if opp_at_idx:
                        afterMyC_coop[j] += 1
                else:
                    afterMyD_count[j] += 1
                    if opp_at_idx:
                        afterMyD_coop[j] += 1
        denom_overall = max(1, rounds_played)
        overall_rate = [total_coop[j] / denom_overall for j in range(m)]
        p_afterC = [afterMyC_coop[j] / max(1, afterMyC_count[j]) for j in range(m)]
        p_afterD = [afterMyD_coop[j] / max(1, afterMyD_count[j]) for j in range(m)]
        UC_rate = 0.8
        UC_overall = 0.75
        CC_delta = 0.25
        PD_overall = 0.2
        classes = []
        for j in range(m):
            if p_afterD[j] >= UC_rate and overall_rate[j] >= UC_overall:
                classes.append('UC')
            elif p_afterC[j] - p_afterD[j] >= CC_delta and p_afterC[j] >= 0.5:
                classes.append('CC')
            elif overall_rate[j] <= PD_overall:
                classes.append('PD')
            else:
                classes.append('U')
        f_UC = classes.count('UC') / m
        f_CC = classes.count('CC') / m
        f_PD = classes.count('PD') / m
        if t <= T_probe:
            if t % 2 == 1:
                return Action.D
            else:
                return Action.C
        in_punishment = self.punish_until_round >= t
        last_my_action = bool(my_actions[-1])
        if last_my_action:
            last_round_opps = opp_actions[-1, :]
            actual_defections_last = int(np.sum((~last_round_opps).astype(np.int64)))
            avg_p_afterC = float(np.mean(p_afterC)) if m > 0 else 0.0
            baseline_defections_afterC = m * (1.0 - avg_p_afterC)
            if actual_defections_last - baseline_defections_afterC >= 2.0:
                available = r - t + 1
                punish_len = min(base_punish_length, max(1, available))
                self.punish_until_round = min(r, t + punish_len - 1)
                in_punishment = self.punish_until_round >= t
        if in_punishment:
            avg_p_afterD = float(np.mean(p_afterD)) if m > 0 else 0.0
            if avg_p_afterD >= 0.5:
                self.punish_until_round = 0
                in_punishment = False
        if in_punishment:
            return Action.D
        if f_PD > 0.5:
            return Action.D
        if f_UC >= 0.6 and f_CC < 0.25:
            return Action.D
        if f_CC >= 0.4:
            return Action.C
        p_ifC_vals = [max(p_afterC[j], overall_rate[j]) for j in range(m)]
        p_ifC = float(np.mean(p_ifC_vals)) if m > 0 else 0.0
        p_ifD = float(np.mean(p_afterD)) if m > 0 else 0.0
        Payoff_ifC = k / n * (1.0 + (n - 1) * p_ifC)
        Payoff_ifD = 1.0 + k / n * ((n - 1) * p_ifD)
        continuation_bonus = 0.05 if f_CC >= 0.25 else 0.0
        if Payoff_ifC + continuation_bonus >= Payoff_ifD + 0.05:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC)

    - Starts by cooperating.
    - Tracks recent (m) rounds of behavior to compute per-player reputations and a
      group cooperation rate G.
    - If recent cooperation is healthy (G >= T) and only a few players are being
      punished, cooperates to sustain the public good.
    - Detects persistent defectors (R_i <= -L) and punishes them for P rounds.
      Punishments are targeted and time-limited; there is a small forgiveness
      probability during punishment rounds to allow recovery.
    - In the final s rounds (endgame) is conservative: cooperates only when near
      unanimity and no active punishments exist.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = 2
        self.P = 2
        self.p_forgive = 0.1
        self.punishments = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t = int(state.round_number)
        n = self.n
        r = self.r
        k = self.k
        m = min(5, max(1, r - 1))
        s = min(3, max(1, math.floor(r / 5)))
        denom = max(1, n - 1)
        T = 0.5 + 0.2 * (k - 1) / denom
        window_start = max(0, t - m)
        window_end = t
        window_length = max(1, t - window_start)
        opp_actions_full = history.opponent_actions
        my_actions_full = history.my_actions
        to_remove = []
        for opp_idx, info in list(self.punishments.items()):
            if info.get('remaining', 0) <= 0:
                to_remove.append(opp_idx)
                continue
            start_r = info.get('start_round', 0)
            if start_r < t:
                if np.any(opp_actions_full[start_r:t, opp_idx]):
                    to_remove.append(opp_idx)
        for opp_idx in to_remove:
            self.punishments.pop(opp_idx, None)
        opp_window = opp_actions_full[window_start:window_end, :]
        if opp_window.size == 0:
            coops_per_opp = np.zeros((0,))
        else:
            coops_per_opp = np.sum(opp_window, axis=0).astype(int)
        R_array = 2 * coops_per_opp - window_length if coops_per_opp.size > 0 else np.array([])
        recent_my = my_actions_full[window_start:window_end]
        per_round_opp_coops = np.sum(opp_window, axis=1) if opp_window.size > 0 else np.zeros((window_length,))
        per_round_coop_fraction = (per_round_opp_coops + recent_my.astype(int)) / float(n)
        G = float(np.mean(per_round_coop_fraction)) if per_round_coop_fraction.size > 0 else 0.0
        for opp_idx in range(max(0, n - 1)):
            if opp_idx >= R_array.size:
                continue
            R_j = int(R_array[opp_idx])
            if R_j <= -self.L:
                self.punishments[opp_idx] = {'remaining': self.P, 'start_round': t}
        in_endgame = t >= r - s
        active_punishments = {idx: info for idx, info in self.punishments.items() if info.get('remaining', 0) > 0}
        n_punish = len(active_punishments)
        punish_limit = math.floor(n / 3)
        if in_endgame:
            if G >= 0.95 and n_punish == 0:
                decision = Action.C
            else:
                decision = Action.D
        else:
            if G >= T and n_punish <= punish_limit:
                decision = Action.C
            else:
                decision = Action.D
            if n_punish > 0:
                if random.random() < self.p_forgive:
                    decision = Action.C
                else:
                    decision = Action.D
        for opp_idx in list(self.punishments.keys()):
            info = self.punishments.get(opp_idx)
            if info is None:
                continue
            remaining = int(info.get('remaining', 0))
            remaining -= 1
            if remaining <= 0:
                self.punishments.pop(opp_idx, None)
            else:
                self.punishments[opp_idx]['remaining'] = remaining
        return decision

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    Collective-Conditional Cooperator with Gradual Punishment and Forgiveness (CCC-GPF).

    - Starts by cooperating.
    - Uses a short lookback window to estimate recent group cooperation.
    - Forgives a single missed cooperator relative to a threshold derived from MPCR.
    - Triggers proportional, short punishments for larger shortfalls.
    - Tightens cooperation requirement in the final rounds (endgame).
    - Deterministic and depends only on game parameters and observed history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.MPCR = k / n if n > 0 else 0.0
        self.w = min(5, max(1, math.floor(r / 10)))
        f_raw = 0.3 + 0.7 * self.MPCR
        self.f_star = max(0.1, min(0.9, f_raw))
        self.T = math.ceil(self.f_star * n)
        self.E = math.ceil(r / 10)
        self.n_players = n
        self.n_rounds = r
        self.remainingPunish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = int(state.round_number)
        r = self.n_rounds
        n = self.n_players
        if t_idx == 0:
            return Action.C
        if t_idx == r - 1:
            return Action.D
        if history is None:
            return Action.D
        if self.remainingPunish > 0:
            self.remainingPunish -= 1
            return Action.D
        try:
            prev_my = bool(history.my_actions[-1])
            prev_opponents_sum = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            return Action.D
        S_prev = prev_opponents_sum + (1 if prev_my else 0)
        if S_prev < self.T - 1:
            r_remaining = r - t_idx
            P = min(r_remaining, 1 + (self.T - S_prev))
            self.remainingPunish = max(0, int(P) - 1)
            return Action.D
        total_rounds_recorded = int(history.my_actions.shape[0])
        last_w = min(self.w, total_rounds_recorded)
        if last_w <= 0:
            return Action.D
        recent_my = history.my_actions[-last_w:].astype(np.int64)
        recent_opponents = history.opponent_actions[-last_w:, :].astype(np.int64)
        per_round_contribs = np.sum(recent_opponents, axis=1) + recent_my
        per_round_frac = per_round_contribs / float(n)
        p_recent = float(np.mean(per_round_frac))
        r_remaining_including_current = r - t_idx
        if r_remaining_including_current <= self.E:
            threshold = min(1.0, self.f_star + 0.1)
        else:
            threshold = self.f_star
        if p_recent >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Graded Reciprocity with Forgiveness and Extinction Detection (GR-FE)

    - Starts by cooperating to seed cooperation.
    - Uses a recent-window fraction of opponent contributions to decide:
        * Cooperate if recent cooperation >= TH
        * Defect if recent cooperation < TL
        * In-between: reciprocate if at least one other cooperated last round
    - Detects sudden collapses (drop > 0.30 vs previous M-window) and punishes
      for a proportional, bounded number of rounds.
    - Detects near-extinction of cooperation and permanently defects thereafter.
    - Defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.M = min(5, max(2, r))
        self.s = 1.0 - k / n
        self.TH = 0.65 + 0.15 * self.s
        self.TL = 0.35 + 0.15 * self.s
        self.P_max = min(3, max(1, math.floor(r / 10) + 1))
        self.E = min(10, r)
        self.CE = 0.1
        self.permanent_defect = False
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == r - 1:
            return Action.D
        if self.permanent_defect:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D
        opp_actions = history.opponent_actions
        num_opponents = opp_actions.shape[1] if opp_actions.ndim >= 2 else 0
        if num_opponents <= 0:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        M = self.M
        W = min(M, t)
        window_start = t - W
        window_end = t
        opp_window = opp_actions[window_start:window_end, :]
        total_contribs = float(np.sum(opp_window))
        denom = float(num_opponents * W) if num_opponents * W > 0 else 1.0
        F = total_contribs / denom
        last_round = opp_actions[t - 1, :]
        last_round_others = float(np.sum(last_round)) / float(num_opponents)
        if W > 0:
            cooperated_always = np.all(opp_window, axis=0)
            defected_at_least_once = np.logical_not(cooperated_always)
            Dcount = int(np.sum(defected_at_least_once))
        else:
            Dcount = 0
        if t > self.E:
            E = self.E
            e_start = t - E
            e_window = opp_actions[e_start:t, :]
            e_total = float(np.sum(e_window))
            e_denom = float(num_opponents * E) if num_opponents * E > 0 else 1.0
            frac_E = e_total / e_denom
            if frac_E < self.CE:
                self.permanent_defect = True
                return Action.D
        if t >= M + 1:
            prev_start = t - M - 1
            prev_end = t - 1
            prev_window = opp_actions[prev_start:prev_end, :]
            prev_total = float(np.sum(prev_window))
            prev_denom = float(num_opponents * M) if num_opponents * M > 0 else 1.0
            F_prev = prev_total / prev_denom
            if F_prev - F > 0.3:
                ratio = float(Dcount) / float(max(1, num_opponents))
                P = 1 + math.floor(ratio * self.P_max)
                P = int(min(self.P_max, max(1, P)))
                self.punish_remaining = P - 1
                return Action.D
        if F >= self.TH:
            return Action.C
        if F < self.TL:
            return Action.D
        if last_round_others > 0.0:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Collective Conditional Cooperation with Proportional Punishment & Forgiveness (CCC-PPF)

    - Start cooperative to invite gains.
    - Monitor recent cooperation rate G over a window W (function of total rounds).
    - If cooperation collapses or someone defects, enter a proportional Punish phase for P rounds.
    - During Punish: usually defect, but with small probability epsilon cooperate (exploratory forgiveness).
      Escalate punishment if defections increase during punishment.
    - After punishment, attempt a short Rebuild of cooperation for R rounds; if rebuild fails, punish again.
    - If cooperation stays collapsed over a long window (2W), switch to permanent protective defection (except final round).
    - Always defect in the final round. For very short games (r <= 3), play C on non-final rounds and D in final.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.phase = 'Cooperate'
        self.punish_remaining = 0
        self.rebuild_remaining = 0
        self.trigger_d = 0
        self.last_punish_length = 1
        self.permanent_defect = False
        self.theta_high = 0.7
        self.theta_low = 0.3
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        round_idx = state.round_number
        if round_idx == 0:
            self.phase = 'Cooperate'
            self.punish_remaining = 0
            self.rebuild_remaining = 0
            self.trigger_d = 0
            self.last_punish_length = 1
            self.permanent_defect = False
            return Action.C
        if round_idx == r - 1:
            return Action.D
        if r <= 3:
            return Action.C
        if history is None:
            return Action.D
        W = max(3, min(10, math.floor(r / 5)))
        available_rounds = round_idx
        w_actual = min(W, available_rounds)
        if w_actual <= 0:
            return Action.C
        num_opponents = n - 1
        my_slice = history.my_actions[-w_actual:]
        opp_slice = history.opponent_actions[-w_actual:, :]
        try:
            total_my_coops = int(np.sum(my_slice))
            total_opp_coops = int(np.sum(opp_slice))
        except Exception:
            total_my_coops = int(sum((bool(x) for x in my_slice)))
            total_opp_coops = int(sum((bool(x) for row in opp_slice for x in row)))
        sum_coops_recent = total_my_coops + total_opp_coops
        G = sum_coops_recent / (n * w_actual)
        w2 = min(2 * W, available_rounds)
        if w2 > 0:
            my_slice2 = history.my_actions[-w2:]
            opp_slice2 = history.opponent_actions[-w2:, :]
            try:
                total_my_coops2 = int(np.sum(my_slice2))
                total_opp_coops2 = int(np.sum(opp_slice2))
            except Exception:
                total_my_coops2 = int(sum((bool(x) for x in my_slice2)))
                total_opp_coops2 = int(sum((bool(x) for row in opp_slice2 for x in row)))
            sum_coops_long = total_my_coops2 + total_opp_coops2
            G_long = sum_coops_long / (n * w2)
        else:
            G_long = 1.0
        if G_long <= self.theta_low:
            self.permanent_defect = True
        prev_opp_actions = history.opponent_actions[-1, :]
        try:
            prev_opp_coops = int(np.sum(prev_opp_actions))
        except Exception:
            prev_opp_coops = int(sum((bool(x) for x in prev_opp_actions)))
        d_last = num_opponents - prev_opp_coops
        if self.permanent_defect:
            self.phase = 'Punish'
            self.punish_remaining = max(1, self.punish_remaining)
            return Action.D
        try:
            opp_coops_each_round = np.sum(history.opponent_actions[-w_actual:, :], axis=1)
            opp_defectors_total = int(np.sum(num_opponents - opp_coops_each_round))
        except Exception:
            opp_defectors_total = 0
            rows = history.opponent_actions[-w_actual:, :]
            for row in rows:
                opp_defectors_total += num_opponents - int(sum((bool(x) for x in row)))
        avg_defectors_per_round = opp_defectors_total / w_actual if w_actual > 0 else 0.0
        avg_defectors_ceil = int(math.ceil(avg_defectors_per_round))
        P_from_avg = max(1, min(4, avg_defectors_ceil))
        if self.phase == 'Cooperate':
            if G <= self.theta_low:
                P = P_from_avg
                self.phase = 'Punish'
                self.punish_remaining = P
                self.last_punish_length = P
                self.trigger_d = d_last if d_last >= 1 else 1
                return Action.D
            if d_last == 0:
                return Action.C
            P = max(1, min(4, d_last))
            self.phase = 'Punish'
            self.punish_remaining = P
            self.last_punish_length = P
            self.trigger_d = d_last
            return Action.D
        elif self.phase == 'Punish':
            d_current = d_last
            if d_current > self.trigger_d:
                P = max(1, min(4, d_current))
                self.punish_remaining = P
                self.last_punish_length = P
                self.trigger_d = d_current
            if random.random() < self.epsilon:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                if self.punish_remaining == 0:
                    R = max(1, int(math.ceil(self.last_punish_length / 2)))
                    self.phase = 'Rebuild'
                    self.rebuild_remaining = R
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                R = max(1, int(math.ceil(self.last_punish_length / 2)))
                self.phase = 'Rebuild'
                self.rebuild_remaining = R
            return Action.D
        elif self.phase == 'Rebuild':
            d_current = d_last
            if d_current > 0:
                P = max(1, min(4, d_current))
                self.phase = 'Punish'
                self.punish_remaining = P
                self.last_punish_length = P
                self.trigger_d = d_current
                return Action.D
            self.rebuild_remaining = max(0, self.rebuild_remaining - 1)
            if self.rebuild_remaining == 0:
                self.phase = 'Cooperate'
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    Probes early with an alternating C/D pattern to estimate opponents' responsiveness to
    my defections, then exploits opponents who do not punish by defecting when the
    immediate gain outweighs estimated future losses (using g and D_threshold).
    Reverts to cooperation (including short conciliatory windows) when faced with
    clear group-wide retaliation. Uses smoothing on response estimates and conservative
    defaults when data is sparse.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = float(game_description.n_players)
        self.n_players = game_description.n_players
        self.r = game_description.n_rounds
        self.k = float(game_description.k)
        self.min_obs = 2
        self.T_probe = min(6, max(2, math.floor(self.r / 5)))
        self.smoothing_alpha = 0.7
        self.calm_until_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.r
        n_players = self.n_players
        opponents = max(0, n_players - 1)
        if t == 0 or history is None:
            return Action.C
        my_actions = np.array(history.my_actions, dtype=bool)
        opp_actions = np.array(history.opponent_actions, dtype=bool)
        rounds_observed = opp_actions.shape[0]
        W = min(5, r)
        W_used = min(W, rounds_observed)
        if W_used > 0 and opponents > 0:
            recent = opp_actions[-W_used:, :]
            baseline_group_coop = float(np.mean(recent))
        else:
            baseline_group_coop = 0.0
        if t < self.T_probe:
            if self.calm_until_round >= t:
                return Action.C
            return Action.C if t % 2 == 0 else Action.D
        last_round_index = r - 1
        penultimate_index = max(0, r - 2)
        if t >= last_round_index:
            return Action.D
        if t == penultimate_index:
            count_unknown = 0
            raw_counts_C = np.zeros(opponents, dtype=int)
            raw_counts_D = np.zeros(opponents, dtype=int)
            if rounds_observed >= 2:
                for s in range(0, rounds_observed - 1):
                    my_a = bool(my_actions[s])
                    for j in range(opponents):
                        if my_a:
                            raw_counts_C[j] += 1
                        else:
                            raw_counts_D[j] += 1
            for j in range(opponents):
                if raw_counts_C[j] + raw_counts_D[j] < self.min_obs:
                    count_unknown += 1
            many_unknown = count_unknown >= math.ceil(0.6 * max(1, opponents))
            if baseline_group_coop > 0.95 and many_unknown:
                return Action.C
            return Action.D
        if self.calm_until_round >= t:
            return Action.C
        denom_C_weight = np.zeros(opponents, dtype=float)
        num_C_weight = np.zeros(opponents, dtype=float)
        denom_D_weight = np.zeros(opponents, dtype=float)
        num_D_weight = np.zeros(opponents, dtype=float)
        raw_counts_C = np.zeros(opponents, dtype=int)
        raw_counts_D = np.zeros(opponents, dtype=int)
        last_transition_index = rounds_observed - 2
        if last_transition_index >= 0:
            for s in range(0, rounds_observed - 1):
                age = last_transition_index - s
                weight = self.smoothing_alpha ** age if age >= 0 else 1.0
                my_a = bool(my_actions[s])
                next_row = opp_actions[s + 1, :]
                if my_a:
                    raw_counts_C += 1
                    denom_C_weight += weight
                    num_C_weight += weight * next_row.astype(float)
                else:
                    raw_counts_D += 1
                    denom_D_weight += weight
                    num_D_weight += weight * next_row.astype(float)
        coop_after_C = np.zeros(opponents, dtype=float)
        coop_after_D = np.zeros(opponents, dtype=float)
        for j in range(opponents):
            if denom_C_weight[j] > 0.0:
                coop_after_C[j] = float(num_C_weight[j] / denom_C_weight[j])
            else:
                coop_after_C[j] = 0.0
            if denom_D_weight[j] > 0.0:
                coop_after_D[j] = float(num_D_weight[j] / denom_D_weight[j])
            else:
                coop_after_D[j] = 0.0
        delta = np.zeros(opponents, dtype=float)
        unknown_mask = np.zeros(opponents, dtype=bool)
        for j in range(opponents):
            obs_count = raw_counts_C[j] + raw_counts_D[j]
            if raw_counts_C[j] < self.min_obs or raw_counts_D[j] < self.min_obs:
                unknown_mask[j] = True
            delta[j] = coop_after_C[j] - coop_after_D[j]
        unknown_delta_value = 0.5 * max(0.25, baseline_group_coop)
        good_count = np.sum(~unknown_mask)
        good_fraction = float(good_count) / max(1, opponents) if opponents > 0 else 1.0
        delta_effective = np.array([delta[j] if not unknown_mask[j] else unknown_delta_value for j in range(opponents)], dtype=float)
        d_estimate = float(np.sum(delta_effective))
        if opponents > 0:
            measured_coop_after_C = np.where(denom_C_weight > 0.0, coop_after_C, baseline_group_coop)
            avg_coop_after_C = float(np.mean(measured_coop_after_C))
        else:
            avg_coop_after_C = 0.0
        g = 1.0 - self.k / float(n_players)
        R_rem = max(1, r - t)
        denom_thresh = self.k * float(R_rem)
        if denom_thresh <= 0.0:
            D_threshold = float('inf')
        else:
            D_threshold = g * float(n_players) / denom_thresh
        if baseline_group_coop < 0.25:
            decision = Action.D
        elif d_estimate < D_threshold:
            decision = Action.D
        elif avg_coop_after_C > 0.9 and d_estimate > 0.0 and (d_estimate < 1.5 * D_threshold):
            denom_for_p = max(D_threshold, 1e-08)
            p_defect = 0.4 + 0.4 * (1.0 - d_estimate / denom_for_p)
            p_defect = min(0.8, max(0.0, p_defect))
            if random.random() < p_defect:
                decision = Action.D
            else:
                decision = Action.C
        else:
            decision = Action.C
        if rounds_observed >= 2:
            s = rounds_observed - 2
            my_a_s = bool(my_actions[s])
            coop_s = int(np.sum(opp_actions[s, :])) + (1 if my_a_s else 0)
            coop_s1 = int(np.sum(opp_actions[s + 1, :])) + (1 if bool(my_actions[s + 1]) else 0) if s + 1 < len(my_actions) else int(np.sum(opp_actions[s + 1, :]))
            avg_delta_recent = 0.0
            if opponents > 0:
                avg_delta_recent = float(np.mean(opp_actions[s, :].astype(float) - opp_actions[s + 1, :].astype(float)))
                avg_delta_recent = max(0.0, -avg_delta_recent)
            drop_detected = False
            if coop_s > 0 and coop_s1 < 0.5 * coop_s:
                drop_detected = True
            if avg_delta_recent > 0.5:
                drop_detected = True
            if not my_a_s and drop_detected:
                T_calm = min(3, max(0, R_rem - 1))
                if T_calm > 0:
                    new_calm_until = t + T_calm - 1
                    if new_calm_until > self.calm_until_round:
                        self.calm_until_round = new_calm_until
                    decision = Action.C
        if decision == Action.C:
            if baseline_group_coop > 0.9:
                num_responsive = np.sum(delta_effective > 0.05)
                if opponents > 0 and num_responsive <= math.floor(0.2 * opponents):
                    decision = Action.D
        return decision

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Starts cooperating.
    - Tracks recent fraction of other players cooperating (window W).
    - Uses threshold T (function of k/n) to decide whether to cooperate.
    - Applies short, bounded punishments (up to P_max rounds) when cooperation falls below T,
      but is lenient about single-round outliers and forgives quickly when others resume cooperation.
    - Handles endgame (last rounds) with safer defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = float(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        raw_T = 0.5 + 0.5 * (1.0 - self.k / self.n)
        self.T = max(0.5, min(0.9, raw_T))
        self.P_max = min(3, max(1, math.floor(self.r / 10)))
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.punishment_timer = 0
            return Action.C
        rem = self.r - state.round_number
        if rem <= 0:
            return Action.D
        completed = state.round_number
        if history is None or completed == 0:
            p = 1.0
            recent_majority = 1.0
            window_size = 0
            rest_avg = None
        else:
            window_size = min(self.W, completed)
            start_idx = completed - window_size
            window_actions = history.opponent_actions[start_idx:completed, :]
            if window_actions.size == 0:
                p = 1.0
                recent_majority = 1.0
                rest_avg = None
            else:
                per_round_f = np.mean(window_actions.astype(float), axis=1)
                p = float(np.mean(per_round_f))
                recent_majority = float(per_round_f[-1])
                if window_size >= 2:
                    rest_avg = float(np.mean(per_round_f[:-1]))
                else:
                    rest_avg = None
        if rem == 1:
            return Action.D
        if rem == 2:
            if recent_majority >= 0.999999:
                return Action.C
            return Action.D
        if rem == 3:
            if p >= 0.9:
                return Action.C
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if p >= self.T:
            return Action.C
        if recent_majority >= 0.8 and rest_avg is not None and (rest_avg >= self.T):
            return Action.C
        new_timer = min(self.P_max, max(0, rem - 1))
        self.punishment_timer = max(0, new_timer - 1)
        return Action.D

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    Opportunistic Conditional Free-rider (OCF)

    Starts cooperative to attract cooperators, estimates recent contribution rate of others (q),
    exploits when others are reliably cooperative by defecting most of the time while sending
    occasional cooperative signals, defects when others are mostly defecting, and enters a short
    punishment episode if my defections appear to provoke retaliation. Tapers cooperation toward
    the end of the game and always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.alpha = self.k / float(self.n)
        self.W_base = min(10, max(3, math.floor(math.sqrt(max(1, self.r)))))
        self.q_high = 0.6
        self.q_low = 0.2
        self.p_signal = 0.1
        self.E = min(3, self.r)
        self.punishment_remaining = 0
        self.processed_defections = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            return Action.C
        r = self.r
        n = self.n
        opponents_count = max(1, n - 1)
        rounds_played = t
        R_rem = r - t

        def compute_q_over_range(start_idx: int, end_idx: int) -> float | None:
            if start_idx > end_idx:
                return None
            seg = history.opponent_actions[start_idx:end_idx + 1, :]
            if seg.size == 0:
                return None
            total_coop = float(np.sum(seg))
            denom = opponents_count * (end_idx - start_idx + 1)
            if denom <= 0:
                return None
            return total_coop / float(denom)
        for d in range(0, t):
            if d in self.processed_defections:
                continue
            try:
                my_action_was_coop = bool(history.my_actions[d])
            except Exception:
                my_action_was_coop = True
            if my_action_was_coop:
                self.processed_defections.add(d)
                continue
            pre_end = d - 1
            pre_start = max(0, d - self.W_base)
            pre_q = compute_q_over_range(pre_start, pre_end)
            post_start = d + 1
            post_end = min(d + 2, t - 1)
            post_q = compute_q_over_range(post_start, post_end)
            self.processed_defections.add(d)
            if pre_q is None or post_q is None:
                continue
            if pre_q - post_q >= 0.2:
                P = min(3, max(0, R_rem))
                if P > 0:
                    if self.punishment_remaining < P:
                        self.punishment_remaining = P
        if R_rem == 1:
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        if R_rem <= self.E:
            if R_rem == 2:
                if random.random() < 0.05:
                    return Action.C
                return Action.D
            if R_rem == 3:
                if random.random() < 0.15:
                    return Action.C
                return Action.D
        W_eff = min(self.W_base, rounds_played)
        if W_eff <= 0:
            q = 0.0
        else:
            start_idx = rounds_played - W_eff
            end_idx = rounds_played - 1
            q_val = compute_q_over_range(start_idx, end_idx)
            q = 0.0 if q_val is None else float(q_val)
        probe_end_idx = min(2, r - 1)
        if t <= probe_end_idx:
            if q <= self.q_low:
                return Action.D
            return Action.C
        if q >= self.q_high:
            if random.random() < self.p_signal:
                return Action.C
            return Action.D
        if q > self.q_low and q < self.q_high:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Collective-First Conditional Cooperation (CFCC)

    Begin by cooperating. Maintain cooperation when recent group cooperation
    meets a parameterized target. If the group under-provides, defect this
    round and set a short, proportional, forgivable punishment for subsequent
    rounds. Near the known finite horizon be more cautious (shorter punishments,
    stricter cooperation requirements).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.m = max(1, min(5, math.floor(r / 10)))
        self.punishment_scale = min(5, max(1, math.ceil(r / 10)))
        self.cooperation_target = 0.5 + 0.25 * ((k - 1.0) / max(1.0, n - 1.0))
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        if state.round_number == 0 or history is None:
            return Action.C
        completed_rounds = state.round_number
        m_use = min(self.m, completed_rounds)
        if m_use <= 0:
            m_use = 1
        start_idx = completed_rounds - m_use
        fractions = []
        for idx in range(start_idx, completed_rounds):
            my_contrib = 1 if bool(history.my_actions[idx]) else 0
            opp_contrib = int(np.sum(history.opponent_actions[idx, :]))
            total_contrib = my_contrib + opp_contrib
            fraction = float(total_contrib) / float(max(1, n))
            fractions.append(fraction)
        avg_recent = float(np.mean(np.array(fractions))) if len(fractions) > 0 else 0.0
        t = state.round_number + 1
        remaining = r - state.round_number
        if self.punish_remaining > 0:
            if avg_recent >= self.cooperation_target:
                self.punish_remaining = 0
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if remaining == 1:
            return Action.D
        endgame_window = max(2, math.ceil(r / 10))
        if remaining <= endgame_window:
            if avg_recent >= 0.9:
                return Action.C
            return Action.D
        if avg_recent >= self.cooperation_target:
            return Action.C
        shortfall = max(0.0, self.cooperation_target - avg_recent)
        punish_length = min(self.punishment_scale, max(1, math.ceil(shortfall * self.punishment_scale)))
        self.punish_remaining = int(punish_length)
        return Action.D

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    Stingy Conditional Exploiter (SCE)

    Quick summary:
    - Probe early to identify cooperators, then exploit when many others cooperate by mostly defecting
      while sending occasional maintenance gifts. Use short targeted punishments for persistent defectors,
      occasional probes in defecting groups, and always defect in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = int(n)
        self.r = int(r)
        self.k = float(game_description.k)
        self.W = min(10, max(3, math.floor(self.r / 4))) if self.r >= 1 else 3
        self.P = min(3, max(1, math.floor(self.r / 10))) if self.r >= 1 else 1
        self.T_high = 0.8
        self.T_low = 0.4
        self.G_high = 0.75
        self.G_mid = 0.4
        self.p_free = 0.85
        self.p_signal = 0.15
        self.p_probe = 0.05
        self.p_rebuild = 0.1
        self.L_punish = min(3, max(1, math.floor(self.r / 20))) if self.r >= 1 else 1
        self.n_opponents = max(0, self.n - 1)
        self.punish_timers = [0] * self.n_opponents
        self.punish_just_finished = [False] * self.n_opponents

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r_total = self.r
        rounds_remaining = r_total - t + 1
        if r_total <= 1:
            return Action.D
        if self.n_opponents == 0:
            return Action.D
        for j in range(self.n_opponents):
            if self.punish_timers[j] > 0:
                self.punish_timers[j] -= 1
                if self.punish_timers[j] == 0:
                    self.punish_just_finished[j] = True
        if state.round_number == 0 or history is None:
            return Action.C
        if rounds_remaining <= 2:
            return Action.D
        rounds_so_far = history.opponent_actions.shape[0]
        window_len = min(self.W, rounds_so_far) if rounds_so_far > 0 else 0
        rates = [0.0] * self.n_opponents
        if window_len > 0:
            recent = history.opponent_actions[-window_len:, :]
            try:
                sums = np.sum(recent.astype(float), axis=0)
                for j in range(self.n_opponents):
                    rates[j] = float(sums[j]) / float(window_len)
            except Exception:
                for j in range(self.n_opponents):
                    s = 0
                    for row in range(-window_len, 0):
                        if history.opponent_actions[row, j]:
                            s += 1
                    rates[j] = s / float(window_len)
        else:
            rates = [0.0] * self.n_opponents
        f = float(np.mean(np.array(rates))) if self.n_opponents > 0 else 0.0
        try:
            c_last = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            c_last = 0
        reliable_idxs = [j for j in range(self.n_opponents) if rates[j] >= self.T_high]
        mostly_defector_idxs = [j for j in range(self.n_opponents) if rates[j] <= self.T_low]
        uncertain_idxs = [j for j in range(self.n_opponents) if j not in reliable_idxs and j not in mostly_defector_idxs]
        if len(mostly_defector_idxs) > 0:
            candidate = None
            worst_rate = 2.0
            for j in mostly_defector_idxs:
                if self.punish_timers[j] == 0 and (not self.punish_just_finished[j]):
                    if rates[j] < worst_rate:
                        worst_rate = rates[j]
                        candidate = j
            if candidate is not None:
                self.punish_timers[candidate] = self.L_punish
                self.punish_just_finished[candidate] = False
        for j in range(self.n_opponents):
            if rates[j] > self.T_low:
                self.punish_timers[j] = 0
                self.punish_just_finished[j] = False
        if t <= self.P:
            half_needed = math.ceil(self.n_opponents / 2)
            if c_last >= half_needed:
                return Action.C
            return Action.D
        if any((self.punish_timers[j] > 0 for j in range(self.n_opponents))):
            return Action.D
        for j in range(self.n_opponents):
            if self.punish_just_finished[j]:
                do_signal = random.random() < self.p_signal
                self.punish_just_finished[j] = False
                if do_signal:
                    return Action.C
                else:
                    break
        if f >= self.G_high:
            if len(reliable_idxs) >= 2:
                if random.random() < self.p_free:
                    return Action.D
                else:
                    return Action.C
            elif random.random() < self.p_free:
                return Action.D
            else:
                return Action.C
        if self.G_mid <= f < self.G_high:
            half_needed = math.ceil(self.n_opponents / 2)
            if c_last >= half_needed:
                prob_defect = self.p_rebuild
                if len(reliable_idxs) >= 2:
                    prob_defect = min(0.9, prob_defect + 0.1)
                if random.random() < prob_defect:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        if f < self.G_mid:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC) with Proportional Punishment and Forgiveness.

    - Starts cooperating (unless single-shot).
    - Uses a short memory (up to 5 rounds) of other players' contributions to compute
      recent_fraction and avg_fraction of cooperators among opponents.
    - Threshold T = 0.5 + 0.25*(1 - k/n) determines whether to cooperate.
    - When avg_fraction >= T, cooperate. When avg_fraction < T, defect and trigger a
      proportional punishment lasting a small number of rounds (up to 3).
    - Periodically (every K rounds) probe with a single cooperation move if defecting
      persistently to test whether cooperation can be restored.
    - On the last round, cooperate only if the immediate previous round's recent_fraction >= T.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter: int = 0
        self.last_probe_round: int = 0
        self.K: int = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        opponents_count = max(1, n - 1)
        T = 0.5 + 0.25 * (1.0 - k / max(1.0, n))
        max_punish = min(3, r)
        if r == 1:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        rounds_played = history.opponent_actions.shape[0]
        if rounds_played >= 1:
            last_row = history.opponent_actions[-1]
            recent_fraction = float(np.mean(last_row)) if last_row.size > 0 else 0.0
        else:
            recent_fraction = 0.0
        M = min(5, max(0, t - 1))
        if M <= 0:
            avg_fraction = 0.0
        else:
            last_M_rows = history.opponent_actions[-M:, :] if M > 0 else history.opponent_actions
            if last_M_rows.size == 0:
                avg_fraction = 0.0
            else:
                row_means = np.mean(last_M_rows, axis=1)
                avg_fraction = float(np.mean(row_means))
        if t == r:
            if rounds_played < 1:
                return Action.D
            return Action.C if recent_fraction >= T else Action.D
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return action
        if avg_fraction >= T:
            return Action.C
        my_actions = history.my_actions
        streak = 0
        for a in my_actions[::-1]:
            if not bool(a):
                streak += 1
            else:
                break
        if self.punishment_counter == 0 and t - self.last_probe_round >= self.K and (streak >= max(1, M)):
            self.last_probe_round = t
            return Action.C
        raw_scale = (T - recent_fraction) * opponents_count
        punish_len = int(round(raw_scale)) + 1
        punish_len = max(1, punish_len)
        punish_len = min(punish_len, max_punish, max(0, r - t))
        if punish_len <= 0 and r - t > 0:
            punish_len = 1
        self.punishment_counter = punish_len
        self.last_probe_round = t
        return Action.D

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    Collective Forgiving Reciprocity (CFR) for the N-player Public Goods Game.

    - Starts cooperating (C) on the first round, defects (D) on the last round.
    - Uses a cooperation threshold T = 0.5 + 0.4*(1 - k/n) (bounded to [0.5,0.9]) which can be
      increased by +0.05 (up to 0.95) if defection episodes repeat.
    - Forgives isolated defections (<=1 defector last round).
    - If group cooperation p < T and more than one defector, initiates a proportional,
      temporary punishment (sequence of Ds). Punishment length scales with (T - p)*n,
      but is capped and never allowed to consume the final round.
    - After punishment ends, resumes cooperation unless the next observed p < T, in which
      case resumes punishment with a reduced timer (half of previous, floored, at least 1).
    - Tracks distinct punishment episodes; every 3 episodes increases T by +0.05 (<=0.95).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        T_init = 0.5 + 0.4 * (1.0 - self.k / max(1.0, self.n))
        T_init = min(max(T_init, 0.5), 0.9)
        self.T = T_init
        self.punish_timer = 0
        self.last_punish_len = 0
        self.just_ended_punishment = False
        self.punish_episodes_since_increase = 0
        self.last_round_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_index = int(state.round_number)
        t = t_index + 1
        remaining = self.r - t_index

        def compute_last_round_stats():
            if history is None or t_index == 0:
                return (None, None)
            opp_last = history.opponent_actions[-1]
            my_last = bool(history.my_actions[-1])
            coop_count = int(np.sum(opp_last)) + (1 if my_last else 0)
            p = coop_count / max(1, self.n)
            d = self.n - coop_count
            return (p, d)
        if t_index == 0:
            action = Action.C
            self.last_round_action = action
            return action
        if remaining == 1:
            action = Action.D
            self.last_round_action = action
            return action
        if remaining == 2:
            p_last, d_last = compute_last_round_stats()
            full_coop = p_last is not None and d_last == 0
            if full_coop and self.punish_timer == 0:
                action = Action.C
            else:
                action = Action.D
            self.last_round_action = action
            return action
        if self.punish_timer > 0:
            action = Action.D
            p_last, _ = compute_last_round_stats()
            if p_last is not None and p_last >= self.T:
                self.punish_timer = 0
                self.last_punish_len = 0
                self.just_ended_punishment = False
            else:
                prev = self.punish_timer
                self.punish_timer = max(0, self.punish_timer - 1)
                if self.punish_timer == 0:
                    self.last_punish_len = prev
                    self.just_ended_punishment = True
            self.last_round_action = action
            return action
        p_last, d_last = compute_last_round_stats()
        if p_last is None:
            action = Action.D
            self.last_round_action = action
            return action
        if d_last <= 1:
            action = Action.C
            self.just_ended_punishment = False
            self.last_round_action = action
            return action
        if p_last >= self.T:
            action = Action.C
            self.just_ended_punishment = False
            self.last_round_action = action
            return action
        if self.just_ended_punishment and self.last_punish_len > 0:
            proposed = max(1, math.floor(self.last_punish_len / 2))
            cap = max(1, min(5, remaining - 1))
            punish_len = min(proposed, cap)
            punish_len = max(1, punish_len)
            self.just_ended_punishment = False
        else:
            gap = max(0.0, self.T - p_last)
            proposed = max(1, math.ceil(gap * self.n))
            cap = max(1, min(5, remaining - 1))
            punish_len = min(proposed, cap)
            punish_len = max(1, punish_len)
        action = Action.D
        self.punish_timer = punish_len
        prev_timer = self.punish_timer
        self.punish_timer = max(0, self.punish_timer - 1)
        if self.punish_timer == 0:
            self.last_punish_len = prev_timer
            self.just_ended_punishment = True
        else:
            self.last_punish_len = prev_timer
            self.just_ended_punishment = False
        self.punish_episodes_since_increase += 1
        if self.punish_episodes_since_increase >= 3:
            newT = min(0.95, self.T + 0.05)
            if newT > self.T:
                self.T = newT
            self.punish_episodes_since_increase = 0
        if self.last_punish_len >= remaining:
            allowed = max(1, remaining - 1)
            self.last_punish_len = allowed
            self.punish_timer = max(0, allowed - 1)
        self.last_round_action = action
        return action

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Starts by cooperating to signal willingness.
    - Defects in the final round.
    - Uses a dynamic threshold T for others' cooperation to decide cooperation.
    - Initiates proportional, time-limited punishments when others fall below T.
    - Forgives immediately if others already restored cooperation above T.
    - Slowly adapts T based on the recent average fraction of cooperating others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        if self.n - 1 > 0:
            T0 = 0.5 + 0.25 * (self.k - 1.0) / float(self.n - 1)
        else:
            T0 = 0.5
        self.T = max(0.5, min(0.9, float(T0)))
        self.Pmax = max(1, math.floor(self.r / 10))
        self.PunishCounter = 0
        self._last_adapt_history_len = -1
        self._last_decision_round = -1
        self._last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = int(state.round_number)
        if current_round == self._last_decision_round and self._last_action is not None:
            return self._last_action
        hist_rounds = 0
        if history is not None and hasattr(history, 'opponent_actions'):
            try:
                hist_rounds = int(history.opponent_actions.shape[0])
            except Exception:
                hist_rounds = 0
        if hist_rounds > 0 and hist_rounds != self._last_adapt_history_len:
            window = min(5, hist_rounds)
            recent = history.opponent_actions[-window:, :]
            denom = max(1, self.n - 1)
            counts = np.sum(recent, axis=1).astype(float)
            fracs = counts / float(denom)
            avg_frac = float(np.mean(fracs)) if fracs.size > 0 else 0.0
            if avg_frac >= self.T:
                self.T = max(0.5, float(self.T) - 0.02)
            else:
                self.T = min(0.95, float(self.T) + 0.02)
            self._last_adapt_history_len = hist_rounds
        if current_round == 0:
            action = Action.C
            self._last_decision_round = current_round
            self._last_action = action
            return action
        if current_round == max(0, self.r - 1):
            action = Action.D
            self._last_decision_round = current_round
            self._last_action = action
            return action
        others_frac = 0.0
        if hist_rounds >= 1:
            try:
                last_row = history.opponent_actions[-1, :]
                coop_count = int(np.sum(last_row))
            except Exception:
                coop_count = 0
            denom = max(1, self.n - 1)
            others_frac = coop_count / float(denom)
        else:
            others_frac = 0.0
        if self.PunishCounter > 0:
            if others_frac >= self.T:
                self.PunishCounter = 0
                action = Action.C
            else:
                self.PunishCounter = max(0, self.PunishCounter - 1)
                action = Action.D
            self._last_decision_round = current_round
            self._last_action = action
            return action
        if others_frac >= self.T:
            action = Action.C
        else:
            shortfall = float(self.T - others_frac)
            scaled = shortfall * float(max(1, self.n - 1))
            try:
                extra = int(round(scaled))
            except Exception:
                extra = int(math.floor(scaled + 0.5))
            punishment_length = 1 + max(0, extra)
            punishment_length = min(self.Pmax, punishment_length)
            self.PunishCounter = int(punishment_length)
            action = Action.D
        self._last_decision_round = current_round
        self._last_action = action
        return action

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Graduated Punishment (ACG)

    - Cooperates on first move, defects on final move.
    - Uses short memory (M = min(5, r)) to evaluate recent group and individual cooperation.
    - If group cooperation falls below alpha (0.5) it defects and starts a short punishment (P = min(3, r)).
    - Punishment consists of P consecutive defect rounds (current round counts), followed by one probation cooperation round.
    - If near-unanimous cooperation (>= 0.9) is observed during punishment, punishment is canceled early (forgiveness).
    - If probation cooperation is not followed by recovery (recent_group_coop >= alpha), punishment is re-entered.
    - If a majority of other players are strong cooperators (>= 80% in memory), prefer cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.M = min(5, self.r)
        self.P = min(3, self.r)
        self.alpha = 0.5
        self.punishment_counter = 0
        self.in_probation = False
        self.just_did_probation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        last_idx = self.r - 1
        if round_idx == 0 or history is None:
            self.punishment_counter = 0
            self.in_probation = False
            self.just_did_probation = False
            return Action.C
        if round_idx == last_idx:
            self.punishment_counter = 0
            self.in_probation = False
            self.just_did_probation = False
            return Action.D
        past_rounds = round_idx
        window = min(self.M, past_rounds)
        recent_group_coop = 0.0
        if window > 0:
            opp = history.opponent_actions
            opp_window = opp[-window:, :] if opp.shape[0] >= window else opp
            total_opp_coops = float(np.sum(opp_window))
            denom = (self.n - 1) * window
            if denom > 0:
                recent_group_coop = total_opp_coops / float(denom)
            else:
                recent_group_coop = 0.0
        if self.just_did_probation:
            self.just_did_probation = False
            if recent_group_coop >= self.alpha:
                pass
            else:
                self.punishment_counter = self.P
                self.punishment_counter -= 1
                if self.punishment_counter == 0:
                    self.in_probation = True
                return Action.D
        if round_idx == last_idx - 1:
            strong_prev = False
            if past_rounds >= 1:
                my_prev = bool(history.my_actions[-1])
                opp_prev_all = bool(np.all(history.opponent_actions[-1, :])) if history.opponent_actions.shape[0] >= 1 else False
                if my_prev and opp_prev_all:
                    strong_prev = True
            if strong_prev or recent_group_coop >= 1.0 - 1e-12:
                return Action.C
            else:
                return Action.D
        if self.punishment_counter > 0:
            if recent_group_coop >= 0.9:
                self.punishment_counter = 0
                self.in_probation = False
            else:
                self.punishment_counter -= 1
                if self.punishment_counter == 0:
                    self.in_probation = True
                return Action.D
        if self.in_probation:
            self.in_probation = False
            self.just_did_probation = True
            return Action.C
        number_coopters = 0
        if window > 0:
            opp_window = history.opponent_actions[-window:, :] if history.opponent_actions.shape[0] >= window else history.opponent_actions
            sums = np.sum(opp_window, axis=0)
            rates = sums / float(window)
            number_coopters = int(np.sum(rates >= 0.8))
        else:
            number_coopters = 0
        majority_needed = math.ceil((self.n - 1) / 2)
        if number_coopters >= majority_needed:
            return Action.C
        if recent_group_coop >= self.alpha:
            return Action.C
        self.punishment_counter = self.P
        self.punishment_counter -= 1
        if self.punishment_counter == 0:
            self.in_probation = True
        return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    Exploitative Conditional-Cooperator (ECC)

    - Starts by cooperating to attract conditional cooperators.
    - Estimates opponents' responsiveness to my cooperation/defection using a recent window.
    - If others are sufficiently responsive (so future losses from defection exceed one-shot gain), cooperate.
    - If opponents are weakly responsive but many cooperate, opportunistically defect with a controlled, reversible probability.
    - Punishes sudden mass defections briefly, then reassesses.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(3, math.floor(self.r / 5)))
        self.m_min = 3
        self.I = min(3, max(0, self.r - 1))
        self.p_exploit_base = 0.15
        self.p_exploit_max = 0.4
        self.p_exploit_step = 0.05
        self.P = 2
        self.epsilon = 0.05
        self.p_exploit = self.p_exploit_base
        self.punishment_counter = 0
        self.last_B = None
        self.prev_b_i = None
        self.last_round_action = None
        self.last_unresponsive_set = set()
        self.rounds_since_escalation_check = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == self.r - 1:
            self.last_round_action = Action.D
            return Action.D
        if t == 0 or history is None:
            if self.r <= 2:
                action = Action.C if t == 0 else Action.D
            else:
                action = Action.C if t < self.I else Action.C
            self.last_round_action = action
            return action
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        n_opponents = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        W = min(self.W, t)
        start = max(0, t - W)
        window_len = t - start
        n_transitions = max(0, window_len - 1)
        if t < self.I:
            self.last_round_action = Action.C
            return Action.C
        if n_transitions < self.m_min:
            self.last_round_action = Action.C
            return Action.C
        b_list = []
        r_list = []
        uncertain_flags = []
        if window_len > 0 and n_opponents > 0:
            window_opp = opp_actions[start:t, :]
            b_arr = np.mean(window_opp.astype(float), axis=0)
        else:
            b_arr = np.zeros((n_opponents,), dtype=float)
        count_c_after = np.zeros((n_opponents,), dtype=int)
        sum_c_after = np.zeros((n_opponents,), dtype=int)
        count_d_after = np.zeros((n_opponents,), dtype=int)
        sum_d_after = np.zeros((n_opponents,), dtype=int)
        for r0 in range(start, t - 1):
            my_a = bool(my_actions[r0])
            next_idx = r0 + 1
            resp_row = opp_actions[next_idx, :] if n_opponents > 0 else np.zeros((n_opponents,), dtype=bool)
            if my_a:
                sum_c_after += resp_row.astype(int)
                count_c_after += 1
            else:
                sum_d_after += resp_row.astype(int)
                count_d_after += 1
        for j in range(n_opponents):
            b_i = float(b_arr[j]) if n_opponents > 0 else 0.0
            b_list.append(b_i)
            c_cnt = int(count_c_after[j])
            d_cnt = int(count_d_after[j])
            pC = float(sum_c_after[j]) / c_cnt if c_cnt > 0 else 0.0
            pD = float(sum_d_after[j]) / d_cnt if d_cnt > 0 else 0.0
            uncertain = c_cnt == 0 or d_cnt == 0
            uncertain_flags.append(uncertain)
            r_i = pC - pD if not uncertain else 0.0
            r_list.append(float(r_i))
        R = float(np.mean(np.array(r_list, dtype=float))) if len(r_list) > 0 else 0.0
        B = float(np.mean(np.array(b_list, dtype=float))) if len(b_list) > 0 else 0.0
        confidence = 0.0
        if n_opponents > 0:
            confidence = float(np.mean([0.0 if u else 1.0 for u in uncertain_flags]))
        denom = self.k * max(1, self.n - 1)
        required_threshold = (self.n - self.k) / denom if denom != 0 else float('inf')
        if self.last_B is not None and self.last_B - B > 0.3:
            self.punishment_counter = self.P
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            self.last_B = B
            self.prev_b_i = b_list.copy()
            self.last_round_action = Action.D
            return Action.D
        if R >= required_threshold:
            self.p_exploit = max(self.p_exploit_base, self.p_exploit - self.p_exploit_step)
            self.last_B = B
            self.prev_b_i = b_list.copy()
            self.last_round_action = Action.C
            return Action.C
        if B < 0.25:
            self.last_B = B
            self.prev_b_i = b_list.copy()
            self.last_round_action = Action.D
            return Action.D
        unresponsive_idxs = []
        for j in range(n_opponents):
            is_cooperator = b_list[j] >= 0.75
            r_i = r_list[j]
            is_unresponsive = uncertain_flags[j] or abs(r_i) <= 0.05
            if is_cooperator and is_unresponsive:
                unresponsive_idxs.append(j)
        frac_exploitable = len(unresponsive_idxs) / max(1, n_opponents) if n_opponents > 0 else 0.0
        group_exploitable = frac_exploitable >= 0.3
        if group_exploitable:
            if self.prev_b_i is not None and len(self.prev_b_i) == len(b_list):
                retaliation_detected = False
                persistence_detected = False
                for j in unresponsive_idxs:
                    prev_b = float(self.prev_b_i[j])
                    curr_b = float(b_list[j])
                    if prev_b - curr_b > 0.05:
                        retaliation_detected = True
                    if prev_b - curr_b <= 0.02:
                        persistence_detected = True
                if retaliation_detected:
                    self.p_exploit = max(self.p_exploit_base, self.p_exploit - 2 * self.p_exploit_step)
                elif persistence_detected:
                    self.p_exploit = min(self.p_exploit_max, self.p_exploit + self.p_exploit_step)
            self.p_exploit = max(self.p_exploit_base, min(self.p_exploit, self.p_exploit_max))
            choose_defect = random.random() < self.p_exploit
            action = Action.D if choose_defect else Action.C
            self.last_B = B
            self.prev_b_i = b_list.copy()
            self.last_unresponsive_set = set(unresponsive_idxs)
            self.last_round_action = action
            return action
        if random.random() < self.epsilon:
            action = Action.D
        else:
            action = Action.C
        self.last_B = B
        self.prev_b_i = b_list.copy()
        self.last_round_action = action
        return action

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Collective-Conditional-Cooperator (CCC)

    Adaptive, forgiving conditional cooperator for the N-player Public Goods Game.
    - Starts by cooperating.
    - Monitors the recent fraction of other players who cooperated (window W).
    - If recent cooperation p >= p*, continues cooperating.
    - If p < p*, defects and begins a short punishment phase of up to P_max rounds.
    - During punishment, if the fraction of cooperators in the last two rounds reaches p* for two consecutive rounds,
      remaining punishment is cancelled (forgiveness) and cooperation resumes thereafter.
    - In the final E rounds, only cooperate if the immediately preceding round was unanimous cooperation and not punishing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.n = n
        self.r = r
        self.k = k
        self.W = max(1, math.floor(r / 10))
        denom = float(n) if n > 0 else 1.0
        self.p_star = 0.5 + 0.25 * (1.0 - k / denom)
        self.P_max = min(3, r)
        self.E = min(2, r)
        self.punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        available_rounds = int(history.my_actions.shape[0])
        if available_rounds == 0:
            return Action.C
        opp_actions = history.opponent_actions
        if opp_actions.size == 0 or opp_actions.shape[1] == 0:
            per_round_fracs = np.zeros(available_rounds, dtype=float)
        else:
            per_round_fracs = np.mean(opp_actions.astype(float), axis=1)
        m = min(self.W, available_rounds)
        if m > 0:
            recent_fracs = per_round_fracs[-m:]
            p = float(np.mean(recent_fracs))
        else:
            p = 0.0

        def early_forgiveness_trigger() -> bool:
            if available_rounds < 2:
                return False
            return float(per_round_fracs[-1]) >= self.p_star and float(per_round_fracs[-2]) >= self.p_star
        in_endgame = state.round_number >= self.r - self.E
        if self.punishment_remaining > 0:
            if early_forgiveness_trigger():
                self.punishment_remaining = 0
            else:
                self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return Action.D
        if in_endgame:
            last_my_action = bool(history.my_actions[-1])
            opponents_last = True
            if opp_actions.size == 0 or opp_actions.shape[1] == 0:
                opponents_last = True
            else:
                opponents_last = bool(np.all(opp_actions[-1, :]))
            if last_my_action and opponents_last and (self.punishment_remaining == 0):
                return Action.C
            return Action.D
        if p >= self.p_star:
            return Action.C
        planned_remaining = max(0, self.P_max - 1)
        self.punishment_remaining = planned_remaining
        return Action.D

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    Exploitative Conditional-Cooperator (ECC)

    - Seeds cooperation on the first round (unless single-round game).
    - Estimates opponents' responsiveness to this player's cooperation using a sliding window.
    - Cooperates only when the estimated persistent benefit (over remaining rounds) outweighs
      the immediate private cost of contributing.
    - Triggers short, calibrated punishments against apparent exploitation, then runs a single
      forgiveness test. If forgiveness (group recovery) occurs, resume normal play; otherwise
      continue calibrated defect cycles until recovery.
    - Occasionally probes (with small probability) by flipping the default decision to gather
      causal data and to harvest nave cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(8, max(0, self.r - 1))
        self.tau_coop = 0.8
        self.tau_defect = 0.2
        self.delta_resp_min = 0.12
        self.probe_rate = 0.04
        self.punish_len = 2
        self.endgame_len = min(2, self.r)
        self.probe_rounds = set()
        self.long_defect_mode = False
        self.punish_end_exclusive = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def was_probe(round_idx: int) -> bool:
            return round_idx in self.probe_rounds
        if t == 0:
            if self.r <= 1:
                return Action.D
            return Action.C
        if history is None:
            return Action.D
        if t >= max(1, self.n - self.endgame_len):
            return Action.D
        opp_actions = np.array(history.opponent_actions, dtype=np.bool_)
        my_actions = np.array(history.my_actions, dtype=np.bool_)
        n_opponents = self.n - 1
        succ_start = max(1, t - self.W)
        succ_end = t - 1
        succ_indices = list(range(succ_start, succ_end + 1)) if succ_end >= succ_start else []
        coop_start = max(0, t - self.W)
        coop_end = t - 1
        coop_indices = list(range(coop_start, coop_end + 1)) if coop_end >= coop_start else []
        coop_rate_j = np.zeros(n_opponents, dtype=float)
        pr_after_myC = np.zeros(n_opponents, dtype=float)
        pr_after_myD = np.zeros(n_opponents, dtype=float)
        resp_j = np.zeros(n_opponents, dtype=float)
        if coop_indices:
            segment = opp_actions[coop_indices, :]
            coop_rate_j = np.mean(segment.astype(float), axis=0)
        else:
            coop_rate_j = np.zeros(n_opponents, dtype=float)
        count_myC = np.zeros(n_opponents, dtype=int)
        count_myD = np.zeros(n_opponents, dtype=int)
        coop_after_myC = np.zeros(n_opponents, dtype=int)
        coop_after_myD = np.zeros(n_opponents, dtype=int)
        for s in succ_indices:
            p = s - 1
            my_p_cooperated = bool(my_actions[p])
            opp_s = opp_actions[s, :]
            if my_p_cooperated:
                count_myC += 1
                coop_after_myC += opp_s.astype(int)
            else:
                count_myD += 1
                coop_after_myD += opp_s.astype(int)
        for j in range(n_opponents):
            if count_myC[j] > 0:
                pr_after_myC[j] = coop_after_myC[j] / float(count_myC[j])
            else:
                pr_after_myC[j] = 0.0
            if count_myD[j] > 0:
                pr_after_myD[j] = coop_after_myD[j] / float(count_myD[j])
            else:
                pr_after_myD[j] = 0.0
            resp_j[j] = pr_after_myC[j] - pr_after_myD[j]
        avg_resp = float(np.mean(np.maximum(resp_j, 0.0))) if n_opponents > 0 else 0.0
        if t - 1 >= 0:
            G = float(np.mean(opp_actions[t - 1, :].astype(float)))
        else:
            G = 0.0
        R_rem = max(0, self.r - (t + 1))
        required_multiplier = 1.0
        if self.r <= 3 and t >= 1:
            required_multiplier = 2.0
        k_over_n = self.k / float(self.n) if self.n > 0 else 0.0
        L = 1.0 - k_over_n
        if R_rem <= 0 or k_over_n <= 0.0 or self.n - 1 <= 0:
            required_avg_resp = float('inf')
        else:
            denom = R_rem * k_over_n * float(self.n - 1)
            if denom <= 0:
                required_avg_resp = float('inf')
            else:
                required_avg_resp = L / denom
        required_avg_resp *= required_multiplier
        if self.long_defect_mode:
            if self.punish_end_exclusive is not None and t < self.punish_end_exclusive:
                default_decision = Action.D
            elif self.punish_end_exclusive is not None and t == self.punish_end_exclusive:
                default_decision = Action.C
            else:
                prev_idx = t - 1
                if prev_idx >= 0:
                    G_prev = float(np.mean(opp_actions[prev_idx, :].astype(float)))
                else:
                    G_prev = 0.0
                if G_prev >= self.tau_coop:
                    self.long_defect_mode = False
                    self.punish_end_exclusive = None
                    default_decision = None
                else:
                    self.punish_end_exclusive = t + self.punish_len
                    default_decision = Action.D
            if default_decision is not None:
                base_decision = default_decision
            else:
                base_decision = None
        else:
            base_decision = None
        if base_decision is None:
            threshold = max(required_avg_resp, self.delta_resp_min)
            if avg_resp >= threshold and G >= 0.4:
                base_decision = Action.C
            else:
                base_decision = Action.D
        do_probe = random.random() < self.probe_rate
        chosen_decision = base_decision
        if do_probe:
            chosen_decision = Action.C if base_decision == Action.D else Action.D
            self.probe_rounds.add(t)
        prev_idx = t - 1
        exploited_now = False
        if prev_idx >= 0:
            my_prev_coop = bool(my_actions[prev_idx])
            G_prev = float(np.mean(opp_actions[prev_idx, :].astype(float)))
            if my_prev_coop and (not was_probe(prev_idx)) and (G_prev < self.tau_defect):
                exploited_now = True
        if exploited_now and (not self.long_defect_mode):
            self.long_defect_mode = True
            self.punish_end_exclusive = t + self.punish_len
            chosen_decision = Action.D
        return chosen_decision

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    Collective Conditional Cooperator with Limited Punishment and Forgiveness (CCC-LPF).

    - Uses a recent-history window (L) to estimate each player's recent cooperation rate.
    - Targets persistent defectors with short punishments (P rounds), but forgives quickly when they cooperate.
    - Prefers cooperation when others reciprocate (R_others >= T_good).
    - Uses a tolerate band (T_tol) to avoid flipping too rapidly.
    - Special endgame behavior on the last and second-to-last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.L = min(10, r)
        self.T_good = 0.5 + 0.25 * (k - 1.0) / max(1, n - 1)
        self.T_tol = 0.4
        self.T_bad = 0.2
        self.P_base = min(3, max(1, math.floor(r / 20)))
        self.punished_counters: dict[int, int] = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = state.round_number
        n = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        if round_idx == 0 or history is None:
            return Action.C
        num_prev = round_idx
        window = min(self.L, num_prev)
        my_actions_hist = history.my_actions
        opp_actions_hist = history.opponent_actions
        if window > 0:
            my_recent = my_actions_hist[-window:]
            opp_recent = opp_actions_hist[-window:, :] if opp_actions_hist.size else np.zeros((window, 0), dtype=bool)
        else:
            my_recent = my_actions_hist
            opp_recent = opp_actions_hist
        try:
            actions_recent = np.concatenate([np.expand_dims(my_recent, axis=1), opp_recent], axis=1)
        except Exception:
            cols = [np.expand_dims(my_recent, axis=1)]
            if opp_recent.size:
                for j in range(opp_recent.shape[1]):
                    cols.append(np.expand_dims(opp_recent[:, j], axis=1))
            actions_recent = np.concatenate(cols, axis=1) if cols else np.zeros((window, n), dtype=bool)
        C_counts = np.sum(actions_recent, axis=0).astype(float)
        r_j = C_counts / float(window)
        if n - 1 > 0:
            R_others = float(np.mean(r_j[1:]))
        else:
            R_others = 0.0
        last_round_actions = actions_recent[-1, :]
        last_two_exist = num_prev >= 2
        if last_two_exist:
            my_last2 = my_actions_hist[-2:]
            opp_last2 = opp_actions_hist[-2:, :] if opp_actions_hist.size else np.zeros((2, 0), dtype=bool)
            try:
                last_two = np.concatenate([np.expand_dims(my_last2, axis=1), opp_last2], axis=1)
            except Exception:
                cols2 = [np.expand_dims(my_last2, axis=1)]
                if opp_last2.size:
                    for j in range(opp_last2.shape[1]):
                        cols2.append(np.expand_dims(opp_last2[:, j], axis=1))
                last_two = np.concatenate(cols2, axis=1) if cols2 else np.zeros((2, n), dtype=bool)
        else:
            last_two = None
        punished_keys = list(self.punished_counters.keys())
        for pid in punished_keys:
            if pid < 0 or pid >= n:
                self.punished_counters.pop(pid, None)
                continue
            if bool(last_round_actions[pid]):
                self.punished_counters.pop(pid, None)
        remaining_rounds = max(0, total_rounds - round_idx)
        for pid in range(1, n):
            if pid in self.punished_counters:
                continue
            if r_j[pid] <= self.T_bad:
                if last_two_exist:
                    if not bool(last_two[-1, pid]) and (not bool(last_two[-2, pid])):
                        if remaining_rounds > 0:
                            self.punished_counters[pid] = min(self.P_base, remaining_rounds)
                else:
                    pass
        is_last_round = round_idx == total_rounds - 1
        is_second_last = round_idx == total_rounds - 2
        if is_last_round:
            if len(self.punished_counters) == 0 and R_others >= 0.5:
                action = Action.C
            else:
                action = Action.D
            self._decrement_punishments_after_round()
            return action
        if is_second_last:
            if len(self.punished_counters) == 0 and R_others >= self.T_good:
                action = Action.C
                self._decrement_punishments_after_round()
                return action
        if len(self.punished_counters) > 0:
            punished_list = list(self.punished_counters.keys())
            if len(punished_list) == 0:
                pass
            else:
                all_changed = True
                for pid in punished_list:
                    if not bool(last_round_actions[pid]):
                        all_changed = False
                        break
                if all_changed:
                    for pid in punished_list:
                        self.punished_counters.pop(pid, None)
                    action = Action.C
                    self._decrement_punishments_after_round()
                    return action
                else:
                    action = Action.D
                    self._decrement_punishments_after_round()
                    return action
        if R_others >= self.T_good:
            action = Action.C
            self._decrement_punishments_after_round()
            return action
        if R_others >= self.T_tol:
            prev_self_coop = bool(my_recent[-1])
            if prev_self_coop:
                action = Action.C
            else:
                action = Action.D
            self._decrement_punishments_after_round()
            return action
        action = Action.D
        self._decrement_punishments_after_round()
        return action

    def _decrement_punishments_after_round(self) -> None:
        """
        Decrement punishment counters by 1 (to be called at end of a round decision so that
        counters reflect remaining rounds for the next invocation). Remove entries that reach zero.
        """
        if not self.punished_counters:
            return
        keys = list(self.punished_counters.keys())
        for pid in keys:
            new_val = self.punished_counters.get(pid, 0) - 1
            if new_val <= 0:
                self.punished_counters.pop(pid, None)
            else:
                self.punished_counters[pid] = new_val

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Collective Conditional Cooperation with Proportional Forgiving Punishment.

    - Starts by cooperating (unless the game has only one round).
    - Tracks recent group contribution rate G over a window W = min(5, r).
    - If G >= tau (0.8) cooperates.
    - If G < tau begins a proportional punishment phase: defect for P rounds where
      P = min(Pmax, P0 + S), S = max(1, round((tau - G) * n)).
    - Punishment phases are tracked internally (inclusive of the round they start).
    - After a punishment phase ends, reassess: forgive if G >= tau, otherwise start
      another (slightly longer, capped) punishment P' = min(Pmax, P + 1).
    - Always defects in the final round. If total rounds r == 1 always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.tau = 0.8
        self.W = min(5, self.r)
        self.P0 = 2
        self.Pmax = min(max(2, math.floor(self.r / 5)), 6)
        self.in_punishment = False
        self.punish_until = -1
        self.last_punishment_length = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        tau = self.tau
        W = self.W
        if r == 1:
            return Action.D
        if t == r - 1:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D

        def compute_G() -> float:
            rounds_available = int(min(t, history.my_actions.shape[0]))
            window_size = min(W, rounds_available)
            if window_size <= 0:
                return 0.0
            my_slice = history.my_actions[-window_size:]
            opp_slice = history.opponent_actions[-window_size:, :]
            total_contribs = float(np.sum(my_slice)) + float(np.sum(opp_slice))
            denom = float(n * window_size)
            if denom <= 0.0:
                return 0.0
            return total_contribs / denom
        if self.in_punishment and t <= self.punish_until:
            return Action.D
        if self.in_punishment and t > self.punish_until:
            self.in_punishment = False
            G = compute_G()
            if G >= tau:
                return Action.C
            else:
                new_P = min(self.Pmax, max(1, self.last_punishment_length + 1))
                self.last_punishment_length = new_P
                self.in_punishment = True
                self.punish_until = t + new_P - 1
                return Action.D
        G = compute_G()
        if G >= tau:
            return Action.C
        raw_S = (tau - G) * n
        S = int(round(raw_S))
        if S < 1:
            S = 1
        P = min(self.Pmax, self.P0 + S)
        self.in_punishment = True
        self.last_punishment_length = P
        self.punish_until = t + P - 1
        return Action.D

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    Exploits reliably cooperative opponents by contributing less than the observed group rate,
    while probing early, reacting to sustained punishment by switching to permanent defection,
    and defecting in the final rounds. Deterministic quotas over a sliding window W implement
    target contribution frequencies without randomness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.P = min(6, max(1, math.floor(self.r / 8)))
        self.W = min(10, self.r)
        self.theta_high = 0.6
        self.theta_low = 0.2
        self.delta = 0.15
        self.Delta = 0.25
        self.D = min(5, max(1, math.floor(self.r / 10)))
        self.T_end = min(3, self.r)
        self.permanent_defect = False
        self.exploit_monitor_active = False
        self.exploit_baseline = None
        self.punishment_counter = 0
        self.exploit_started = False
        self.last_mode = None
        self.epsilon_exploit = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0:
            if self.r == 1:
                self.last_mode = 'endgame_defect'
                return Action.D
            self.last_mode = 'probe'
            return Action.C
        if history is None:
            return Action.D
        if t >= self.r - self.T_end:
            self.last_mode = 'endgame_defect'
            return Action.D
        if self.permanent_defect:
            self.last_mode = 'permanent_defect'
            return Action.D
        if t < self.P:
            self.last_mode = 'probe'
            return Action.C
        rounds_observed = t
        window = min(self.W, rounds_observed)
        opp_actions_window = history.opponent_actions[-window:, :]
        my_actions_window = history.my_actions[-window:]
        if window <= 0:
            self.last_mode = 'no_history_defect'
            return Action.D
        try:
            q_j = np.mean(opp_actions_window.astype(float), axis=0)
        except Exception:
            q_j = np.array([float(np.mean(opp_actions_window[:, j])) for j in range(opp_actions_window.shape[1])])
        if q_j.size > 0:
            q_others = float(np.mean(q_j))
        else:
            q_others = 0.0
        try:
            opp_sum_per_round = np.sum(opp_actions_window.astype(float), axis=1)
        except Exception:
            opp_sum_per_round = np.array([float(np.sum(opp_actions_window[i, :])) for i in range(opp_actions_window.shape[0])])
        my_sum_per_round = np.sum(my_actions_window.astype(float))
        mean_opp_per_round = float(np.mean(opp_sum_per_round))
        mean_my_per_round = float(np.mean(my_actions_window.astype(float)))
        group_avg_contribution = (mean_opp_per_round + mean_my_per_round) / float(self.n)
        near_uncond = False
        min_rounds_for_uncond = max(self.P, self.W)
        if rounds_observed >= min_rounds_for_uncond:
            recent_for_uncond = history.opponent_actions[-min_rounds_for_uncond:, :]
            recent_means = np.mean(recent_for_uncond.astype(float), axis=0)
            if np.any(recent_means >= 0.9):
                near_uncond = True
        majority_threshold = math.ceil((self.n - 1) / 2)
        check_window_safety = min(self.D, rounds_observed)
        if check_window_safety > 0:
            recent_opp = history.opponent_actions[-check_window_safety:, :]
            recent_opp_means = np.mean(recent_opp.astype(float), axis=0)
            num_low = int(np.sum(recent_opp_means <= self.theta_low))
            if self.exploit_started and num_low >= majority_threshold:
                self.permanent_defect = True
                self.last_mode = 'safety_trigger_defect'
                return Action.D
        current_mode = 'defect'
        p_target = 0.0
        if q_others >= self.theta_high:
            current_mode = 'exploit'
            p_target = max(0.02, q_others - self.delta)
            if near_uncond:
                p_target = max(0.02, p_target - self.epsilon_exploit)
        elif q_others > self.theta_low:
            current_mode = 'conditional'
            p_target = max(0.05, 0.7 * q_others)
            if near_uncond:
                p_target = max(0.02, p_target - self.epsilon_exploit)
        else:
            current_mode = 'defect'
            p_target = 0.0
        if current_mode == 'exploit':
            if not self.exploit_started:
                self.exploit_started = True
            if not self.exploit_monitor_active:
                self.exploit_baseline = group_avg_contribution
                self.exploit_monitor_active = True
                self.punishment_counter = 0
        else:
            self.exploit_monitor_active = False
            self.exploit_baseline = None
            self.punishment_counter = 0
        if self.exploit_monitor_active and self.exploit_baseline is not None:
            drop = self.exploit_baseline - group_avg_contribution
            if drop >= self.Delta:
                self.punishment_counter += 1
            else:
                self.punishment_counter = 0
            if self.punishment_counter >= self.D:
                self.permanent_defect = True
                self.exploit_monitor_active = False
                self.last_mode = 'punishment_detected_defect'
                return Action.D
        if current_mode in ('exploit', 'conditional'):
            current_window = window
            quota = int(math.floor(current_window * p_target))
            my_contribs = int(np.sum(my_actions_window.astype(int)))
            if my_contribs < quota:
                self.last_mode = current_mode
                return Action.C
            else:
                self.last_mode = current_mode
                return Action.D
        self.last_mode = 'defect'
        return Action.D

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    Collective Conditional Cooperator with Proportional Punishment and Forgiveness (C3-PuF).

    - Starts by cooperating.
    - Uses a recent-memory window W to evaluate fraction of other players cooperating.
    - If recent cooperation >= threshold T (depends on k/n) then cooperate.
    - If recent cooperation < T, defect this round and schedule a short proportional punishment
      (P total defect rounds including current), but never schedule punishment that would
      extend into the final round.
    - Punishments are measured as number of future rounds to defect in addition to the current one;
      stored in self.punish_counter.
    - Any unanimous cooperation in past rounds immediately resets punish_counter to 0 (fast forgiveness).
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(8, self.r)
        self.M = max(1, math.floor(self.r / 10))
        if self.k / self.n >= 0.6:
            self.T = 0.5
        else:
            self.T = 0.65
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0:
            self.punish_counter = 0
            return Action.C
        if t == self.r - 1:
            return Action.D
        if history is None:
            return Action.C
        try:
            opp_all_coop = np.all(history.opponent_actions, axis=1)
            my_coop = history.my_actions.astype(np.bool_)
            unanimous_rounds = np.logical_and(opp_all_coop, my_coop)
            if np.any(unanimous_rounds):
                self.punish_counter = 0
        except Exception:
            self.punish_counter = 0
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        window = min(self.W, t)
        if window <= 0:
            coop_rate = 1.0
        else:
            recent_opponents = history.opponent_actions[-window:, :]
            try:
                coop_rate = float(np.mean(recent_opponents))
            except Exception:
                coop_rate = 1.0
        if coop_rate >= self.T:
            return Action.C
        diff_frac = max(0.0, (self.T - coop_rate) / self.T)
        P = 1 + math.floor(diff_frac * self.M)
        remaining_rounds = self.r - t
        cap_P = max(1, remaining_rounds - 1)
        if P > cap_P:
            P = cap_P
        self.punish_counter = max(0, int(P) - 1)
        return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    Cooperates only when observed conditional responses of others to my cooperation
    make the expected future benefits exceed the immediate cost. Probes early to
    learn, retains small exploration noise, exploits unconditional cooperators,
    and switches to safe permanent defection if cooperation provokes persistent,
    costly retaliation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.WINDOW = max(10, self.r)
        self.EPSILON = 0.05
        self.MIN_SAMPLES = 4
        self.DECAY = 1.0
        self.SAFETY_MARGIN = 0.0
        self.perm_defect = False
        self.reconcile_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def flip_action(a: Action) -> Action:
            return Action.D if a is Action.C else Action.C

        def clamp01(x: float) -> float:
            if x != x:
                return 0.0
            if x < 0.0:
                return 0.0
            if x > 1.0:
                return 1.0
            return float(x)
        remaining_rounds = max(0, self.r - state.round_number)
        if state.round_number == 0 or history is None:
            base_action = Action.C
            if random.random() < self.EPSILON:
                base_action = flip_action(base_action)
            return base_action
        if self.perm_defect:
            action = Action.D
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        if remaining_rounds <= 1:
            action = Action.D
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        T = int(my_actions.shape[0])
        other_count = 1
        try:
            other_count = int(opp_actions.shape[1])
        except Exception:
            other_count = max(1, self.n - 1)
        n_minus_1 = max(1, other_count)
        if T <= 1:
            count_C = 0
            count_D = 0
            p_C = 0.0
            p_D = 0.0
            others_next_list_C = []
            others_next_list_D = []
        else:
            start_idx = max(0, T - self.WINDOW - 1)
            end_idx = T - 2
            others_next_list_C = []
            others_next_list_D = []
            for i in range(start_idx, end_idx + 1):
                row = opp_actions[i + 1]
                coop_count = float(np.sum(row))
                frac = coop_count / float(n_minus_1)
                if bool(my_actions[i]):
                    others_next_list_C.append(frac)
                else:
                    others_next_list_D.append(frac)
            count_C = len(others_next_list_C)
            count_D = len(others_next_list_D)
            p_C = float(np.mean(others_next_list_C)) if count_C > 0 else 0.0
            p_D = float(np.mean(others_next_list_D)) if count_D > 0 else 0.0
        retaliation_detected = False
        if count_C >= self.MIN_SAMPLES and count_D >= self.MIN_SAMPLES:
            if p_C - p_D > 0.25:
                recent_def_indices = []
                lookback = min(6, max(0, T - 1))
                for i in range(max(0, T - 1 - lookback), T - 1):
                    if not bool(my_actions[i]):
                        recent_def_indices.append(i)
                recent_others = []
                for i in recent_def_indices:
                    recent_others.append(float(np.sum(opp_actions[i + 1])) / float(n_minus_1))
                if len(recent_others) >= 3:
                    recent_mean = float(np.mean(recent_others))
                    if recent_mean < p_C - 0.1:
                        retaliation_detected = True
                if retaliation_detected:
                    e_next_temp = (p_C - p_D) * float(n_minus_1)
                    Benefit_temp = self.k / float(self.n) * e_next_temp * float(remaining_rounds - 1) * self.DECAY
                    Cost_temp = 1.0 - self.k / float(self.n)
                    if Benefit_temp <= Cost_temp:
                        self.perm_defect = True
                        action = Action.D
                        if random.random() < self.EPSILON:
                            action = flip_action(action)
                        return action
                    elif self.reconcile_rounds == 0:
                        self.reconcile_rounds = 2
        if self.reconcile_rounds > 0:
            self.reconcile_rounds = max(0, self.reconcile_rounds - 1)
            action = Action.C
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        if count_C < self.MIN_SAMPLES or count_D < self.MIN_SAMPLES:
            prob_coop = 0.7
            action = Action.C if random.random() < prob_coop else Action.D
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        if abs(p_C - p_D) < 0.05 and p_C > 0.75:
            action = Action.D
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        e_next = (p_C - p_D) * float(n_minus_1)
        Benefit = self.k / float(self.n) * e_next * float(remaining_rounds - 1) * self.DECAY
        Cost = 1.0 - self.k / float(self.n)
        margin = float(self.SAFETY_MARGIN)
        if Benefit > Cost + margin:
            action = Action.C
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        if Benefit < Cost - margin:
            action = Action.D
            if random.random() < self.EPSILON:
                action = flip_action(action)
            return action
        denom = 2.0 * margin + 1e-09
        p_raw = (Benefit - Cost + margin) / denom
        p = clamp01(p_raw)
        action = Action.C if random.random() < p else Action.D
        if random.random() < self.EPSILON:
            action = flip_action(action)
        return action

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR)

    - Starts cooperating.
    - Tracks short-run reputations S_j for each opponent over a sliding window W.
    - Targets persistent defectors (S_j <= F_threshold) if group cooperation recently dropped,
      and applies short, proportional punishments followed by probation.
    - Outside punishment: conditional collective cooperation based on group_mean with
      graded (probabilistic) response in the middle band and tiny exploratory cooperations.
    - Monitors own recent payoff vs hypothetical always-defect payoff; if exploited, temporarily
      tightens trust (lowers T_high) and shortens probation windows for K rounds.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(10, max(3, math.ceil(self.r / 10)))
        self.T_high_base = 0.7
        self.T_low = 0.4
        self.F_threshold = 0.2
        self.P_base = max(1, math.ceil(self.r / 20))
        self.Q_base = min(3, math.ceil(self.r / 20))
        self.Epsilon_explore = min(0.05, 2.0 / (self.r + 1.0))
        self.P_few = min(2, math.ceil(self.r / 10))
        self.K_protect = min(5, math.ceil(self.r / 10))
        self.targeted_set = set()
        self.in_punishment = False
        self.punishment_end_round = 0
        self.probation_end_round = 0
        self.T_high_current = float(self.T_high_base)
        self.protection_cooldown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t1 = state.round_number + 1
        n = self.n
        r = self.r
        if t1 == 1:
            return Action.C
        if t1 == r:
            return Action.D
        rounds_played = 0
        opp_actions = None
        my_actions = None
        my_payoffs = None
        opp_payoffs = None
        if history is not None:
            my_actions = history.my_actions
            my_payoffs = history.my_payoffs
            opp_actions = history.opponent_actions
            opp_payoffs = history.opponent_payoffs
            rounds_played = my_actions.shape[0]
        T_high_eff = float(self.T_high_current)
        if t1 > r - self.P_few:
            T_high_eff = T_high_eff - 0.1
        if T_high_eff < 0.05:
            T_high_eff = 0.05
        Q_eff = max(1, self.Q_base - (1 if self.protection_cooldown > 0 else 0))
        n_opponents = n - 1
        S = [0.0] * n_opponents
        if rounds_played > 0 and opp_actions is not None:
            window_len = min(self.W, rounds_played)
            start_idx = rounds_played - window_len
            sub = opp_actions[start_idx:start_idx + window_len, :]
            try:
                col_sums = np.sum(sub.astype(np.int32), axis=0)
                for j in range(n_opponents):
                    S[j] = float(col_sums[j]) / float(window_len)
            except Exception:
                for j in range(n_opponents):
                    s = 0
                    for rr in range(start_idx, start_idx + window_len):
                        if bool(opp_actions[rr, j]):
                            s += 1
                    S[j] = s / window_len
        else:
            S = [0.0] * n_opponents
        recent_group_mean = 0.0
        prior_group_mean = None
        if rounds_played > 0 and opp_actions is not None:
            recent_len = min(self.W, rounds_played)
            recent_start = rounds_played - recent_len
            total_coop_counts = []
            for rr in range(recent_start, rounds_played):
                opp_coops = int(np.sum(opp_actions[rr, :])) if hasattr(np, 'sum') else int(sum(opp_actions[rr, :]))
                my_coop = 1 if bool(my_actions[rr]) else 0
                total_coop_counts.append(opp_coops + my_coop)
            if len(total_coop_counts) > 0:
                recent_group_mean = float(np.mean([c / float(n) for c in total_coop_counts]))
            else:
                recent_group_mean = 0.0
            prior_len = min(self.W, max(0, rounds_played - recent_len))
            if prior_len > 0:
                prior_start = rounds_played - recent_len - prior_len
                prior_counts = []
                for rr in range(prior_start, prior_start + prior_len):
                    opp_coops = int(np.sum(opp_actions[rr, :])) if hasattr(np, 'sum') else int(sum(opp_actions[rr, :]))
                    my_coop = 1 if bool(my_actions[rr]) else 0
                    prior_counts.append(opp_coops + my_coop)
                prior_group_mean = float(np.mean([c / float(n) for c in prior_counts]))
        group_drop = False
        if prior_group_mean is not None:
            if prior_group_mean - recent_group_mean >= 0.05:
                group_drop = True
        newly_targeted = False
        if group_drop:
            for j in range(n_opponents):
                if S[j] <= self.F_threshold and j not in self.targeted_set:
                    self.targeted_set.add(j)
                    newly_targeted = True
        if newly_targeted:
            self.in_punishment = True
            self.punishment_end_round = t1 + self.P_base * max(1, len(self.targeted_set))
        if self.in_punishment and t1 <= self.punishment_end_round:
            if len(self.targeted_set) == 0:
                self.in_punishment = False
            else:
                if self.protection_cooldown > 0:
                    self.protection_cooldown = max(0, self.protection_cooldown - 1)
                    if self.protection_cooldown == 0:
                        self.T_high_current = float(self.T_high_base)
                return Action.D
        if self.in_punishment and t1 == self.punishment_end_round + 1:
            self.in_punishment = False
            self.probation_end_round = t1 + Q_eff
        if self.probation_end_round >= t1 and len(self.targeted_set) > 0:
            removed = set()
            for j in list(self.targeted_set):
                if S[j] > self.F_threshold:
                    removed.add(j)
            if removed:
                for j in removed:
                    self.targeted_set.discard(j)
            if len(self.targeted_set) == 0:
                self.probation_end_round = 0
            elif t1 <= self.probation_end_round:
                if self.protection_cooldown > 0:
                    self.protection_cooldown = max(0, self.protection_cooldown - 1)
                    if self.protection_cooldown == 0:
                        self.T_high_current = float(self.T_high_base)
                return Action.C
        if self.probation_end_round > 0 and t1 == self.probation_end_round + 1 and (len(self.targeted_set) > 0):
            self.in_punishment = True
            self.punishment_end_round = t1 + self.P_base * max(1, len(self.targeted_set))
            if self.protection_cooldown > 0:
                self.protection_cooldown = max(0, self.protection_cooldown - 1)
                if self.protection_cooldown == 0:
                    self.T_high_current = float(self.T_high_base)
            return Action.D
        group_mean = 0.0
        if n_opponents > 0:
            group_mean = float(sum(S) / float(n_opponents))
        prefer_C_due_recent_majority = False
        if rounds_played > 0 and opp_actions is not None:
            last_opp_coops = int(np.sum(opp_actions[-1, :])) if hasattr(np, 'sum') else int(sum(opp_actions[-1, :]))
            last_my_coop = 1 if bool(my_actions[-1]) else 0
            last_total = last_opp_coops + last_my_coop
            if last_total >= math.ceil(n / 2):
                prefer_C_due_recent_majority = True
        decision_C = False
        if prefer_C_due_recent_majority:
            decision_C = True
        elif group_mean >= T_high_eff:
            decision_C = True
        elif group_mean <= self.T_low:
            decision_C = False
        else:
            denom = T_high_eff - self.T_low
            if denom <= 0:
                decision_C = group_mean >= self.T_low + 0.5 * (T_high_eff - self.T_low)
            else:
                p = (group_mean - self.T_low) / denom
                decision_C = random.random() < p
        if not decision_C and random.random() < self.Epsilon_explore:
            decision_C = True
        if rounds_played > 0 and my_payoffs is not None and (opp_actions is not None):
            L = min(self.W, rounds_played)
            start_idx = rounds_played - L
            realized_vals = []
            hypothetical_vals = []
            for rr in range(start_idx, rounds_played):
                realized_vals.append(float(my_payoffs[rr]))
                others_coops = int(np.sum(opp_actions[rr, :])) if hasattr(np, 'sum') else int(sum(opp_actions[rr, :]))
                hypo = 1.0 + self.k / float(n) * float(others_coops)
                hypothetical_vals.append(hypo)
            if len(realized_vals) > 0 and len(hypothetical_vals) > 0:
                realized_avg = float(np.mean(realized_vals))
                hypothetical_avg = float(np.mean(hypothetical_vals))
                if hypothetical_avg > 0 and realized_avg < 0.95 * hypothetical_avg:
                    self.protection_cooldown = self.K_protect
                    self.T_high_current = max(0.0, float(self.T_high_base) - 0.1)
        if self.protection_cooldown > 0:
            self.protection_cooldown = max(0, self.protection_cooldown - 1)
            if self.protection_cooldown == 0:
                self.T_high_current = float(self.T_high_base)
        if decision_C:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    Exploitative Adaptive Public-Goods Strategy.

    - Probes early to find cooperators.
    - Uses exponential smoothing per-opponent scores to classify cooperators/defectors/uncertain.
    - Exploits stable cooperators by mostly defecting, with occasional soft cooperation to sustain them.
    - Detects retaliatory drops and runs short repair phases of cooperation.
    - Refuses cooperation in final T_end rounds and in very short games.
    - Falls back to permanent defection after repeated punish-repair cycles, except for rare probes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.n_opponents = max(0, self.n - 1)
        self.alpha = 0.7
        self.theta_high = 0.8
        self.theta_low = 0.2
        self.p_soft = 0.1
        self.p_probe = 0.2
        self.L_repair = 2
        self.delta_drop = 0.2
        self.T_end = min(3, max(1, math.floor(self.r / 10)))
        self.repair_remaining = 0
        self.punish_repair_cycles = 0
        self.permanent_defect = False
        self.caution_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if self.r <= 3:
            return Action.D
        if t >= self.r - self.T_end:
            return Action.D
        if t == 0:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        if history is None:
            return Action.D
        num_past_rounds = history.opponent_actions.shape[0]
        S = np.full((self.n_opponents,), 0.5, dtype=float)
        if num_past_rounds > 0:
            for r_index in range(num_past_rounds):
                c = history.opponent_actions[r_index, :].astype(float)
                S = self.alpha * S + (1.0 - self.alpha) * c
        S_before_last = np.full((self.n_opponents,), 0.5, dtype=float)
        if num_past_rounds >= 2:
            for r_index in range(num_past_rounds - 1):
                c = history.opponent_actions[r_index, :].astype(float)
                S_before_last = self.alpha * S_before_last + (1.0 - self.alpha) * c
        avg_S = float(np.mean(S)) if self.n_opponents > 0 else 0.0
        n_coop = int(np.sum(S >= self.theta_high))
        n_defect = int(np.sum(S <= self.theta_low))
        majority_needed = math.ceil(self.n_opponents / 2) if self.n_opponents > 0 else 0
        last_contribs = 0
        prev_contribs = None
        if num_past_rounds >= 1:
            last_contribs = int(np.sum(history.opponent_actions[-1, :]))
        if num_past_rounds >= 2:
            prev_contribs = int(np.sum(history.opponent_actions[-2, :]))
        detected_retaliation = False
        if num_past_rounds >= 2:
            my_last_action = bool(history.my_actions[-1])
            if not my_last_action and prev_contribs is not None:
                prop_prev = prev_contribs / max(1, self.n_opponents)
                prop_last = last_contribs / max(1, self.n_opponents)
                if prop_prev - prop_last > self.delta_drop:
                    n_coop_before = int(np.sum(S_before_last >= self.theta_high))
                    if n_coop_before >= majority_needed:
                        detected_retaliation = True
        if detected_retaliation:
            if self.repair_remaining <= 0:
                self.punish_repair_cycles += 1
            self.repair_remaining = max(self.repair_remaining, self.L_repair)
            if self.punish_repair_cycles > 3:
                self.permanent_defect = True
        if self.repair_remaining > 0:
            n_coop_now = int(np.sum(S >= self.theta_high))
            if n_coop_now >= majority_needed:
                self.repair_remaining = 0
            else:
                self.repair_remaining = max(0, self.repair_remaining - 1)
                return Action.C
        if self.permanent_defect:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        if self.caution_mode:
            if np.any(S >= self.theta_high):
                self.caution_mode = False
            else:
                if random.random() < self.p_probe / 2:
                    return Action.C
                return Action.D
        if n_coop >= majority_needed and majority_needed > 0:
            if random.random() < self.p_soft:
                return Action.C
            return Action.D
        if n_coop == 0:
            if random.random() < self.p_probe / 2:
                return Action.C
            return Action.D
        coop_indices = S >= self.theta_high
        if np.any(coop_indices):
            subset_avg = float(np.mean(S[coop_indices])) if np.sum(coop_indices) > 0 else 0.0
            if subset_avg >= self.theta_high:
                if random.random() < self.p_soft:
                    return Action.C
                return Action.D
            else:
                if num_past_rounds >= 1 and bool(history.my_actions[-1]):
                    if last_contribs == 0:
                        self.caution_mode = True
                        return Action.D
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    Collective Reciprocity with Graduated Punishment and Quick Forgiveness.

    - Signals cooperation in the first round (unless single-round game).
    - Uses a short memory m = min(5, r) to compute recent cooperation rate R among opponents.
    - Cooperates if the entire group cooperated last round, or if recent cooperation R >= T (0.6).
    - If R < T, punishes by defecting for L = min(Pmax, 1 + D_last) rounds (current round counts as first).
      While punishing, if opponents' recent cooperation R >= T is observed, punishment is stopped
      and the agent returns to cooperating immediately (quick forgiveness).
    - Endgame cautiousness: always defect in the final round; in the last G rounds (including final)
      only cooperate if every other player cooperated in the immediately previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(5, self.r)
        self.T = 0.6
        self.Pmax = min(4, self.r)
        self.G = min(2, max(0, math.floor(self.r / 10)))
        self.punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if t_idx == self.r - 1:
            self.punishment_remaining = 0
            return Action.D
        if t_idx == 0 or history is None:
            return Action.C
        if self.G > 0 and t_idx >= self.r - self.G:
            last_opponents = history.opponent_actions[-1, :]
            if last_opponents.size == 0:
                return Action.D
            if bool(np.all(last_opponents)):
                return Action.C
            self.punishment_remaining = 0
            return Action.D

        def compute_recent_R():
            start = max(0, t_idx - self.m)
            end = t_idx
            if end <= start:
                return 0.0
            opp_slice = history.opponent_actions[start:end, :]
            denom = max(1, opp_slice.shape[1])
            fractions = np.sum(opp_slice, axis=1) / float(denom)
            if fractions.size == 0:
                return 0.0
            return float(np.mean(fractions))
        R = compute_recent_R()
        last_round_opponents = history.opponent_actions[-1, :]
        D_last = int(last_round_opponents.size - int(np.sum(last_round_opponents)))
        if self.punishment_remaining > 0:
            if R >= self.T:
                self.punishment_remaining = 0
                return Action.C
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return Action.D
        my_last = bool(history.my_actions[-1])
        if my_last and bool(np.all(last_round_opponents)):
            return Action.C
        if R >= self.T:
            return Action.C
        L = min(self.Pmax, 1 + D_last)
        self.punishment_remaining = max(0, L - 1)
        return Action.D

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    Opportunistic Conditional-Exploiter (OCE)

    Probes early to estimate opponents' unconditional and conditional cooperation rates,
    then exploits when cooperating does not increase immediate expected payoff.
    Maintains short punishments, quick forgiveness, occasional recon probes, and always
    defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        W_choice = max(3, min(10, math.floor(self.r / 5)))
        self.W = W_choice
        self.D = max(2, math.floor(self.W / 2))
        self.R_t = 0.25
        self.U_t = 0.85
        self.P_t = 0.25
        self.L_punish = 1
        self.q = 0.05
        self.punish_until = -1
        self.last_maintenance_round = -9999
        self.maintenance_K = 7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds_played = 0 if history is None else history.my_actions.shape[0]
        current_round = state.round_number
        rounds_left = self.r - current_round
        if current_round >= self.r - 1:
            return Action.D
        probing_rounds = min(2, self.W)
        if current_round == 0:
            return Action.C
        if current_round < probing_rounds:
            return Action.C
        if current_round <= self.punish_until:
            return Action.D
        if history is None:
            return Action.D
        total_rounds_played = history.my_actions.shape[0]
        window_size = min(self.W, total_rounds_played)
        window_start = max(0, total_rounds_played - window_size)
        m = self.n - 1
        if m <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        recent_opp = opp_actions[window_start:total_rounds_played, :]
        if recent_opp.size == 0:
            coop_rate = np.zeros(m, dtype=float)
        else:
            coop_rate = np.array(np.mean(recent_opp.astype(float), axis=0), dtype=float)
        p_if_my_coop = np.zeros(m, dtype=float)
        p_if_my_defect = np.zeros(m, dtype=float)
        trials_if_my_coop = np.zeros(m, dtype=int)
        trials_if_my_defect = np.zeros(m, dtype=int)
        for r_idx in range(window_start, total_rounds_played):
            if r_idx == 0:
                continue
            prev_my_coop = bool(history.my_actions[r_idx - 1])
            row = opp_actions[r_idx, :]
            if prev_my_coop:
                trials_if_my_coop += 1
                p_if_my_coop += row.astype(float)
            else:
                trials_if_my_defect += 1
                p_if_my_defect += row.astype(float)
        for j in range(m):
            if trials_if_my_coop[j] > 0:
                p_if_my_coop[j] = p_if_my_coop[j] / trials_if_my_coop[j]
            else:
                p_if_my_coop[j] = 0.0
            if trials_if_my_defect[j] > 0:
                p_if_my_defect[j] = p_if_my_defect[j] / trials_if_my_defect[j]
            else:
                p_if_my_defect[j] = 0.0
        responsiveness = p_if_my_coop - p_if_my_defect
        is_unconditional = coop_rate >= self.U_t
        is_defector = coop_rate <= 1.0 - self.U_t
        is_conditional = (responsiveness >= self.R_t) & (coop_rate >= 0.2)
        is_punisher = p_if_my_defect <= p_if_my_coop - self.P_t
        last_round_idx = total_rounds_played - 1
        if last_round_idx >= 0:
            last_round_row = opp_actions[last_round_idx, :]
            cooperators_last = int(np.sum(last_round_row))
            if cooperators_last >= math.floor(m / 2) + 1:
                recent_coop_threshold = 0.5
                exploited_indices = []
                for j in range(m):
                    if not bool(last_round_row[j]) and coop_rate[j] >= recent_coop_threshold:
                        exploited_indices.append(j)
                if len(exploited_indices) > 0 and current_round < self.r - 1:
                    self.punish_until = current_round + self.L_punish - 1
                    return Action.D
        prob_j_C = np.zeros(m, dtype=float)
        prob_j_D = np.zeros(m, dtype=float)
        for j in range(m):
            if trials_if_my_coop[j] >= self.D:
                prob_j_C[j] = p_if_my_coop[j]
            else:
                prob_j_C[j] = coop_rate[j]
            if trials_if_my_defect[j] >= self.D:
                prob_j_D[j] = p_if_my_defect[j]
            else:
                prob_j_D[j] = coop_rate[j]
        expected_others_if_C = float(np.sum(prob_j_C))
        expected_others_if_D = float(np.sum(prob_j_D))
        if expected_others_if_C <= 1e-08 and expected_others_if_D <= 1e-08:
            return Action.D
        if rounds_left <= 2:
            return Action.D
        payoff_C = self.k / self.n * (1.0 + expected_others_if_C)
        payoff_D = 1.0 + self.k / self.n * expected_others_if_D
        delta = payoff_C - payoff_D
        if delta > 0.0:
            return Action.C
        R_mask = (responsiveness >= self.R_t) | is_punisher
        R_frac = float(np.sum(R_mask)) / float(m)
        threshold_expected = max(1, math.floor(m / 3))
        if R_frac >= 0.25 and expected_others_if_C >= threshold_expected:
            return Action.C
        num_uncond = int(np.sum(is_unconditional))
        if num_uncond > math.floor(m / 2):
            if current_round - self.last_maintenance_round >= self.maintenance_K:
                self.last_maintenance_round = current_round
                return Action.C
            return Action.D
        if random.random() < self.q:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    Seeks to sustain cooperation when others reciprocate, uses short finite punishments
    against persistent defectors, forgives isolated/noisy defections, and defects in the
    final round. Uses only game parameters and observed contribution history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.delta = 0.05
        self.tol = 0.2
        self.punish_remaining = 0
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        k = self.k
        p_star = (n - k) / (k * (n - 1))
        W = min(5, r)
        P = min(3, r)
        current_round = int(state.round_number)
        if current_round == r - 1:
            self.last_action = Action.D
            self.punish_remaining = 0
            return Action.D
        if current_round == 0 or history is None:
            self.last_action = Action.C
            return Action.C
        available_rounds = int(history.opponent_actions.shape[0])
        opponents = n - 1
        rounds_considered = min(W, available_rounds, current_round)
        if rounds_considered <= 0:
            self.last_action = Action.C
            return Action.C
        start_idx = available_rounds - rounds_considered
        end_idx = available_rounds
        window = history.opponent_actions[start_idx:end_idx, :]
        total_other_C = float(np.sum(window))
        denom = float(opponents * rounds_considered)
        if denom <= 0.0:
            p_hat = 1.0
        else:
            p_hat = total_other_C / denom
        last_row = history.opponent_actions[available_rounds - 1, :]
        last_row_coops = float(np.sum(last_row))
        last_round_defect_fraction = 1.0 - last_row_coops / float(opponents) if opponents > 0 else 0.0
        noise_override = False
        if rounds_considered >= 2:
            last_row_sum = float(np.sum(window[-1, :]))
            excl_total_other_C = total_other_C - last_row_sum
            excl_denom = float(opponents * (rounds_considered - 1))
            if excl_denom > 0.0:
                p_hat_excl_last = excl_total_other_C / excl_denom
            else:
                p_hat_excl_last = p_hat
            if last_round_defect_fraction <= self.tol and p_hat_excl_last > p_star - self.delta:
                noise_override = True
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            self.last_action = Action.D
            return action
        if p_hat >= p_star + self.delta:
            self.last_action = Action.C
            return Action.C
        if noise_override:
            self.last_action = Action.C
            return Action.C
        if p_hat <= p_star - self.delta:
            self.punish_remaining = max(0, P - 1)
            self.last_action = Action.D
            return Action.D
        if self.last_action == Action.C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    Selfish adaptive exploiter for the N-player Public Goods Game.

    - Probes early to estimate conditional cooperation.
    - Computes expected personal payoff of cooperating vs defecting using
      observed conditional cooperation rates.
    - Exploits when defecting yields higher expected payoff, with limited
      randomized maintenance cooperation to sustain exploitables.
    - Detects punishers (responsive players) and switches to a short
      conditional-cooperation "truce" to avoid destructive wars.
    - Adds small random noise to avoid determinism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        if self.r < 6:
            self.T_probe = 1
        else:
            self.T_probe = min(max(3, math.ceil(0.05 * self.r)), 6)
        self.T_end = min(3, math.ceil(0.05 * self.r))
        self.W = min(max(5, math.ceil(0.15 * self.r)), self.r)
        self.eps_resp = 0.15
        self.eps_exploit = 0.85
        self.p_probe = 0.2
        self.p_noise = 0.05
        self.tiny_margin = 0.01
        self.cond_mode_remaining = 0
        self.cond_mode_length = min(5, math.ceil(0.1 * self.r)) if self.r > 0 else 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        opponents = n - 1
        if t == 0 or history is None:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        if t > self.r - self.T_end - 1:
            return Action.D
        if t < self.T_probe:
            if random.random() < self.p_probe:
                planned_action = Action.C
            else:
                planned_action = Action.D
            if random.random() < self.p_noise:
                planned_action = Action.C if planned_action == Action.D else Action.D
            return planned_action
        available_rounds = t
        window_size = min(self.W, available_rounds)
        if window_size <= 0:
            return Action.D
        my_actions = history.my_actions[-window_size:]
        opp_actions = history.opponent_actions[-window_size:, :]
        if opponents <= 0:
            return Action.D
        coop_rates = np.zeros(opponents, dtype=float)
        p_if_myC = np.zeros(opponents, dtype=float)
        p_if_myD = np.zeros(opponents, dtype=float)
        responsiveness = np.zeros(opponents, dtype=float)
        exploitable_flags = np.zeros(opponents, dtype=bool)
        punisher_flags = np.zeros(opponents, dtype=bool)
        my_bool = np.array(my_actions, dtype=bool)
        opp_bool = np.array(opp_actions, dtype=bool)
        denom_myC = np.sum(my_bool)
        denom_myD = window_size - denom_myC
        for j in range(opponents):
            opp_j = opp_bool[:, j]
            C_j = float(np.sum(opp_j))
            coop_rates[j] = C_j / window_size if window_size > 0 else 0.0
            if denom_myC > 0:
                numer_myC = float(np.sum(opp_j & my_bool))
                p_myC = numer_myC / denom_myC
            else:
                p_myC = coop_rates[j]
            if denom_myD > 0:
                numer_myD = float(np.sum(opp_j & ~my_bool))
                p_myD = numer_myD / denom_myD
            else:
                p_myD = coop_rates[j]
            p_if_myC[j] = p_myC
            p_if_myD[j] = p_myD
            responsiveness[j] = p_myC - p_myD
            exploitable_flags[j] = coop_rates[j] >= self.eps_exploit and responsiveness[j] <= self.eps_resp
            punisher_flags[j] = responsiveness[j] >= self.eps_resp
        E_others_if_C = float(np.sum(p_if_myC))
        E_others_if_D = float(np.sum(p_if_myD))
        EU_C = self.k / n * (1.0 + E_others_if_C)
        EU_D = 1.0 + self.k / n * E_others_if_D
        num_punishers = int(np.sum(punisher_flags))
        punish_threshold = max(2, math.ceil(n / 4))
        num_exploitables = int(np.sum(exploitable_flags))
        fraction_exploitables = num_exploitables / opponents if opponents > 0 else 0.0
        if num_punishers >= punish_threshold and EU_D <= EU_C + 0.05:
            self.cond_mode_remaining = self.cond_mode_length
        if self.cond_mode_remaining > 0:
            T_thresh = max(1, round(0.5 * (n - 1)))
            if available_rounds >= 1:
                prev_coops = int(np.sum(history.opponent_actions[-1, :]))
            else:
                prev_coops = 0
            if prev_coops >= T_thresh:
                planned = Action.C
            else:
                planned = Action.D
            self.cond_mode_remaining = max(0, self.cond_mode_remaining - 1)
            if random.random() < self.p_noise:
                planned = Action.C if planned == Action.D else Action.D
            return planned
        planned = Action.D
        if EU_D > EU_C + self.tiny_margin:
            advantage = EU_D - EU_C
            if num_exploitables > 0 and advantage < 0.15:
                p_maintain = min(0.25, 2.0 * fraction_exploitables)
                if random.random() < p_maintain:
                    planned = Action.C
                else:
                    planned = Action.D
            else:
                planned = Action.D
        elif EU_C > EU_D + self.tiny_margin:
            planned = Action.C
        else:
            planned = Action.D
        if np.all(coop_rates < self.eps_exploit / 2.0):
            planned = Action.D
        if random.random() < self.p_noise:
            planned = Action.C if planned == Action.D else Action.D
        return planned

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    Collective Conditional Cooperation with Adaptive, Lenient Punishment (CCCAP).

    - Opens with cooperation (unless the game has a single round).
    - Tracks recent group cooperation over a window w and compares to an adaptive threshold tau that depends on n and k.
    - If recent cooperation f >= tau, cooperate; if f < tau, issue a short proportionate punishment (defect for P rounds).
    - Punishment length scales with the shortfall (tau - f) and is constrained by remaining rounds.
    - Rapid forgiveness: if the most recent round was near-unanimous cooperation (n-1 or n cooperators), immediately resume cooperating.
    - While punishing, if the group responds (f >= tau_forgive) stop punishing early and resume cooperation.
    - Endgame caution: always defect in the final round; in the last E rounds (excluding final) require a stricter threshold max(tau, 0.8) to cooperate and do not begin new punishments there.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = max(1, math.floor(self.r / 5))
        self.E = min(2, self.r)
        raw_tau = 0.5 + (self.n - self.k) / (2.0 * max(1, self.n - 1))
        self.tau = min(0.95, max(0.5, raw_tau))
        self.S = 4
        self.tau_forgive = max(0.4, self.tau - 0.1)
        self.P_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining = self.r - t
        if self.r == 1:
            return Action.D
        if remaining == 1:
            self.P_timer = 0
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D

        def total_cooperators_in_round(s: int) -> int:
            opp_coops = int(np.sum(history.opponent_actions[s, :]))
            my_coop = 1 if bool(history.my_actions[s]) else 0
            return opp_coops + my_coop
        denom = min(self.w, t)
        if denom == 0:
            f = 1.0
        else:
            start = t - denom
            total_coops = 0
            for s in range(start, t):
                total_coops += total_cooperators_in_round(s)
            f = total_coops / (self.n * denom)
        last_round_total = total_cooperators_in_round(t - 1)
        if not bool(history.my_actions[-1]) and last_round_total >= self.n - 1:
            self.P_timer = 0
            return Action.C
        if remaining <= self.E:
            effective_tau = max(self.tau, 0.8)
            if f >= effective_tau:
                return Action.C
            else:
                self.P_timer = 0
                return Action.D
        if self.P_timer > 0:
            if f >= self.tau_forgive:
                self.P_timer = 0
                return Action.C
            self.P_timer = max(0, self.P_timer - 1)
            return Action.D
        if f >= self.tau:
            return Action.C
        shortfall = max(0.0, self.tau - f)
        proposed_P = max(1, math.ceil(shortfall * self.S))
        max_allowed = max(1, remaining - 1)
        P = min(max_allowed, proposed_P)
        self.P_timer = max(0, P - 1)
        return Action.D

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (AC3)

    - Starts cooperating.
    - Uses a recent-window coop rate per other player to identify reliable cooperators.
    - Continues cooperating when a recent majority cooperated or many players have been reliable.
    - Proportionally punishes when cooperation collapses (temporary punishments whose length scales
      with the fraction of defectors).
    - After punishment, plays a test cooperation round and awards forgiveness if majority returns.
    - Probes periodically when mutual defection persists to try to re-start cooperation.
    - Has endgame safeguards to avoid being a last-round sucker while preserving cooperation if the
      group is already cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, self.r)
        self.punish_rounds_remaining = 0
        self.next_round_is_test = False
        self.awaiting_test_response = False
        self.T_probe = max(3, max(1, round(self.r / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)

        def act_cooperate():
            return Action.C

        def act_defect():
            return Action.D
        if self.n <= 1:
            return act_cooperate()
        if t == 0 or history is None:
            self.punish_rounds_remaining = 0
            self.next_round_is_test = False
            self.awaiting_test_response = False
            return act_cooperate()
        rounds_played = int(history.my_actions.shape[0])
        window = min(self.W, max(1, rounds_played))
        opp_actions = history.opponent_actions
        num_others = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        if num_others <= 0:
            num_others = max(0, self.n - 1)
        if rounds_played >= 1 and opp_actions.size != 0:
            recent_slice = opp_actions[-window:, :] if window > 0 else opp_actions
            coop_rates = np.mean(recent_slice.astype(float), axis=0)
            if coop_rates.shape[0] != num_others:
                if coop_rates.shape[0] < num_others:
                    pad = np.zeros(num_others - coop_rates.shape[0], dtype=float)
                    coop_rates = np.concatenate([coop_rates, pad])
                else:
                    coop_rates = coop_rates[:num_others]
        else:
            coop_rates = np.zeros(num_others, dtype=float)
        S = int(np.sum(coop_rates >= 0.6))
        if rounds_played >= 1 and opp_actions.size != 0:
            last_row = opp_actions[-1, :]
            OTHER_FRACTION_PREV = float(np.sum(last_row.astype(float)) / max(1, num_others))
        else:
            OTHER_FRACTION_PREV = 1.0
        DEFECT_SEVERITY = 1.0 - OTHER_FRACTION_PREV
        rounds_remaining = max(0, self.r - t)
        k_over_n = self.k / max(1.0, float(self.n))
        main_other_threshold = 0.5
        main_S_threshold = (self.n - 1) / 2.0
        if k_over_n >= 0.9:
            main_other_threshold = 0.4
            main_S_threshold = max(1, math.floor((self.n - 1) * 0.45))
        elif k_over_n <= 0.4:
            main_other_threshold = 0.6
            main_S_threshold = max(1, math.ceil(0.75 * (self.n - 1)))
        if rounds_remaining <= 2:
            if OTHER_FRACTION_PREV >= main_other_threshold:
                return act_cooperate()
            if OTHER_FRACTION_PREV < main_other_threshold and S < math.floor((self.n - 1) / 2.0 + 1e-09):
                return act_defect()
        if self.next_round_is_test:
            self.next_round_is_test = False
            self.awaiting_test_response = True
            return act_cooperate()
        if self.punish_rounds_remaining and self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining = max(0, self.punish_rounds_remaining - 1)
            if self.punish_rounds_remaining == 0:
                self.next_round_is_test = True
            self.awaiting_test_response = False
            return act_defect()
        if self.awaiting_test_response:
            prev_majority_cooperated = OTHER_FRACTION_PREV >= main_other_threshold
            self.awaiting_test_response = False
            if prev_majority_cooperated:
                pass
            else:
                PUNISH_LENGTH = 1 + int(round(3.0 * DEFECT_SEVERITY))
                PUNISH_LENGTH = min(max(1, PUNISH_LENGTH), max(0, rounds_remaining))
                self.punish_rounds_remaining = PUNISH_LENGTH
                self.punish_rounds_remaining = max(0, self.punish_rounds_remaining - 1)
                if self.punish_rounds_remaining == 0:
                    self.next_round_is_test = True
                return act_defect()
        consecutive_mutual_defect_count = 0
        if rounds_played >= 1 and opp_actions.size != 0:
            for idx in range(rounds_played - 1, -1, -1):
                row = opp_actions[idx, :]
                frac = float(np.sum(row.astype(float)) / max(1, num_others))
                if frac < 0.5:
                    consecutive_mutual_defect_count += 1
                else:
                    break
        if consecutive_mutual_defect_count > self.T_probe:
            if self.T_probe > 0 and t % self.T_probe == 0:
                self.awaiting_test_response = True
                return act_cooperate()
        if OTHER_FRACTION_PREV >= main_other_threshold or S >= main_S_threshold:
            return act_cooperate()
        PUNISH_LENGTH = 1 + int(round(3.0 * DEFECT_SEVERITY))
        PUNISH_LENGTH = min(max(1, PUNISH_LENGTH), max(0, rounds_remaining))
        self.punish_rounds_remaining = PUNISH_LENGTH
        self.punish_rounds_remaining = max(0, self.punish_rounds_remaining - 1)
        if self.punish_rounds_remaining == 0:
            self.next_round_is_test = True
        self.awaiting_test_response = False
        return act_defect()

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    Adaptive Exploiter (AE) for N-Player Public Goods Game.

    Behavior summary:
    - Probe early to find cooperators (cooperate for ProbeRounds).
    - Classify opponents over a sliding window W to find stable cooperators.
    - Exploit (defect) opportunistically when there is a reliable pool of cooperators
      or high overall cooperation, subject to cooldowns and consecutive-exploit caps.
    - Monitor for drops in others' cooperation after exploiting; punish for PunishDuration if detected.
    - Use conditional cooperation (cooperate if a majority of others cooperated last round)
      when exploitation is not safe.
    - Be forgiving after punishment to re-attract cooperators.
    - Always defect in the final EndgameRounds (including the final round).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(0, self.r - 1))
        self.ProbeRounds = min(2, max(1, self.r))
        self.StableCoopThresh = 0.8
        self.Majority = math.ceil((self.n - 1) / 2)
        self.ExploitCooldown = 1
        self.MaxConsecutiveExploits = 3
        self.PunishDuration = min(4, max(1, math.ceil(0.1 * self.r)))
        self.EndgameRounds = min(2, max(1, self.r))
        self.DropThreshold = 0.25 * (self.n - 1)
        self.consecutive_exploits = 0
        self.last_exploit_round = None
        self.last_exploit_baseline = None
        self.exploit_monitored_stage = 2
        self.in_punish_until_round = None
        self.majority_before_punish = False
        self.last_punish_end_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if t >= self.r - self.EndgameRounds:
            return Action.D
        if t == 0 or history is None:
            if self.ProbeRounds >= 1:
                return Action.C
            return Action.D
        history_len = t
        opponents = history.opponent_actions
        if self.in_punish_until_round is not None and t > self.in_punish_until_round:
            self.last_punish_end_round = self.in_punish_until_round
            self.in_punish_until_round = None
        if self.last_exploit_round is not None and self.exploit_monitored_stage < 2:
            if self.exploit_monitored_stage == 0:
                idx = self.last_exploit_round
                if history_len - 1 >= idx:
                    current_count = int(opponents[idx, :].sum())
                    baseline = 0 if self.last_exploit_baseline is None else int(self.last_exploit_baseline)
                    absolute_drop = baseline - current_count
                    relative_drop = absolute_drop / baseline if baseline > 0 else 0.0
                    if absolute_drop >= self.DropThreshold or relative_drop >= 0.3:
                        self.in_punish_until_round = min(self.r - 1, t + self.PunishDuration - 1)
                        prev_idx = max(0, t - 1)
                        self.majority_before_punish = int(opponents[prev_idx, :].sum()) >= self.Majority
                        self.consecutive_exploits = 0
                        self.last_exploit_round = None
                        self.last_exploit_baseline = None
                        self.exploit_monitored_stage = 2
                        return Action.D
                    else:
                        self.exploit_monitored_stage = 1
            if self.exploit_monitored_stage == 1:
                idx = self.last_exploit_round + 1
                if history_len - 1 >= idx:
                    current_count = int(opponents[idx, :].sum())
                    baseline = 0 if self.last_exploit_baseline is None else int(self.last_exploit_baseline)
                    absolute_drop = baseline - current_count
                    relative_drop = absolute_drop / baseline if baseline > 0 else 0.0
                    if absolute_drop >= self.DropThreshold or relative_drop >= 0.3:
                        self.in_punish_until_round = min(self.r - 1, t + self.PunishDuration - 1)
                        prev_idx = max(0, t - 1)
                        self.majority_before_punish = int(opponents[prev_idx, :].sum()) >= self.Majority
                        self.consecutive_exploits = 0
                        self.last_exploit_round = None
                        self.last_exploit_baseline = None
                        self.exploit_monitored_stage = 2
                        return Action.D
                    else:
                        self.exploit_monitored_stage = 2
        if self.in_punish_until_round is not None and t <= self.in_punish_until_round:
            return Action.D
        if self.last_punish_end_round is not None and t == self.last_punish_end_round + 1:
            if self.majority_before_punish:
                self.last_punish_end_round = None
                self.majority_before_punish = False
                return Action.C
            self.last_punish_end_round = None
            self.majority_before_punish = False
        last_w = min(self.W, history_len) if self.W > 0 else history_len
        if last_w <= 0:
            last_w = history_len
        if last_w <= 0:
            return Action.D
        start_idx = max(0, history_len - last_w)
        window = opponents[start_idx:history_len, :]
        per_opponent_counts = np.array(window.sum(axis=0), dtype=float)
        p_j = per_opponent_counts / float(last_w)
        stable_mask = p_j >= self.StableCoopThresh
        StableCount = int(stable_mask.sum())
        per_round_counts = np.array(window.sum(axis=1), dtype=float)
        AvgOtherContrib = float(per_round_counts.mean()) if per_round_counts.size > 0 else 0.0
        if AvgOtherContrib <= 0.2 * (self.n - 1):
            return Action.D
        near_unconditional_mask = p_j >= 0.98
        has_near_unconditional = bool(near_unconditional_mask.any())
        aggressive_mode = has_near_unconditional and AvgOtherContrib >= 0.4 * (self.n - 1)
        stable_pool_condition = StableCount >= self.Majority
        high_overall_condition = AvgOtherContrib >= 0.6 * (self.n - 1)
        exploit_pool = stable_pool_condition or high_overall_condition
        cooldown_ok = self.last_exploit_round is None or t - self.last_exploit_round >= self.ExploitCooldown + 1
        exploit_allowed = False
        if exploit_pool and cooldown_ok:
            if self.consecutive_exploits < self.MaxConsecutiveExploits:
                exploit_allowed = True
            elif aggressive_mode:
                exploit_allowed = True
            else:
                exploit_allowed = False
        if exploit_allowed:
            if history_len >= 1:
                baseline_count = int(opponents[-1, :].sum())
            else:
                baseline_count = 0
            self.last_exploit_baseline = baseline_count
            self.last_exploit_round = t
            self.consecutive_exploits = min(self.consecutive_exploits + 1, max(self.MaxConsecutiveExploits, self.consecutive_exploits + 1))
            self.exploit_monitored_stage = 0
            return Action.D
        prev_idx = history_len - 1
        prev_contribs = int(opponents[prev_idx, :].sum()) if prev_idx >= 0 else 0
        if prev_contribs >= self.Majority:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    Proportional Reciprocity with Forgiveness (PRF)

    - Leads with cooperation.
    - Uses a sliding window to estimate each opponent's recent cooperation rate.
    - Rewards unanimous last-round cooperation immediately.
    - Uses high/low thresholds (q_high, q_low) to decide cooperation/defection;
      in between it mixes probabilistically.
    - Detects persistent defectors and initiates short, proportional punishments.
    - During punishment: usually defects, but occasionally (every T_test rounds)
      probes with a small probability of cooperating. If group cooperation
      recovers (avg_others >= q_high), punishment is cancelled (forgiveness).
    - Defects in the final round by default, unless the group has been stably
      cooperative (then contributes).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W_default = min(10, max(3, math.floor(self.r / 4)))
        self.q_high = 0.8
        self.q_low = 0.5
        self.L = 2
        self.T_test = 3
        self.my_current_punish_timer = 0
        self._punish_elapsed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = state.round_number
        last_round_index = t_idx - 1
        r = self.r
        if t_idx == 0 or history is None:
            return Action.C
        rounds_played = history.opponent_actions.shape[0]
        W = min(self.W_default, rounds_played)
        if W <= 0:
            W = 1
        recent = history.opponent_actions[-W:, :]
        coop_counts = np.sum(recent.astype(np.int32), axis=0)
        coop_rates = coop_counts / float(W)
        if coop_rates.size == 0:
            avg_others = 0.0
        else:
            avg_others = float(np.mean(coop_rates))
        last_round_all = bool(np.all(history.opponent_actions[-1, :])) if history.opponent_actions.shape[0] >= 1 else False
        persistent_mask = coop_rates <= 0.2
        num_persistent = int(np.sum(persistent_mask))
        if t_idx == r - 1:
            if last_round_all and avg_others >= self.q_high:
                return Action.C
            return Action.D
        if self.my_current_punish_timer > 0 and avg_others >= self.q_high:
            self.my_current_punish_timer = 0
            self._punish_elapsed = 0
            return Action.C
        if self.my_current_punish_timer > 0:
            is_test_round = self._punish_elapsed % self.T_test == 0
            action = Action.D
            if is_test_round:
                if random.random() < 0.25:
                    action = Action.C
            self.my_current_punish_timer = max(0, self.my_current_punish_timer - 1)
            self._punish_elapsed += 1
            return action
        if last_round_all:
            chosen_action = Action.C
        elif avg_others >= self.q_high:
            chosen_action = Action.C
        elif avg_others <= self.q_low:
            chosen_action = Action.D
        else:
            denom = self.q_high - self.q_low
            if denom <= 0:
                p_coop = 0.5
            else:
                p_coop = (avg_others - self.q_low) / denom
                if p_coop < 0.0:
                    p_coop = 0.0
                elif p_coop > 1.0:
                    p_coop = 1.0
            chosen_action = Action.C if random.random() < p_coop else Action.D
        if num_persistent > 0 and avg_others >= 0.5:
            rounds_remaining = max(0, r - (t_idx + 1))
            proposed_punish = self.L * num_persistent
            punish_length = min(proposed_punish, rounds_remaining)
            if punish_length > 0:
                self.my_current_punish_timer = int(punish_length)
                self._punish_elapsed = 0
        return chosen_action

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    Selective Reciprocal Exploiter

    - Probes once on round 0 (unless endgame).
    - Tracks opponent cooperation rates and whether opponents tend to respond to
      this player's cooperation vs defection.
    - Exploits persistent naive cooperators when there are too few reciprocators.
    - Cooperates in a lenient group tit-for-tat when there are enough reciprocators
      and sufficient remaining rounds.
    - Uses short, targeted punishment periods when reciprocators appear to punish
      this player's defections, then attempts to repair cooperation.
    - Defects in the final few rounds (endgame/backward induction).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.UNCOOP_RATE = 0.95
        self.RECIPROCATOR_DELTA = 0.2
        self.RECIPROCATOR_MIN_RATE = 0.3
        self.RECIPROCATOR_FRACTION_TO_COOPERATE = 0.2
        self.MIN_ROUNDS_TO_SUPPORT_COOP = 3
        self.PUNISH_MIN = 1
        self.PUNISH_MAX = 3
        self.ENDGAME_BASE = 2
        self.punish_counter = 0
        self._last_seen_round_number = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        round_index = int(state.round_number)
        remaining = r - round_index
        ENDGAME_THRESHOLD = min(self.ENDGAME_BASE, r // 10 + 1)
        PUNISH_LENGTH = min(self.PUNISH_MAX, max(self.PUNISH_MIN, remaining // 6))
        if remaining <= ENDGAME_THRESHOLD:
            return Action.D
        if round_index == 0:
            return Action.C
        if history is None:
            return Action.D
        T = int(history.opponent_actions.shape[0])
        if T <= 0:
            return Action.D
        n_opponents = int(history.opponent_actions.shape[1])
        if n_opponents < 1:
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=np.int32)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.int32)
        C_j = np.sum(opp_actions, axis=0)
        T_j = T
        p_j = np.zeros(n_opponents, dtype=float)
        if T_j > 0:
            p_j = C_j.astype(float) / float(T_j)
        if T_j >= 2:
            my_actions_prev = my_actions[:-1]
            opp_next = opp_actions[1:, :]
            mask_after_myC = my_actions_prev == 1
            mask_after_myD = my_actions_prev == 0
            if np.any(mask_after_myC):
                Cj_after_myC = np.sum(opp_next[mask_after_myC, :], axis=0)
                N_after_myC = int(np.sum(mask_after_myC))
            else:
                Cj_after_myC = np.zeros(n_opponents, dtype=int)
                N_after_myC = 0
            if np.any(mask_after_myD):
                Cj_after_myD = np.sum(opp_next[mask_after_myD, :], axis=0)
                N_after_myD = int(np.sum(mask_after_myD))
            else:
                Cj_after_myD = np.zeros(n_opponents, dtype=int)
                N_after_myD = 0
        else:
            Cj_after_myC = np.zeros(n_opponents, dtype=int)
            Cj_after_myD = np.zeros(n_opponents, dtype=int)
            N_after_myC = 0
            N_after_myD = 0
        denom_after_myC = max(1, N_after_myC)
        denom_after_myD = max(1, N_after_myD)
        coop_after_myC = Cj_after_myC.astype(float) / float(denom_after_myC)
        coop_after_myD = Cj_after_myD.astype(float) / float(denom_after_myD)
        response_delta_j = coop_after_myC - coop_after_myD
        labels = []
        num_uncond = 0
        num_recip = 0
        num_defect = 0
        num_mixed = 0
        for j in range(n_opponents):
            pj = float(p_j[j])
            delta = float(response_delta_j[j])
            if pj >= self.UNCOOP_RATE and delta <= 0.05:
                labels.append('uncond')
                num_uncond += 1
            elif delta >= self.RECIPROCATOR_DELTA and pj >= self.RECIPROCATOR_MIN_RATE:
                labels.append('recip')
                num_recip += 1
            elif pj <= 0.1:
                labels.append('defect')
                num_defect += 1
            else:
                labels.append('mixed')
                num_mixed += 1
        fraction_recips = float(num_recip) / float(max(1, n))
        last_round_opps = opp_actions[-1, :]
        my_last = int(my_actions[-1])
        cooperators_last = int(np.sum(last_round_opps)) + my_last
        G_prev = float(cooperators_last) / float(n)
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if num_uncond > 0 and fraction_recips < self.RECIPROCATOR_FRACTION_TO_COOPERATE:
            punishment_detected = False
            for j in range(n_opponents):
                if labels[j] == 'recip':
                    baseline = float(p_j[j])
                    after_myD_rate = float(coop_after_myD[j])
                    if baseline - after_myD_rate >= 0.15:
                        punishment_detected = True
                        break
            if punishment_detected:
                self.punish_counter = PUNISH_LENGTH
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
            return Action.D
        if num_uncond > 0 and fraction_recips < self.RECIPROCATOR_FRACTION_TO_COOPERATE:
            return Action.D
        if fraction_recips >= self.RECIPROCATOR_FRACTION_TO_COOPERATE and remaining >= self.MIN_ROUNDS_TO_SUPPORT_COOP:
            if G_prev > 0.5:
                return Action.C
            elif abs(G_prev - 0.5) < 1e-09:
                my_recent_rate = float(np.sum(my_actions)) / float(max(1, T))
                if my_recent_rate > 0.4:
                    return Action.C
                return Action.D
            else:
                return Action.D
        for j in range(n_opponents):
            if labels[j] == 'recip':
                baseline = float(p_j[j])
                after_myD_rate = float(coop_after_myD[j])
                if baseline - after_myD_rate >= 0.2:
                    self.punish_counter = PUNISH_LENGTH
                    self.punish_counter = max(0, self.punish_counter - 1)
                    return Action.D
        if num_uncond == 0 and num_recip < max(1, int(self.RECIPROCATOR_FRACTION_TO_COOPERATE * n)):
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR)

    - Starts cooperative to signal intent.
    - Tracks short-term (recent window) and long-term cooperation rates among others.
    - Uses thresholds that depend on lambda = k/n to decide whether to cooperate, defect,
      or enter a short collective punishment period after a sharp drop in cooperation.
    - Forgives after finite punishments and is more conservative in very short games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_index = int(state.round_number)
        t = round_index + 1
        n = self.n
        r = self.r
        k = self.k
        lam = k / n if n > 0 else 0.0
        w = min(max(3, math.floor(r / 10)), r)
        if round_index == 0 or history is None or getattr(history, 'opponent_actions', None) is None:
            return Action.C
        opp_actions = history.opponent_actions
        num_past = int(opp_actions.shape[0])
        num_opponents = int(opp_actions.shape[1]) if opp_actions.ndim >= 2 else 0
        if num_opponents <= 0:
            if t == r:
                return Action.D
            return Action.C
        coop_counts = np.sum(opp_actions.astype(np.int64), axis=1)
        coop_rates = coop_counts / float(max(1, num_opponents))
        if num_past > 0:
            R_long = float(np.mean(coop_rates))
        else:
            R_long = 1.0
        if num_past > 0:
            recent = int(min(w, num_past))
            if recent <= 0:
                R_short = R_long
            else:
                R_short = float(np.mean(coop_rates[-recent:]))
        else:
            R_short = R_long

        def clamp(x, a, b):
            return a if x < a else b if x > b else x
        H = clamp(0.7 - 0.3 * lam, 0.45, 0.7)
        L = max(H - 0.3, 0.05)
        if num_past >= 1:
            coop_last = float(coop_rates[-1])
            threshold_drop = max(0.5 * R_long, 0.25)
            if coop_last <= threshold_drop and R_long >= 0.6:
                floor_r4 = max(1, math.floor(r / 4))
                P_candidate = max(1, math.ceil((1.0 - R_short) * 4.0))
                P = min(P_candidate, floor_r4, 4)
                new_punish_until = t + int(P) - 1
                if new_punish_until > self.punish_until:
                    self.punish_until = new_punish_until
        if t <= int(self.punish_until):
            return Action.D
        if r <= 4 and t > 1:
            if R_short >= H and R_long >= 0.7:
                return Action.C
            return Action.D
        if t == r:
            if R_long >= 0.8 and R_short >= H:
                return Action.C
            return Action.D
        if R_short >= H:
            return Action.C
        if R_short <= L:
            return Action.D
        if R_long >= 0.6:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    Collective Harmony  conditional, forgiving, and proportional cooperation.

    Starts cooperating, looks at a recent window of past rounds to classify players
    as reliable, cooperates when a clear recent majority is reliable, punishes
    briefly and proportionally when exploitation appears, forgives via occasional
    exploratory cooperation, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.epsilon = 0.1
        self.base_P = 2
        self._punishment_remaining = 0
        self._punishment_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        k = self.k
        W = min(5, max(0, r - 1))
        kn_ratio = k / n if n > 0 else 0.0
        if kn_ratio >= 0.7:
            phi = 0.5
        elif kn_ratio <= 0.4:
            phi = 0.7
        else:
            phi = 0.6
        M = int(math.ceil(phi * n))
        is_first = t == 0
        is_last = t == r - 1
        is_penultimate = t == r - 2
        if r <= 0:
            return Action.D
        if r == 1:
            return Action.D
        if is_first:
            return Action.C
        if history is None:
            if is_last:
                return Action.D
            return Action.C
        if t >= 1:
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :] if history.opponent_actions.shape[0] >= 1 else np.array([], dtype=bool)
            L = int(last_my) + int(np.sum(last_opponents))
        else:
            L = 0
        past_rounds = min(W, t)
        if past_rounds > 0:
            my_slice = history.my_actions[-past_rounds:].astype(np.int8).reshape((past_rounds, 1))
            opp_slice = history.opponent_actions[-past_rounds:, :].astype(np.int8) if history.opponent_actions.shape[0] >= past_rounds else np.zeros((past_rounds, max(0, n - 1)), dtype=np.int8)
            try:
                contributions = np.concatenate([my_slice, opp_slice], axis=1)
            except Exception:
                contributions = np.zeros((past_rounds, n), dtype=np.int8)
                contributions[:, 0] = my_slice[:, 0]
                if opp_slice.shape[1] > 0:
                    contributions[:, 1:1 + opp_slice.shape[1]] = opp_slice
        else:
            contributions = np.zeros((0, n), dtype=np.int8)
        if past_rounds > 0:
            sums_per_player = np.sum(contributions, axis=0)
            f_j = sums_per_player.astype(float) / float(past_rounds)
            reliable_flags = f_j >= phi
            R = int(np.sum(reliable_flags))
        else:
            reliable_flags = np.array([False] * n)
            f_j = np.zeros((n,), dtype=float)
            R = 0
        if is_penultimate:
            stable_count = 0
            if past_rounds > 0:
                totals_per_round = np.sum(contributions, axis=1)
                stable_count = int(np.sum(totals_per_round >= M))
            if R >= M and stable_count >= 2 and (self._punishment_remaining <= 0):
                return Action.C
            return Action.D
        if is_last:
            return Action.D
        if self._punishment_remaining > 0:
            if R >= M:
                self._punishment_remaining = 0
                return Action.C
            coop_prob = random.random()
            self._punishment_remaining = max(0, self._punishment_remaining - 1)
            if coop_prob < self.epsilon:
                return Action.C
            return Action.D
        if R >= M:
            return Action.C
        if L >= M:
            return Action.C
        if R >= n - 2:
            return Action.C
        mass_defection_threshold = max(1, int(math.floor(n * 0.3)))
        if R <= mass_defection_threshold:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        P = self.base_P
        if self._punishment_count >= 2:
            P = 3
        remaining_rounds_after_this = r - t - 1
        effective_P = min(P, max(0, remaining_rounds_after_this))
        if effective_P <= 0:
            return Action.D
        self._punishment_count += 1
        self._punishment_remaining = max(0, effective_P - 1)
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    Collective Gradual Reciprocity (CGR)

    - Starts by cooperating.
    - Monitors recent group cooperation (window w).
    - If recent cooperation >= , cooperate.
    - If recent cooperation < , punish by defecting for a length proportional
      to the number of defectors in the last round (capped by P_max).
    - After a punishment phase ends, play one probation cooperative round.
    - Occasionally flip action with small probability eps to probe/forgive.
    - In persistent low-cooperation environments, stay defecting except for
      occasional eps probes.
    - Be conservative near the end: linearly reduce  in the last s rounds,
      and in the final round defect unless the previous round was unanimous C.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        self.eps = 0.05
        self.P_max = min(4, max(1, math.floor(self.r / 5)))
        tau_base = 0.65
        leniency = 0.15 * ((self.k - 1.0) / max(1.0, self.n - 1))
        self.tau_static = max(0.5, tau_base - leniency)
        self.s_endgame = min(3, math.floor(self.r / 10) + 1)
        self.punish_until = -1
        self._prev_punish_until = -1
        self._probation_consumed_round = -1
        self._last_seen_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            self.punish_until = -1
            self._prev_punish_until = -1
            self._probation_consumed_round = -1
            self._last_seen_round = 0
            return Action.C
        self._last_seen_round = t
        rounds_available = int(history.my_actions.shape[0])
        start_idx = max(0, rounds_available - self.w)
        idxs = list(range(start_idx, rounds_available))
        coop_fracs = []
        for idx in idxs:
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opps = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            total_c = my_c + opps
            frac = total_c / float(self.n)
            coop_fracs.append(frac)
        cooperation_rate = float(np.mean(coop_fracs)) if len(coop_fracs) > 0 else 0.0
        tau = float(self.tau_static)
        if self.s_endgame > 0:
            start_last_s = max(0, self.r - self.s_endgame)
            if t >= start_last_s:
                f = (t - start_last_s + 1) / float(self.s_endgame)
                f = min(max(f, 0.0), 1.0)
                tau = self.tau_static - (self.tau_static - 0.5) * f
                if tau < 0.5:
                    tau = 0.5
        very_low = False
        window_for_very_low = 2 * self.w
        if rounds_available >= window_for_very_low:
            start_vlow = max(0, rounds_available - window_for_very_low)
            fracs_vlow = []
            for idx in range(start_vlow, rounds_available):
                my_c = 1 if bool(history.my_actions[idx]) else 0
                opps = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
                total_c = my_c + opps
                fracs_vlow.append(total_c / float(self.n))
            avg_vlow = float(np.mean(fracs_vlow)) if len(fracs_vlow) > 0 else 0.0
            if avg_vlow < 0.2:
                very_low = True
        is_final_round = t == self.r - 1
        prev_punish_until = int(self.punish_until) if self.punish_until is not None else -1
        last_idx = rounds_available - 1
        coop_last_round = 0
        if last_idx >= 0:
            coop_last_round = (1 if bool(history.my_actions[last_idx]) else 0) + (int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size > 0 else 0)
        defectors_last_round = self.n - coop_last_round
        if cooperation_rate < tau:
            punishment_length = min(self.P_max, max(1, int(defectors_last_round)))
            new_until = t + punishment_length - 1
            if new_until > self.punish_until:
                self.punish_until = new_until
        in_punishment = t <= self.punish_until
        probation = False
        if not in_punishment and prev_punish_until >= 0 and (t == prev_punish_until + 1):
            if self._probation_consumed_round != t:
                probation = True
        if very_low:
            planned = Action.C if random.random() < self.eps else Action.D
            self._prev_punish_until = self.punish_until
            if probation and planned == Action.C:
                self._probation_consumed_round = t
            return planned
        if is_final_round:
            unanimous_prev = False
            if last_idx >= 0 and coop_last_round == self.n:
                unanimous_prev = True
            planned = Action.C if unanimous_prev else Action.D
            if random.random() < self.eps:
                planned = Action.C if planned == Action.D else Action.D
            self._prev_punish_until = self.punish_until
            return planned
        if in_punishment:
            planned = Action.D
            self._prev_punish_until = self.punish_until
            return Action.C if random.random() < self.eps and planned == Action.D else planned
        if probation:
            planned = Action.C
            self._probation_consumed_round = t
            if random.random() < self.eps:
                planned = Action.D if planned == Action.C else Action.C
            self._prev_punish_until = self.punish_until
            return planned
        if cooperation_rate >= tau:
            planned = Action.C
        else:
            planned = Action.D
        if random.random() < self.eps:
            planned = Action.C if planned == Action.D else Action.D
        self._prev_punish_until = self.punish_until
        return planned

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    Cautious Opportunist for N-Player Public Goods Game.

    - Lures conditional cooperators by opening with cooperation and gathering statistics.
    - Identifies conditional cooperators (responsive to my cooperation) using Laplace-smoothed
      probabilities and conservative thresholds.
    - Enters Exploit mode when a majority of opponents look like conditional cooperators:
      mostly cooperates to maintain cooperation, but periodically defects a single round
      ("grab") to extract one-shot gains. Adjusts grab frequency based on observed retaliation.
    - If exploitation provokes durable collapse, punish briefly then test for recovery.
    - Otherwise (few reliable reciprocators) remains Defensive (defect each round), with
      occasional cheap cooperative tests.
    - Always defects in the final round(s).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.min_samples = 4
        self.p_thresh = 0.6
        self.rec_thresh = 0.2
        self.exploit_interval_base = 4
        self.punishment_len = 2
        self.last_rounds_defect = 2 if self.r <= 6 else 1
        self.recent_window = min(20, max(5, self.r))
        self.def_test_M = max(8, max(1, self.r // 10))
        self.mode = 'test'
        self.cooper_cycle = max(1, self.exploit_interval_base)
        self.last_processed_grab = -1
        self.punishment_counter = 0
        self.post_punish_test = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number >= self.r - self.last_rounds_defect:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        rounds_played = int(history.my_actions.shape[0])
        if rounds_played == 0:
            return Action.C
        window = min(self.recent_window, rounds_played)
        start_idx = rounds_played - window
        n_opponents = history.opponent_actions.shape[1]
        N_j = [0] * n_opponents
        C_j = [0] * n_opponents
        N_after_myC = [0] * n_opponents
        C_after_myC = [0] * n_opponents
        N_after_myD = [0] * n_opponents
        C_after_myD = [0] * n_opponents
        for t in range(start_idx, rounds_played):
            for j in range(n_opponents):
                act = bool(history.opponent_actions[t, j])
                N_j[j] += 1
                if act:
                    C_j[j] += 1
        for t in range(start_idx, rounds_played - 1):
            my_act = bool(history.my_actions[t])
            for j in range(n_opponents):
                opp_next = bool(history.opponent_actions[t + 1, j])
                if my_act:
                    N_after_myC[j] += 1
                    if opp_next:
                        C_after_myC[j] += 1
                else:
                    N_after_myD[j] += 1
                    if opp_next:
                        C_after_myD[j] += 1
        p_j = [0.5] * n_opponents
        p_j_after_C = [0.5] * n_opponents
        p_j_after_D = [0.5] * n_opponents
        rec_j = [0.0] * n_opponents
        for j in range(n_opponents):
            denom = N_j[j] + 2
            if denom > 0:
                p_j[j] = (C_j[j] + 1) / denom
            denomC = N_after_myC[j] + 2
            p_j_after_C[j] = (C_after_myC[j] + 1) / denomC
            denomD = N_after_myD[j] + 2
            p_j_after_D[j] = (C_after_myD[j] + 1) / denomD
            rec_j[j] = p_j_after_C[j] - p_j_after_D[j]
        try:
            P_group = float(np.mean(np.array(p_j)))
        except Exception:
            P_group = 0.0
        S_rec = []
        for j in range(n_opponents):
            if N_j[j] >= self.min_samples and p_j_after_C[j] >= self.p_thresh and (rec_j[j] >= self.rec_thresh):
                S_rec.append(j)
        m = len(S_rec)
        m_frac = m / max(1, self.n - 1)
        few_obs = sum((1 for j in range(n_opponents) if N_j[j] < self.min_samples))
        if few_obs > n_opponents // 2:
            if P_group < 0.2:
                self.mode = 'defensive'
                return Action.D
            self.mode = 'test'
            return Action.C
        recent_payoffs = history.my_payoffs[start_idx:rounds_played]
        my_avg = float(np.mean(recent_payoffs)) if len(recent_payoffs) > 0 else 0.0
        expected_others_coop = P_group * (self.n - 1)
        baseline_defect_payoff = 1.0 + self.k / self.n * expected_others_coop
        if my_avg < baseline_defect_payoff - 1e-06:
            self.mode = 'defensive'
            return Action.D
        exploit_unconditional = False
        if m_frac >= 0.5:
            intended_mode = 'exploit'
        else:
            many_unconditional = sum((1 for j in range(n_opponents) if p_j[j] >= self.p_thresh and rec_j[j] < self.rec_thresh))
            if P_group >= 0.9 and many_unconditional >= n_opponents // 2:
                intended_mode = 'exploit'
                exploit_unconditional = True
            else:
                intended_mode = 'defensive'
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                self.post_punish_test = 2
                self.mode = 'test'
            return Action.D
        if self.post_punish_test > 0:
            self.post_punish_test -= 1
            self.mode = 'test'
            return Action.C
        self.mode = intended_mode
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        my_defect_rounds = [t for t in range(rounds_played) if not bool(my_actions[t])]
        processed_any = False
        for g in my_defect_rounds:
            if g <= self.last_processed_grab:
                continue
            if g + 1 >= rounds_played:
                continue
            response_round = g + 1
            coop_counts = float(np.sum(opp_actions[response_round, :]))
            group_coop_rate_response = coop_counts / max(1.0, float(n_opponents))
            if len(S_rec) > 0:
                avg_p_after_D = float(np.mean([p_j_after_D[j] for j in S_rec]))
                avg_p_after_C = float(np.mean([p_j_after_C[j] for j in S_rec]))
                observed_after_D = 0.0
                for j in S_rec:
                    if bool(opp_actions[response_round, j]):
                        observed_after_D += 1.0
                observed_frac_after_D = observed_after_D / max(1.0, float(len(S_rec)))
                drop = avg_p_after_C - observed_frac_after_D
                if drop > 0.15:
                    self.cooper_cycle = min(self.cooper_cycle + 1, max(10, max(1, self.r // 4)))
                elif drop <= 0.05:
                    self.cooper_cycle = max(1, self.cooper_cycle - 1)
                else:
                    pass
            else:
                self.cooper_cycle = min(self.cooper_cycle + 1, max(10, max(1, self.r // 4)))
            collapse = True
            for tcheck in range(response_round, min(rounds_played, response_round + self.punishment_len)):
                coop_t = float(np.sum(opp_actions[tcheck, :])) / max(1.0, float(n_opponents))
                if coop_t >= 0.5:
                    collapse = False
                    break
            if collapse:
                self.punishment_counter = self.punishment_len
                self.mode = 'punish'
                self.last_processed_grab = g
                processed_any = True
                break
            self.last_processed_grab = g
            processed_any = True
        if self.mode == 'defensive':
            if self.def_test_M > 0 and state.round_number % self.def_test_M == 0:
                many_now_coop = sum((1 for j in range(n_opponents) if p_j[j] >= 0.75))
                if many_now_coop >= max(1, n_opponents // 4):
                    return Action.C
            return Action.D
        if self.mode == 'exploit':
            last_my_defect = -1
            for t in range(rounds_played - 1, -1, -1):
                if not bool(history.my_actions[t]):
                    last_my_defect = t
                    break
            if last_my_defect < 0:
                rounds_since_grab = rounds_played
            else:
                rounds_since_grab = rounds_played - 1 - last_my_defect
            if rounds_since_grab >= self.cooper_cycle:
                return Action.D
            else:
                return Action.C
        self.mode = 'test'
        return Action.C

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    Adaptive Exploiter (AE)
    - Probe early to find consistent cooperators.
    - Exploit players who keep cooperating after I defect, with rare cooperative "keep-alive" gestures.
    - If no exploitable players, cultivate cooperation by cooperating for short periods.
    - If exploitation causes broad retaliation, punish briefly and re-probe.
    - Endgame: always defect in the last two rounds; ramp down cooperation shortly before endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.W = min(10, max(3, math.floor(self.r / 10)))
        self.Probe_length = min(3, max(1, math.floor(self.r / 20)))
        self.cooperator_threshold = 0.75
        self.exploitable_response_threshold = 0.6
        self.forgiveness_tolerance = 2
        self.p_guard_coop = max(0.03, min(0.1, 5.0 / max(1, self.r)))
        self.Endgame_hard_defect = 2
        self.Rampdown_period = min(5, math.floor(self.r / 10))
        self.Cultivate_length_default = min(self.W, max(2, math.floor(self.r / 10)))
        self.Punish_length = min(self.W, 3)
        self.punish_rounds_left = 0
        self.cultivate_rounds_left = 0
        self.exploit_active = False
        self.exploit_start_round = None
        self.last_global_recent_coop_snapshot = None
        self.post_punish_probe_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        rounds_left = max(0, self.r - t)
        if rounds_left <= self.Endgame_hard_defect:
            self.exploit_active = False
            self.exploit_start_round = None
            self.last_global_recent_coop_snapshot = None
            self.cultivate_rounds_left = 0
            self.punish_rounds_left = 0
            self.post_punish_probe_remaining = 0
            return Action.D
        if t == 0 or history is None:
            self.punish_rounds_left = 0
            self.cultivate_rounds_left = 0
            self.exploit_active = False
            self.exploit_start_round = None
            self.last_global_recent_coop_snapshot = None
            self.post_punish_probe_remaining = 0
            if t < self.Probe_length:
                return Action.C
            return Action.D
        if self.punish_rounds_left > 0:
            self.punish_rounds_left -= 1
            if self.punish_rounds_left == 0:
                self.post_punish_probe_remaining = self.Probe_length
                self.exploit_active = False
                self.exploit_start_round = None
                self.last_global_recent_coop_snapshot = None
            return Action.D
        if self.post_punish_probe_remaining > 0:
            self.post_punish_probe_remaining -= 1
            return Action.C
        if t < self.Probe_length:
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        rounds_played = opp_actions.shape[0]
        n_opponents = max(0, opp_actions.shape[1])
        window_len = min(self.W, rounds_played)
        if window_len <= 0:
            if rounds_left <= self.Rampdown_period + self.Endgame_hard_defect:
                return Action.D
            return Action.C
        window_start = rounds_played - window_len
        recent_slice = slice(window_start, rounds_played)
        total_coop = np.sum(opp_actions, axis=0).astype(float)
        recent_counts = np.sum(opp_actions[recent_slice, :], axis=0).astype(float)
        recent_coop = recent_counts / float(window_len)
        cond_coop_after_my_D = np.zeros(n_opponents, dtype=float)
        triggers = np.zeros(n_opponents, dtype=float)
        successes = np.zeros(n_opponents, dtype=float)
        for idx in range(window_start, rounds_played - 0):
            if not bool(my_actions[idx]):
                if idx + 1 < rounds_played:
                    opp_next = opp_actions[idx + 1, :]
                    triggers += 1.0
                    successes += opp_next.astype(float)
        for j in range(n_opponents):
            if triggers[j] > 0.0:
                cond_coop_after_my_D[j] = successes[j] / triggers[j]
            else:
                cond_coop_after_my_D[j] = float(recent_coop[j])
        if n_opponents > 0:
            global_recent_coop = float(np.mean(recent_coop))
        else:
            global_recent_coop = 0.0
        ExploitSet_indices = [j for j in range(n_opponents) if recent_coop[j] >= self.cooperator_threshold and cond_coop_after_my_D[j] >= self.exploitable_response_threshold]
        if n_opponents > 0:
            retaliator_frac = float(np.mean((cond_coop_after_my_D < 0.4).astype(float)))
        else:
            retaliator_frac = 0.0
        predicted_others = float(np.sum(recent_coop))
        if predicted_others <= 0.5:
            predicted_others_low = True
        else:
            predicted_others_low = False
        in_rampdown = rounds_left <= self.Rampdown_period + self.Endgame_hard_defect and self.Rampdown_period > 0
        if self.cultivate_rounds_left > 0:
            if len(ExploitSet_indices) > 0 and global_recent_coop >= 0.3 and (retaliator_frac <= 0.5) and (not in_rampdown):
                self.cultivate_rounds_left = 0
                self.exploit_active = True
                self.exploit_start_round = t
                self.last_global_recent_coop_snapshot = global_recent_coop
            else:
                self.cultivate_rounds_left -= 1
                return Action.C
        want_exploit = False
        if len(ExploitSet_indices) > 0 and global_recent_coop >= 0.3 and (retaliator_frac <= 0.5) and (not in_rampdown):
            want_exploit = True
        if retaliator_frac > 0.5:
            want_exploit = False
        if predicted_others_low:
            want_exploit = False
        if self.exploit_active and self.exploit_start_round is not None:
            if self.last_global_recent_coop_snapshot is not None:
                drop = self.last_global_recent_coop_snapshot - global_recent_coop
                if drop > 0.2:
                    self.exploit_active = False
                    self.exploit_start_round = None
                    self.last_global_recent_coop_snapshot = None
                    self.punish_rounds_left = self.Punish_length
                    return Action.D
                num_now_exploitable = sum((1 for j in range(n_opponents) if recent_coop[j] >= self.cooperator_threshold and cond_coop_after_my_D[j] >= self.exploitable_response_threshold))
                if num_now_exploitable == 0:
                    self.exploit_active = False
                    self.exploit_start_round = None
                    self.last_global_recent_coop_snapshot = None
                    want_exploit = False
            if t - (self.exploit_start_round or 0) >= self.W:
                self.exploit_active = False
                self.exploit_start_round = None
                self.last_global_recent_coop_snapshot = None
        if want_exploit:
            if not self.exploit_active:
                self.exploit_active = True
                self.exploit_start_round = t
                self.last_global_recent_coop_snapshot = global_recent_coop
            guard_prob = self.p_guard_coop
            if in_rampdown:
                guard_prob = max(0.0, guard_prob * 0.5)
            if random.random() < guard_prob:
                return Action.C
            else:
                return Action.D
        if self.cultivate_rounds_left <= 0:
            self.cultivate_rounds_left = self.Cultivate_length_default
        if in_rampdown and rounds_left <= self.Rampdown_period + self.Endgame_hard_defect:
            if self.cultivate_rounds_left > 0:
                self.cultivate_rounds_left -= 1
                return Action.C
            return Action.D
        if self.cultivate_rounds_left > 0:
            self.cultivate_rounds_left -= 1
            return Action.C
        if global_recent_coop < 0.3:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    Begins by cooperating, conditions cooperation on recent group behavior and long-run
    reliability of players, punishes limitedly, probes periodically to restore cooperation,
    and defects in a short endgame. Resistant to noise by using windows and long-run
    frequencies. Reduces willingness to restore cooperation if a persistent minority
    free-rides.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.endgame_length = min(2, self.n_rounds)
        self.high_threshold = 0.8
        self.low_threshold = 0.2
        self.majority_threshold = 0.5
        self.forgiveness_punish_length = 2
        self.probe_interval = 7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = state.round_number
        t = round_idx + 1
        if round_idx >= self.n_rounds - self.endgame_length:
            return Action.D
        if round_idx == 0 or history is None:
            return Action.C
        rounds_played = history.my_actions.shape[0]
        n_others = max(1, self.n_players - 1)
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        opponent_freqs = np.mean(opp_actions.astype(float), axis=0)
        my_freq = float(np.mean(my_actions.astype(float))) if rounds_played > 0 else 0.0
        last_round_coop_rate = float(np.mean(opp_actions[-1, :].astype(float)))
        recent_window = min(3, t - 1)
        recent_coop_rate = None
        if recent_window > 0:
            recent_slice = opp_actions[-recent_window:, :]
            per_round_rates = np.mean(recent_slice.astype(float), axis=1)
            recent_coop_rate = float(np.mean(per_round_rates))
        group_long_run_coop_rate = float(np.mean((opponent_freqs >= 0.6).astype(float)))
        persistent_free_rider_share = float(np.mean((opponent_freqs <= 0.2).astype(float)))
        consecutive_my_defections = 0
        for val in reversed(my_actions):
            if not bool(val):
                consecutive_my_defections += 1
            else:
                break
        restricted = persistent_free_rider_share >= 0.4

        def can_resume_cooperation() -> bool:
            if not restricted:
                return True
            strong_recent = recent_coop_rate is not None and recent_coop_rate >= 0.75
            strong_long_run = group_long_run_coop_rate > 0.6
            return strong_recent or strong_long_run
        if recent_window > 0 and recent_coop_rate is not None and (recent_coop_rate >= self.high_threshold):
            if can_resume_cooperation():
                return Action.C
            else:
                return Action.D
        if last_round_coop_rate >= self.majority_threshold:
            if can_resume_cooperation():
                return Action.C
            else:
                return Action.D
        if group_long_run_coop_rate > 0.5:
            if can_resume_cooperation():
                return Action.C
            else:
                return Action.D
        if consecutive_my_defections >= self.forgiveness_punish_length and t % self.probe_interval == 0:
            if can_resume_cooperation():
                return Action.C
        recent_low = recent_coop_rate is not None and recent_coop_rate <= self.low_threshold
        last_below_majority = last_round_coop_rate < self.majority_threshold
        if recent_low or last_below_majority:
            return Action.D
        if last_round_coop_rate >= 0.4 and group_long_run_coop_rate >= 0.4:
            if can_resume_cooperation():
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    Exploitative Conditional Cooperator (ECC)

    - Starts by cooperating to signal willingness to cooperate.
    - In normal play, alternates short exploit bursts (single defection) when others have been
      highly cooperative, to free-ride while preserving cooperative signals.
    - Uses short, finite punishments against persistent defectors, then a brief forgiveness/test
      period. If cooperation does not recover, withdraw cooperation until signals improve.
    - Defects in the final R_end rounds (endgame).
    - Protects against being the lone consistent cooperator by switching to permanent defection
      if it has cooperated >=3 rounds in a row while group cooperation remains low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.q_high = 0.8
        self.q_low = 0.4
        self.punish_remaining = 0
        self.test_remaining = 0
        self.withdraw = False
        self.permanent_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        R_end = min(2, max(0, self.r - 1))
        if state.round_number >= self.r - R_end:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        if self.permanent_defect:
            return Action.D
        r_remaining = max(0, self.r - state.round_number)
        W = min(5, state.round_number)
        M = min(3, state.round_number)
        cooperation_rate = None
        if W > 0:
            recent_others = history.opponent_actions[-W:, :]
            SumOthers = int(np.sum(recent_others))
            denom = W * (self.n - 1)
            if denom > 0:
                cooperation_rate = SumOthers / denom
            else:
                cooperation_rate = 0.0
        else:
            cooperation_rate = None
        S = 3
        if state.round_number >= S:
            my_last_S = history.my_actions[-S:]
            if bool(np.all(my_last_S)):
                if cooperation_rate is not None and cooperation_rate < self.q_low:
                    self.permanent_defect = True
                    return Action.D
        persistent_defector_found = False
        if M > 0:
            recent_op = history.opponent_actions[-M:, :]
            sums_per_op = np.sum(recent_op, axis=0)
            if np.any(sums_per_op == 0):
                persistent_defector_found = True
        if persistent_defector_found:
            P = min(4, max(0, r_remaining))
            if P > self.punish_remaining:
                self.punish_remaining = P
            self.test_remaining = 0
            self.withdraw = False
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                F = min(2, max(0, r_remaining - 1))
                if F > 0:
                    self.test_remaining = F
                else:
                    self.test_remaining = 0
            return Action.D
        if self.test_remaining > 0:
            self.test_remaining -= 1
            W_now = min(5, state.round_number) if state.round_number > 0 else 0
            if W_now > 0:
                recent_others_now = history.opponent_actions[-W_now:, :]
                SumOthers_now = int(np.sum(recent_others_now))
                denom_now = W_now * (self.n - 1)
                if denom_now > 0:
                    cooperation_rate_now = SumOthers_now / denom_now
                else:
                    cooperation_rate_now = 0.0
            else:
                cooperation_rate_now = None
            if cooperation_rate_now is not None and cooperation_rate_now >= self.q_low:
                self.withdraw = False
                self.test_remaining = 0
            if self.test_remaining == 0:
                if cooperation_rate_now is None or cooperation_rate_now < self.q_low:
                    self.withdraw = True
            return Action.C
        if self.withdraw:
            if cooperation_rate is not None and cooperation_rate >= self.q_low:
                self.withdraw = False
            else:
                return Action.D
        if cooperation_rate is None:
            last_coops = int(np.sum(history.opponent_actions[-1, :]))
            if last_coops > (self.n - 1) / 2:
                return Action.C
            return Action.D
        if cooperation_rate >= self.q_high:
            my_last = bool(history.my_actions[-1])
            if my_last:
                return Action.D
            else:
                return Action.C
        if self.q_low <= cooperation_rate < self.q_high:
            last_round_coops = int(np.sum(history.opponent_actions[-1, :]))
            if last_round_coops > (self.n - 1) / 2:
                return Action.C
            return Action.D
        if cooperation_rate < self.q_low:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    Collective Adaptive Conditional Cooperator (CACC)

    Adaptive public-goods strategy that starts cooperatively, rewards cooperative environments,
    applies short proportional punishments to deter persistent free-riding, and forgives
    once cooperation recovers. Uses sliding-window statistics and simple reciprocity detection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = max(3, min(10, math.floor(self.r / 5)))
        self.B = min(4, max(1, math.floor(self.r / 10)))
        self.K_end = min(3, max(1, math.floor(self.r * 0.05)))
        self.T_good = 0.8
        self.T_bad = 0.35
        self.delta = 0.4
        self.P_min = 1
        self.P_max = 4
        self.punishment_counter = 0
        self.recently_punished = False
        self.last_window_f_group = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            return Action.C
        if t >= self.r - self.K_end:
            return Action.D
        if 1 <= t <= max(0, self.B - 1):
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        total_rounds_available = my_actions.shape[0]
        W = min(self.W, total_rounds_available)
        start_idx = max(0, total_rounds_available - W)
        window_len = total_rounds_available - start_idx
        if window_len <= 0:
            return Action.C
        f_js = []
        if self.n - 1 > 0:
            opp_window = opp_actions[start_idx:total_rounds_available, :].astype(float)
            opp_sums = np.sum(opp_window, axis=0) if opp_window.size else np.zeros(self.n - 1)
        else:
            opp_sums = np.array([], dtype=float)
        for s in opp_sums:
            f_js.append(float(s) / max(1, window_len))
        my_window = my_actions[start_idx:total_rounds_available].astype(float)
        my_sum = float(np.sum(my_window)) if my_window.size else 0.0
        f_js.append(my_sum / max(1, window_len))
        total_contribs = my_sum + (np.sum(opp_sums) if opp_sums.size else 0.0)
        f_group = float(total_contribs) / (self.n * max(1, window_len))
        r_js = []
        c_after_c_counts = []
        c_after_d_counts = []
        denom_c_prev = 0
        denom_d_prev = 0
        if window_len >= 2:
            for r_idx in range(start_idx + 1, total_rounds_available):
                my_prev = bool(my_actions[r_idx - 1])
                if my_prev:
                    denom_c_prev += 1
                else:
                    denom_d_prev += 1
            for opp_idx in range(max(0, self.n - 1)):
                c_after_c = 0
                c_after_d = 0
                for r_idx in range(start_idx + 1, total_rounds_available):
                    opp_act = bool(opp_actions[r_idx, opp_idx])
                    my_prev = bool(my_actions[r_idx - 1])
                    if opp_act and my_prev:
                        c_after_c += 1
                    if opp_act and (not my_prev):
                        c_after_d += 1
                c_after_c_counts.append(c_after_c)
                c_after_d_counts.append(c_after_d)
        else:
            for _ in range(max(0, self.n - 1)):
                c_after_c_counts.append(0)
                c_after_d_counts.append(0)
        for idx in range(self.n - 1):
            denom_c = max(1, denom_c_prev)
            denom_d = max(1, denom_d_prev)
            r_val = c_after_c_counts[idx] / denom_c - c_after_d_counts[idx] / denom_d
            r_js.append(float(r_val))
        r_js.append(0.0)
        prev_f_group = f_group
        if total_rounds_available >= 2 * W:
            prev_start = max(0, total_rounds_available - 2 * W)
            prev_end = start_idx
            prev_window_len = start_idx - prev_start
            if prev_window_len > 0:
                prev_my_sum = float(np.sum(my_actions[prev_start:start_idx].astype(float)))
                if self.n - 1 > 0:
                    prev_opp_sum = float(np.sum(opp_actions[prev_start:start_idx, :].astype(float)))
                else:
                    prev_opp_sum = 0.0
                prev_f_group = float(prev_my_sum + prev_opp_sum) / (self.n * max(1, prev_window_len))
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            self.recently_punished = True
            self.last_window_f_group = f_group
            return Action.D
        threshold_fr = max(0.15, f_group - self.delta)
        free_rider_count = 0
        for idx in range(self.n - 1):
            if f_js[idx] <= threshold_fr:
                free_rider_count += 1
        FR = float(free_rider_count) / float(self.n)
        drop = prev_f_group - f_group
        trigger_punish = FR >= 0.2 or drop > 0.25
        if trigger_punish:
            P = min(self.P_max, self.P_min + math.ceil(3 * FR))
            self.punishment_counter = max(0, int(P) - 1)
            self.recently_punished = True
            self.last_window_f_group = f_group
            return Action.D
        has_strong_reciprocator = False
        for idx in range(self.n - 1):
            if r_js[idx] >= 0.3 and f_js[idx] >= 0.5:
                has_strong_reciprocator = True
                break
        pos_recip_count = sum((1 for idx in range(self.n - 1) if r_js[idx] > 0))
        majority_pos = pos_recip_count > self.n / 2
        recovery_threshold = max(self.T_good - 0.05, 0.7)
        if self.recently_punished and f_group >= recovery_threshold:
            self.recently_punished = False
            self.punishment_counter = 0
        if f_group >= self.T_good:
            self.last_window_f_group = f_group
            if random.random() < 0.02:
                return Action.D
            return Action.C
        if f_group <= self.T_bad:
            if has_strong_reciprocator:
                self.last_window_f_group = f_group
                return Action.C
            self.last_window_f_group = f_group
            return Action.D
        denom = max(1e-08, self.T_good - self.T_bad)
        p_coop = (f_group - self.T_bad) / denom
        if majority_pos:
            p_coop = min(1.0, p_coop + 0.15)
        rand_val = random.random()
        self.last_window_f_group = f_group
        if rand_val < p_coop:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    Exploitative Conditional Maintainer (ECM)

    Brief:
    - Probe briefly to find persistent cooperators, then mostly defect to exploit them.
    - Occasionally perform low-frequency maintenance cooperations to sustain conditional cooperators.
    - If exploitation provokes sustained punishment (drop in others' cooperation), attempt a short recovery.
    - If recovery fails or payoff falls below the all-defect baseline, switch to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        P_raw = max(1, min(5, math.floor(r / 10)))
        self.P = min(P_raw, max(0, r - 1))
        if r <= 1:
            self.P = 0
        self.W = min(10, r)
        self.M = max(3, math.floor(r / 10))
        self.alpha = 0.6
        self.delta_drop = 0.25
        self.R = min(3, r)
        self.E = 2
        self.mode = 'PROBE' if self.P > 0 and r > 1 else 'PROBE'
        self.exploit_enter_round = None
        self.probe_marked = False
        self.candidate_cooperators = set()
        self.probe_group_avg = 0.0
        self.recover_remaining = 0
        self.last_mode_change_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k

        def rounds_remaining():
            return r - t

        def safe_mean(arr):
            if arr is None or arr.size == 0:
                return 0.0
            return float(np.mean(arr))

        def group_avg_over_rows(start_row_inclusive, end_row_exclusive):
            if history is None:
                return 0.0
            if start_row_inclusive >= end_row_exclusive:
                return 0.0
            sub = history.opponent_actions[start_row_inclusive:end_row_exclusive, :]
            if sub.size == 0:
                return 0.0
            return float(np.mean(sub))

        def group_avg_last_W():
            if history is None:
                return 0.0
            t_past = history.opponent_actions.shape[0]
            start = max(0, t_past - self.W)
            return group_avg_over_rows(start, t_past)

        def group_avg_probe():
            if history is None or self.P <= 0:
                return 0.0
            available = history.opponent_actions.shape[0]
            end = min(available, self.P)
            if end <= 0:
                return 0.0
            return group_avg_over_rows(0, end)

        def my_avg_payoff_last_W():
            if history is None:
                return 0.0
            t_past = history.my_payoffs.shape[0]
            start = max(0, t_past - self.W)
            slice_ = history.my_payoffs[start:t_past]
            return safe_mean(slice_)

        def i_have_cooperated_in_last(m):
            if history is None:
                return False
            if m <= 0:
                return False
            t_past = history.my_actions.shape[0]
            if t_past == 0:
                return False
            start = max(0, t_past - m)
            return np.any(history.my_actions[start:t_past])
        if t == 0:
            if r <= 1:
                self.mode = 'DEFECT'
                self.last_mode_change_round = 0
                return Action.D
            self.mode = 'PROBE'
            self.last_mode_change_round = 0
            return Action.C
        if rounds_remaining() <= self.E:
            self.mode = 'DEFECT'
            return Action.D
        if not self.probe_marked:
            if self.P > 0 and history.opponent_actions.shape[0] >= self.P:
                probe_rows = self.P
                if probe_rows > 0:
                    opps = history.opponent_actions[0:probe_rows, :]
                    freqs = np.mean(opps, axis=0)
                    self.candidate_cooperators = set((int(i) for i, f in enumerate(freqs) if f >= self.alpha))
                    self.probe_group_avg = float(np.mean(opps)) if opps.size > 0 else 0.0
                else:
                    self.candidate_cooperators = set()
                    self.probe_group_avg = 0.0
                self.probe_marked = True
        if t <= max(0, self.P - 1):
            seen_probe_rounds = min(history.opponent_actions.shape[0], self.P)
            if seen_probe_rounds > 0:
                probe_block = history.opponent_actions[0:seen_probe_rounds, :]
                per_round_sums = np.sum(probe_block, axis=1)
                if np.all(per_round_sums == 0):
                    self.mode = 'DEFECT'
                    self.last_mode_change_round = t
                    return Action.D
            return Action.C
        if self.probe_marked and self.mode == 'PROBE':
            if len(self.candidate_cooperators) >= 1:
                self.mode = 'EXPLOIT'
                self.exploit_enter_round = t
                self.last_mode_change_round = t
            else:
                self.mode = 'DEFECT'
                self.last_mode_change_round = t
        if self.mode == 'DEFECT':
            L = max(1, max(self.P, self.W))
            t_past = history.opponent_actions.shape[0]
            if t_past >= L:
                start = t_past - L
                window_avg = group_avg_over_rows(start, t_past)
                if window_avg >= 0.9:
                    my_avg = my_avg_payoff_last_W()
                    if my_avg <= 1.1:
                        self.mode = 'PROBE'
                        self.probe_marked = False
                        self.candidate_cooperators = set()
                        self.probe_group_avg = 0.0
                        self.last_mode_change_round = t
                        return Action.C
            return Action.D
        if self.recover_remaining > 0 or self.mode == 'RECOVER':
            if self.mode != 'RECOVER':
                self.mode = 'RECOVER'
                self.last_mode_change_round = t
            if self.recover_remaining <= 0:
                self.recover_remaining = self.R
            self.recover_remaining -= 1
            if self.recover_remaining <= 0:
                pass
            return Action.C
        if self.mode == 'EXPLOIT':
            if self.exploit_enter_round is None:
                self.exploit_enter_round = t
            rounds_since_exploit = t - self.exploit_enter_round
            lastW_avg = group_avg_last_W()
            probe_avg = self.probe_group_avg if self.probe_marked else group_avg_probe()
            recent_defected = False
            if history is not None:
                t_past = history.my_actions.shape[0]
                if t_past >= 1 and (not history.my_actions[-1]):
                    recent_defected = True
                elif t_past >= 2 and (not history.my_actions[-1]) and (not history.my_actions[-2]):
                    recent_defected = True
                elif t_past >= 2 and (not history.my_actions[-1] or not history.my_actions[-2]):
                    recent_defected = True
            drop = probe_avg - lastW_avg
            if recent_defected and drop >= self.delta_drop:
                self.mode = 'RECOVER'
                self.recover_remaining = self.R
                self.last_mode_change_round = t
                self.recover_remaining -= 1
                return Action.C
            if self.M > 0 and rounds_since_exploit >= 1 and (rounds_since_exploit % self.M == 0):
                if lastW_avg >= self.alpha:
                    if not i_have_cooperated_in_last(self.M):
                        return Action.C
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    Adaptive Proportional Conditional Cooperator (APC)

    - Starts cooperative.
    - Uses a sliding window to estimate recent cooperation among others.
    - Cooperates when others mostly cooperate (>= TH_high), defects when cooperation is clearly low (<= TH_low).
    - In intermediate band, cooperates with probability proportional to how close others are to TH_high.
    - On observed spikes of defection, issues a finite proportional punishment (pure defection for L rounds).
    - After punishment, tries a short cooperative probe of length R to restore cooperation; if probe fails, punish again.
    - Defects in the final round (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        self.TH_high = 0.75
        self.TH_low = 0.25
        self.E = 1.0 / max(1, self.n)
        self.S = 3.0
        self.R = 1
        self.P = 0
        self.B = False
        self.probe_remaining = 0
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.n
        n_rounds = self.r
        t = int(state.round_number)
        if t == 0 or history is None:
            self.last_action = Action.C
            return Action.C
        if t == n_rounds - 1:
            self.last_action = Action.D
            return Action.D
        W_eff = min(self.W, t)
        if W_eff <= 0:
            self.last_action = Action.C
            return Action.C
        opp_actions = history.opponent_actions
        try:
            recent = np.array(opp_actions[-W_eff:, :], dtype=float)
            denom = max(1, n_players - 1)
            per_round_frac = recent.sum(axis=1) / denom
            f = float(np.mean(per_round_frac)) if per_round_frac.size > 0 else 0.0
            last_row = np.array(opp_actions[-1, :], dtype=float)
            f_last = float(last_row.sum() / denom) if last_row.size > 0 else 0.0
        except Exception:
            self.last_action = Action.D
            return Action.D
        if self.P > 0:
            action = Action.D
            self.P -= 1
            if self.P <= 0:
                self.P = 0
                remaining_after = n_rounds - (t + 1)
                if remaining_after >= 1 and self.R >= 1:
                    self.B = True
                    self.probe_remaining = min(self.R, remaining_after)
                else:
                    self.B = False
                    self.probe_remaining = 0
            self.last_action = action
            return action
        if self.B:
            if f >= self.TH_high:
                self.B = False
                self.probe_remaining = 0
                self.last_action = Action.C
                return Action.C
            if f < self.TH_low:
                L = 1 + math.ceil(self.S * max(0.0, self.TH_high - f))
                remaining_after = n_rounds - t
                max_allowed = max(0, remaining_after - 1)
                L = min(L, max_allowed)
                if L <= 0:
                    self.B = False
                    self.probe_remaining = 0
                    self.last_action = Action.D
                    return Action.D
                self.P = L
                self.B = False
                self.probe_remaining = 0
                self.last_action = Action.D
                return Action.D
            action = Action.C
            self.probe_remaining = max(0, self.probe_remaining - 1)
            if self.probe_remaining <= 0:
                self.B = False
            self.last_action = action
            return action
        if f_last <= self.TH_low:
            L = 1 + math.ceil(self.S * max(0.0, self.TH_high - f_last))
            remaining_after = n_rounds - t
            max_allowed = max(0, remaining_after - 1)
            L = min(L, max_allowed)
            if L <= 0:
                self.last_action = Action.D
                return Action.D
            self.P = L
            self.last_action = Action.D
            return Action.D
        if f >= self.TH_high - self.E:
            self.last_action = Action.C
            return Action.C
        if f <= self.TH_low + self.E:
            self.last_action = Action.D
            return Action.D
        denom_band = self.TH_high - self.TH_low
        if denom_band <= 0:
            action = self.last_action
            self.last_action = action
            return action
        p = (f - self.TH_low) / denom_band
        if p < 0.0:
            p = 0.0
        elif p > 1.0:
            p = 1.0
        if random.random() < p:
            action = Action.C
        else:
            action = Action.D
        self.last_action = action
        return action

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    Opportunistic Conditional Exploiter (OCE)

    Exploitative strategy for the repeated public goods game:
    - Probe early to classify opponents (cooperator / conditional / defector).
    - Enter Exploitation Mode if there is at least one likely cooperator and group cooperation is nontrivial:
        Cooperate except with probability p_exploit (randomized defections), adapting p_exploit upward
        when cooperators tolerate exploitation and downward when they retaliate.
    - Enter Defection Mode otherwise: defect each round, with rare cooperative probes to discover new cooperators.
    - Always defect during the final end_phase (no future inducement possible).
    - Maintain per-opponent statistics and short-term blacklists for betrayers.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.early_phase = max(3, math.floor(self.r / 5))
        self.end_phase = max(3, math.floor(self.r / 5))
        self.W = max(3, math.floor(self.r / 10))
        self.coop_high = 0.6
        self.coop_low = 0.2
        self.resp_threshold = 0.15
        self.p_exploit_initial = 0.3
        self.p_exploit_max = 0.6
        self.p_exploit_min = 0.0
        self.exploit_adjust_step = 0.1
        self.tolerated_drop = 0.08
        self.blacklist_recovery = max(3, math.floor(self.r / 20))
        self.betrayal_threshold = 0.35
        if self.n in (2, 3):
            self.p_exploit_max = max(self.p_exploit_min, self.p_exploit_max - 0.1)
        self.probe_interval = max(10, math.floor(self.r / 10))
        self.p_exploit = float(self.p_exploit_initial)
        self.in_exploit_mode = False
        self.last_p_exploit_change_round = -1
        self.coop_rate_before_change = 0.0
        self.stable_windows = 0
        self.n_opponents = max(0, self.n - 1)
        self.blacklist_until = [0] * self.n_opponents
        self.has_been_in_exploit = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.r <= 5:
            return Action.D
        if state.round_number >= self.r - self.end_phase:
            return Action.D
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        T = int(my_actions.shape[0])
        if self.n_opponents <= 0:
            return Action.D
        opp_int = opp_actions.astype(int)
        C_js = np.sum(opp_int, axis=0)
        r_js = np.array(C_js / max(1, T), dtype=float)
        if T >= 2:
            my_prev = my_actions[:-1].astype(int)
            opp_after = opp_int[1:, :]
            myC_events = int(np.sum(my_prev))
            myD_events = max(0, T - 1 - myC_events)
            if myC_events > 0:
                coop_after_myC = np.sum(opp_after[my_prev.astype(bool), :], axis=0)
                p_j_myC = np.array(coop_after_myC / myC_events, dtype=float)
            else:
                p_j_myC = np.zeros(self.n_opponents, dtype=float)
            if myD_events > 0:
                mask_myD = ~my_prev.astype(bool)
                coop_after_myD = np.sum(opp_after[mask_myD, :], axis=0) if mask_myD.any() else np.zeros(self.n_opponents, dtype=int)
                p_j_myD = np.array(coop_after_myD / myD_events, dtype=float)
            else:
                p_j_myD = np.zeros(self.n_opponents, dtype=float)
        else:
            p_j_myC = np.zeros(self.n_opponents, dtype=float)
            p_j_myD = np.zeros(self.n_opponents, dtype=float)
        blacklisted = [state.round_number < self.blacklist_until[j] for j in range(self.n_opponents)]
        unconditional_defectors = [r_js[j] <= self.coop_low for j in range(self.n_opponents)]
        likely_cooperators = []
        for j in range(self.n_opponents):
            if unconditional_defectors[j]:
                continue
            cond_responsiveness = float(p_j_myC[j] - p_j_myD[j])
            if r_js[j] >= self.coop_high or (cond_responsiveness >= self.resp_threshold and r_js[j] >= 0.3):
                if not blacklisted[j]:
                    likely_cooperators.append(j)
        window = min(self.W, T)
        if window <= 0:
            group_mean = 0.0
        else:
            recent_my = my_actions[-window:].astype(int)
            recent_opp = opp_int[-window:, :]
            total_coops_per_round = np.sum(recent_opp, axis=1) + recent_my
            coop_fracs = total_coops_per_round / float(self.n)
            group_mean = float(np.mean(coop_fracs))
        enter_exploit = len(likely_cooperators) >= 1 and group_mean >= 0.3
        if enter_exploit:
            if not self.in_exploit_mode:
                self.in_exploit_mode = True
                self.has_been_in_exploit = True
                self.p_exploit = float(self.p_exploit_initial)
                if len(likely_cooperators) > 0 and window > 0:
                    recent_coops = opp_int[-window:, :]
                    sums = np.array([np.sum(recent_coops[:, j]) for j in likely_cooperators], dtype=float)
                    coop_rate_now = float(np.mean(sums / float(window)))
                else:
                    coop_rate_now = 0.0
                self.coop_rate_before_change = coop_rate_now
                self.last_p_exploit_change_round = state.round_number
                self.stable_windows = 0
        else:
            self.in_exploit_mode = False
        if self.in_exploit_mode:
            if len(likely_cooperators) > 0 and window > 0:
                recent_coops = opp_int[-window:, :]
                sums = np.array([np.sum(recent_coops[:, j]) for j in likely_cooperators], dtype=float)
                coop_rate_now = float(np.mean(sums / float(window)))
            else:
                coop_rate_now = 0.0
            if coop_rate_now + self.tolerated_drop < self.coop_rate_before_change:
                old_p = self.p_exploit
                self.p_exploit = max(self.p_exploit_min, self.p_exploit - self.exploit_adjust_step)
                self.coop_rate_before_change = coop_rate_now
                self.last_p_exploit_change_round = state.round_number
                self.stable_windows = 0
            else:
                if abs(coop_rate_now - self.coop_rate_before_change) <= self.tolerated_drop:
                    self.stable_windows += 1
                else:
                    self.stable_windows = 0
                if self.stable_windows >= 2 and self.p_exploit < self.p_exploit_max:
                    self.p_exploit = min(self.p_exploit_max, self.p_exploit + self.exploit_adjust_step)
                    self.coop_rate_before_change = coop_rate_now
                    self.last_p_exploit_change_round = state.round_number
                    self.stable_windows = 0
            for j in range(self.n_opponents):
                if blacklisted[j] or unconditional_defectors[j]:
                    continue
                if r_js[j] >= 0.3:
                    delta = float(p_j_myC[j] - p_j_myD[j])
                    if delta >= self.betrayal_threshold:
                        self.blacklist_until[j] = state.round_number + self.blacklist_recovery
                        self.p_exploit = max(self.p_exploit_min, self.p_exploit - self.exploit_adjust_step / 2.0)
                        self.coop_rate_before_change = 0.0
                        self.stable_windows = 0
            draw = random.random()
            if draw < self.p_exploit:
                return Action.D
            else:
                return Action.C
        rounds_remaining = self.r - state.round_number
        if state.round_number % self.probe_interval == 0 and rounds_remaining > self.end_phase + 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    Collective Conditional Cooperation with Finite Punishment (CCCFP).

    - Cooperates initially (unless very short horizon).
    - If the previous round had a strict majority cooperating, cooperate.
    - If the previous round had sub-majority cooperation, initiate a finite P-round
      group punishment (everyone defects for P rounds). Punishment is shared.
    - If a majority cooperation is observed at the start of a round, clear any
      ongoing punishment immediately and cooperate (forgiveness).
    - In the final E rounds (E = 3) for r > 3, always defect (endgame window).
    - For very short games (r <= 3) always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.E = 3
        self.m = math.ceil(self.n / 2)
        self.P = max(1, math.ceil(self.r / 10))
        self.punishment_counter = 0
        self.last_round_cooperators = 0
        self.my_last_action = Action.D if self.r <= 3 else Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        if self.r <= 3:
            self.my_last_action = Action.D
            return Action.D
        if self.r > 3 and t > self.r - self.E:
            self.my_last_action = Action.D
            return Action.D
        if state.round_number > 0 and history is not None:
            prev = state.round_number - 1
            try:
                opp_sum = int(np.sum(history.opponent_actions[prev, :]))
            except Exception:
                opp_sum = int(np.sum(history.opponent_actions[prev]) if history.opponent_actions.size > 0 else 0)
            try:
                my_prev = int(history.my_actions[prev])
            except Exception:
                my_prev = 0
            last_cooperators = opp_sum + my_prev
            self.last_round_cooperators = last_cooperators
        else:
            last_cooperators = 0
        if last_cooperators >= self.m and self.punishment_counter != 0:
            self.punishment_counter = 0
        if self.punishment_counter > 0:
            decision = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.my_last_action = decision
            return decision
        if t == 1:
            self.my_last_action = Action.C
            return Action.C
        if last_cooperators >= self.m:
            self.my_last_action = Action.C
            return Action.C
        else:
            self.punishment_counter = self.P
            decision = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.my_last_action = decision
            return decision

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    Exploitative Conditional Free-rider (ECF)

    - Probes occasionally with small probability epsilon.
    - Exploits reliably cooperating opponents by defecting when others cooperate.
    - Detects retaliators (opponents who reduce cooperation after being exploited) and attempts a short repair.
    - Falls back to permanent defection if group cooperation collapses or repair fails.
    - Always cooperates in the very first round; always defects in the final T_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.perm_defect = False
        self.repair_in_progress = False
        self.repair_remaining = 0
        self.pre_repair_F_group = None
        self.n_players = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        n = self.n_players
        t = state.round_number
        W = min(10, max(1, math.floor(r / 5)))
        eps = min(0.1, 3.0 / max(1, r))
        TH_high = 0.9
        TH_collapse = 0.2
        K_repair = 2
        T_end = min(2, r)
        if t == 0 or history is None:
            return Action.C
        opp_actions = history.opponent_actions
        rounds_so_far = opp_actions.shape[0]
        if t >= r - T_end:
            return Action.D
        window_size = min(W, rounds_so_far)
        window_start = max(0, rounds_so_far - window_size)
        if window_size <= 0:
            f_js = np.zeros(opp_actions.shape[1], dtype=float)
        else:
            recent_slice = opp_actions[window_start:rounds_so_far, :]
            f_js = np.array(np.mean(recent_slice, axis=0), dtype=float)
        if f_js.size == 0:
            F_group = 0.0
        else:
            F_group = float(np.mean(f_js))
        if F_group < TH_collapse:
            self.perm_defect = True
            return Action.D
        if self.perm_defect:
            return Action.D

        def detect_retaliators():
            retaliators = set()
            my_actions = history.my_actions
            num_opponents = opp_actions.shape[1]
            for t0 in range(rounds_so_far):
                if not bool(my_actions[t0]):
                    start_b = max(0, t0 - W)
                    end_b = t0
                    start_a = t0 + 1
                    end_a = min(rounds_so_far, t0 + 1 + W)
                    if end_b <= start_b or end_a <= start_a:
                        continue
                    before_slice = opp_actions[start_b:end_b, :]
                    after_slice = opp_actions[start_a:end_a, :]
                    f_before = np.array(np.mean(before_slice, axis=0), dtype=float)
                    f_after = np.array(np.mean(after_slice, axis=0), dtype=float)
                    drops = f_before - f_after >= 0.3
                    for j in range(num_opponents):
                        if drops[j]:
                            retaliators.add(j)
            return retaliators
        retaliators = detect_retaliators()
        if retaliators and (not self.repair_in_progress):
            self.repair_in_progress = True
            self.repair_remaining = K_repair
            self.pre_repair_F_group = F_group
        if self.repair_in_progress:
            if self.repair_remaining > 0:
                self.repair_remaining -= 1
                return Action.C
            else:
                rounds_so_far_new = opp_actions.shape[0]
                window_size_new = min(W, rounds_so_far_new)
                window_start_new = max(0, rounds_so_far_new - window_size_new)
                if window_size_new <= 0:
                    F_group_after = 0.0
                else:
                    recent_slice_new = opp_actions[window_start_new:rounds_so_far_new, :]
                    F_group_after = float(np.mean(np.mean(recent_slice_new, axis=0)) if recent_slice_new.size else 0.0)
                if self.pre_repair_F_group is None or F_group_after <= self.pre_repair_F_group:
                    self.perm_defect = True
                self.repair_in_progress = False
                self.repair_remaining = 0
                self.pre_repair_F_group = None
                if self.perm_defect:
                    return Action.D
        last_round_coop_count = int(np.sum(opp_actions[-1, :])) if opp_actions.shape[0] >= 1 else 0
        if last_round_coop_count == n - 1:
            if random.random() < eps:
                return Action.C
            return Action.D
        high_coop_count = int(np.sum(f_js >= TH_high)) if f_js.size > 0 else 0
        if high_coop_count >= math.ceil((n - 1) / 2):
            if random.random() < eps:
                return Action.C
            return Action.D
        if random.random() < eps:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    Collective-Conditional Proportional Reciprocity (CCPR)

    Start cooperative, sustain cooperation when a recent substantial fraction of
    other players has been reliably cooperative (threshold adapts to k/n),
    punish proportionally and briefly when previously reliable cooperators defect,
    forgive after short punishments, and defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r))
        self.alpha = 0.5 + 0.3 * (1.0 - self.k / float(self.n))
        self.beta = 0.2
        self.P_max = min(3, max(1, int(math.floor(self.r / 10))))
        self.P = 0
        opp_count = max(0, self.n - 1)
        self.prev_reliable = np.zeros((opp_count,), dtype=bool)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        last_round_idx = max(0, self.r - 1)
        if t == 0 or history is None:
            opp_count = max(0, self.n - 1)
            if self.prev_reliable.shape[0] != opp_count:
                self.prev_reliable = np.zeros((opp_count,), dtype=bool)
            return Action.C
        if t == last_round_idx:
            return Action.D
        opp_actions = history.opponent_actions
        rounds_played = int(opp_actions.shape[0])
        opp_count = int(opp_actions.shape[1]) if opp_actions.ndim > 1 else 0
        if self.prev_reliable.shape[0] != opp_count:
            self.prev_reliable = np.zeros((opp_count,), dtype=bool)
        last_m = min(self.w, rounds_played)
        start_idx = rounds_played - last_m
        if opp_count > 0 and last_m > 0:
            recent = opp_actions[start_idx:rounds_played, :]
            s_j = np.sum(recent, axis=0).astype(int)
        else:
            s_j = np.zeros((opp_count,), dtype=int)
        threshold_count = int(math.ceil(self.alpha * last_m)) if last_m > 0 else 1
        current_reliable = s_j >= threshold_count
        F = float(current_reliable.sum()) / float(opp_count) if opp_count > 0 else 0.0
        last_round_actions = opp_actions[-1, :] if rounds_played > 0 and opp_count > 0 else np.zeros((opp_count,), dtype=bool)
        f_last = float(np.mean(last_round_actions)) if opp_count > 0 else 0.0
        if opp_count > 0:
            new_defectors = int(np.sum(self.prev_reliable & ~last_round_actions))
        else:
            new_defectors = 0
        action = Action.D
        if self.P > 0:
            action = Action.D
            self.P = max(0, self.P - 1)
        elif F > self.alpha:
            action = Action.C
        elif F <= self.beta:
            action = Action.D
        elif f_last > 0.5:
            action = Action.C
        else:
            action = Action.D
        if new_defectors > 0 and self.P == 0:
            self.P = min(self.P_max, 1 + int(new_defectors))
            if action == Action.C:
                action = Action.D
                self.P = max(0, self.P - 1)
        self.prev_reliable = np.array(current_reliable, dtype=bool)
        return action

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    Adaptive Exploiter with Controlled Forgiveness.

    - Uses moving-window statistics of opponents' contributions to decide whether to exploit,
      sustain cooperation, punish persistent defectors, or probe for recovery.
    - Parameters (window size, thresholds, probabilities, memory) are derived from game_description.
    - Uses only observed history (my actions/payoffs and opponents' actions/payoffs).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.w = int(min(10, max(3, math.floor(r / 10))))
        self.H = 0.7
        self.L = 0.3
        self.P_e = 0.8
        self.P_g = 0.8
        self.epsilon = 0.05
        self.M = 3
        self.T_reset = int(max(5, self.w))
        k_over_n = float(k) / float(n) if n > 0 else 0.0
        if k_over_n > 0.7:
            self.H = max(0.0, self.H - 0.1)
            self.P_g = min(1.0, self.P_g + 0.1)
        if k_over_n < 0.4:
            self.H = min(1.0, self.H + 0.1)
            self.P_g = max(0.0, self.P_g - 0.1)
        self.punished_for = None
        self.suspend_exploit_until = -1
        self.det_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = max(0, n - 1)
        if self.punished_for is None:
            self.punished_for = [0] * m
        if state.round_number == r - 1:
            return Action.D
        rounds_played = int(len(history.my_actions))
        if rounds_played == 0:
            return Action.C
        window = int(min(self.w, rounds_played))
        opp_actions = np.asarray(history.opponent_actions)
        if opp_actions.ndim == 1:
            opp_actions = opp_actions.reshape((opp_actions.shape[0], 1))
        recent_slice = opp_actions[-window:, :]
        coop_counts = np.sum(recent_slice.astype(np.int64), axis=0)
        coop_rates = coop_counts / float(window) if window > 0 else np.zeros(m, dtype=float)
        group_recent = float(np.mean(coop_rates)) if m > 0 else 0.0
        last_round_opp = opp_actions[-1, :] if rounds_played >= 1 else np.zeros((m,), dtype=bool)
        coop_last = int(np.sum(last_round_opp.astype(np.int64)))
        persistent = [False] * m
        for j in range(m):
            is_low_rate = coop_rates[j] < 0.2
            if is_low_rate:
                if rounds_played >= self.M:
                    last_M = opp_actions[-self.M:, j]
                    if int(np.sum(last_M.astype(np.int64))) == 0:
                        persistent[j] = True
                    else:
                        persistent[j] = False
                else:
                    persistent[j] = True
            else:
                persistent[j] = False
        num_persistent = sum((1 for p in persistent if p))
        for j in range(m):
            if coop_rates[j] > 0.4 and self.punished_for[j] != 0:
                self.punished_for[j] = 0
        can_check_T = rounds_played >= self.T_reset
        if can_check_T:
            recent_my = np.asarray(history.my_actions)[-self.T_reset:]
            recent_opp = opp_actions[-self.T_reset:, :]
            all_defect_my = int(np.sum(recent_my.astype(np.int64))) == 0
            all_defect_opp = int(np.sum(recent_opp.astype(np.int64))) == 0
            if all_defect_my and all_defect_opp:
                if random.random() < 0.2:
                    return Action.C
        currently_punishing = any((pf > 0 for pf in self.punished_for))
        suspended = False
        if history.my_actions[-1] == False and rounds_played >= 2:
            before_window = int(min(self.w, rounds_played - 1))
            if before_window >= 1:
                before_slice = opp_actions[-(before_window + 1):-1, :]
                before_counts = np.sum(before_slice.astype(np.int64), axis=0)
                before_rates = before_counts / float(before_window)
                group_before = float(np.mean(before_rates)) if m > 0 else 0.0
                drop = group_before - group_recent
                if drop > 0.15:
                    self.suspend_exploit_until = state.round_number + 2
                    suspended = True
                    return Action.C
        if state.round_number < self.suspend_exploit_until:
            return Action.C
        if num_persistent > 1:
            for j in range(m):
                if persistent[j]:
                    if self.punished_for[j] < self.M:
                        self.punished_for[j] += 1
                    else:
                        self.punished_for[j] = 0
                else:
                    self.punished_for[j] = 0
            return Action.D
        if num_persistent == 1:
            victim_idx = next((j for j in range(m) if persistent[j]), None)
            if victim_idx is not None:
                if self.punished_for[victim_idx] < self.M:
                    self.punished_for[victim_idx] += 1
                    return Action.D
                else:
                    self.punished_for[victim_idx] = 0
        chosen_action = None
        if group_recent >= self.H:
            if num_persistent >= 1:
                for j in range(m):
                    if persistent[j]:
                        if self.punished_for[j] < self.M:
                            self.punished_for[j] += 1
                chosen_action = Action.D
            elif random.random() < self.P_e:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        elif self.L <= group_recent < self.H:
            half_threshold = math.ceil(m / 2) if m > 0 else 0
            num_good = int(np.sum((coop_rates >= 0.5).astype(np.int64)))
            cond_half_players = num_good >= half_threshold
            cond_recent_round = coop_last >= math.ceil((n - 1) / 2)
            if cond_half_players or cond_recent_round:
                if random.random() < self.P_g:
                    chosen_action = Action.C
                else:
                    chosen_action = Action.D
            else:
                chosen_action = Action.D
        elif random.random() < self.epsilon:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        if currently_punishing:
            if chosen_action == Action.C:
                chosen_action = Action.D
            for j in range(m):
                if self.punished_for[j] > 0:
                    if self.punished_for[j] < self.M:
                        self.punished_for[j] += 1
                    else:
                        self.punished_for[j] = 0
        self.det_counter = (self.det_counter + 1) % 10
        return chosen_action

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    Opportunistic Classifier-and-Exploit (OCE)

    Exploit clear unconditional cooperators, sustain cooperation with responsive reciprocators
    when profitable, probe occasionally, and defect in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r <= 3:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        W = min(10, max(1, math.floor(self.r / 5)))
        epsilon = min(0.15, 3.0 / max(10, self.r))
        K_end = min(3, math.floor(self.r / 10))
        M_prey = max(1, math.ceil((self.n - 1) / 3))
        T_coop = max(1, math.ceil((self.n - 1) / 2))
        F = min(3, W)
        t_now = state.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        my_payoffs = history.my_payoffs
        n_opponents = self.n - 1
        win_start = max(0, t_now - W)
        window_len = t_now - win_start
        if window_len <= 0:
            c_rates = np.zeros(n_opponents, dtype=float)
            responsiveness = np.zeros(n_opponents, dtype=float)
        else:
            window_opp = opp_actions[win_start:t_now, :]
            c_rates = np.array(np.mean(window_opp.astype(float), axis=0))
            if window_len <= 1:
                responsiveness = np.zeros(n_opponents, dtype=float)
            else:
                idx_after = list(range(win_start + 1, t_now))
                my_prev = my_actions[np.array(idx_after) - 1]
                opp_after = opp_actions[np.array(idx_after), :]
                responsiveness = np.zeros(n_opponents, dtype=float)
                for j in range(n_opponents):
                    opp_j_after = opp_after[:, j]
                    mask_C = my_prev.astype(bool)
                    mask_D = (~my_prev).astype(bool)
                    if mask_C.sum() > 0:
                        p_after_C = float(np.mean(opp_j_after[mask_C].astype(float)))
                    else:
                        p_after_C = c_rates[j]
                    if mask_D.sum() > 0:
                        p_after_D = float(np.mean(opp_j_after[mask_D].astype(float)))
                    else:
                        p_after_D = c_rates[j]
                    responsiveness[j] = p_after_C - p_after_D
        RESP_LOW = 0.1
        RESP_HIGH = 0.25
        classification = np.array(['Neutral'] * n_opponents, dtype=object)
        for j in range(n_opponents):
            if c_rates[j] >= 0.9 and responsiveness[j] <= RESP_LOW:
                classification[j] = 'Unconditional'
            elif c_rates[j] <= 0.2:
                classification[j] = 'Defector'
            elif c_rates[j] >= 0.5 and responsiveness[j] >= RESP_HIGH:
                classification[j] = 'Reciprocator'
            else:
                classification[j] = 'Neutral'
        if F >= 1 and t_now >= F:
            last_F = opp_actions[t_now - F:t_now, :]
            for j in range(n_opponents):
                if classification[j] == 'Reciprocator':
                    if np.all(last_F[:, j].astype(bool)):
                        classification[j] = 'Neutral'
        num_unconditional = int(np.sum(classification == 'Unconditional'))
        num_defectors = int(np.sum(classification == 'Defector'))
        num_reciprocator = int(np.sum(classification == 'Reciprocator'))
        num_neutral = int(np.sum(classification == 'Neutral'))
        final_round = t_now == self.r - 1
        in_endgame_zone = K_end > 0 and t_now >= self.r - K_end
        if t_now >= 1:
            last_round_others = int(np.sum(opp_actions[-1, :].astype(int)))
            contributors_idx = np.where(opp_actions[-1, :].astype(bool))[0]
            contributors = int(len(contributors_idx))
        else:
            last_round_others = 0
            contributors = 0
            contributors_idx = np.array([], dtype=int)
        recent_punished = False
        if window_len >= 1 and t_now >= 2:
            recent_payoffs = my_payoffs[win_start:t_now]
            if recent_payoffs.size > 0:
                recent_mean = float(np.mean(recent_payoffs))
                for t in range(win_start + 1, t_now):
                    if not bool(my_actions[t - 1]):
                        payoff_t = float(my_payoffs[t])
                        if recent_mean - payoff_t >= 0.25:
                            recent_punished = True
                            break
        if final_round:
            return Action.D
        if num_unconditional >= M_prey:
            chosen = Action.D
        else:
            will_cooperate_by_rule3 = False
            if contributors >= T_coop:
                good_contrib = int(np.sum(~(classification[contributors_idx] == 'Defector')))
                if contributors > 0 and good_contrib * 2 >= contributors:
                    will_cooperate_by_rule3 = True
            if recent_punished and (not will_cooperate_by_rule3):
                if contributors >= max(1, T_coop - 1):
                    good_contrib = int(np.sum(~(classification[contributors_idx] == 'Defector'))) if contributors > 0 else 0
                    if contributors > 0 and good_contrib * 2 >= contributors:
                        will_cooperate_by_rule3 = True
            if in_endgame_zone:
                if will_cooperate_by_rule3:
                    chosen = Action.C
                else:
                    chosen = Action.D
            else:
                chosen = Action.C if will_cooperate_by_rule3 else Action.D
        if random.random() < epsilon:
            chosen = Action.C if chosen == Action.D else Action.D
        if recent_punished and chosen == Action.D and (last_round_others >= 1) and (random.random() < epsilon / 2):
            chosen = Action.C
        return chosen

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR)

    - Starts cooperative, defects in the final round.
    - Uses a sliding window W to track recent cooperation rates (per-player and group).
    - If group cooperation falls below thresholds, executes short, proportional punishment phases.
    - Forgives quickly when the group response improves.
    - Adjusts generosity and punishment length based on k/n and small-group considerations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.E = 3
        self.p_low = 0.2
        self.T_base = 0.5
        self.g = 0.1 if self.k / max(1, self.n) >= 0.6 else 0.0
        self.L_base = 2
        kn_ratio = self.k / max(1.0, self.n)
        if kn_ratio >= 0.6:
            self.L_adj = 1
        elif kn_ratio <= 0.4:
            self.L_adj = 3
        else:
            self.L_adj = self.L_base
        self._punish_until: None | int = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        last_round_index = max(0, self.r - 1)
        if t == 0 or history is None:
            self._punish_until = None
            return Action.C
        if t == last_round_index:
            self._punish_until = None
            return Action.D
        start_idx = max(0, t - self.W)
        end_idx = t - 1
        num_past_rounds = end_idx - start_idx + 1
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        prev_idx = t - 1
        coop_prev = 0
        if prev_idx < my_actions.shape[0]:
            coop_prev += 1 if bool(my_actions[prev_idx]) else 0
        if prev_idx < opp_actions.shape[0]:
            coop_prev += int(np.sum(opp_actions[prev_idx, :])) if opp_actions.shape[1] > 0 else 0
        frac_prev = coop_prev / max(1, self.n)
        if num_past_rounds <= 0:
            avg_recent = 0.0
        else:
            fracs = []
            for ridx in range(start_idx, end_idx + 1):
                c = 0
                if ridx < my_actions.shape[0]:
                    c += 1 if bool(my_actions[ridx]) else 0
                if ridx < opp_actions.shape[0] and opp_actions.shape[1] > 0:
                    c += int(np.sum(opp_actions[ridx, :]))
                fracs.append(c / max(1, self.n))
            avg_recent = float(np.mean(np.array(fracs))) if len(fracs) > 0 else 0.0
        coop_rates = []
        if num_past_rounds <= 0:
            self_rate = 0.0
        else:
            slice_start = start_idx
            slice_end = end_idx + 1
            my_slice = my_actions[slice_start:slice_end] if slice_start < my_actions.shape[0] else np.array([], dtype=bool)
            if my_slice.size == 0:
                self_rate = 0.0
            else:
                self_rate = float(np.mean(my_slice.astype(float)))
        coop_rates.append(self_rate)
        if num_past_rounds <= 0 or opp_actions.shape[1] == 0:
            for _ in range(self.n - 1):
                coop_rates.append(0.0)
        else:
            rows_available = opp_actions[start_idx:end_idx + 1, :] if start_idx < opp_actions.shape[0] else np.zeros((0, opp_actions.shape[1]), dtype=bool)
            if rows_available.shape[0] == 0:
                for _ in range(opp_actions.shape[1]):
                    coop_rates.append(0.0)
            else:
                col_means = np.mean(rows_available.astype(float), axis=0)
                for v in col_means:
                    coop_rates.append(float(v))
            while len(coop_rates) < self.n:
                coop_rates.append(0.0)
        persistent_defectors = sum((1 for rate in coop_rates if rate <= self.p_low))
        kn_ratio = self.k / max(1.0, self.n)
        T = self.T_base - (self.g if kn_ratio >= 0.6 else 0.0)
        if self.n <= 3:
            T = T - 0.05
        if T < 0.0:
            T = 0.0
        if T > 1.0:
            T = 1.0
        remaining_rounds = self.r - t
        if remaining_rounds <= self.E:
            if frac_prev >= 0.75 or avg_recent >= 0.8:
                self._punish_until = None
                return Action.C
            else:
                self._punish_until = None
                return Action.D
        if persistent_defectors >= max(0, self.n - 2):
            self._punish_until = None
            return Action.D
        if frac_prev >= T or avg_recent >= 0.8:
            self._punish_until = None
            return Action.C
        if kn_ratio >= 0.6:
            L = 1
        elif kn_ratio <= 0.4:
            L = 3
        else:
            L = self.L_base
        max_punish_until = max(0, last_round_index - 1)
        if self._punish_until is not None and t <= self._punish_until:
            return Action.D
        if self._punish_until is not None and t > self._punish_until:
            self._punish_until = None
            if frac_prev >= T or avg_recent >= 0.6:
                return Action.C
        tentative_until = t + L - 1
        if tentative_until > max_punish_until:
            tentative_until = max_punish_until
        if tentative_until < t:
            self._punish_until = None
            return Action.D
        self._punish_until = tentative_until
        return Action.D

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    Adaptive Selective Exploiter (ASE)

    Probes initially, sustains cooperation with responsive partners, takes
    occasional one-shot defections to extract extra payoff, adapts exploitation
    frequency based on observed retaliation, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = int(min(8, max(3, math.floor(self.r / 10) + 1)))
        m0 = int(max(4, math.ceil(5 * float(self.n) / float(max(self.k, 1e-09)))))
        self.M0 = m0
        self.M = int(m0)
        self.last_exploit_round = -1
        self.f_before_exploit = None
        self.recover_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if self.r <= 1:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D
        rounds_played = int(history.opponent_actions.shape[0])
        opponents_count = int(history.opponent_actions.shape[1])
        if opponents_count <= 0:
            return Action.D
        L = min(self.W, rounds_played)
        coop_rates = []
        if L > 0:
            recent = history.opponent_actions[-L:, :]
            coop_counts = np.sum(recent, axis=0).astype(float)
            coop_rates = (coop_counts / float(L)).tolist()
        else:
            coop_rates = [0.0 for _ in range(opponents_count)]
        f = float(np.mean(np.array(coop_rates, dtype=float))) if coop_rates else 0.0
        reliable_cooperators = {j for j, rate in enumerate(coop_rates) if rate >= 0.8}
        likely_defectors = {j for j, rate in enumerate(coop_rates) if rate <= 0.2}
        if self.last_exploit_round >= 0 and self.last_exploit_round == t - 1 and (self.f_before_exploit is not None):
            delta = 0.1
            if f < self.f_before_exploit - delta:
                newM = int(min(self.M * 2, max(1, self.r)))
                self.M = newM
                self.recover_rounds = self.W
            else:
                self.M = int(max(self.M - 1, 2))
            self.f_before_exploit = None
        final_round = self.r - 1
        penultimate_round = max(0, self.r - 2)
        if t == final_round:
            return Action.D
        if t == penultimate_round:
            if f >= 0.6 and self.M <= 3:
                return Action.D
        if f < 0.25:
            return Action.D
        if len(reliable_cooperators) > 0:
            if len(reliable_cooperators) + len(likely_defectors) == opponents_count and len(likely_defectors) > 0:
                return Action.D
        if len(likely_defectors) >= math.ceil(opponents_count / 2):
            return Action.D
        coop_majority_needed = math.ceil(opponents_count / 2)
        if len(reliable_cooperators) >= coop_majority_needed:
            if self.recover_rounds > 0:
                self.recover_rounds = max(0, self.recover_rounds - 1)
                return Action.C
            if self.last_exploit_round >= 0 and t - self.last_exploit_round >= self.M:
                self.last_exploit_round = t
                self.f_before_exploit = f
                return Action.D
            return Action.C
        if f >= 0.5:
            if self.M < self.M0:
                self.M = int(self.M0)
            if self.recover_rounds > 0:
                self.recover_rounds = max(0, self.recover_rounds - 1)
                return Action.C
            if self.last_exploit_round >= 0 and t - self.last_exploit_round >= self.M:
                self.last_exploit_round = t
                self.f_before_exploit = f
                return Action.D
            return Action.C
        if 0.25 <= f < 0.5:
            probe_interval = max(1, math.ceil(self.W))
            if t % probe_interval == 0:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    - Learns per-opponent tendencies (pure cooperator, conditional/reciprocal, punisher, defector)
      using a recent window of history.
    - Estimates how much my cooperation increases others' cooperation (influence_est).
    - Exploits pure cooperators and generally defects in endgame.
    - Cooperates only when the expected future benefit (via induced cooperation) outweighs the
      immediate cost; otherwise seeds cooperation rarely to probe/maintain conditionals.
    - Uses short calibrated punishments when it detects systematic exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.last_punish_round = -9999
        self.pure_coop_thresh = 0.95
        self.conditional_thresh = 0.2
        self.punisher_thresh = 0.6
        self.exploit_thresh = 0.6
        self.low_coop_threshold = 0.1
        self.defector_thresh = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        MPCR = k / n
        t = int(state.round_number)
        R = max(0, r - (t + 1))
        if t == 0 or history is None or history.my_actions.size == 0:
            if r >= 20:
                return Action.C
            if r >= 5:
                return Action.C
            return Action.D
        if t >= r - 1 or R <= 2:
            self.punishment_timer = 0
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        num_opponents = opp_actions.shape[1]
        W = min(10, max(3, r // 10))
        start_idx = max(0, t - W)
        trans_start = max(1, t - W)
        recent_opp = opp_actions[start_idx:t, :] if t > start_idx else np.zeros((0, num_opponents), dtype=bool)
        recent_len = recent_opp.shape[0]
        coop_counts = recent_opp.sum(axis=0).astype(float)
        p_j = np.zeros(num_opponents, dtype=float)
        if recent_len > 0:
            p_j = coop_counts / float(recent_len)
        trans_range = list(range(trans_start, t))
        coop_after_my_C_counts = np.zeros(num_opponents, dtype=float)
        coop_after_my_D_counts = np.zeros(num_opponents, dtype=float)
        occ_after_my_C = 0.0
        occ_after_my_D = 0.0
        for i in trans_range:
            prev_my = bool(my_actions[i - 1])
            row = opp_actions[i, :]
            if prev_my:
                coop_after_my_C_counts += row.astype(float)
                occ_after_my_C += 1.0
            else:
                coop_after_my_D_counts += row.astype(float)
                occ_after_my_D += 1.0
        coop_after_my_C_frac = np.zeros(num_opponents, dtype=float)
        coop_after_my_D_frac = np.zeros(num_opponents, dtype=float)
        if occ_after_my_C > 0:
            coop_after_my_C_frac = coop_after_my_C_counts / occ_after_my_C
        if occ_after_my_D > 0:
            coop_after_my_D_frac = coop_after_my_D_counts / occ_after_my_D
        is_pure_cooperator = p_j >= self.pure_coop_thresh
        is_conditional = coop_after_my_C_frac - coop_after_my_D_frac >= self.conditional_thresh
        prob_defect_after_my_D = 1.0 - coop_after_my_D_frac
        is_punisher = prob_defect_after_my_D >= self.punisher_thresh
        is_defector = (p_j <= self.defector_thresh) & ~is_conditional & ~is_pure_cooperator
        pure_count = int(is_pure_cooperator.sum())
        conditional_count = int(is_conditional.sum())
        punisher_count = int(is_punisher.sum())
        defector_count = int(is_defector.sum())
        if recent_len > 0:
            p_others_recent = float(np.mean(p_j))
        else:
            last_round = opp_actions[-1, :] if opp_actions.shape[0] > 0 else np.zeros(num_opponents, dtype=bool)
            p_others_recent = float(np.mean(last_round.astype(float))) if num_opponents > 0 else 0.0
        avg_coop_after_C = 0.0
        avg_coop_after_D = 0.0
        if occ_after_my_C > 0:
            frac_list = []
            for i in trans_range:
                if my_actions[i - 1]:
                    frac_list.append(float(opp_actions[i, :].mean()))
            if len(frac_list) > 0:
                avg_coop_after_C = float(np.mean(np.array(frac_list, dtype=float)))
        if occ_after_my_D > 0:
            frac_list = []
            for i in trans_range:
                if not my_actions[i - 1]:
                    frac_list.append(float(opp_actions[i, :].mean()))
            if len(frac_list) > 0:
                avg_coop_after_D = float(np.mean(np.array(frac_list, dtype=float)))
        influence_est = avg_coop_after_C - avg_coop_after_D
        if occ_after_my_C + occ_after_my_D <= 0:
            influence_est = 0.0
        if p_others_recent <= self.low_coop_threshold:
            return Action.D
        if pure_count > 0:
            return Action.D
        punisher_fraction = punisher_count / float(num_opponents) if num_opponents > 0 else 0.0
        if p_others_recent >= self.exploit_thresh and punisher_fraction <= 0.3:
            return Action.D
        exploited_mask = coop_after_my_C_frac + 0.15 < coop_after_my_D_frac
        exploited_count = int(exploited_mask.sum())
        exploited_fraction = exploited_count / float(num_opponents) if num_opponents > 0 else 0.0
        if exploited_fraction >= 0.3:
            T_pun = min(3, max(1, math.ceil(max(1, r - (t + 1)) / 10.0)))
            self.punishment_timer = int(T_pun) - 1
            self.last_punish_round = t
            return Action.D
        expected_future_net_gain = MPCR * influence_est * R - 1.0
        if expected_future_net_gain > 0.0:
            return Action.C
        if influence_est > 0.0:
            p_seed = min(0.25, 0.5 * float(influence_est))
            if random.random() < p_seed:
                return Action.C
        conditional_fraction = conditional_count / float(num_opponents) if num_opponents > 0 else 0.0
        if conditional_fraction >= 0.2 and punisher_fraction >= 0.1:
            if recent_len > 0 and conditional_count > 0:
                cond_indices = np.where(is_conditional)[0]
                cond_coop = recent_opp[:, cond_indices].mean() if cond_indices.size > 0 else 0.0
                cond_recent = float(cond_coop)
            else:
                cond_recent = 0.0
            coop_prob = 0.8 if cond_recent >= 0.5 else 0.5
            if random.random() < coop_prob:
                return Action.C
            return Action.D
        if occ_after_my_C + occ_after_my_D < max(2, W // 2):
            p_probe = 0.25
            if random.random() < p_probe:
                return Action.C
            return Action.D
        if p_others_recent >= 0.5 and punisher_fraction <= 0.4:
            return Action.D
        if random.random() < 0.05:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    Exploit-or-Invest strategy for the N-player Public Goods Game.

    - Defects by default, cooperates only when measured responsiveness () implies
      that the future gain from raising others' cooperation outweighs the immediate cost.
    - Uses a rolling window of recent rounds to estimate how others respond to this
      player's cooperations vs defections (mean_q_after_C, mean_q_after_D).
    - Adds small randomized probing to detect changes in opponent behavior.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(10, max(1, self.r))
        self.epsilon_probe = 0.05
        self.epsilon_exploit = 0.01
        self.delta_min = 0.02
        self.retaliation_drop_thresh = 0.15
        self.retaliation_penalty = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r = self.r
        k = self.k
        if n <= 1:
            return Action.D
        if t >= r - 1:
            return Action.D
        if t == 0 or history is None:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D

        def opponent_coop_fraction(idx: int) -> float:
            row = history.opponent_actions[idx, :]
            denom = max(1, row.shape[0])
            return float(np.sum(row) / denom)
        current_mean_q = 0.0
        if t - 1 >= 0:
            current_mean_q = opponent_coop_fraction(t - 1)
        window_start = max(0, t - self.W)
        q_after_C = []
        q_after_D = []
        for s in range(window_start, t - 1):
            my_action_s = bool(history.my_actions[s])
            q_s1 = opponent_coop_fraction(s + 1)
            if my_action_s:
                q_after_C.append(q_s1)
            else:
                q_after_D.append(q_s1)
        if len(q_after_C) > 0:
            mean_q_after_C = float(np.mean(q_after_C))
        else:
            mean_q_after_C = current_mean_q
        if len(q_after_D) > 0:
            mean_q_after_D = float(np.mean(q_after_D))
        else:
            mean_q_after_D = current_mean_q
        delta = mean_q_after_C - mean_q_after_D
        if delta < 0.0:
            delta = 0.0
        if delta < self.delta_min:
            delta = 0.0
        immediate_loss = 1.0 - k / n
        R_remain = r - (t + 1)
        if R_remain < 1:
            primary_action = Action.D
        else:
            expected_future_gain = k / n * delta * R_remain
            qs_in_window = []
            for s in range(window_start, t):
                qs_in_window.append(opponent_coop_fraction(s))
            if len(qs_in_window) >= 2:
                m = min(3, len(qs_in_window))
                recent_avg = float(np.mean(qs_in_window[-m:]))
                prev_part = qs_in_window[:-m]
                if len(prev_part) > 0:
                    prev_avg = float(np.mean(prev_part))
                    if prev_avg - recent_avg > self.retaliation_drop_thresh:
                        expected_future_gain *= self.retaliation_penalty
            my_actions_window = history.my_actions[window_start:t]
            last_defect_idx = None
            for offset, act in enumerate(my_actions_window):
                if not bool(act):
                    last_defect_idx = window_start + offset
            if last_defect_idx is not None and last_defect_idx + 1 < t:
                before_qs = []
                after_qs = []
                for s in range(window_start, t):
                    qval = opponent_coop_fraction(s)
                    if s <= last_defect_idx:
                        before_qs.append(qval)
                    else:
                        after_qs.append(qval)
                if len(before_qs) > 0 and len(after_qs) > 0:
                    before_avg = float(np.mean(before_qs))
                    after_avg = float(np.mean(after_qs))
                    if before_avg - after_avg > self.retaliation_drop_thresh:
                        expected_future_gain *= self.retaliation_penalty
            primary_action = Action.C if expected_future_gain > immediate_loss else Action.D
        if primary_action == Action.D:
            if random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        else:
            if random.random() < self.epsilon_exploit:
                return Action.D
            return Action.C

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)
    - Starts cooperative to signal willingness.
    - Reciprocates recent majority cooperation and long-run high cooperation.
    - Short, targeted punishment of persistent defectors, with quick forgiveness on a single coop.
    - Occasional probes when otherwise defecting to test for return to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.n = n
        self.r = r
        self.k = self.game_description.k
        self.MajorityThreshold = math.ceil(n / 2)
        self.HighCoopThreshold = 0.75
        self.NearUnanimous = max(0, n - 1)
        self.ProbeRate = max(0.02, 1.0 / max(1, r))
        self.ShortPunishLength = min(3, max(1, math.ceil(r / 10)))
        self.punish_end_round = -1
        self.punished_players = set()
        self.forgiven_players = set()
        self.last_probe_round = -1
        self.pre_probe_G = None
        self._initialized = False
        self.last_G = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.punish_end_round = -1
            self.punished_players = set()
            self.forgiven_players = set()
            self.last_probe_round = -1
            self.pre_probe_G = None
            self.last_G = None
            self._initialized = True
            return Action.C
        t = state.round_number
        last_round_idx = t - 1
        n_players = self.n
        opponent_count = max(0, n_players - 1)
        rounds_so_far = len(history.my_actions)
        if rounds_so_far == 0:
            return Action.C
        if opponent_count > 0:
            coop_counts_by_opponent = np.sum(history.opponent_actions.astype(np.int32), axis=0)
            C_j_arr = coop_counts_by_opponent.astype(np.float64) / float(rounds_so_far)
            C_bar = float(np.mean(C_j_arr)) if opponent_count > 0 else 0.0
        else:
            C_j_arr = np.array([], dtype=np.float64)
            C_bar = 0.0
        opp_last = np.sum(history.opponent_actions[last_round_idx, :].astype(np.int32)) if opponent_count > 0 else 0
        my_last = bool(history.my_actions[-1])
        G_last = int(opp_last) + (1 if my_last else 0)
        MyLast = my_last
        if self.punished_players and opponent_count > 0:
            newly_forgiven = {j for j in list(self.punished_players) if history.opponent_actions[last_round_idx, j]}
            if newly_forgiven:
                for j in newly_forgiven:
                    self.punished_players.discard(j)
                    self.forgiven_players.add(j)
        PersistentDefectors = set()
        for j in range(opponent_count):
            if j in self.forgiven_players:
                continue
            if C_j_arr.size > 0 and C_j_arr[j] <= 0.2:
                PersistentDefectors.add(j)
        if self.last_probe_round == last_round_idx and self.pre_probe_G is not None:
            if G_last >= self.pre_probe_G + 1:
                self.last_probe_round = -1
                self.pre_probe_G = None
                self.last_G = G_last
                return Action.C
            else:
                self.last_probe_round = -1
                self.pre_probe_G = None
        if t == self.r - 1:
            if C_bar >= self.HighCoopThreshold or G_last >= self.NearUnanimous:
                self.last_G = G_last
                return Action.C
            else:
                self.last_G = G_last
                return Action.D
        exploiter_indices = set()
        for j in PersistentDefectors:
            last_action = bool(history.opponent_actions[last_round_idx, j])
            if not last_action:
                exploiter_indices.add(j)
                continue
            recent_len = min(3, rounds_so_far)
            if recent_len >= 1:
                recent_window = history.opponent_actions[-recent_len:, j].astype(np.int32)
                recent_rate = float(np.sum(recent_window)) / float(recent_len)
                if recent_rate + 1e-09 < float(C_j_arr[j]) - 0.1:
                    exploiter_indices.add(j)
        if exploiter_indices:
            new_end = t + self.ShortPunishLength - 1
            if new_end > self.punish_end_round:
                self.punish_end_round = new_end
            for j in exploiter_indices:
                self.punished_players.add(j)
            if t <= self.punish_end_round:
                self.last_G = G_last
                return Action.D
        if t <= self.punish_end_round:
            self.last_G = G_last
            return Action.D
        if G_last >= self.MajorityThreshold:
            self.last_G = G_last
            return Action.C
        if C_bar >= self.HighCoopThreshold:
            self.last_G = G_last
            return Action.C
        if MyLast and G_last >= 1:
            self.last_G = G_last
            return Action.C
        do_probe = random.random() < self.ProbeRate
        if do_probe:
            self.last_probe_round = t
            self.pre_probe_G = G_last
            self.last_G = G_last
            return Action.C
        self.last_G = G_last
        return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    Opportunistic ProberExploiter (OPE)

    Exploitative public-goods strategy that:
    - Probes briefly for cooperators,
    - Exploits when others are sufficiently cooperative (defecting to free-ride),
    - Repairs cooperation if defecting triggers punishment,
    - Searches (sporadic probes) when cooperation is scarce,
    - Always defects in the final round (endgame exploitation).

    State machine modes: 'PROBE', 'EXPLOIT', 'REPAIR', 'SEARCH', 'DEFECT_FOREVER'
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_probe = max(1, min(5, math.floor(self.r / 8))) if self.r > 1 else 1
        self.T_repair = max(1, min(4, math.floor(self.r / 10))) if self.r > 1 else 1
        self.W = min(self.r, max(5, math.floor(self.r / 4))) if self.r > 1 else 1
        self.T_end = 1 if self.r > 5 else min(2, self.r)
        self.p_high = 0.6
        self.p_low = 0.2
        self.delta_drop = 0.2
        self.epsilon_seed = 0.05
        self.search_q = 0.2
        self.search_S_rounds = max(3, math.floor(self.W / 5))
        self.mode = 'PROBE'
        self.probe_started = False
        self.repair_remaining = 0
        self.repair_attempts = 0
        self.max_repair_attempts = 2
        self.last_round_defected = False
        self.baseline_before_last_defection = None
        self.last_round_seeded = False
        self.baseline_before_last_seed = None
        self.rounds_since_last_probe = 0
        self.search_fail_count = 0
        self.search_fail_threshold = max(2, math.floor(self.W / 2))
        self.defect_forever = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = int(state.round_number)
        if self.r <= 1:
            self.mode = 'DEFECT_FOREVER'
            self.defect_forever = True
            return Action.D
        if current_round >= max(0, self.r - self.T_end):
            self.mode = 'DEFECT_FOREVER'
            self.defect_forever = True
            self.last_round_defected = False
            self.last_round_seeded = False
            return Action.D
        if current_round == 0 or history is None:
            self.mode = 'PROBE'
            self.probe_started = True
            self.last_round_defected = False
            self.last_round_seeded = False
            return Action.C

        def opponents_in_round_fraction(round_index: int) -> float:
            if round_index < 0 or round_index >= history.opponent_actions.shape[0]:
                return 0.0
            num_opp = history.opponent_actions.shape[1]
            if num_opp <= 0:
                return 0.0
            count = int(np.sum(history.opponent_actions[round_index, :]))
            return count / float(num_opp)

        def opponents_in_last_avail_fraction(avail_rounds: int) -> float:
            total_rounds = min(avail_rounds, history.opponent_actions.shape[0])
            if total_rounds <= 0:
                return 0.0
            num_opp = history.opponent_actions.shape[1]
            total_coop = int(np.sum(history.opponent_actions[-total_rounds:, :]))
            denom = float(num_opp * total_rounds) if num_opp * total_rounds > 0 else 1.0
            return total_coop / denom
        p_others_W = opponents_in_last_avail_fraction(self.W)
        if self.last_round_defected:
            p_after = opponents_in_round_fraction(history.opponent_actions.shape[0] - 1)
            baseline = 0.0 if self.baseline_before_last_defection is None else self.baseline_before_last_defection
            if baseline - p_after > self.delta_drop:
                self.mode = 'REPAIR'
                self.repair_remaining = self.T_repair
                self.repair_attempts += 1
                self.last_round_defected = False
                self.baseline_before_last_defection = None
            else:
                self.last_round_defected = False
                self.baseline_before_last_defection = None
                if p_others_W < self.p_low and self.mode == 'EXPLOIT':
                    self.mode = 'SEARCH'
        if self.last_round_seeded:
            p_after = opponents_in_round_fraction(history.opponent_actions.shape[0] - 1)
            baseline = 0.0 if self.baseline_before_last_seed is None else self.baseline_before_last_seed
            if p_after - baseline > self.delta_drop / 2.0:
                self.last_round_seeded = False
                self.baseline_before_last_seed = None
                if p_others_W >= self.p_high:
                    self.mode = 'EXPLOIT'
                elif p_others_W <= self.p_low:
                    self.mode = 'SEARCH'
            else:
                self.last_round_seeded = False
                self.baseline_before_last_seed = None
                self.mode = 'SEARCH'
                self.search_fail_count += 1
                if self.search_fail_count >= self.search_fail_threshold:
                    self.mode = 'DEFECT_FOREVER'
                    self.defect_forever = True
        if self.mode == 'PROBE':
            if current_round < self.T_probe:
                self.probe_started = True
                return Action.C
            else:
                avail = min(self.W, history.opponent_actions.shape[0])
                p_others_after_probe = opponents_in_last_avail_fraction(avail)
                if p_others_after_probe >= self.p_high:
                    self.mode = 'EXPLOIT'
                else:
                    self.mode = 'SEARCH'
        if self.mode == 'DEFECT_FOREVER' or self.defect_forever:
            self.last_round_defected = False
            self.last_round_seeded = False
            return Action.D
        if self.mode == 'REPAIR' and self.repair_remaining > 0:
            action = Action.C
            self.repair_remaining -= 1
            self.last_round_defected = False
            self.last_round_seeded = False
            return action
        if self.mode == 'SEARCH':
            if p_others_W <= self.p_low:
                self.search_fail_count += 1
            else:
                self.search_fail_count = max(0, self.search_fail_count - 1)
            if self.search_fail_count >= self.search_fail_threshold:
                self.mode = 'DEFECT_FOREVER'
                self.defect_forever = True
                return Action.D
            do_probe = False
            if random.random() < self.search_q:
                do_probe = True
            if self.rounds_since_last_probe >= self.search_S_rounds:
                do_probe = True
            if do_probe:
                baseline_len = min(self.W, 3, history.opponent_actions.shape[0])
                baseline = opponents_in_last_avail_fraction(baseline_len) if baseline_len > 0 else 0.0
                self.baseline_before_last_seed = baseline
                self.last_round_seeded = True
                self.rounds_since_last_probe = 0
                return Action.C
            else:
                self.rounds_since_last_probe += 1
                baseline_len = min(self.W, 3, history.opponent_actions.shape[0])
                baseline = opponents_in_last_avail_fraction(baseline_len) if baseline_len > 0 else 0.0
                self.baseline_before_last_defection = baseline
                self.last_round_defected = True
                return Action.D
        if self.mode == 'EXPLOIT':
            if p_others_W <= self.p_low:
                self.mode = 'SEARCH'
                self.last_round_defected = False
                self.last_round_seeded = False
                return self.__call__(state, history)
            if random.random() < self.epsilon_seed:
                baseline_len = min(self.W, 3, history.opponent_actions.shape[0])
                baseline = opponents_in_last_avail_fraction(baseline_len) if baseline_len > 0 else 0.0
                self.baseline_before_last_seed = baseline
                self.last_round_seeded = True
                self.last_round_defected = False
                return Action.C
            else:
                baseline_len = min(self.W, 3, history.opponent_actions.shape[0])
                baseline = opponents_in_last_avail_fraction(baseline_len) if baseline_len > 0 else 0.0
                self.baseline_before_last_defection = baseline
                self.last_round_defected = True
                self.last_round_seeded = False
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    Opportunistic Free-Rider with Probe-and-Guard.

    - Starts by cooperating for R0 rounds to build reputation.
    - Performs a single probe (defection) at round R0 to test punishments.
    - Classifies environment as 'punitive' or 'non-punitive' after observing responses.
    - In non-punitive environments, probabilistically free-rides on reliably cooperative opponents and adapts p_exploit.
    - In punitive environments, uses a cautious conditional-cooperation rule to avoid provoking sustained punishment.
    - In the last T_end rounds always defects. If group cooperation collapses persistently, switches to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(2, math.floor(self.r / 5)))
        self.R0 = min(2, max(1, math.floor(self.r / 10)))
        self.probe_index = self.R0
        self.T_end = min(2, math.floor(self.r / 10))
        self.coop_high = 0.8
        self.coop_low = 0.3
        self.punish_fraction = 0.5
        self.retaliation_drop = 0.4
        self.p_exploit_init = 0.5
        self.p_exploit = float(self.p_exploit_init)
        self.p_exploit_max = 0.9
        self.p_exploit_min = 0.05
        self._inc_factor = 1.1
        self._dec_factor = 0.5
        self.ENV = 'unknown'
        self.probe_classified = False
        self.last_monitored_round = None
        self.monitoring_exploit_round = None
        self.monitor_baseline = None
        self.monitor_end_round = None
        self.cautious_until = -1
        self.permanent_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_number = state.round_number
        n = self.n
        r = self.r
        W = self.W
        if round_number >= r - self.T_end:
            return Action.D
        if round_number == 0 or history is None:
            return Action.C
        rounds_played = history.opponent_actions.shape[0]
        m = n - 1
        window = min(W, rounds_played)
        recent_opponent_actions = history.opponent_actions[-window:, :]
        coop_rate_j = np.mean(recent_opponent_actions.astype(float), axis=0) if window > 0 else np.zeros(m, dtype=float)
        group_coop_rate_last_round = float(np.mean(history.opponent_actions[-1, :].astype(float))) if rounds_played >= 1 else 0.0
        long_run_group_coop_rate = float(np.mean(recent_opponent_actions.astype(float))) if window > 0 else 0.0
        if long_run_group_coop_rate < self.coop_low:
            self.permanent_defect = True
        if self.permanent_defect:
            return Action.D
        if round_number <= self.R0 - 1:
            return Action.C
        if round_number == self.probe_index:
            return Action.D
        if self.ENV == 'unknown' and round_number == self.probe_index + 1:
            return Action.C
        if not self.probe_classified and rounds_played >= self.probe_index + 2:
            pre_start = max(0, self.probe_index - self.W)
            pre_end = self.probe_index
            if pre_end - pre_start <= 0:
                pre_start = 0
                pre_end = self.probe_index
            if pre_end - pre_start > 0:
                pre_window_actions = history.opponent_actions[pre_start:pre_end, :]
                coop_rate_before_probe = np.mean(pre_window_actions.astype(float), axis=0)
            else:
                coop_rate_before_probe = np.zeros(m, dtype=float)
            post_probe_actions = history.opponent_actions[self.probe_index + 1, :]
            prev_cooperators_mask = coop_rate_before_probe >= self.coop_high
            reductions = coop_rate_before_probe - post_probe_actions.astype(float)
            retaliator_mask = prev_cooperators_mask & (reductions >= self.retaliation_drop)
            num_prev_cooperators = int(np.sum(prev_cooperators_mask))
            num_retaliators = int(np.sum(retaliator_mask))
            fraction_retaliators = num_retaliators / num_prev_cooperators if num_prev_cooperators > 0 else 0.0
            if fraction_retaliators >= self.punish_fraction:
                self.ENV = 'punitive'
            else:
                self.ENV = 'nonpunitive'
            self.probe_classified = True
        last_finished_round = rounds_played - 1
        if last_finished_round - 1 >= 0:
            d = last_finished_round - 1
            my_action_d = bool(history.my_actions[d])
            if not my_action_d:
                pre_start = max(0, d - self.W)
                pre_end = d
                if pre_end - pre_start > 0:
                    pre_actions = history.opponent_actions[pre_start:pre_end, :]
                    coop_rate_before_d = np.mean(pre_actions.astype(float), axis=0)
                else:
                    coop_rate_before_d = np.zeros(m, dtype=float)
                post_actions = history.opponent_actions[d + 1, :].astype(float)
                prev_cooperators_mask = coop_rate_before_d >= self.coop_high
                reductions = coop_rate_before_d - post_actions
                retaliator_mask = prev_cooperators_mask & (reductions >= self.retaliation_drop)
                num_prev_cooperators = int(np.sum(prev_cooperators_mask))
                num_retaliators = int(np.sum(retaliator_mask))
                fraction_retaliators = num_retaliators / num_prev_cooperators if num_prev_cooperators > 0 else 0.0
                if fraction_retaliators >= self.punish_fraction:
                    self.ENV = 'punitive'
                    self.probe_classified = True
        if last_finished_round >= 0:
            my_action_last = bool(history.my_actions[-1])
            group_cooperators_last = int(np.sum(history.opponent_actions[-1, :].astype(int)))
            group_coop_rate_in_last_round = float(group_cooperators_last) / float(m) if m > 0 else 0.0
            exploit_threshold = 0.6
            exploit_round_idx = last_finished_round
            if not my_action_last and group_coop_rate_in_last_round >= exploit_threshold:
                if self.last_monitored_round != exploit_round_idx:
                    pre_start = max(0, exploit_round_idx - self.W)
                    pre_end = exploit_round_idx
                    if pre_end - pre_start > 0:
                        pre_actions = history.opponent_actions[pre_start:pre_end, :]
                        baseline = float(np.mean(pre_actions.astype(float)))
                    else:
                        baseline = float(np.mean(history.opponent_actions[:exploit_round_idx, :].astype(float))) if exploit_round_idx > 0 else 0.0
                    self.monitoring_exploit_round = exploit_round_idx
                    self.monitor_baseline = baseline
                    self.monitor_end_round = exploit_round_idx + self.W
                    self.last_monitored_round = exploit_round_idx
            if self.monitoring_exploit_round is not None and rounds_played > self.monitor_end_round:
                start = self.monitoring_exploit_round + 1
                end = start + self.W
                if start < rounds_played:
                    clipped_end = min(end, rounds_played)
                    post_actions = history.opponent_actions[start:clipped_end, :]
                    if post_actions.size > 0:
                        post_avg = float(np.mean(post_actions.astype(float)))
                    else:
                        post_avg = float(np.mean(history.opponent_actions[start:rounds_played, :].astype(float))) if start < rounds_played else 0.0
                else:
                    post_avg = 0.0
                baseline = float(self.monitor_baseline) if self.monitor_baseline is not None else 0.0
                drop = baseline - post_avg
                if drop > self.coop_low:
                    self.p_exploit = max(self.p_exploit_min, self.p_exploit * self._dec_factor)
                    self.cautious_until = round_number + 3
                else:
                    self.p_exploit = min(self.p_exploit_max, self.p_exploit * self._inc_factor)
                self.monitoring_exploit_round = None
                self.monitor_baseline = None
                self.monitor_end_round = None
        env = self.ENV if self.ENV in ('punitive', 'nonpunitive') else 'nonpunitive'
        if env == 'punitive':
            if group_coop_rate_last_round >= 0.5:
                return Action.C
            else:
                return Action.D
        S_mask = coop_rate_j >= self.coop_high
        num_S = int(np.sum(S_mask))
        if round_number < self.cautious_until:
            if group_coop_rate_last_round >= 0.5:
                return Action.C
            return Action.D
        if num_S >= 1:
            p = float(self.p_exploit)
            p = max(self.p_exploit_min, min(self.p_exploit_max, p))
            if random.random() < p:
                return Action.D
            else:
                return Action.C
        elif group_coop_rate_last_round >= 0.6:
            if random.random() < 0.3:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    Conditional Collective Reciprocity (CCR)  graded punishment, generous forgiveness.

    - Starts cooperative, defects in final round.
    - Uses a recent-window statistic to measure group and individual cooperation.
    - Forgives isolated defections, punishes persistent defectors with short collective
      punishments (up to two phases), and uses cautious probing after sustained collapse.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(10, max(1, math.floor(self.r / 4)))
        self.majority_threshold = 0.75
        self.collapse_threshold = 0.2
        self.lenient_threshold = 0.4
        self.personal_trust_threshold = 0.7
        self.p_max = min(3, max(1, math.floor(self.r / 10)))
        self.in_punishment = False
        self.punish_turns_remaining = 0
        self.punish_phases_done = 0
        self.punish_targets = tuple()
        self.consecutive_rebound_count = 0
        self.collapse_window_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.in_punishment = False
            self.punish_turns_remaining = 0
            self.punish_phases_done = 0
            self.punish_targets = tuple()
            self.consecutive_rebound_count = 0
            self.collapse_window_count = 0
            return Action.C
        if state.round_number >= self.r - 1:
            return Action.D
        rounds_played = history.opponent_actions.shape[0]
        window_size = min(self.w, rounds_played)
        recent = history.opponent_actions[-window_size:, :]
        rem = max(1, self.r - state.round_number)
        if window_size <= 0 or recent.size == 0:
            return Action.C
        personal_coop_rates = np.array(np.mean(recent.astype(float), axis=0))
        defect_counts = window_size - np.array(np.sum(recent.astype(int), axis=0))
        total_other_coop = float(np.sum(recent.astype(int)))
        group_coop_rate = total_other_coop / ((self.n - 1) * window_size)
        trusted_players = int(np.sum(personal_coop_rates >= self.personal_trust_threshold))
        persistent_mask = (personal_coop_rates <= 0.5) & (defect_counts >= 2)
        persistent_defectors_indices = tuple((int(i) for i in np.nonzero(persistent_mask)[0]))
        players_with_any_defection = int(np.sum(defect_counts > 0))
        isolated_single_defection = players_with_any_defection == 1 and group_coop_rate >= self.majority_threshold
        if group_coop_rate < self.collapse_threshold:
            self.collapse_window_count += 1
        else:
            self.collapse_window_count = 0
        if group_coop_rate >= self.majority_threshold:
            self.consecutive_rebound_count += 1
        else:
            self.consecutive_rebound_count = 0
        if self.in_punishment:
            if self.punish_turns_remaining > 0:
                self.punish_turns_remaining -= 1
                return Action.D
            improved = True
            if self.punish_targets:
                for idx in self.punish_targets:
                    if 0 <= idx < personal_coop_rates.shape[0]:
                        if personal_coop_rates[idx] <= 0.5:
                            improved = False
                            break
            if improved or self.punish_phases_done >= 2:
                self.in_punishment = False
                self.punish_turns_remaining = 0
                self.punish_phases_done = 0
                self.punish_targets = tuple()
            else:
                if self.punish_phases_done == 1:
                    p = min(self.p_max, max(1, rem - 1))
                    if p > 0:
                        self.punish_phases_done = 2
                        self.punish_turns_remaining = p - 1
                        return Action.D
                self.in_punishment = False
                self.punish_turns_remaining = 0
                self.punish_phases_done = 0
                self.punish_targets = tuple()
        if not self.in_punishment and persistent_defectors_indices and (not isolated_single_defection):
            if rem - 1 >= 1:
                p = min(self.p_max, max(1, rem - 1))
                self.in_punishment = True
                self.punish_phases_done = 1
                self.punish_targets = persistent_defectors_indices
                self.punish_turns_remaining = p - 1
                return Action.D
        if self.collapse_window_count > 1:
            s = max(2, math.floor(rem / 4)) if rem >= 1 else 2
            if state.round_number % s == 0:
                return Action.C
            return Action.D
        if isolated_single_defection:
            return Action.C
        if group_coop_rate >= self.majority_threshold:
            return Action.C
        if group_coop_rate <= self.lenient_threshold:
            return Action.D
        needed = math.floor((self.n - 1) / 2)
        if trusted_players >= needed:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    Adaptive Opportunistic Exploiter (AOE)

    - Starts with a cooperative signal.
    - Tracks per-opponent recent cooperation rates over a window w.
    - When a sufficient set of opponents are reliably cooperative and group cooperation is healthy,
      enters an exploit mode: defect most rounds but cooperate deterministically once every L rounds
      (maintenance). If exploitation becomes unsafe (others collapse or punish), switches to rebuild
      mode to restore cooperation or punish defectors.
    - Always defects in a short endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.n = int(n)
        self.r = int(r)
        self.k = float(k)
        self.w = min(10, max(1, math.floor(self.r / 4)))
        self.endgame = min(3, max(1, math.ceil(self.r / 10)))
        self.T_high = 0.75
        self.T_many = math.ceil((self.n - 1) / 2)
        self.p_maint = max(0.05, min(0.5, self.k / float(self.n)))
        self.L = math.ceil(1.0 / self.p_maint) if self.p_maint > 0 else math.ceil(1.0 / 0.05)
        self.T_rebuild = 0.6
        self.T_fail = 0.35
        self._in_exploit_mode = False
        self._consec_low_mean = 0
        self._consec_high_mean = 0
        self._exploit_suspended_until = -1
        self._last_mean = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t = int(state.round_number)
        if t >= self.r - self.endgame:
            return Action.D
        rounds_played = int(history.opponent_actions.shape[0])
        w_effective = min(self.w, rounds_played) if rounds_played > 0 else 0
        if w_effective == 0:
            return Action.D
        recent_slice = history.opponent_actions[-w_effective:, :]
        coop_counts = np.sum(recent_slice.astype(np.int64), axis=0)
        rate_j = coop_counts.astype(float) / float(w_effective)
        reliable_cooperators = int(np.sum(rate_j >= self.T_high))
        recent_others_coop_count = int(np.sum(history.opponent_actions[-1, :].astype(np.int64)))
        mean_others_rate = float(np.mean(rate_j)) if rate_j.size > 0 else 0.0
        if mean_others_rate < self.T_fail:
            self._consec_low_mean += 1
        else:
            self._consec_low_mean = 0
        if mean_others_rate >= self.T_rebuild:
            self._consec_high_mean += 1
        else:
            self._consec_high_mean = 0
        if t < self._exploit_suspended_until:
            self._in_exploit_mode = False
        else:
            majority_needed = max(1, math.ceil((self.n - 1) / 2))
            if reliable_cooperators >= majority_needed and mean_others_rate >= self.T_rebuild:
                if self._in_exploit_mode or self._consec_high_mean >= 2:
                    self._in_exploit_mode = True
            if self._consec_low_mean >= self.w:
                self._in_exploit_mode = False
        if self._in_exploit_mode:
            if recent_others_coop_count < self.T_many:
                self._exploit_suspended_until = t + self.w
                self._in_exploit_mode = False
            else:
                L_eff = self.L
                if t >= 2:
                    my_last2 = history.my_actions[-2:].astype(np.bool_)
                    if my_last2.shape[0] == 2 and bool(my_last2[0]) and bool(my_last2[1]):
                        opp_last2 = history.opponent_actions[-2:, :].astype(np.bool_)
                        opp_last2_all_defect = np.logical_not(opp_last2).all(axis=0)
                        prev_reliable = rate_j >= self.T_high
                        punishers = np.logical_and(prev_reliable, opp_last2_all_defect)
                        if np.any(punishers):
                            L_eff = max(1, self.L * 2)
                if self._consec_low_mean >= self.w:
                    self._in_exploit_mode = False
                elif t % L_eff == 0:
                    return Action.C
                else:
                    return Action.D
        if recent_others_coop_count >= self.T_many:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    Adaptive Probe-and-Exploit (APEX)

    - Probes early and mid tournament to detect whether the group punishes defections.
    - If group is non-punishing (or many opponents are near-unconditional cooperators), exploit:
      defect by default, but make maintenance contributions periodically and when group cooperation falls
      below a target fraction of baseline cooperation.
    - If group is punishing, play conditional cooperation (majority-match) with one-round forgiveness.
    - Always defect in the final EndgameK rounds.
    - For very short games (r <= ShortGameCutoff) always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r = int(game_description.n_rounds)
        self.n = int(game_description.n_players)
        self.ShortGameCutoff = 4
        self.W = min(5, max(1, self.r))
        self.PunishmentDrop = 0.2
        self.MaintenanceInterval = 4
        self.EndgameK = min(3, self.r)
        probe1 = 2
        probe2 = 2 + math.floor(self.r / 3)
        probes = []
        if probe1 < self.r:
            probes.append(probe1)
        if probe2 < self.r and probe2 != probe1:
            probes.append(probe2)
        self.scheduled_probes = [p for p in probes if p < self.r - self.EndgameK]
        self.processed_probes = set()
        self.probe_results = []
        self.mode = 'unknown'
        self.exploit_start_round = None
        self.last_chosen_action = None
        self.exploit_hard = False
        self.conditional_extra_probe_used = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        opponents = n - 1
        if r <= self.ShortGameCutoff:
            self.last_chosen_action = Action.D
            return Action.D
        if t >= r - self.EndgameK:
            self.last_chosen_action = Action.D
            return Action.D
        if t == 0:
            self.last_chosen_action = Action.C
            return Action.C
        if t == 1:
            self.last_chosen_action = Action.C
            return Action.C
        if history is None:
            self.last_chosen_action = Action.D
            return Action.D

        def coop_fraction_in_window(start_idx, end_idx_exclusive):
            if start_idx >= end_idx_exclusive:
                return None
            start = max(0, start_idx)
            end = min(end_idx_exclusive, t)
            if start >= end:
                return None
            window = history.opponent_actions[start:end, :]
            if window.size == 0:
                return None
            return float(np.mean(window))
        for p in list(self.scheduled_probes):
            if p in self.processed_probes:
                continue
            post_end = p + 1 + self.W
            if post_end <= t:
                baseline_start = p - self.W
                baseline_end = p
                baseline = coop_fraction_in_window(baseline_start, baseline_end)
                post = coop_fraction_in_window(p + 1, p + 1 + self.W)
                if baseline is None:
                    baseline = coop_fraction_in_window(0, p)
                if baseline is None or post is None:
                    self.processed_probes.add(p)
                    continue
                label = 'punishing' if post <= baseline - self.PunishmentDrop else 'nonpunishing'
                self.probe_results.append({'probe': p, 'baseline': float(baseline), 'post': float(post), 'label': label})
                self.processed_probes.add(p)
        if len(self.probe_results) > 0:
            latest = self.probe_results[-1]
            if latest['label'] == 'punishing':
                if self.mode != 'conditional':
                    self.mode = 'conditional'
                self.exploit_hard = False
            else:
                if self.mode != 'exploit':
                    self.mode = 'exploit'
                    self.exploit_start_round = t
                if history.opponent_actions.size > 0:
                    per_player_rates = np.mean(history.opponent_actions[:t, :], axis=0)
                    very_cooperative = np.sum(per_player_rates > 0.95)
                    if very_cooperative >= math.ceil(opponents * 0.5):
                        self.exploit_hard = True
        recent_window_len = min(self.W, t)
        recent_start = t - recent_window_len
        recent_coop_frac = coop_fraction_in_window(recent_start, t)
        if recent_coop_frac is None:
            recent_coop_frac = 0.0
        if self.mode == 'conditional' and (not self.conditional_extra_probe_used):
            candidate = t + max(2, math.floor(self.r / 4))
            if candidate < r - self.EndgameK:
                if candidate not in self.scheduled_probes and candidate not in self.processed_probes:
                    self.scheduled_probes.append(candidate)
                    self.conditional_extra_probe_used = True
        if t in self.scheduled_probes and t < r - self.EndgameK:
            self.last_chosen_action = Action.D
            return Action.D
        if self.mode == 'conditional':
            try:
                my_last_action = bool(history.my_actions[-1])
            except Exception:
                my_last_action = True
            last_round_opponents_coop = None
            if t >= 1:
                last_round_opponents_coop = float(np.mean(history.opponent_actions[-1, :]))
            else:
                last_round_opponents_coop = 0.0
            if not my_last_action and last_round_opponents_coop >= 0.5:
                self.last_chosen_action = Action.C
                return Action.C
            if last_round_opponents_coop >= 0.5:
                self.last_chosen_action = Action.C
                return Action.C
            else:
                self.last_chosen_action = Action.D
                return Action.D
        if self.mode == 'exploit' or self.exploit_hard:
            action = Action.D
            if self.exploit_start_round is None:
                self.exploit_start_round = t
            if (t - self.exploit_start_round) % self.MaintenanceInterval == 0:
                action = Action.C
            baseline_coop = None
            if len(self.probe_results) > 0:
                baseline_coop = self.probe_results[-1]['baseline']
            else:
                early_coop = coop_fraction_in_window(0, min(2, t))
                baseline_coop = early_coop if early_coop is not None else 0.0
            target = 0.6 * float(baseline_coop)
            if recent_coop_frac < target:
                action = Action.C
            self.last_chosen_action = action
            return action
        if self.mode == 'unknown':
            last_round_opponents_coop = 0.0
            if t >= 1:
                last_round_opponents_coop = float(np.mean(history.opponent_actions[-1, :]))
            if last_round_opponents_coop >= 0.5:
                self.last_chosen_action = Action.C
                return Action.C
            else:
                self.last_chosen_action = Action.D
                return Action.D
        self.last_chosen_action = Action.D
        return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    Reciprocal Conditional Cooperator with Graduated Punishment and Probing (RCC-GPP).

    - Cooperates first round to signal willingness.
    - Uses up to m = min(3, r) rounds of memory to compute recent cooperation fraction.
    - If recent cooperation >= T (0.5) cooperates.
    - If significant breach in last round, starts a short punishment ( P_max rounds).
    - After punishment, does a single-round rebuild (cooperate).
    - Uses small-probability probes () when defecting to detect return to cooperation.
    - Always defects on the final round. Shortens punishments near end so rebuild is possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.R = int(self.game_description.n_rounds)
        self.n = int(self.game_description.n_players)
        self.k = float(self.game_description.k)
        self.m = min(3, self.R)
        self.T = 0.5
        self.P_max = 3
        self.epsilon = min(0.1, 2.0 / (self.R + 1)) if self.R > 0 else 0.05
        self.punishment_remaining = 0
        self.in_rebuild = False
        self.consecutive_low_windows = 0
        self.conservative_mode = False
        self.last_was_probe = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining = self.R - t
        if t == 0 or history is None:
            self.last_was_probe = False
            return Action.C
        if remaining == 1:
            self.punishment_remaining = 0
            self.in_rebuild = False
            self.last_was_probe = False
            return Action.D
        if remaining <= self.P_max:
            allowed = max(0, remaining - 1)
            if self.punishment_remaining > allowed:
                self.punishment_remaining = allowed
                if self.punishment_remaining == 0:
                    self.in_rebuild = True
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            if self.punishment_remaining == 0:
                self.in_rebuild = True
            self.last_was_probe = False
            return Action.D
        if self.in_rebuild:
            self.in_rebuild = False
            self.last_was_probe = True
            return Action.C
        available = min(self.m, t)
        if available <= 0:
            self.last_was_probe = False
            return Action.C
        start_idx = t - available
        fractions = []
        for idx in range(start_idx, t):
            opp_row = history.opponent_actions[idx, :]
            coop_count = int(np.sum(opp_row)) + (1 if bool(history.my_actions[idx]) else 0)
            fractions.append(float(coop_count) / float(self.n))
        f_recent = float(np.mean(fractions))
        f_last = float(fractions[-1])
        if self.last_was_probe:
            if f_last >= self.T:
                self.conservative_mode = False
                self.consecutive_low_windows = 0
            self.last_was_probe = False
        if f_recent < self.T:
            self.consecutive_low_windows += 1
        else:
            self.consecutive_low_windows = 0
            self.conservative_mode = False
        if self.consecutive_low_windows >= 2:
            self.conservative_mode = True
        if f_recent >= self.T:
            self.last_was_probe = False
            return Action.C
        severe_threshold = self.T - 0.25
        if f_last <= severe_threshold:
            punishment_length = min(self.P_max, max(0, remaining - 1))
            if punishment_length <= 0:
                self.last_was_probe = False
                return Action.D
            self.punishment_remaining = punishment_length - 1
            if self.punishment_remaining == 0:
                self.in_rebuild = True
            self.last_was_probe = False
            return Action.D
        probe_prob = float(self.epsilon)
        roll = random.random()
        if self.conservative_mode:
            if roll < probe_prob:
                self.last_was_probe = True
                return Action.C
            else:
                self.last_was_probe = False
                return Action.D
        elif roll < probe_prob:
            self.last_was_probe = True
            return Action.C
        else:
            self.last_was_probe = False
            return Action.D

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    - Maintains short-window statistics about opponents' cooperation rates.
    - Tries to appear cooperative most of the time, but defects opportunistically
      against naive cooperators while avoiding costly retaliation.
    - Forgives after punishment for a short number of rounds to rebuild reputation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        computed = math.floor(self.r / 10)
        self.L = min(10, max(3, computed))
        self.COOP_ENV_THRESHOLD = 0.6
        self.MAJORITY_LAST_ROUND = math.ceil((self.n - 1) / 2)
        self.RETALIATE_SENSITIVITY = 0.15
        self.NAIVE_COOP_THRESHOLD = 0.95
        self.BASE_EXPLOIT_PROB = 0.2
        self.SAFE_FORGIVENESS_ROUNDS = 2
        self.forgiveness_timer = 0
        self.exploit_bias = 0.0
        self._punishment_events = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        L = self.L
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        if self.forgiveness_timer > 0:
            self.forgiveness_timer -= 1
            return Action.C
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        rounds_played = my_actions.shape[0]
        if rounds_played == 0:
            return Action.C
        window_len = min(L, rounds_played)
        start_idx = max(0, rounds_played - window_len)
        recent_opp = opp_actions[start_idx:rounds_played, :]
        coop_rates = np.mean(recent_opp.astype(float), axis=0)
        if coop_rates.size == 0:
            group_coop_rate = 0.0
        else:
            group_coop_rate = float(np.mean(coop_rates))
        last_round_idx = rounds_played - 1
        last_round_contributors = int(np.sum(opp_actions[last_round_idx, :].astype(int)))
        my_recent_window = my_actions[start_idx:rounds_played]
        my_recent_defections = int(np.sum((~my_recent_window).astype(int)))
        naive_cooperators = []
        retaliators = []
        persistent_defectors = []
        conditional = []
        recent_defection_offsets = np.where(my_recent_window == False)[0]
        recent_defection_indices = [start_idx + int(off) for off in recent_defection_offsets]
        for j in range(n - 1):
            cr_j = float(coop_rates[j])
            if cr_j >= self.NAIVE_COOP_THRESHOLD:
                naive_cooperators.append(j)
            if cr_j <= 0.1:
                persistent_defectors.append(j)
            is_retaliator = False
            for di in recent_defection_indices:
                bstart = max(0, di - window_len)
                b_end = di
                a_start = di + 1
                a_end = min(rounds_played, di + 1 + window_len)
                len_before = b_end - bstart
                len_after = a_end - a_start
                if len_before <= 0 or len_after <= 0:
                    continue
                before_slice = opp_actions[bstart:b_end, j]
                after_slice = opp_actions[a_start:a_end, j]
                coop_before = float(np.mean(before_slice.astype(float)))
                coop_after = float(np.mean(after_slice.astype(float)))
                if coop_before - coop_after >= self.RETALIATE_SENSITIVITY:
                    is_retaliator = True
                    break
            if is_retaliator:
                retaliators.append(j)
            elif j not in naive_cooperators and j not in persistent_defectors:
                conditional.append(j)
        coord_punish_count = 0
        for di in recent_defection_indices:
            bstart = max(0, di - max(1, window_len))
            b_end = di
            a_start = di + 1
            a_end = min(rounds_played, di + 1 + max(1, window_len))
            len_before = b_end - bstart
            len_after = a_end - a_start
            if len_before <= 0 or len_after <= 0:
                continue
            before_group = float(np.mean(np.mean(opp_actions[bstart:b_end, :].astype(float), axis=1)))
            after_group = float(np.mean(np.mean(opp_actions[a_start:a_end, :].astype(float), axis=1)))
            if before_group - after_group >= 0.2:
                coord_punish_count += 1
                self._punishment_events.append(di)
        coordinated_punisher_detected = coord_punish_count >= 2
        punished_now = False
        if rounds_played >= 2:
            prev_window_end = rounds_played - 1
            prev_start = max(0, prev_window_end - window_len)
            if prev_window_end - prev_start > 0:
                prev_group_slice = opp_actions[prev_start:prev_window_end, :]
                prev_group_coop_rate = float(np.mean(prev_group_slice.astype(float)))
                if my_actions[-1] == False and prev_group_coop_rate - group_coop_rate >= 0.1:
                    punished_now = True
                    self.forgiveness_timer = self.SAFE_FORGIVENESS_ROUNDS
                    self._punishment_events.append(rounds_played - 1)
        if len(persistent_defectors) >= self.MAJORITY_LAST_ROUND:
            return Action.D
        if group_coop_rate < self.COOP_ENV_THRESHOLD or last_round_contributors < self.MAJORITY_LAST_ROUND:
            return Action.D
        if len(retaliators) > 0 and my_recent_defections > 0:
            self.forgiveness_timer = self.SAFE_FORGIVENESS_ROUNDS
            return Action.C
        if coordinated_punisher_detected:
            self.exploit_bias = max(self.exploit_bias - 0.02, -0.1)
            return Action.C
        t1 = state.round_number + 1
        p_scale = 1.0 + t1 / max(1, r)
        p_exploit = self.BASE_EXPLOIT_PROB * p_scale + self.exploit_bias
        if t1 > 3 * r / 4:
            p_exploit = p_exploit * 1.25
        p_exploit = max(0.0, min(0.95, p_exploit))
        if len(naive_cooperators) > 0:
            if random.random() < p_exploit:
                self.exploit_bias = min(self.exploit_bias + 0.01, 0.1)
                return Action.D
            else:
                return Action.C
        if group_coop_rate >= 0.8:
            probe_p = min(p_exploit, 0.25)
            if random.random() < probe_p:
                self.exploit_bias = min(self.exploit_bias + 0.005, 0.05)
                return Action.D
            else:
                return Action.C
        if self.COOP_ENV_THRESHOLD <= group_coop_rate < 0.8:
            strong_majority_threshold = math.ceil(0.75 * (n - 1))
            if last_round_contributors >= strong_majority_threshold:
                return Action.C
            else:
                return Action.D
        if random.random() < p_exploit * 0.5:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    Adaptive Collective Pavlov with Proportional Punishment (ACPP).

    - Starts by cooperating.
    - Uses a Pavlov (win-stay, lose-shift) rule comparing actual vs counterfactual payoff.
    - When switching C->D because of exploitation, punishes by defecting for a proportional, bounded
      number of rounds (based on number of defectors observed). Punishment is cancellable by
      quick group recovery (near-universal cooperation in a short lookback window).
    - Always defects in the final round. Includes a small heuristic for the penultimate round.
    - State is inferred from public history (no external mutable timers); punishment windows are
      detected from past switching events caused by exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(3, self.r)
        self.P_max = min(3, max(1, self.r // 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            return Action.C
        if t == self.r - 1:
            return Action.D

        def total_contrib_at(idx: int) -> int:
            opp_sum = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_act = int(bool(history.my_actions[idx]))
            return opp_sum + my_act
        last_idx = t - 1
        M_prev = total_contrib_at(last_idx)
        a_prev = bool(history.my_actions[last_idx])
        common_share = self.k / self.n * float(M_prev)
        pi_prev = (0.0 if a_prev else 1.0) + common_share
        if a_prev:
            pi_cf = 1.0 + self.k / self.n * float(max(0, M_prev - 1))
        else:
            pi_cf = 0.0 + self.k / self.n * float(M_prev + 1)
        window_len = min(self.W, t)
        window_start = max(0, t - window_len)
        totals_window = [total_contrib_at(i) for i in range(window_start, t)]
        avg_total_window = float(np.mean(np.array(totals_window))) if totals_window else 0.0
        if avg_total_window >= self.n - 1:
            return Action.C
        rounds_remaining = self.r - t
        if not a_prev and M_prev >= self.n - 1 and (rounds_remaining >= 2):
            return Action.C
        last_exploit_switch_idx = None
        exploited_count_recent = 0
        for idx in range(last_idx, -1, -1):
            M_i = total_contrib_at(idx)
            a_i = bool(history.my_actions[idx])
            common_share_i = self.k / self.n * float(M_i)
            pi_prev_i = (0.0 if a_i else 1.0) + common_share_i
            if a_i:
                pi_cf_i = 1.0 + self.k / self.n * float(max(0, M_i - 1))
            else:
                pi_cf_i = 0.0 + self.k / self.n * float(M_i + 1)
            if idx >= window_start and a_i and (pi_cf_i > pi_prev_i):
                exploited_count_recent += 1
            if idx < last_idx:
                next_act = bool(history.my_actions[idx + 1])
                if a_i and pi_cf_i > pi_prev_i and (not next_act):
                    last_exploit_switch_idx = idx
                    break
        active_punishment = False
        if last_exploit_switch_idx is not None:
            M_at_exploit = total_contrib_at(last_exploit_switch_idx)
            if exploited_count_recent >= self.W:
                P_for_event = self.P_max
            else:
                P_for_event = min(self.P_max, max(1, self.n - int(M_at_exploit)))
            punish_start = last_exploit_switch_idx + 1
            punish_end = last_exploit_switch_idx + P_for_event
            if t >= punish_start and t <= punish_end:
                active_punishment = True
        if active_punishment:
            return Action.D
        if t == self.r - 2:
            if avg_total_window <= 1.0:
                return Action.D
        if pi_prev >= pi_cf:
            return Action.C if a_prev else Action.D
        else:
            return Action.D if a_prev else Action.C

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    Selective Free-Riding with Reputation Management.

    - Exploits unconditional cooperators.
    - Protects sizeable groups of conditional cooperators.
    - Punishes persistent punishers and defects in the endgame.
    - Probes sparsely and forgives after sustained recovery.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.p_exploit = 0.15
        self.high_coop = 0.85
        self.low_coop = 0.15
        self.cond_sensitivity = 0.4
        self.punish_tolerance = 2
        r = max(1, int(self.n_rounds))
        self.L_template = min(10, max(3, r // 4))
        self.min_exploit_gap = max(1, int(math.floor(1.0 / max(1e-09, self.p_exploit))))
        self.n_opponents = max(0, self.n_players - 1)
        self.punisher_flags = [False] * self.n_opponents
        self.punisher_since = [-1] * self.n_opponents
        self.last_exploit_round = -10000
        self.broad_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            return Action.C
        r = max(1, self.n_rounds)
        if r == 1:
            return Action.D
        if t >= r - 2:
            return Action.D
        L = self.L_template
        lookback_len = min(L, t)
        start_idx = t - lookback_len
        end_idx = t
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        if self.n_opponents == 0:
            return Action.D
        C = [0.0] * self.n_opponents
        if lookback_len > 0:
            window = opp_actions[start_idx:end_idx, :]
            sums = np.sum(window.astype(np.int64), axis=0)
            for i in range(self.n_opponents):
                C[i] = float(sums[i]) / float(lookback_len)
        else:
            C = [0.0] * self.n_opponents
        denom = 0
        if lookback_len >= 2:
            for idx in range(start_idx, end_idx - 1):
                if not bool(my_actions[idx]):
                    denom += 1
        reductions = [0] * self.n_opponents
        if denom > 0:
            for idx in range(start_idx, end_idx - 1):
                if not bool(my_actions[idx]):
                    row_now = opp_actions[idx, :]
                    row_next = opp_actions[idx + 1, :]
                    for i in range(self.n_opponents):
                        if bool(row_now[i]) and (not bool(row_next[i])):
                            reductions[i] += 1
            P = [reductions[i] / denom for i in range(self.n_opponents)]
        else:
            P = [0.0] * self.n_opponents
        recovery_window = max(1, min(self.L_template, lookback_len)) if lookback_len > 0 else self.L_template
        punish_check_start = max(1, t - self.punish_tolerance)
        for i in range(self.n_opponents):
            if self.punisher_flags[i]:
                if recovery_window <= lookback_len:
                    rec_start = t - recovery_window
                    rec_window = opp_actions[rec_start:t, i]
                    all_coop = bool(np.all(rec_window.astype(np.bool_))) if rec_window.size > 0 else False
                    if all_coop:
                        self.punisher_flags[i] = False
                        self.punisher_since[i] = -1
            if not self.punisher_flags[i]:
                consec = 0
                j_start = max(1, t - self.punish_tolerance)
                j_end = t
                if j_end - j_start >= self.punish_tolerance:
                    j_iter = range(t - self.punish_tolerance, t)
                else:
                    j_iter = range(j_start, j_end)
                consecutive_ok = True
                count_found = 0
                for j in j_iter:
                    if j - 1 < 0 or j >= t:
                        consecutive_ok = False
                        break
                    my_def_at_prev = not bool(my_actions[j - 1])
                    opp_prev = bool(opp_actions[j - 1, i])
                    opp_now = bool(opp_actions[j, i])
                    punished_here = my_def_at_prev and opp_prev and (not opp_now)
                    if punished_here:
                        count_found += 1
                    else:
                        consecutive_ok = False
                        break
                if consecutive_ok and count_found >= self.punish_tolerance and (count_found > 0):
                    self.punisher_flags[i] = True
                    self.punisher_since[i] = t
        num_punishers = sum((1 for f in self.punisher_flags if f))
        punish_many_threshold = max(1, int(math.floor(0.5 * self.n_opponents)))
        if num_punishers >= punish_many_threshold:
            self.broad_defection = True
        elif num_punishers < punish_many_threshold:
            self.broad_defection = False
        unconditional_count = 0
        conditional_count = 0
        defector_count = 0
        conditional_indices = []
        for i in range(self.n_opponents):
            if C[i] >= self.high_coop and P[i] <= 0.25:
                unconditional_count += 1
            if P[i] >= self.cond_sensitivity and C[i] >= 0.3:
                conditional_count += 1
                conditional_indices.append(i)
            if C[i] <= self.low_coop:
                defector_count += 1
        expected_loss = sum((P[i] for i in conditional_indices))
        collapse_threshold = max(1.0, 0.5 * float(self.n_players))
        if self.broad_defection:
            return Action.D
        if num_punishers >= max(1, int(math.floor(0.5 * self.n_opponents))):
            return Action.D
        if expected_loss > collapse_threshold:
            cooperating_to_protect = True
        else:
            cooperating_to_protect = False
        if not cooperating_to_protect:
            if unconditional_count >= 1 and expected_loss <= collapse_threshold:
                return Action.D
            defector_threshold = max(1, int(math.floor(0.5 * self.n_opponents)))
            if defector_count >= defector_threshold:
                return Action.D
            if conditional_count == 0:
                return Action.D
        recent_punishers = False
        for i in range(self.n_opponents):
            if self.punisher_flags[i] and self.punisher_since[i] >= 0:
                if t - self.punisher_since[i] <= max(1, lookback_len):
                    recent_punishers = True
                    break
        can_exploit_now = t - self.last_exploit_round >= self.min_exploit_gap
        if cooperating_to_protect and (not recent_punishers) and can_exploit_now:
            if random.random() < self.p_exploit:
                self.last_exploit_round = t
                return Action.D
        if cooperating_to_protect:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    Collective-First Conditional-Cooperation (C4)

    - Prefer cooperation when the recent majority of others cooperated.
    - If cooperation falls below threshold, issue a short proportional punishment (defect for P rounds),
      then perform a single-round recovery test (cooperate). If the test indicates cooperation has
      returned, resume cooperating; otherwise repeat punishment (capped).
    - Forgives isolated mistakes (single-round majority counts as cooperative).
    - Always cooperate on the very first round. Always defect on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.W = min(5, max(1, self.n_rounds - 1))
        self.theta = 0.6
        self.MaxPunish = 3
        self.phase = 'cooperate'
        self.punish_remaining = 0
        self.P_current = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.phase = 'cooperate'
            self.punish_remaining = 0
            self.P_current = 0
            return Action.C
        if state.round_number == self.n_rounds - 1:
            self.phase = 'cooperate'
            self.punish_remaining = 0
            self.P_current = 0
            return Action.D
        if history is None:
            return Action.D
        n_others = max(1, self.n_players - 1)
        played_rounds = history.opponent_actions.shape[0]
        W_eff = min(self.W, played_rounds)
        if W_eff <= 0:
            recent_rate = 1.0
            last_round_majority = True
        else:
            window = history.opponent_actions[-W_eff:, :]
            total_other_coops = int(window.sum())
            recent_rate = total_other_coops / (W_eff * n_others)
            last_row = history.opponent_actions[-1, :]
            last_round_coops = int(last_row.sum())
            last_round_majority = last_round_coops / n_others > 0.5
        if self.phase == 'punish' and self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.phase = 'await_test'
            return Action.D
        if self.phase == 'await_test':
            self.phase = 'evaluate_test'
            return Action.C
        if self.phase == 'evaluate_test':
            test_success = False
            if W_eff > 0:
                last_row = history.opponent_actions[-1, :]
                last_round_coops = int(last_row.sum())
                if last_round_coops / n_others >= self.theta or recent_rate >= self.theta or last_round_majority:
                    test_success = True
            if test_success:
                self.phase = 'cooperate'
                self.punish_remaining = 0
                self.P_current = 0
            else:
                shortfall = max(0.0, self.theta - recent_rate)
                raw_P = math.ceil(shortfall / self.theta * self.MaxPunish) if self.theta > 0 else self.MaxPunish
                P = max(1, raw_P)
                P = min(P, self.MaxPunish)
                max_possible_P = max(0, self.n_rounds - 1 - state.round_number)
                if max_possible_P <= 0:
                    self.phase = 'final_defect'
                    self.punish_remaining = 0
                    self.P_current = 0
                    return Action.D
                P = min(P, max_possible_P)
                self.phase = 'punish'
                self.P_current = P
                self.punish_remaining = P
                self.punish_remaining -= 1
                if self.punish_remaining == 0:
                    self.phase = 'await_test'
                return Action.D
        if self.phase == 'final_defect':
            return Action.D
        if W_eff > 0 and recent_rate >= self.theta or last_round_majority:
            self.phase = 'cooperate'
            self.punish_remaining = 0
            self.P_current = 0
            return Action.C
        shortfall = max(0.0, self.theta - recent_rate)
        raw_P = math.ceil(shortfall / self.theta * self.MaxPunish) if self.theta > 0 else self.MaxPunish
        P = max(1, raw_P)
        P = min(P, self.MaxPunish)
        max_possible_P = max(0, self.n_rounds - 1 - state.round_number)
        if max_possible_P <= 0:
            self.phase = 'final_defect'
            self.punish_remaining = 0
            self.P_current = 0
            return Action.D
        P = min(P, max_possible_P)
        self.phase = 'punish'
        self.P_current = P
        self.punish_remaining = P
        self.punish_remaining -= 1
        if self.punish_remaining == 0:
            self.phase = 'await_test'
        return Action.D

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    Opportunistic Probe-and-Exploit (OPE)

    - Early probing to detect punishers (short sequence of C then a single D).
    - Classify opponents as punishers if their cooperation rate drops after my probe-defection.
    - If group is non-punishing: permanently defect except deterministic scheduled re-probes (one C every S rounds)
      and final L rounds always defect.
    - If group is punishing: maintenance mode where I match a recent majority of others (rule a),
      but after S consecutive cooperative rounds perform a single planned defection to harvest surplus.
      If that defection provokes punishment (detected by classification around that defection),
      suspend further planned exploitation until 3 rounds of stable cooperation are observed.
    - Continuously re-classify after any defection by me (initial probe, re-probes, planned exploits).
    - Handle very short games and missing data by using available rounds when computing statistics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.P = min(3, self.r)
        self.W = min(10, self.r)
        self.S = max(3, math.floor(math.sqrt(self.r)))
        self.L = min(2, self.r)
        self.tau = 0.25
        self.Phi = 0.25
        self.M = math.ceil((self.n - 1) / 2)
        self.probe_round = max(0, self.P - 1)
        self.classified = False
        self.is_punishing = False
        self.punishers_mask = None
        self.last_classify_round = -1
        self.last_probe_round = self.probe_round
        self.last_reprobe_round = -9999
        self.collapsed = False
        self.exploit_cooldown = False
        self.awaiting_punishment_check = False
        self.last_planned_exploit_round = -9999
        self.stable_coop_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.r
        P = self.P
        W = self.W
        S = self.S
        L = self.L
        tau = self.tau
        Phi = self.Phi
        M = self.M
        n = self.n
        if r == 1:
            return Action.D
        if history is None:
            past_rounds = 0
            my_past = None
            opp_past = None
        else:
            past_rounds = history.my_actions.shape[0]
            my_past = history.my_actions
            opp_past = history.opponent_actions
        if t <= self.probe_round:
            if P == 2:
                if t == 0:
                    return Action.C
                else:
                    return Action.D
            elif t < self.probe_round:
                return Action.C
            else:
                return Action.D
        if history is None:
            return Action.D

        def find_last_my_defection_index() -> int | None:
            if past_rounds == 0:
                return None
            arr = my_past
            defects = ~arr
            inds = np.nonzero(defects)[0]
            if inds.size == 0:
                return None
            return int(inds[-1])

        def classify_around_defection(defect_index: int) -> tuple[bool, np.ndarray, float]:
            """
            Classify punishers around a specific defection round index (defect_index).
            Returns (is_punishing, punishers_mask, fraction_punishers)
            Uses W rounds before defect_index (as available) and up to 2 rounds after defect_index (as available).
            """
            if defect_index is None:
                return (False, np.zeros((n - 1,), dtype=bool), 0.0)
            before_start = max(0, defect_index - W)
            before_end = defect_index
            after_start = defect_index + 1
            after_end = min(past_rounds, defect_index + 3)
            before_len = max(0, before_end - before_start)
            after_len = max(0, after_end - after_start)
            punishers = np.zeros((n - 1,), dtype=bool)
            if before_len == 0 and after_len == 0:
                return (False, punishers, 0.0)
            if before_len > 0:
                before_rates = np.array(np.mean(opp_past[before_start:before_end, :], axis=0))
            else:
                before_rates = np.zeros((n - 1,), dtype=float)
            if after_len > 0:
                after_rates = np.array(np.mean(opp_past[after_start:after_end, :], axis=0))
            else:
                after_rates = np.zeros((n - 1,), dtype=float)
            diffs = before_rates - after_rates
            punishers = diffs >= tau
            frac = float(np.sum(punishers) / max(1, n - 1))
            is_punish = frac >= Phi
            return (is_punish, punishers, frac)
        last_def_index = find_last_my_defection_index()
        if last_def_index is not None:
            is_pun, pun_mask, frac = classify_around_defection(last_def_index)
            changed = not self.classified or is_pun != self.is_punishing or self.punishers_mask is None or (not np.array_equal(self.punishers_mask, pun_mask))
            if changed:
                self.classified = True
                self.is_punishing = bool(is_pun)
                self.punishers_mask = pun_mask.copy() if pun_mask is not None else np.zeros((n - 1,), dtype=bool)
                self.last_classify_round = t - 1
                if self.awaiting_punishment_check:
                    if frac >= Phi and np.sum(self.punishers_mask) > 0:
                        self.exploit_cooldown = True
                        self.stable_coop_streak = 0
                    self.awaiting_punishment_check = False
        recent_end = past_rounds
        recent_start = max(0, recent_end - W)
        recent_len = max(0, recent_end - recent_start)
        recent_coop_avg = 0.0
        if recent_len > 0:
            my_section = my_past[recent_start:recent_end].astype(float)
            opp_section = opp_past[recent_start:recent_end, :].astype(float)
            per_round_totals = np.sum(opp_section, axis=1) + my_section
            per_round_avgs = per_round_totals / float(n)
            recent_coop_avg = float(np.mean(per_round_avgs))
        else:
            recent_coop_avg = 0.0
        if recent_coop_avg < 0.2:
            self.collapsed = True
            self.is_punishing = False
            self.classified = True
        elif self.collapsed and recent_coop_avg >= 0.2:
            self.collapsed = False
            self.exploit_cooldown = False
            self.stable_coop_streak = 0
        prev_round_exists = past_rounds >= 1
        prev_opp_cooperators = 0
        if prev_round_exists:
            prev_opp_cooperators = int(np.sum(opp_past[-1, :]))
        rounds_left = r - t
        if rounds_left <= L:
            return Action.D
        if self.classified and (not self.is_punishing) or self.collapsed:
            base_for_spacing = self.last_classify_round if self.last_classify_round >= 0 else self.probe_round
            if base_for_spacing < 0:
                base_for_spacing = self.probe_round
            is_reprobe_round = t - base_for_spacing > 0 and (t - base_for_spacing) % S == 0
            if is_reprobe_round and t != self.last_reprobe_round and (not self.collapsed):
                self.last_reprobe_round = t
                return Action.C
            if self.collapsed and is_reprobe_round and (t != self.last_reprobe_round):
                self.last_reprobe_round = t
                return Action.C
            return Action.D
        if past_rounds > 0:
            last_all_coop = history.my_actions[-1] and np.all(history.opponent_actions[-1, :])
            if last_all_coop:
                self.stable_coop_streak += 1
            else:
                self.stable_coop_streak = 0
        else:
            self.stable_coop_streak = 0
        if self.exploit_cooldown and self.stable_coop_streak >= 3:
            self.exploit_cooldown = False
        available_window = min(S, past_rounds)
        stable_last_S = False
        if available_window > 0:
            start_idx = past_rounds - available_window
            my_win = history.my_actions[start_idx:past_rounds]
            opp_win = history.opponent_actions[start_idx:past_rounds, :]
            if np.all(my_win) and np.all(opp_win):
                stable_last_S = True
            else:
                stable_last_S = False
        else:
            stable_last_S = False
        enough_time_since_last_exploit = t - self.last_planned_exploit_round >= S
        if stable_last_S and (not self.exploit_cooldown) and enough_time_since_last_exploit:
            self.last_planned_exploit_round = t
            self.awaiting_punishment_check = True
            return Action.D
        if prev_round_exists and prev_opp_cooperators >= M:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    Exploitative Public Goods strategy.

    - First move: Cooperate to probe.
    - Last move: Always defect.
    - Track recent opponent cooperation (last_W_avg) over a short window W.
    - If group cooperation is high (>= thr): exploit by defecting with probability p_exploit,
      occasionally cooperating to sustain contributors. If exploitation appears to cause a
      sustained drop in cooperation, trigger a short rebuild.
    - If cooperation is moderate: cooperate deterministically to encourage recovery.
    - If cooperation collapses (<= 0.10) for several consecutive window evaluations, give up:
      defect forever except for periodic probes. If probes show recovery, exit give-up and
      enter a short rebuild.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.W = min(5, max(1, math.floor(r / 10) + 1))
        self.thr = 0.5 + 0.25 * (k / n)
        self.p_exploit = 0.85
        self.R = 2
        self.L = min(4, max(2, math.floor(r / 10)))
        self.P = max(3, math.floor(r / 10))
        self.consecutive_low_windows = 0
        self.in_rebuild_until = -1
        self.give_up_mode = False
        self._last_window_avg_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == r - 1:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.C if t == 0 else Action.D
        opponents_count = max(1, n - 1)
        available_rounds = history.opponent_actions.shape[0]
        usable_rounds = min(available_rounds, t)

        def frac_at(idx: int) -> float:
            if idx < 0 or idx >= usable_rounds:
                return 0.0
            coop_count = int(np.sum(history.opponent_actions[idx, :]))
            return coop_count / float(opponents_count)
        window_size = min(self.W, usable_rounds)
        if window_size <= 0:
            last_W_avg = None
        else:
            start_idx = usable_rounds - window_size
            f_values = [frac_at(i) for i in range(start_idx, usable_rounds)]
            last_W_avg = float(np.mean(f_values)) if len(f_values) > 0 else None
        f_prev = frac_at(usable_rounds - 1) if usable_rounds - 1 >= 0 else 0.0
        if self.give_up_mode:
            if last_W_avg is not None and last_W_avg >= self.thr:
                self.give_up_mode = False
                self.consecutive_low_windows = 0
                self.in_rebuild_until = t + self.R - 1
                return Action.C
            if t % self.P == 0:
                return Action.C
            return Action.D
        if t <= self.in_rebuild_until:
            return Action.C
        if last_W_avg is None:
            return Action.C
        if last_W_avg > self.thr:
            self.consecutive_low_windows = 0
        if f_prev >= 1.0 - 1e-12:
            return Action.D
        if last_W_avg >= self.thr:
            exploit_draw = random.random()
            chose_defect = exploit_draw < self.p_exploit
            chosen_action = Action.D if chose_defect else Action.C
            sustained_drop = False
            if usable_rounds >= 2:
                prev_indices = list(range(0, max(0, usable_rounds - window_size)))
                if len(prev_indices) > 0:
                    prev_f_vals = [frac_at(i) for i in prev_indices]
                    prev_mean = float(np.mean(prev_f_vals))
                    if prev_mean - last_W_avg >= 0.05:
                        sustained_drop = True
            if chose_defect and sustained_drop:
                self.in_rebuild_until = t + self.R - 1
            return chosen_action
        if last_W_avg < self.thr and last_W_avg > 0.1:
            return Action.C
        if last_W_avg <= 0.1:
            self.consecutive_low_windows += 1
            if self.consecutive_low_windows >= self.L:
                self.give_up_mode = True
                return Action.D
            else:
                self.in_rebuild_until = t + self.R - 1
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    Exploitative Adaptive Public-Goods Strategy.

    - Tracks short-term responsiveness of each opponent to my cooperation/defection.
    - Probes early to gather data, exploits unconditional cooperators,
      cooperates when expected future gains outweigh immediate costs,
      and avoids defecting when punishers would inflict larger future loss.
    - Uses a rolling window, Laplace smoothing, and occasional randomized probes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(10, self.r)
        self.T_probe = min(3, self.r)
        self.epsilon = 0.01
        self.p_probe_defect = 0.05
        self.p_probe_coop = 0.05
        self.endgame_horizon = 2
        self.unconditional_coop_threshold = 0.9
        self.punisher_delta_threshold = -0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t0 = state.round_number
        if t0 == 0 or history is None:
            if random.random() < self.p_probe_defect:
                return Action.D
            return Action.C
        opp_actions = history.opponent_actions
        rounds_played = history.my_actions.shape[0]
        num_opponents = opp_actions.shape[1] if opp_actions.ndim >= 2 else 0
        window_rounds = min(self.W, rounds_played)
        window_start = max(0, rounds_played - window_rounds)
        window_slice = slice(window_start, rounds_played)
        if window_rounds > 0 and num_opponents > 0:
            recent_opponent_matrix = opp_actions[window_slice, :]
            total_coops_per_opp = recent_opponent_matrix.sum(axis=0).astype(float)
            overall_coop_rate = (total_coops_per_opp + 1.0) / (window_rounds + 2.0)
        else:
            overall_coop_rate = np.full((num_opponents,), 0.5)
        transition_start = window_start
        transition_end = rounds_played - 1
        n_transitions = max(0, transition_end - transition_start)
        P_afterC = np.zeros((num_opponents,), dtype=float)
        P_afterD = np.zeros((num_opponents,), dtype=float)
        P_uncond = np.zeros((num_opponents,), dtype=float)
        delta = np.zeros((num_opponents,), dtype=float)
        my_actions = history.my_actions
        if n_transitions > 0:
            s_indices = np.arange(transition_start, transition_start + n_transitions)
            s_plus1_indices = s_indices + 1
            my_actions_s = my_actions[s_indices]
            opp_actions_splus1 = opp_actions[s_plus1_indices, :]
        else:
            my_actions_s = np.array([], dtype=bool)
            opp_actions_splus1 = np.zeros((0, num_opponents), dtype=bool)
        for j in range(num_opponents):
            if window_rounds > 0:
                P_uncond[j] = float(total_coops_per_opp[j]) / float(window_rounds)
            else:
                P_uncond[j] = 0.5
            if n_transitions > 0:
                mask_afterC = my_actions_s.astype(bool)
                mask_afterD = ~mask_afterC
                denom_afterC = int(mask_afterC.sum())
                denom_afterD = int(mask_afterD.sum())
                if denom_afterC > 0:
                    num_coop_afterC = int(opp_actions_splus1[mask_afterC, j].sum())
                    P_afterC[j] = (num_coop_afterC + 1.0) / (denom_afterC + 2.0)
                else:
                    P_afterC[j] = float(overall_coop_rate[j])
                if denom_afterD > 0:
                    num_coop_afterD = int(opp_actions_splus1[mask_afterD, j].sum())
                    P_afterD[j] = (num_coop_afterD + 1.0) / (denom_afterD + 2.0)
                else:
                    P_afterD[j] = float(overall_coop_rate[j])
            else:
                P_afterC[j] = float(overall_coop_rate[j])
                P_afterD[j] = float(overall_coop_rate[j])
            delta[j] = P_afterC[j] - P_afterD[j]
        total_responsiveness = float(delta.sum())
        negative_deltas = delta[delta < 0.0]
        sum_negative_deltas = float((-negative_deltas).sum()) if negative_deltas.size > 0 else 0.0
        L_remain = max(0, self.r - (t0 + 1))
        if t0 == self.r - 1:
            return Action.D
        if L_remain <= self.endgame_horizon:
            cost_now = 1.0 - self.k / float(self.n)
            punitive_future_loss = L_remain * (self.k / float(self.n)) * sum_negative_deltas
            if punitive_future_loss > cost_now + self.epsilon:
                chosen = Action.C
            else:
                chosen = Action.D
            if chosen == Action.C:
                if random.random() < self.p_probe_defect:
                    return Action.D
            else:
                low_confidence = rounds_played < max(3, self.T_probe + 1)
                if low_confidence and random.random() < self.p_probe_coop:
                    return Action.C
            return chosen
        if t0 + 1 <= self.T_probe:
            if random.random() < self.p_probe_defect:
                return Action.D
            return Action.C
        if num_opponents > 0:
            num_unconditional = int((P_uncond >= self.unconditional_coop_threshold).sum())
            if num_unconditional >= math.floor(num_opponents / 2) + 1:
                if random.random() < self.p_probe_coop:
                    return Action.C
                return Action.D
        cost_now = 1.0 - self.k / float(self.n)
        benefit_future = L_remain * (self.k / float(self.n)) * total_responsiveness
        if benefit_future > cost_now + self.epsilon:
            chosen = Action.C
        else:
            punitive_future_loss = L_remain * (self.k / float(self.n)) * sum_negative_deltas
            if punitive_future_loss > cost_now + self.epsilon:
                chosen = Action.C
            else:
                chosen = Action.D
        low_confidence = rounds_played < max(3, self.T_probe + 1) or n_transitions < max(2, self.T_probe)
        if chosen == Action.C:
            if random.random() < self.p_probe_defect:
                return Action.D
        elif low_confidence and random.random() < self.p_probe_coop:
            return Action.C
        return chosen

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    Conditional Exploiter with Probing and Recovery (CEPR)

    - Probes early to estimate opponents' cooperation rates.
    - In rich cooperative environments, occasionally defects to exploit,
      but watches for coordinated drops in opponent cooperation and
      enters a recovery phase if exploitation provokes retaliation.
    - In poorer environments, behaves conservatively (SUSTAIN).
    - Always defects in the final few rounds (ENDGAME) and in very short games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_probe = min(3, max(1, math.floor(self.r / 6)))
        self.gamma_high = 0.6
        self.gamma_low = 0.25
        self.collapse_sensitivity = 0.2
        self.recover_len = 3
        self.exploit_spacing = 1
        self.endgame_margin = min(3, math.floor(self.r / 10))
        self.p_probe = 0.05
        self.mode = 'PROBE'
        self.rounds_since_probe = 0
        self.last_exploit_round = -9999
        self.recover_remaining = 0
        self.last_was_exploit_flag = False
        self.classified_after_probe = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r <= 2:
            return Action.D
        t = state.round_number
        if t == 0 or history is None:
            if self.r > 2:
                self.mode = 'PROBE'
                self.rounds_since_probe = 1
                return Action.C
            else:
                return Action.D
        rounds_seen = int(history.opponent_actions.shape[0])
        n_opponents = max(1, self.n - 1)
        opp_coop_counts = np.array(history.opponent_actions.sum(axis=0), dtype=float)
        coop_rate_j = opp_coop_counts / max(1, rounds_seen)
        avg_coop_excl_me = float(np.mean(coop_rate_j)) if coop_rate_j.size > 0 else 0.0
        recent_group_contrib = int(history.opponent_actions[-1].sum()) if rounds_seen >= 1 else 0
        recent_group_frac = recent_group_contrib / n_opponents
        if t >= self.r - self.endgame_margin:
            self.mode = 'ENDGAME'
            return Action.D
        if not self.classified_after_probe and t <= self.T_probe:
            self.mode = 'PROBE'
            self.rounds_since_probe = t
            if t == self.T_probe:
                pass
            return Action.C
        if not self.classified_after_probe and t > self.T_probe:
            if avg_coop_excl_me >= self.gamma_high:
                self.mode = 'EXPLOIT'
            elif avg_coop_excl_me <= self.gamma_low:
                self.mode = 'SUSTAIN'
            else:
                self.mode = 'SUSTAIN'
            self.classified_after_probe = True
        if self.last_exploit_round >= 0:
            e = self.last_exploit_round
            if rounds_seen - 1 >= e + 1:
                baseline_idx = e - 1 if e - 1 >= 0 else e
                try:
                    baseline_contrib = int(history.opponent_actions[baseline_idx].sum())
                    after_contrib = int(history.opponent_actions[e + 1].sum())
                except Exception:
                    baseline_contrib = int(history.opponent_actions[e].sum())
                    after_contrib = int(history.opponent_actions[e + 1].sum()) if e + 1 < rounds_seen else baseline_contrib
                baseline_frac = baseline_contrib / n_opponents
                after_frac = after_contrib / n_opponents
                if baseline_frac - after_frac > self.collapse_sensitivity:
                    self.mode = 'RECOVER'
                    self.recover_remaining = self.recover_len
                    self.last_exploit_round = -9999
                    self.last_was_exploit_flag = False
        if self.mode == 'RECOVER':
            if self.recover_remaining > 0:
                self.recover_remaining -= 1
                if self.recover_remaining == 0:
                    if avg_coop_excl_me >= self.gamma_high:
                        self.mode = 'EXPLOIT'
                    elif avg_coop_excl_me <= self.gamma_low:
                        self.mode = 'SUSTAIN'
                    else:
                        self.mode = 'SUSTAIN'
                return Action.C
            else:
                self.mode = 'SUSTAIN'
        if self.mode == 'SUSTAIN':
            if avg_coop_excl_me >= 0.4:
                if random.random() < self.p_probe:
                    return Action.C
                return Action.C
            else:
                if random.random() < self.p_probe:
                    return Action.C
                return Action.D
        if self.mode == 'EXPLOIT':
            last_round_idx = rounds_seen - 1
            last_was_exploit = False
            if last_round_idx >= 0:
                try:
                    my_last_action = bool(history.my_actions[-1])
                except Exception:
                    my_last_action = False
                if last_round_idx >= 1:
                    prev_counts = np.array(history.opponent_actions[:last_round_idx].sum(axis=0), dtype=float)
                    prev_rates = prev_counts / max(1, last_round_idx)
                    prev_avg = float(np.mean(prev_rates))
                else:
                    prev_avg = avg_coop_excl_me
                if not my_last_action and prev_avg >= self.gamma_high:
                    last_was_exploit = True
            if not last_was_exploit and avg_coop_excl_me >= self.gamma_high:
                self.last_exploit_round = t
                self.last_was_exploit_flag = True
                return Action.D
            else:
                self.last_was_exploit_flag = False
                return Action.C
        if self.mode == 'ENDGAME':
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    Collective Conditional-Cooperation with Proportional Punishment and Forgiveness.

    - Starts by cooperating.
    - Uses a majority threshold M = ceil(n/2) to judge group reciprocity.
    - Base punishment length B = max(1, ceil(r/10)).
    - If a majority cooperated last round, continue cooperating.
    - If a majority did not cooperate, start a proportional punishment (defect now and for L remaining rounds),
      where L = min(R_rem - 1, B + gap) and gap = M - prev_total_C.
    - While punishing: defect each round, decrement remaining punishment rounds;
      if two consecutive past rounds have total contributions >= M, end punishment early (forgive).
    - Do not start new punishments in the late game when R_rem <= B (instead: cooperate only if everyone cooperated last round).
    - Always defect on the final (last-index) round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = int(math.ceil(self.n / 2))
        self.B = max(1, int(math.ceil(self.r / 10)))
        self.in_punishment = False
        self.L_rem = 0
        self.punish_start_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = int(state.round_number)
        R_rem = self.r - t_idx
        if t_idx == 0 or history is None:
            self.in_punishment = False
            self.L_rem = 0
            self.punish_start_round = None
            return Action.C
        if t_idx == self.r - 1:
            self.in_punishment = False
            self.L_rem = 0
            self.punish_start_round = None
            return Action.D

        def total_cooperators_at_round(idx: int) -> int:
            opp_sum = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            my_action = int(bool(history.my_actions[idx]))
            return opp_sum + my_action
        prev_total_C = total_cooperators_at_round(-1)
        if self.in_punishment:
            recovered = False
            if t_idx >= 2:
                last_total = total_cooperators_at_round(-1)
                prev_last_total = total_cooperators_at_round(-2)
                if last_total >= self.M and prev_last_total >= self.M:
                    recovered = True
                    self.in_punishment = False
                    self.L_rem = 0
                    self.punish_start_round = None
            action = Action.D
            if not recovered:
                if self.L_rem > 0:
                    self.L_rem -= 1
                if self.L_rem <= 0:
                    self.in_punishment = False
                    self.L_rem = 0
                    self.punish_start_round = None
            return action
        if R_rem <= self.B:
            if prev_total_C == self.n:
                return Action.C
            return Action.D
        if prev_total_C >= self.M:
            return Action.C
        else:
            gap = self.M - prev_total_C
            L = int(min(max(0, R_rem - 1), self.B + gap))
            if L > 0:
                self.in_punishment = True
                self.L_rem = L
                self.punish_start_round = t_idx
            else:
                self.in_punishment = False
                self.L_rem = 0
                self.punish_start_round = None
            return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    Collective Conditional-Cooperation with Graduated Punishment and Forgiveness.

    - Start by cooperating.
    - Reciprocate when a majority cooperated last round.
    - If I was exposed (I cooperated but group failed to reach majority), punish
      for P = min(Pcap, M - G) rounds (Pcap=2) by defecting, but forgive early
      if recent cooperation is nearly unanimous.
    - Treat very small deviations (>= n - A cooperators) as anomalies and forgive.
    - Be conservative in the last few rounds and in the final round follow the
      unanimous-cooperation requirement.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        M = math.ceil(n / 2)
        A = max(1, math.floor((n - 1) / 4))
        W = min(3, max(1, r - 1))
        Pcap = 2
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        past_rounds = t
        Gs = []
        for s in range(past_rounds):
            my_coop = bool(history.my_actions[s])
            opp_coops = int(np.sum(history.opponent_actions[s, :])) if history.opponent_actions.size else 0
            Gs.append(opp_coops + (1 if my_coop else 0))
        G_prev = Gs[-1]
        if t == r - 1:
            unanimous_before = all((gs == n for gs in Gs[:-0])) if len(Gs) > 0 else True
            if unanimous_before and all((gs == n for gs in Gs)):
                return Action.C
            return Action.D
        remaining = r - (t + 1)
        if remaining < 3:
            start_idx = max(0, past_rounds - W)
            window = Gs[start_idx:past_rounds]
            avg_recent = float(np.mean(window)) if window else 0.0
            if G_prev >= M and avg_recent >= n - A:
                return Action.C
            return Action.D
        start_idx = max(0, past_rounds - W)
        recent_window = Gs[start_idx:past_rounds]
        avg_recent = float(np.mean(recent_window)) if recent_window else 0.0
        if avg_recent >= n - A:
            return Action.C
        if G_prev >= n - A:
            return Action.C
        for s in range(past_rounds):
            my_coop_s = bool(history.my_actions[s])
            Gs_s = Gs[s]
            if my_coop_s and Gs_s < M:
                P_s = min(Pcap, M - Gs_s)
                punish_start = s + 1
                punish_end = s + P_s
                if punish_start <= t <= punish_end:
                    return Action.D
        if G_prev >= M:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    - Probes with initial cooperation to identify cooperators.
    - Exploits reliably cooperative environments by mostly defecting while
      occasionally cooperating to sustain cooperation.
    - Defects in hopeless environments and enters short recovery phases if
      punished after defecting.
    - Always defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.effective_return = self.k / self.n
        self.w = min(10, max(3, math.floor(self.r / 5))) if self.r > 0 else 3
        self.initial_rounds = min(3, self.r)
        self.endgame_rounds = min(3, max(1, math.floor(0.1 * self.r))) if self.r > 0 else 1
        self.maintenance_threshold = 0.5 + 0.25 * self.effective_return
        self.low_threshold = 0.15 + 0.15 * self.effective_return
        self.exploit_coop_prob = 0.3
        self.recovery_coop_prob = 0.75
        self.recovery_rounds = 2
        self.punishment_drop = 0.15
        self.recovery_remaining = 0
        self.recovery_prev_avg = None
        self.forced_non_coop = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if self.r <= 1:
            return Action.D
        rounds_remaining = max(0, self.r - t)
        if rounds_remaining <= self.endgame_rounds:
            return Action.D
        if t < self.initial_rounds:
            return Action.C
        if history is None:
            return Action.D
        num_opponents = max(1, self.n - 1)
        total_played = t
        win = min(self.w, total_played) if total_played > 0 else 0

        def compute_coop_rates_over_window(end_idx_exclusive: int, window_len: int) -> np.ndarray:
            if window_len <= 0 or end_idx_exclusive <= 0:
                return np.zeros(num_opponents, dtype=float)
            start = max(0, end_idx_exclusive - window_len)
            slice_actions = history.opponent_actions[start:end_idx_exclusive, :]
            actual_len = slice_actions.shape[0]
            if actual_len == 0:
                return np.zeros(num_opponents, dtype=float)
            sums = np.sum(slice_actions.astype(int), axis=0)
            return sums.astype(float) / float(actual_len)
        coop_rates_recent = compute_coop_rates_over_window(total_played, win)
        long_win = min(total_played, max(10, 2 * self.w)) if total_played > 0 else 0
        coop_rates_long = compute_coop_rates_over_window(total_played, long_win) if long_win > 0 else coop_rates_recent
        persistent_threshold = 0.05
        persistent_mask = coop_rates_long <= persistent_threshold
        if np.all(persistent_mask) and num_opponents > 0:
            effective_mask = np.ones_like(persistent_mask, dtype=bool)
        else:
            effective_mask = ~persistent_mask
        if np.any(effective_mask):
            avg_others = float(np.mean(coop_rates_recent[effective_mask]))
        else:
            avg_others = float(np.mean(coop_rates_recent)) if coop_rates_recent.size > 0 else 0.0
        last_round_others = int(np.sum(history.opponent_actions[-1, :])) if total_played >= 1 else 0
        punishment_detected = False
        drop_in_avg = 0.0
        if total_played >= 1:
            we_defected_last = not bool(history.my_actions[-1])
            if we_defected_last:
                win_before = min(self.w, max(0, total_played - 1))
                coop_rates_before = compute_coop_rates_over_window(total_played - 1, win_before)
                if np.any(effective_mask):
                    avg_before = float(np.mean(coop_rates_before[effective_mask]))
                else:
                    avg_before = float(np.mean(coop_rates_before)) if coop_rates_before.size > 0 else avg_others
                drop_in_avg = avg_before - avg_others
                if drop_in_avg > self.punishment_drop:
                    punishment_detected = True
        if punishment_detected and self.recovery_remaining <= 0:
            self.recovery_remaining = self.recovery_rounds
            self.recovery_prev_avg = avg_others
            self.forced_non_coop = False
        if avg_others <= self.low_threshold:
            environment = 'non-cooperative'
        elif avg_others >= self.maintenance_threshold:
            environment = 'cooperative'
        else:
            environment = 'mixed'
        if self.forced_non_coop:
            environment = 'non-cooperative'
        seed = t + int(np.sum(history.opponent_actions.astype(int))) * 100 + int(np.sum(history.my_actions.astype(int))) * 10000 + int(1000 * avg_others)
        rng = random.Random(seed)
        action = Action.D
        if environment == 'non-cooperative':
            action = Action.D
        elif environment == 'cooperative':
            if self.recovery_remaining > 0:
                if rng.random() < self.recovery_coop_prob:
                    action = Action.C
                else:
                    action = Action.D
            else:
                majority_needed = math.ceil((self.n - 1) / 2)
                if last_round_others >= majority_needed:
                    if rng.random() < self.exploit_coop_prob:
                        action = Action.C
                    else:
                        action = Action.D
                else:
                    action = Action.D
        else:
            majority_needed = math.ceil((self.n - 1) / 2)
            if last_round_others >= majority_needed:
                action = Action.C
            else:
                action = Action.D
        if self.recovery_remaining > 0:
            self.recovery_remaining -= 1
            if self.recovery_remaining == 0 and self.recovery_prev_avg is not None:
                total_played_now = t
                win_now = min(self.w, total_played_now) if total_played_now > 0 else 0
                coop_rates_now = compute_coop_rates_over_window(total_played_now, win_now)
                if np.any(effective_mask):
                    avg_now = float(np.mean(coop_rates_now[effective_mask]))
                else:
                    avg_now = float(np.mean(coop_rates_now)) if coop_rates_now.size > 0 else 0.0
                if avg_now <= self.recovery_prev_avg + 1e-08:
                    self.forced_non_coop = True
                self.recovery_prev_avg = None
        if self.forced_non_coop and avg_others >= self.maintenance_threshold:
            self.forced_non_coop = False
        return action

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Punishment and Probing Forgiveness (CCG-P)

    - Starts cooperative, defects in final round.
    - Tracks recent cooperation rates in a sliding window W.
    - If a strong majority of opponents are reliably cooperative, cooperate.
    - If recent group cooperation G >= q, cooperate.
    - Otherwise enter a proportional punishment of length LastP = clamp(1 + ceil((q-G)*W), 1, P_max).
    - After punishment finishes, schedule a single cooperative probe round. If the probe's observed cooperation
      among others >= q, resume normal behavior; otherwise escalate punishment modestly and begin punishment immediately.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.W = min(10, max(1, math.floor(r / 10)))
        self.q = 0.5
        self.s_high = 0.8
        self.R_req = 0.7
        self.P_max = min(5, max(1, r - 1))
        self.PunishRemaining = 0
        self.LastP = 0
        self.schedule_probe = False
        self.awaiting_probe_result = False
        self.probe_round_index = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        opponents_count = max(0, n - 1)
        if t == 0:
            self.PunishRemaining = 0
            self.LastP = 0
            self.schedule_probe = False
            self.awaiting_probe_result = False
            self.probe_round_index = None
            return Action.C
        if t == r - 1:
            self.awaiting_probe_result = False
            self.schedule_probe = False
            return Action.D
        if history is None:
            return Action.D
        if self.awaiting_probe_result:
            probe_idx = self.probe_round_index if self.probe_round_index is not None else t - 1
            if probe_idx < 0 or probe_idx >= history.opponent_actions.shape[0]:
                self.awaiting_probe_result = False
                self.LastP = min(self.P_max, max(1, self.LastP + 1))
                self.PunishRemaining = self.LastP
                self.PunishRemaining -= 1
                if self.PunishRemaining <= 0:
                    self.PunishRemaining = 0
                    self.schedule_probe = True
                return Action.D
            if opponents_count == 0:
                G_probe = 0.0
            else:
                row = history.opponent_actions[probe_idx, :]
                cooperators = int(np.count_nonzero(row))
                G_probe = cooperators / float(opponents_count)
            self.awaiting_probe_result = False
            self.probe_round_index = None
            if G_probe >= self.q:
                pass
            else:
                self.LastP = min(self.P_max, max(1, self.LastP + 1))
                self.PunishRemaining = self.LastP
                self.PunishRemaining -= 1
                if self.PunishRemaining <= 0:
                    self.PunishRemaining = 0
                    self.schedule_probe = True
                return Action.D
        if self.schedule_probe:
            self.schedule_probe = False
            self.awaiting_probe_result = True
            self.probe_round_index = t
            return Action.C
        total_rounds_available = history.opponent_actions.shape[0]
        last_k = min(self.W, total_rounds_available)
        if last_k <= 0 or opponents_count == 0:
            S = np.zeros((0,), dtype=float)
            G = 0.0
            R_frac = 0.0
        else:
            recent = history.opponent_actions[-last_k:, :]
            coop_counts = np.count_nonzero(recent, axis=0).astype(float)
            S = coop_counts / float(last_k)
            if S.size == 0:
                G = 0.0
                R_frac = 0.0
            else:
                G = float(np.mean(S))
                R_frac = float(np.count_nonzero(S >= self.s_high)) / float(opponents_count)
        if R_frac >= self.R_req:
            return Action.C
        if self.PunishRemaining > 0:
            self.PunishRemaining -= 1
            if self.PunishRemaining <= 0:
                self.PunishRemaining = 0
                self.schedule_probe = True
            return Action.D
        if G >= self.q:
            return Action.C
        gap = max(0.0, self.q - G)
        computed = 1 + math.ceil(gap * float(self.W))
        LastP_new = int(max(1, min(self.P_max, int(computed))))
        self.LastP = LastP_new
        self.PunishRemaining = self.LastP
        self.PunishRemaining -= 1
        if self.PunishRemaining <= 0:
            self.PunishRemaining = 0
            self.schedule_probe = True
        return Action.D

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    Starts by cooperating to recruit conditional cooperators, learns who reciprocates or punishes,
    then attempts to exploit identified reciprocators by mostly defecting and sending sparse
    maintenance gifts. Reverts to cautious conditional cooperation if many punishers are detected
    or cooperation collapses. Always defects in the final ENDGAME rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.BUILD = min(6, max(1, math.floor(self.r / 5)))
        self.ENDGAME = min(3, max(1, math.ceil(self.r / 10)))
        self.MAINTENANCE_CYCLE = 4
        self.PUNISH_THRESHOLD = 0.25
        self.DROP_DET_THRESHOLD = 0.1
        self.TARGET_GROUP_COOP = 0.6
        self.H = min(6, max(1, math.floor(self.r / 6)))
        self.last_exploit_gift_round = -999
        self.last_mode = 'BUILD'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        H = self.H
        if r <= 1:
            return Action.D
        if t == 0 or history is None:
            self.last_exploit_gift_round = -999
            self.last_mode = 'BUILD'
            return Action.C
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        m = opp_actions.shape[1] if opp_actions.ndim >= 2 else 0
        if m == 0:
            return Action.D
        if t >= r - self.ENDGAME:
            return Action.D
        window_start = max(0, t - H)
        window_end = t
        recent_window = opp_actions[window_start:window_end, :] if window_end > window_start else np.zeros((0, m), dtype=bool)
        if t >= 1:
            last_round_opponents = opp_actions[t - 1, :]
            last_round_group_coop = float(np.mean(last_round_opponents)) if last_round_opponents.size > 0 else 0.0
        else:
            last_round_group_coop = 0.0
        if recent_window.size == 0:
            group_recent_coop = 0.0
        else:
            group_recent_coop = float(np.mean(recent_window))
        times_I_cooperated = int(np.sum(my_actions))
        times_I_defected = int(my_actions.size - times_I_cooperated)
        total_coop_j = np.sum(opp_actions, axis=0) if t > 0 else np.zeros(m, dtype=int)
        recent_coop_j = np.mean(recent_window, axis=0) if recent_window.size > 0 else np.zeros(m, dtype=float)
        prob_after_my_C_1 = np.zeros(m, dtype=float)
        prob_after_my_D_1 = np.zeros(m, dtype=float)
        prob_after_my_D_2 = np.zeros(m, dtype=float)
        counts_after_C = 0
        counts_after_D = 0
        if t >= 2:
            q_indices = range(0, t - 1)
            for q in q_indices:
                my_action_q = bool(my_actions[q])
                next_round_idx = q + 1
                opp_next = opp_actions[next_round_idx, :]
                if my_action_q:
                    prob_after_my_C_1 += opp_next.astype(float)
                    counts_after_C += 1
                else:
                    prob_after_my_D_1 += opp_next.astype(float)
                    counts_after_D += 1
            if counts_after_C > 0:
                prob_after_my_C_1 = prob_after_my_C_1 / float(counts_after_C)
            else:
                prob_after_my_C_1 = np.zeros(m, dtype=float)
            if counts_after_D > 0:
                prob_after_my_D_1 = prob_after_my_D_1 / float(counts_after_D)
            else:
                prob_after_my_D_1 = np.zeros(m, dtype=float)
        else:
            prob_after_my_C_1 = np.zeros(m, dtype=float)
            prob_after_my_D_1 = np.zeros(m, dtype=float)
        sum_after_D_two = np.zeros(m, dtype=float)
        count_after_D_two = 0
        if t >= 2:
            for q in range(0, t):
                if not bool(my_actions[q]):
                    vals = []
                    if q + 1 < t:
                        vals.append(opp_actions[q + 1, :].astype(float))
                    if q + 2 < t:
                        vals.append(opp_actions[q + 2, :].astype(float))
                    if len(vals) > 0:
                        avg_vals = np.mean(np.vstack(vals), axis=0)
                        sum_after_D_two += avg_vals
                        count_after_D_two += 1
        if count_after_D_two > 0:
            prob_after_my_D_2 = sum_after_D_two / float(count_after_D_two)
        else:
            prob_after_my_D_2 = np.array(recent_coop_j, dtype=float)
        delta_CD = prob_after_my_C_1 - prob_after_my_D_1
        reciprocator_mask = (delta_CD >= -0.05) & (prob_after_my_C_1 >= 0.4)
        fraction_reciprocators = float(np.sum(reciprocator_mask) / float(m))
        punish_mask = recent_coop_j - prob_after_my_D_2 >= self.DROP_DET_THRESHOLD
        fraction_punishers = float(np.sum(punish_mask) / float(m))

        def detect_recent_group_punishment():
            if t < 1:
                return False
            recent_def_indices = [idx for idx in range(max(0, t - 3), t) if not bool(my_actions[idx])]
            for d_idx in recent_def_indices:
                before_start = max(0, d_idx - H)
                before_end = d_idx
                if before_end - before_start <= 0:
                    group_before = float(np.mean(opp_actions[0:d_idx, :])) if d_idx > 0 else 0.0
                else:
                    group_before = float(np.mean(opp_actions[before_start:before_end, :]))
                after_vals = []
                if d_idx + 1 < t:
                    after_vals.append(np.mean(opp_actions[d_idx + 1, :]))
                if d_idx + 2 < t:
                    after_vals.append(np.mean(opp_actions[d_idx + 2, :]))
                if len(after_vals) == 0:
                    continue
                group_after = float(np.mean(np.array(after_vals)))
                if group_before - group_after >= self.DROP_DET_THRESHOLD:
                    return True
            return False
        recent_group_punished = detect_recent_group_punishment()
        current_group_coop = group_recent_coop
        mode = 'NEUTRAL'
        if fraction_punishers > self.PUNISH_THRESHOLD or current_group_coop < 0.2:
            mode = 'CAUTIOUS'
        elif fraction_reciprocators >= 0.3 and current_group_coop >= self.TARGET_GROUP_COOP:
            mode = 'EXPLOIT'
        else:
            mode = 'NEUTRAL'
        if recent_group_punished:
            mode = 'CAUTIOUS'
        if t < self.BUILD:
            self.last_mode = 'BUILD'
            exploited_last_round = False
            if t >= 1:
                my_payoff_last = float(history.my_payoffs[-1]) if history.my_payoffs.size > 0 else 0.0
                if bool(my_actions[-1]) and last_round_group_coop < 0.5:
                    if my_payoff_last < 0.5:
                        exploited_last_round = True
            if exploited_last_round:
                return Action.D
            else:
                return Action.C
        action = Action.D
        if mode == 'EXPLOIT':
            action = Action.D
            last_gift_gap = t - self.last_exploit_gift_round if self.last_exploit_gift_round >= 0 else 9999
            if last_gift_gap >= self.MAINTENANCE_CYCLE and last_round_group_coop >= self.TARGET_GROUP_COOP - 0.1:
                action = Action.C
                self.last_exploit_gift_round = t
            if current_group_coop < self.TARGET_GROUP_COOP - 0.15:
                mode = 'CAUTIOUS'
                action = Action.D
        elif mode == 'CAUTIOUS':
            action = Action.C if last_round_group_coop >= 0.5 else Action.D
        else:
            majority_last = False
            if t >= 1:
                coopers_last = int(np.sum(opp_actions[t - 1, :]))
                majority_last = coopers_last >= math.floor((m + 1) / 2)
            if majority_last or current_group_coop >= 0.5:
                action = Action.C
            else:
                action = Action.D
        rounds_left = r - t
        if rounds_left <= 3:
            if action == Action.C and mode == 'EXPLOIT':
                action = Action.D
        self.last_mode = mode
        if mode == 'EXPLOIT' and action == Action.C:
            self.last_exploit_gift_round = t
        return action

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    Collective Conditional Cooperation with Forgiveness, Targeted Punishment and Occasional Testing.

    - Cooperates initially to signal willingness.
    - Tracks recent opponents' cooperation over a memory window W (up to 5 rounds).
    - Detects sharp drops in cooperation and responds with a short punishment (P rounds).
    - After punishment, performs a Test cooperation round to probe recovery.
    - Uses thresholds T_high and T_low for straightforward cooperate/defect decisions.
    - In ambiguous regions uses a smoothly varying cooperation probability.
    - Scales down cooperation in the final H_end rounds (avoids endgame exploitation).
    - Occasionally probes with small probability p_test when the group appears deadlocked.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W = min(5, self.r)
        self.P = 2
        self.delta_drop = 0.4
        self.T_high = 0.7
        self.T_low = 0.3
        self.p_test = 0.05
        self.H_end = min(3, self.r)
        self.remaining_punish = 0
        self.test_pending = False
        self.last_test_round = None
        self.pre_test_avg = None
        self.stuck_deadlock = False
        self.recovery = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        if t == 0 or history is None:
            return Action.C
        if t == r - 1:
            return Action.D
        if self.remaining_punish > 0:
            self.remaining_punish -= 1
            if self.remaining_punish == 0:
                self.test_pending = True
            return Action.D
        opp_actions = history.opponent_actions
        rounds_available = opp_actions.shape[0]
        recent_w = min(self.W, rounds_available)
        if recent_w <= 0:
            opp_avg_recent = 0.0
        else:
            opp_avg_recent = float(np.mean(opp_actions[-recent_w:, :]))
        prev_w = min(self.W, max(0, rounds_available - recent_w))
        previous_avg = None
        if prev_w >= 1:
            previous_slice_start = -recent_w - prev_w
            previous_slice_end = -recent_w
            previous_avg = float(np.mean(opp_actions[previous_slice_start:previous_slice_end, :]))
        if previous_avg is not None and previous_avg - opp_avg_recent > self.delta_drop:
            self.remaining_punish = self.P
            self.pre_test_avg = opp_avg_recent
            self.remaining_punish -= 1
            if self.remaining_punish == 0:
                self.test_pending = True
            self.stuck_deadlock = False
            self.recovery = False
            return Action.D
        if self.test_pending:
            self.test_pending = False
            self.last_test_round = t
            if self.pre_test_avg is None:
                self.pre_test_avg = opp_avg_recent
            return Action.C
        if self.last_test_round is not None and t > self.last_test_round:
            if self.pre_test_avg is not None and opp_avg_recent >= self.pre_test_avg + 0.05:
                self.recovery = True
                self.stuck_deadlock = False
            else:
                self.recovery = False
                if opp_avg_recent <= 0.05:
                    self.stuck_deadlock = True
                else:
                    self.stuck_deadlock = False
            self.last_test_round = None
            self.pre_test_avg = None
        if self.stuck_deadlock:
            if random.random() < self.p_test:
                return Action.C
            return Action.D
        if opp_avg_recent >= self.T_high:
            return Action.C
        if opp_avg_recent <= self.T_low:
            if opp_avg_recent <= 0.05 and random.random() < self.p_test:
                return Action.C
            return Action.D
        p = 0.2 + 0.8 * opp_avg_recent
        if self.H_end > 0 and t >= r - self.H_end and (t < r - 1):
            remaining_rounds = r - t - 1
            scale = max(0.0, min(1.0, remaining_rounds / float(self.H_end)))
            p = p * scale
        if p <= 0.0:
            return Action.D
        if p >= 1.0:
            return Action.C
        if opp_avg_recent <= 0.05 and random.random() < self.p_test:
            return Action.C
        if random.random() < p:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    Builds cooperative reputation early, opportunistically defects when many others
    still cooperate and the group is unlikely to punish, quickly switches to a short
    defensive punish mode if retaliation is detected, and periodically probes to
    rediscover exploitation opportunities. Always defects in the final round(s).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.k = float(game_description.k)
        self.r = int(game_description.n_rounds)
        self.initial_rounds = min(3, max(0, self.r - 1))
        self.endgame_rounds = 1
        self.min_coop_for_exploit = 0.7
        self.punish_threshold = 0.25
        self.probe_prob_base = 0.15
        self.exploit_prob_max = 0.6
        self.punish_cooldown = 2
        self.punish_detect_drop = 0.3
        self.incentive = max(0.0, 1.0 - self.k / max(1.0, float(self.n)))
        self.mode = 'NORMAL'
        self.punish_timer = 0
        self.locked_defect = False
        self.low_coop_streak = 0
        self.rebuild_timer = 0
        self.consecutive_probes_no_retaliation = 0
        self.last_checked_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= 1:
            return Action.D
        if t >= max(0, self.r - self.endgame_rounds):
            return Action.D
        if t == 0 or history is None:
            if self.initial_rounds > 0:
                return Action.C
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_played = opp_actions.shape[0]
        W = min(10, max(1, rounds_played))
        if opp_actions.size == 0:
            coop_rate_j = np.zeros((max(0, self.n - 1),), dtype=float)
        else:
            coop_rate_j = np.mean(opp_actions.astype(float), axis=0) if opp_actions.shape[0] > 0 else np.zeros((opp_actions.shape[1],), dtype=float)
        if opp_actions.shape[0] >= 1:
            recent = opp_actions[-W:, :] if opp_actions.shape[0] >= W else opp_actions
            per_round_frac = np.mean(recent.astype(float), axis=1) if recent.size > 0 else np.array([0.0])
            R = float(np.mean(per_round_frac)) if per_round_frac.size > 0 else 0.0
        else:
            R = 0.0
        num_opponents = max(0, self.n - 1)
        retaliate_j = np.zeros((num_opponents,), dtype=float)
        if rounds_played >= 2 and num_opponents > 0:
            my_def_indices = np.where(my_actions == False)[0]
            valid_def_indices = my_def_indices[my_def_indices + 1 < rounds_played]
            if valid_def_indices.size > 0:
                counts = np.zeros((num_opponents,), dtype=float)
                totals = np.zeros((num_opponents,), dtype=float)
                for d in valid_def_indices:
                    after_row = opp_actions[d + 1, :].astype(float)
                    punished = 1.0 - after_row
                    counts += punished
                    totals += 1.0
                mask = totals > 0
                retaliate_j[mask] = counts[mask] / totals[mask]
                retaliate_j[~mask] = 0.0
            else:
                retaliate_j[:] = 0.0
        else:
            retaliate_j[:] = 0.0
        P_avg = float(np.mean(retaliate_j)) if retaliate_j.size > 0 else 0.0
        punishment_detected = False
        if rounds_played >= 2:
            my_def_indices = np.where(my_actions == False)[0]
            for d in my_def_indices:
                if d + 1 >= rounds_played:
                    continue
                start_before = max(0, d - W)
                end_before = d
                if end_before - start_before <= 0:
                    continue
                before_slice = opp_actions[start_before:end_before, :]
                if before_slice.size == 0:
                    continue
                before_avg = float(np.mean(before_slice.astype(float)))
                after_avg = float(np.mean(opp_actions[d + 1, :].astype(float)))
                if before_avg - after_avg > self.punish_detect_drop:
                    punishment_detected = True
                    break
        if punishment_detected and self.mode != 'PUNISH':
            self.mode = 'PUNISH'
            self.punish_timer = int(self.punish_cooldown)
        check_window = max(3, W)
        if opp_actions.shape[0] >= 1:
            recent_all = opp_actions[-check_window:, :] if opp_actions.shape[0] >= check_window else opp_actions
            recent_mean = float(np.mean(recent_all.astype(float))) if recent_all.size > 0 else 0.0
            if recent_mean < self.min_coop_for_exploit * 0.5:
                self.low_coop_streak += 1
            else:
                self.low_coop_streak = 0
        else:
            self.low_coop_streak = 0
        if self.low_coop_streak >= max(3, self.punish_cooldown * 2):
            self.locked_defect = True
        if self.locked_defect and R >= self.min_coop_for_exploit:
            self.locked_defect = False
            self.rebuild_timer = int(self.initial_rounds)
            self.low_coop_streak = 0
        if self.rebuild_timer > 0:
            self.rebuild_timer -= 1
            return Action.C
        if self.locked_defect:
            return Action.D
        if self.mode == 'PUNISH' and self.punish_timer > 0:
            self.punish_timer -= 1
            if self.punish_timer <= 0:
                self.mode = 'NORMAL'
            return Action.D
        if t < self.initial_rounds:
            return Action.C
        p_exploit = 0.0
        if R < self.min_coop_for_exploit:
            p_exploit = 0.0
        elif P_avg > self.punish_threshold:
            p_exploit = 0.0
        else:
            denom = max(1e-09, 1.0 - self.min_coop_for_exploit)
            frac = (R - self.min_coop_for_exploit) / denom
            frac = max(0.0, min(1.0, frac))
            p_exploit = self.probe_prob_base + (self.exploit_prob_max - self.probe_prob_base) * frac * self.incentive
            p_exploit = max(0.0, min(self.exploit_prob_max, p_exploit))
        draw = random.random()
        choose_defect = draw < p_exploit
        if choose_defect:
            self.consecutive_probes_no_retaliation += 1
            if self.consecutive_probes_no_retaliation > 10:
                self.consecutive_probes_no_retaliation = 10
            return Action.D
        self.consecutive_probes_no_retaliation = 0
        return Action.C

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    Adaptive Exploitative Conditional Defector (AECD)

    Exploit when the measured risk of sustained retaliation is low, probe early to identify
    unconditional cooperators, use small-probability maintenance cooperation to avoid
    mass collapse, and enter recovery mode when group cooperation falls after my defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        default_w = round(self.r / 6) if self.r >= 6 else 5
        self.W = min(30, max(5, default_w))
        self.delta = 0.25
        self.p_maintain = max(0.05, min(0.15, 3 / self.r)) if self.r > 0 else 0.05
        self.G_low = 0.15
        self.GRF_high = 0.4
        self.forgiveness_window = max(1, self.W // 3)
        self.soft_cooperators = set()
        self.rehab_counters = dict()
        self.prev_resp = None
        self.recovery_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C if self.r > 1 else Action.D
        rounds_played = int(state.round_number)
        opponents_count = self.n - 1
        R_remain = max(0, self.r - state.round_number)
        W = self.W
        window_start = max(0, rounds_played - W)
        window_end = rounds_played
        my_actions_full = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions_full = np.asarray(history.opponent_actions, dtype=np.bool_)
        my_window = my_actions_full[window_start:window_end]
        opp_window = opp_actions_full[window_start:window_end, :]
        w = my_window.shape[0]
        if w == 0:
            CR = np.zeros(opponents_count, dtype=float)
        else:
            CR = np.mean(opp_window.astype(float), axis=0)
        if w <= 1:
            Resp = np.zeros(opponents_count, dtype=float)
        else:
            curr_my = my_window[:-1]
            next_opp = opp_window[1:, :]
            coop_mask = curr_my.astype(bool)
            defect_mask = ~coop_mask
            coop_count = int(np.sum(coop_mask))
            defect_count = int(np.sum(defect_mask))
            if coop_count > 0:
                avg_after_coop = np.sum(next_opp[coop_mask, :].astype(float), axis=0) / coop_count
            else:
                avg_after_coop = CR.copy()
            if defect_count > 0:
                avg_after_def = np.sum(next_opp[defect_mask, :].astype(float), axis=0) / defect_count
            else:
                avg_after_def = CR.copy()
            Resp = avg_after_coop - avg_after_def
        GCR = float(np.mean(CR)) if opponents_count > 0 else 0.0
        GRF = float(np.mean((Resp >= self.delta).astype(float))) if opponents_count > 0 else 0.0
        for idx in range(opponents_count):
            if CR[idx] >= 0.95 and Resp[idx] < 0.05:
                self.soft_cooperators.add(idx)
            else:
                pass
        if self.prev_resp is None:
            for i in range(opponents_count):
                self.rehab_counters[i] = 1 if Resp[i] < self.delta else 0
        else:
            for i in range(opponents_count):
                if Resp[i] < self.delta:
                    self.rehab_counters[i] = self.rehab_counters.get(i, 0) + 1
                else:
                    self.rehab_counters[i] = 0
        rehabilitated = {i for i, c in self.rehab_counters.items() if c >= 2}
        recovery_triggered = False
        if w >= max(3, self.forgiveness_window * 2):
            third = max(1, w // 3)
            first_third = opp_window[:third, :]
            last_third = opp_window[-third:, :]
            mean_first = float(np.mean(first_third.astype(float))) if first_third.size > 0 else 0.0
            mean_last = float(np.mean(last_third.astype(float))) if last_third.size > 0 else 0.0
            drop = mean_first - mean_last
            we_defected_recently = np.any(~my_window.astype(bool))
            if drop >= 0.15 and we_defected_recently:
                recovery_triggered = True
        if recovery_triggered and self.recovery_remaining <= 0:
            self.recovery_remaining = max(1, self.forgiveness_window)
        if self.recovery_remaining > 0:
            self.recovery_remaining -= 1
            self.prev_resp = Resp.copy()
            return Action.C
        if GCR <= self.G_low:
            self.prev_resp = Resp.copy()
            return Action.D
        if state.round_number < min(3, self.r):
            self.prev_resp = Resp.copy()
            return Action.C
        near_end_threshold = max(1, round(W / 6))
        if R_remain <= near_end_threshold:
            severe_retaliators = np.sum((Resp >= 0.5).astype(int))
            if opponents_count > 0 and severe_retaliators / opponents_count >= 0.7:
                self.prev_resp = Resp.copy()
                return Action.C
            else:
                self.prev_resp = Resp.copy()
                return Action.D
        g = 1.0 - self.k / float(max(1, self.n))
        delta_pos = np.maximum(Resp, 0.0)
        sum_delta = float(np.sum(delta_pos))
        D_est = float(min(W, R_remain))
        per_round_loss = self.k / float(max(1, self.n)) * sum_delta
        L = D_est * per_round_loss
        if len(self.soft_cooperators) > 0 and GRF < self.GRF_high:
            if random.random() < self.p_maintain:
                self.prev_resp = Resp.copy()
                return Action.C
            else:
                self.prev_resp = Resp.copy()
                return Action.D
        if g > L and GRF < self.GRF_high:
            if random.random() < self.p_maintain:
                self.prev_resp = Resp.copy()
                return Action.C
            else:
                self.prev_resp = Resp.copy()
                return Action.D
        self.prev_resp = Resp.copy()
        return Action.C

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    Exploitative Conditional Cooperator (ECC)

    Continuously estimate recent group cooperation; undercut cooperative groups by contributing
    slightly less (free-riding) while withholding contributions from defectors. Detects whether
    defections provoke retaliation and increases cooperation to avoid costly punishment.
    Always defects in the final few rounds. Includes small randomized probing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(20, max(1, math.floor(self.r / 10)))
        self.last_rounds = min(3, self.r)
        self.T_high = 0.7
        self.T_low = 0.3
        self.exploit_margin = 0.25
        self.punish_sensitivity = 0.15
        self.forgiveness_boost = 0.4
        self.test_prob = 0.03
        self.min_support = 0.05
        self.first_round_probe = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t > self.r - self.last_rounds - 1:
            return Action.D
        if t == 0 or history is None:
            return Action.C if random.random() < self.first_round_probe else Action.D
        start = max(0, t - self.W)
        end = t
        L = end - start
        if L <= 0:
            return Action.C if random.random() < self.first_round_probe else Action.D
        opp_slice = history.opponent_actions[start:end, :]
        my_slice = history.my_actions[start:end]
        m = opp_slice.shape[1] if opp_slice.ndim == 2 else 0
        if m > 0:
            f_j = np.mean(opp_slice.astype(float), axis=0)
            F = float(np.mean(f_j)) if f_j.size > 0 else 0.0
        else:
            F = 0.0
        others_per_round = np.sum(opp_slice.astype(float), axis=1)
        coop_mask = np.asarray(my_slice).astype(bool)
        defect_mask = ~coop_mask
        if coop_mask.any():
            G_coop = float(np.mean(others_per_round[coop_mask]))
        else:
            G_coop = float(np.mean(others_per_round)) if L > 0 else 0.0
        if defect_mask.any():
            G_def = float(np.mean(others_per_round[defect_mask]))
        else:
            G_def = float(np.mean(others_per_round)) if L > 0 else 0.0
        delta = G_coop - G_def
        retaliated = bool(delta > self.punish_sensitivity)
        if retaliated:
            p = min(1.0, F + self.forgiveness_boost)
        elif F >= self.T_high:
            p = max(self.min_support, F - self.exploit_margin)
        elif F <= self.T_low:
            p = 0.0
        else:
            p = max(0.0, F - self.exploit_margin / 2.0)
        if p < 0.0:
            p = 0.0
        elif p > 1.0:
            p = 1.0
        if random.random() < self.test_prob:
            return Action.D
        return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    Collective Responsive Conditional Cooperator (CRCC)

    - Starts cooperative (unless the game is a single-round game, in which case defects).
    - Reciprocates when a clear majority of others cooperated in the previous round.
    - Forgives a single isolated defector immediately.
    - If cooperation is not observed, defects and issues a short, proportional punishment
      whose length scales with distinct defectors observed in a recent window.
    - Punishments can be extended if more distinct defectors appear during punishment.
    - After punishment ends, probes with small probability epsilon to test for restored cooperation.
    - In the short endgame (last few rounds before final), favors defection on close/tie-like signals.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = max(1, min(5, math.floor(self.r / 4)))
        self.majority_threshold = math.ceil((self.n - 1) / 2)
        self.max_punish = min(3, max(1, math.floor(self.r / 10)))
        self.epsilon = 0.1
        self.endgame_k = min(3, math.floor(self.r / 10))
        self.punish_until_round = 0
        self._last_seen_punish_until = -1
        self._tie_margin = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            past_rounds = 0
        else:
            past_rounds = 0 if history.opponent_actions is None else history.opponent_actions.shape[0]
        if t == 0:
            if self.r == 1:
                self._last_seen_punish_until = self.punish_until_round
                return Action.D
            else:
                self._last_seen_punish_until = self.punish_until_round
                return Action.C
        if t == self.r - 1:
            self._last_seen_punish_until = self.punish_until_round
            return Action.D
        if history is None:
            self._last_seen_punish_until = self.punish_until_round
            return Action.D
        last_w_start = max(0, t - self.w)
        window_len = t - last_w_start
        if window_len <= 0:
            window_len = 1
            last_w_start = max(0, t - 1)
        other_coops_last = int(np.sum(history.opponent_actions[t - 1, :]))
        other_defectors_last = self.n - 1 - other_coops_last
        window_actions = history.opponent_actions[last_w_start:t, :]
        try:
            defected_any = np.any(~window_actions, axis=0)
            distinct_defectors_in_window = int(np.sum(defected_any))
        except Exception:
            distinct_defectors_in_window = int(np.sum(~history.opponent_actions[t - 1, :]))
        total_possible = (self.n - 1) * window_len
        coop_count = int(np.sum(window_actions))
        if total_possible > 0:
            recent_coop_fraction = coop_count / total_possible
        else:
            recent_coop_fraction = 0.0
        in_endgame = False
        if self.endgame_k > 0:
            endgame_start = max(0, self.r - self.endgame_k - 1)
            endgame_end = max(0, self.r - 2)
            in_endgame = t >= endgame_start and t <= endgame_end
        first_round_after_punish = self._last_seen_punish_until >= t - 1 and self._last_seen_punish_until < t
        if t <= self.punish_until_round:
            L_prime = 1 + min(self.max_punish - 1, distinct_defectors_in_window)
            new_punish_until = t + L_prime - 1
            if new_punish_until > self.punish_until_round:
                self.punish_until_round = new_punish_until
            self._last_seen_punish_until = self.punish_until_round
            return Action.D
        if first_round_after_punish:
            if random.random() < self.epsilon:
                self._last_seen_punish_until = self.punish_until_round
                return Action.C
        if other_coops_last >= self.majority_threshold:
            if in_endgame and abs(recent_coop_fraction - 0.5) <= self._tie_margin:
                self._last_seen_punish_until = self.punish_until_round
                return Action.D
            self._last_seen_punish_until = self.punish_until_round
            return Action.C
        if other_coops_last == self.majority_threshold - 1 and other_defectors_last == 1:
            if in_endgame and abs(recent_coop_fraction - 0.5) <= self._tie_margin:
                self._last_seen_punish_until = self.punish_until_round
                return Action.D
            self._last_seen_punish_until = self.punish_until_round
            return Action.C
        L = 1 + min(self.max_punish - 1, distinct_defectors_in_window)
        self.punish_until_round = t + L - 1
        self._last_seen_punish_until = self.punish_until_round
        return Action.D

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    Adaptive Exploit-Reciprocator

    - Probe early rounds to classify opponents as Safe (S), Reciprocator (R), or Punisher/Defector (P).
    - If many Safe Cooperators exist, enter an exploit mode (defect) but monitor for retaliation and abort quickly.
    - Otherwise use Reciprocal mode: cooperate when group cooperation was high last round, perform occasional test defections,
      and apply short, targeted punishments (L_punish rounds) to retaliators then forgive.
    - Always defect in last L_end rounds. If cooperation collapses (safety fallback), switch to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_probe = min(8, max(3, math.floor(self.r / 10)))
        self.L_end = max(1, math.floor(self.r / 5))
        self.L_punish = min(3, max(1, math.floor(self.r / 10)))
        self.p_test = min(0.12, 4.0 / max(1.0, float(self.r)))
        self.W_window = max(3, math.floor(self.r / 10))
        self.exploit_mode = False
        self.exploit_start_round = None
        self.exploit_safe_indices = []
        self.punishment_counter = 0
        self.permanent_defect = False
        self.last_exploit_check = -1
        self.known_retaliators = set()
        self._eps = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t >= max(0, self.r - self.L_end):
            return Action.D
        if history is None or t == 0:
            return Action.C
        rounds_so_far = history.my_actions.shape[0]
        opp_count = max(0, self.n - 1)
        window_len = min(self.W_window, rounds_so_far)
        if window_len >= 1:
            start_idx = rounds_so_far - window_len
            opp_slice = history.opponent_actions[start_idx:, :] if opp_count > 0 else np.zeros((window_len, 0), dtype=np.bool_)
            my_slice = history.my_actions[start_idx:]
            coop_counts = np.sum(opp_slice.astype(np.int64), axis=1) + my_slice.astype(np.int64)
            avg_coop_fraction = float(np.mean(coop_counts / max(1, self.n)))
            if avg_coop_fraction < 0.3 - self._eps:
                self.permanent_defect = True
        if self.permanent_defect:
            return Action.D

        def compute_p_i(probe_only: bool=True):
            if opp_count == 0:
                return np.array([], dtype=float)
            if probe_only:
                length = min(self.T_probe, rounds_so_far)
                if length <= 0:
                    return np.zeros(opp_count, dtype=float)
                slice_actions = history.opponent_actions[0:length, :]
            else:
                slice_actions = history.opponent_actions[:, :]
            if slice_actions.size == 0:
                return np.zeros(opp_count, dtype=float)
            return np.asarray(np.mean(slice_actions.astype(float), axis=0))

        def compute_r_i():
            if opp_count == 0:
                return np.array([], dtype=float)
            my_actions = history.my_actions[:rounds_so_far]
            opp_actions = history.opponent_actions[:rounds_so_far, :]
            defect_rounds = [idx for idx in range(rounds_so_far) if not bool(my_actions[idx])]
            if len(defect_rounds) == 0:
                return np.zeros(opp_count, dtype=float)
            diffs = []
            for d in defect_rounds:
                b0 = max(0, d - 2)
                b1 = d - 1
                if b1 < b0:
                    before = np.zeros(opp_count, dtype=float)
                else:
                    before = np.mean(opp_actions[b0:b1 + 1, :].astype(float), axis=0)
                a0 = d + 1
                a1 = min(rounds_so_far - 1, d + 2)
                if a0 > a1:
                    after = np.zeros(opp_count, dtype=float)
                else:
                    after = np.mean(opp_actions[a0:a1 + 1, :].astype(float), axis=0)
                diffs.append(after - before)
            if len(diffs) == 0:
                return np.zeros(opp_count, dtype=float)
            diffs_arr = np.vstack(diffs)
            return np.mean(diffs_arr, axis=0)
        p_probe = compute_p_i(probe_only=True)
        p_total = compute_p_i(probe_only=False)
        r_vals = compute_r_i()

        def classify_players(p_array, r_array):
            classes = ['P'] * opp_count
            for idx in range(opp_count):
                p_i = float(p_array[idx]) if idx < len(p_array) else 0.0
                r_i = float(r_array[idx]) if idx < len(r_array) else 0.0
                if p_i >= 0.8 - self._eps and r_i <= 0.15 + self._eps:
                    classes[idx] = 'S'
                elif p_i >= 0.4 - self._eps and r_i >= 0.15 - self._eps:
                    classes[idx] = 'R'
                else:
                    classes[idx] = 'P'
            return classes
        p_for_class = p_probe if rounds_so_far >= 1 else p_total
        classes = classify_players(p_for_class, r_vals)
        for idx, r_i in enumerate(r_vals):
            if r_i >= 0.15 - self._eps:
                self.known_retaliators.add(idx)
        if t < self.T_probe:
            if t == 1:
                return Action.D
            if random.random() < self.p_test:
                return Action.D
            return Action.C
        safe_indices = [i for i, c in enumerate(classes) if c == 'S']
        count_S = len(safe_indices)
        exploit_threshold = math.ceil(self.n / 2)
        if not self.exploit_mode and count_S >= exploit_threshold:
            if self.punishment_counter <= 0:
                self.exploit_mode = True
                self.exploit_start_round = t
                self.exploit_safe_indices = list(safe_indices)
        if self.exploit_mode:
            if count_S < exploit_threshold:
                self.exploit_mode = False
                self.exploit_start_round = None
                self.exploit_safe_indices = []
            else:
                if self.exploit_start_round is not None and t <= self.exploit_start_round + 2:
                    pre_b = max(0, self.exploit_start_round - 2)
                    pre_e = max(0, self.exploit_start_round - 1)
                    post_b = self.exploit_start_round
                    post_e = min(rounds_so_far - 1, self.exploit_start_round + 1)
                    if pre_e >= pre_b and post_e >= post_b and (len(self.exploit_safe_indices) > 0):
                        opp_actions = history.opponent_actions
                        before_rates = []
                        after_rates = []
                        for idx in self.exploit_safe_indices:
                            before_slice = opp_actions[pre_b:pre_e + 1, idx] if pre_e >= pre_b else np.array([], dtype=np.bool_)
                            after_slice = opp_actions[post_b:post_e + 1, idx] if post_e >= post_b else np.array([], dtype=np.bool_)
                            before_rate = float(np.mean(before_slice.astype(float))) if before_slice.size > 0 else 0.0
                            after_rate = float(np.mean(after_slice.astype(float))) if after_slice.size > 0 else 0.0
                            before_rates.append(before_rate)
                            after_rates.append(after_rate)
                        reduced_count = sum((1 for b, a in zip(before_rates, after_rates) if b - a >= 0.2 - self._eps))
                        frac_reduced = reduced_count / max(1, len(self.exploit_safe_indices))
                        if frac_reduced >= 0.25 - self._eps:
                            self.exploit_mode = False
                            self.exploit_start_round = None
                            self.exploit_safe_indices = []
                if self.punishment_counter > 0:
                    self.punishment_counter -= 1
                    return Action.D
                return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        for idx, r_i in enumerate(r_vals):
            if r_i >= 0.15 - self._eps and idx not in self.known_retaliators:
                self.known_retaliators.add(idx)
                self.punishment_counter = self.L_punish
                self.punishment_counter -= 1
                return Action.D
        last_round_idx = rounds_so_far - 1
        last_round_cooperators = 0
        if opp_count > 0:
            last_round_cooperators += int(np.sum(history.opponent_actions[last_round_idx, :].astype(np.int64)))
        last_round_cooperators += int(bool(history.my_actions[last_round_idx]))
        if last_round_cooperators >= math.ceil(self.n / 2):
            near_unanimous = last_round_cooperators >= max(0, self.n - 1)
            high_or_safe = 0
            for idx in range(opp_count):
                if classes[idx] == 'S' or (p_total.size > idx and p_total[idx] >= 0.8 - self._eps):
                    high_or_safe += 1
            if near_unanimous and high_or_safe >= 2 and (random.random() < self.p_test):
                return Action.D
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    Exploitative Public Goods strategy.

    - Probe briefly to estimate which opponents respond to my cooperation.
    - Maintain sets of Unconditional cooperators and Coaxable (responsive) players.
    - Default to defect; free-ride when a safe pool of cooperators exists.
    - Use short, credible punishments for betrayals, and occasional cooperation to rebuild the pool.
    - Always defect in the final endgame rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = max(3, min(10, math.floor(self.r / 4)))
        self.probe_rounds = min(6, max(1, math.ceil(self.r * 0.08)))
        self.endgame_len = min(2, max(1, math.ceil(self.r * 0.05)))
        self.coop_rate_thresh = 0.5
        self.resp_thresh = 0.15
        self.safe_exploit_count = max(1, (self.n - 1) // 3)
        self.punish_rounds = min(3, max(1, math.ceil(self.r * 0.03)))
        self.rebuild_interval = max(3, math.ceil(self.n / 2))
        self.n_opponents = max(0, self.n - 1)
        self.coaxable = set()
        self.unconditional = set()
        self.punished_until = {}
        self.punish_remaining = 0
        self.last_punish_start = None
        self.prev_S = 0
        self.prev_coop_rates = [0.0] * self.n_opponents
        self.prev_coaxable = set()
        self.prev_unconditional = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        if r <= 1:
            return Action.D
        if r <= 3:
            if t == 0:
                return Action.D
            return Action.D
        if t >= r - self.endgame_len:
            return Action.D
        if history is None or history.my_actions.size == 0:
            if t == 0:
                return Action.C
            return Action.D
        rounds_so_far = int(history.my_actions.shape[0])
        opp_shape_rounds = history.opponent_actions.shape[0]
        opp_shape_players = history.opponent_actions.shape[1] if opp_shape_rounds > 0 else self.n_opponents
        n_obs_opponents = int(opp_shape_players)
        if t < self.probe_rounds:
            return Action.C if t % 2 == 0 else Action.D
        window_len = min(self.w, t)
        window_start = max(0, t - window_len)
        prev_window_len = min(self.w, max(0, t - window_len))
        prev_window_start = max(0, t - 2 * window_len)
        coop_rates = [0.0] * n_obs_opponents
        coop_after_myC = [0] * n_obs_opponents
        total_after_myC = [0] * n_obs_opponents
        coop_after_myD = [0] * n_obs_opponents
        total_after_myD = [0] * n_obs_opponents
        coop_rates_prev = [0.0] * n_obs_opponents
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        if window_len > 0:
            seg = opp_actions[window_start:t, :] if window_start < t else np.zeros((0, n_obs_opponents), dtype=bool)
            if seg.shape[0] > 0:
                sums = np.sum(seg, axis=0)
                for i in range(n_obs_opponents):
                    coop_rates[i] = float(sums[i]) / float(window_len)
            else:
                coop_rates = [0.0] * n_obs_opponents
        if t - 1 >= window_start:
            for s in range(window_start, t - 1):
                myC = bool(my_actions[s])
                next_row = opp_actions[s + 1, :]
                for i in range(n_obs_opponents):
                    if myC:
                        total_after_myC[i] += 1
                        if bool(next_row[i]):
                            coop_after_myC[i] += 1
                    else:
                        total_after_myD[i] += 1
                        if bool(next_row[i]):
                            coop_after_myD[i] += 1
        if prev_window_len > 0 and prev_window_start < t - window_len:
            prev_seg = opp_actions[prev_window_start:t - window_len, :] if t - window_len > prev_window_start else np.zeros((0, n_obs_opponents), dtype=bool)
            if prev_seg.shape[0] > 0:
                prev_sums = np.sum(prev_seg, axis=0)
                for i in range(n_obs_opponents):
                    coop_rates_prev[i] = float(prev_sums[i]) / float(prev_seg.shape[0])
            else:
                coop_rates_prev = [0.0] * n_obs_opponents
        else:
            coop_rates_prev = [0.0] * n_obs_opponents
        new_unconditional = set()
        new_coaxable = set()
        for i in range(n_obs_opponents):
            punished_until_round = int(self.punished_until.get(i, -1))
            if punished_until_round > t:
                continue
            coop_rate_i = coop_rates[i]
            denomC = max(1, total_after_myC[i])
            denomD = max(1, total_after_myD[i])
            resp_i = coop_after_myC[i] / denomC - coop_after_myD[i] / denomD
            if coop_rate_i >= 0.8:
                new_unconditional.add(i)
            elif coop_rate_i >= self.coop_rate_thresh and resp_i >= self.resp_thresh:
                new_coaxable.add(i)
        betrayal_detected = False
        for i in range(n_obs_opponents):
            was_in_pool = i in self.prev_coaxable or i in self.prev_unconditional
            if not was_in_pool:
                continue
            drop = coop_rates_prev[i] - coop_rates[i]
            denomC = max(1, total_after_myC[i])
            coop_after_myC_rate = coop_after_myC[i] / denomC
            consistent_betrayal = total_after_myC[i] >= 2 and coop_after_myC_rate < 0.2
            if drop > 0.3 or consistent_betrayal:
                self.punished_until[i] = t + self.punish_rounds
                if i in new_coaxable:
                    new_coaxable.discard(i)
                if i in new_unconditional:
                    new_unconditional.discard(i)
                self.punish_remaining = self.punish_rounds
                self.last_punish_start = t
                betrayal_detected = True
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            self.coaxable = new_coaxable
            self.unconditional = new_unconditional
            self.prev_S = len(self.coaxable) + len(self.unconditional)
            self.prev_coaxable = set(self.coaxable)
            self.prev_unconditional = set(self.unconditional)
            self.prev_coop_rates = coop_rates
            return action
        S = len(new_unconditional) + len(new_coaxable)
        if S >= self.safe_exploit_count:
            allow_rebuild = True
            if self.last_punish_start is not None and t - self.last_punish_start <= self.punish_rounds // 2:
                allow_rebuild = False
            if S == self.safe_exploit_count and self.rebuild_interval > 0 and (t % self.rebuild_interval == 0) and allow_rebuild:
                chosen = Action.C
            else:
                chosen = Action.D
            self.coaxable = new_coaxable
            self.unconditional = new_unconditional
            self.prev_S = S
            self.prev_coaxable = set(self.coaxable)
            self.prev_unconditional = set(self.unconditional)
            self.prev_coop_rates = coop_rates
            return chosen
        if S == 0:
            chosen = Action.C
            self.coaxable = new_coaxable
            self.unconditional = new_unconditional
            self.prev_S = S
            self.prev_coaxable = set(self.coaxable)
            self.prev_unconditional = set(self.unconditional)
            self.prev_coop_rates = coop_rates
            return chosen
        fragile_condition = S == max(0, self.safe_exploit_count - 1)
        if fragile_condition and random.random() < 0.25:
            chosen = Action.C
            self.coaxable = new_coaxable
            self.unconditional = new_unconditional
            self.prev_S = S
            self.prev_coaxable = set(self.coaxable)
            self.prev_unconditional = set(self.unconditional)
            self.prev_coop_rates = coop_rates
            return chosen
        last_my_action = bool(my_actions[-1])
        if last_my_action is False:
            chosen = Action.C
        else:
            chosen = Action.D
        self.coaxable = new_coaxable
        self.unconditional = new_unconditional
        self.prev_S = S
        self.prev_coaxable = set(self.coaxable)
        self.prev_unconditional = set(self.unconditional)
        self.prev_coop_rates = coop_rates
        return chosen

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    Adaptive Conditional Exploiter (ACE)

    - Signals cooperation first round to attract cooperators.
    - Uses a sliding window estimate of recent group cooperation (G).
    - Probabilistically "probes" (one-shot defects) when group cooperation is high,
      scaled down when public-good multiplier k/n is high.
    - Swiftly applies finite punishment when cooperation collapses or when clearly
      made a sucker, then conducts a cooperative test and conditionally rebuilds trust.
    - Always defects in a short endgame; be aggressive when the total number of rounds is small.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.E = min(2, self.r)
        self.P = min(4, max(1, math.floor(self.r / 10)))
        self.T_coop = 0.6
        self.base_exploit_max_init = 0.25
        self.s_mult = 1.0 - self.k / max(1.0, float(self.n))
        self.punish_timer = 0
        self.base_exploit_max = float(self.base_exploit_max_init)
        self._pre_punish_base_exploit_max = float(self.base_exploit_max_init)
        self.pending_probes = []
        self.post_punish_test_pending = False
        self.awaiting_post_punish_evaluation = False
        self.post_punish_evaluate_after = None
        self.post_punish_failed = False
        self.last_G = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n_players = self.n
        rounds_total = self.r
        if t == 0 or history is None:
            self.pending_probes = []
            self.post_punish_test_pending = False
            self.awaiting_post_punish_evaluation = False
            self.post_punish_failed = False
            self.punish_timer = 0
            self.last_G = 1.0
            return Action.C
        if rounds_total <= 4 and t > 0:
            return Action.D
        if t >= rounds_total - self.E:
            return Action.D

        def compute_G_over_window(start_idx: int, end_idx: int) -> float:
            if start_idx > end_idx:
                return None
            opp = history.opponent_actions
            max_idx = opp.shape[0] - 1
            if max_idx < 0:
                return None
            s = max(0, start_idx)
            e = min(max_idx, end_idx)
            if s > e:
                return None
            counts = np.sum(opp[s:e + 1, :], axis=1).astype(float)
            fractions = counts / max(1, n_players - 1)
            return float(np.mean(fractions))
        last_round_index = t - 1
        window_end = last_round_index
        window_start = max(0, t - self.W)
        G = compute_G_over_window(window_start, window_end)
        if G is None:
            G = 1.0
        prev_end = window_start - 1
        prev_start = max(0, window_start - self.W)
        G_prev = compute_G_over_window(prev_start, prev_end)
        if G_prev is None:
            Delta = 0.0
        else:
            Delta = G - G_prev
        self.last_G = G
        remaining_probes = []
        for p in self.pending_probes:
            probe_round = p.get('round', -999)
            pre_G = p.get('pre_G', G)
            age = t - probe_round
            if 1 <= age <= 2:
                if pre_G - G > 0.1:
                    self.base_exploit_max = max(0.0, self.base_exploit_max * 0.5)
                    if pre_G - G >= 0.25:
                        self.punish_timer = self.P
                        self._pre_punish_base_exploit_max = self.base_exploit_max
                else:
                    pass
            elif age <= 0:
                remaining_probes.append(p)
            else:
                pass
        self.pending_probes = remaining_probes
        was_sucker = False
        try:
            if history.my_actions.shape[0] >= 1:
                if bool(history.my_actions[-1]) is True:
                    opp_last = int(np.sum(history.opponent_actions[-1, :]))
                    if opp_last == 0:
                        was_sucker = True
        except Exception:
            was_sucker = False
        if was_sucker or Delta <= -0.25:
            self.punish_timer = self.P
            self._pre_punish_base_exploit_max = self.base_exploit_max
        if self.punish_timer > 0:
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.post_punish_test_pending = True
            return Action.D
        if self.post_punish_test_pending:
            self.post_punish_test_pending = False
            self.awaiting_post_punish_evaluation = True
            self.post_punish_evaluate_after = t + self.W
            self._pre_punish_base_exploit_max = max(self.base_exploit_max, self._pre_punish_base_exploit_max)
            return Action.C
        if self.awaiting_post_punish_evaluation and t >= (self.post_punish_evaluate_after or t + 1):
            eval_window_end = t - 1
            eval_window_start = max(0, t - self.W)
            G_post = compute_G_over_window(eval_window_start, eval_window_end)
            if G_post is None:
                G_post = 0.0
            if G_post >= self.T_coop:
                self.base_exploit_max = 0.5 * float(self._pre_punish_base_exploit_max)
                self.awaiting_post_punish_evaluation = False
                self.post_punish_failed = False
            else:
                self.awaiting_post_punish_evaluation = False
                self.post_punish_failed = True
                self.base_exploit_max = min(self.base_exploit_max, 0.5 * float(self._pre_punish_base_exploit_max))
        if self.post_punish_failed:
            if G < self.T_coop:
                return Action.D
            else:
                self.post_punish_failed = False
                self.base_exploit_max = 0.5 * float(self._pre_punish_base_exploit_max)
        if G < self.T_coop:
            return Action.D
        denom = max(1e-09, 1.0 - self.T_coop)
        raw = self.base_exploit_max * ((G - self.T_coop) / denom)
        prob_probe = raw * max(0.0, float(self.s_mult))
        prob_probe = max(0.0, min(prob_probe, self.base_exploit_max))
        do_probe = random.random() < prob_probe if prob_probe > 0.0 else False
        if do_probe:
            self.pending_probes.append({'round': t, 'pre_G': G})
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    Conditional cooperator for the N-player Public Goods Game.

    - Start by cooperating.
    - Use Win-Stay Lose-Shift (WSLS) based on actual vs. counterfactual payoff from the last round.
    - Detect persistent exploiters (low cooperation relative to group) and apply short punishments.
    - Be lenient when the group has been highly cooperative recently.
    - Small random probing to escape deadlocks and to test recovery during punishments.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(5, max(1, math.floor(self.r / 5)))
        self.exploiter_rate = 0.4
        self.group_good = 0.75
        self.p_probe = 0.02
        self._punish_counters: dict[int, int] = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == self.r - 1:
            self._punish_counters.clear()
            return Action.D
        if t == 0 or history is None:
            return Action.C
        window_start = max(0, t - self.W)
        num_window_rounds = t - window_start
        my_window = history.my_actions[window_start:t]
        opp_window = history.opponent_actions[window_start:t, :]
        self_coops = int(np.sum(my_window))
        self_rate = self_coops / num_window_rounds
        opp_coops_counts = np.sum(opp_window.astype(np.int64), axis=0) if num_window_rounds > 0 else np.zeros(self.n - 1, dtype=int)
        opp_rates = (opp_coops_counts / num_window_rounds).tolist()
        total_coops_window = self_coops + int(np.sum(opp_coops_counts))
        group_coop_rate = total_coops_window / (self.n * num_window_rounds) if num_window_rounds > 0 else 0.0
        flagged = []
        for j, rate in enumerate(opp_rates):
            if rate <= self.exploiter_rate and group_coop_rate - rate >= 0.2:
                flagged.append(j)
        remaining_rounds_for_punish = max(0, self.r - (t + 1))
        P = min(3, remaining_rounds_for_punish)
        for j in flagged:
            if P > 0 and j not in self._punish_counters:
                self._punish_counters[j] = P
        to_remove = []
        for j, remaining in list(self._punish_counters.items()):
            if j < len(opp_rates) and opp_rates[j] > self.exploiter_rate:
                to_remove.append(j)
        for j in to_remove:
            self._punish_counters.pop(j, None)
        active_punishments = [j for j, rem in self._punish_counters.items() if rem > 0]
        is_punishing = len(active_punishments) > 0
        if is_punishing:
            if random.random() < self.p_probe:
                action = Action.C
            else:
                action = Action.D
            for j in list(self._punish_counters.keys()):
                self._punish_counters[j] -= 1
                if self._punish_counters[j] <= 0:
                    self._punish_counters.pop(j, None)
            return action
        my_action_prev = bool(history.my_actions[-1])
        last_opp_actions = history.opponent_actions[-1, :]
        others_coops_last = int(np.sum(last_opp_actions))
        total_prev = int(my_action_prev) + others_coops_last
        my_payoff_prev = float(history.my_payoffs[-1])
        c_switched = 0 if my_action_prev else 1
        counterfactual = 1 - c_switched + self.k / self.n * (others_coops_last + c_switched)
        if my_payoff_prev >= counterfactual:
            action = Action.C if my_action_prev else Action.D
        else:
            action = Action.D if my_action_prev else Action.C
        if action == Action.D and group_coop_rate >= self.group_good:
            if random.random() < 0.9:
                action = Action.C
        if random.random() < self.p_probe:
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    Adaptive Exploitative Public-Goods Strategy.

    - Starts in a short PROBE phase (cooperate) to build reputation.
    - If others continue to contribute when I defect and few punishers exist, switch to EXPLOIT and defect mostly.
    - If others punish my defection (reduce contributions), switch to RECIPROCATE:
      conditional cooperation (cooperate when majority cooperated previously) and wait out punishers.
    - Always defect in the final round.
    - Small randomization (epsilon) to avoid brittle deterministic behavior and to probe.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.MPCR = self.k / max(1, self.n)
        self.T_probe = min(3, max(1, math.floor(self.r / 10)))
        self.L = min(10, max(3, math.floor(self.r / 5)))
        self.exploit_threshold = 0.6
        self.punish_sensitivity = 0.15
        self.punish_duration = 3
        self.epsilon = 0.03
        self.mode = 'PROBE'
        self.mode_rounds = 0
        self.bad_windows_count = 0
        self.recip_rounds_left = 0
        self.permanent_reciprocate = False
        self.awaiting_probe_result = False
        self.pending_one_shot_probe = False
        self.last_probe_round = -999
        self.probe_failures = 0
        self.max_probe_failures = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == self.r - 1:
            self.mode_rounds += 1
            self.awaiting_probe_result = False
            self.pending_one_shot_probe = False
            return Action.D
        if t == 0 or history is None:
            self.mode = 'PROBE'
            self.mode_rounds = 1
            return Action.C
        total_past = len(history.my_actions)
        start_idx = max(0, total_past - self.L)
        indices = list(range(start_idx, total_past))
        if len(indices) > 0:
            opp_window = history.opponent_actions[start_idx:total_past, :]
            my_window = history.my_actions[start_idx:total_past]
        else:
            opp_window = history.opponent_actions[0:0, :]
            my_window = history.my_actions[0:0]
        if opp_window.size == 0:
            per_round_opp_coop = np.array([], dtype=float)
        else:
            per_round_opp_coop = np.mean(opp_window.astype(float), axis=1)
        def_round_mask = None
        if my_window.size == 0:
            group_coop_when_I_def = 0.0
        else:
            def_round_mask = my_window == False
            if np.any(def_round_mask):
                arr = per_round_opp_coop[np.where(def_round_mask)[0]] if per_round_opp_coop.size > 0 else np.array([])
                if arr.size > 0:
                    group_coop_when_I_def = float(np.mean(arr))
                else:
                    group_coop_when_I_def = 0.0
            else:
                group_coop_when_I_def = float(np.mean(per_round_opp_coop)) if per_round_opp_coop.size > 0 else 0.0
        opp_count = max(0, self.n - 1)
        retaliation = np.zeros(opp_count, dtype=float)
        after_C_rows = []
        after_D_rows = []
        for r in range(max(1, start_idx + 1), total_past):
            prev_idx = r - 1
            prev_in_window_idx = prev_idx - start_idx
            cur_in_window_idx = r - start_idx
            if prev_in_window_idx < 0 or cur_in_window_idx < 0:
                continue
            if bool(history.my_actions[prev_idx]):
                after_C_rows.append(cur_in_window_idx)
            else:
                after_D_rows.append(cur_in_window_idx)
        if opp_window.size == 0:
            after_C_rate = np.zeros(opp_count, dtype=float)
            after_D_rate = np.zeros(opp_count, dtype=float)
        else:
            if len(after_C_rows) > 0:
                after_C_data = opp_window[np.array(after_C_rows, dtype=int), :]
                after_C_rate = np.mean(after_C_data.astype(float), axis=0)
            else:
                after_C_rate = np.zeros(opp_count, dtype=float)
            if len(after_D_rows) > 0:
                after_D_data = opp_window[np.array(after_D_rows, dtype=int), :]
                after_D_rate = np.mean(after_D_data.astype(float), axis=0)
            else:
                after_D_rate = np.zeros(opp_count, dtype=float)
        retaliation = after_C_rate - after_D_rate
        punishers_mask = retaliation >= self.punish_sensitivity
        punishers = int(np.sum(punishers_mask)) if opp_count > 0 else 0
        punisher_fraction = punishers / max(1, opp_count)
        if self.awaiting_probe_result:
            if group_coop_when_I_def >= self.exploit_threshold and punisher_fraction <= 0.25 and (not self.permanent_reciprocate):
                self.mode = 'EXPLOIT'
                self.mode_rounds = 1
                self.bad_windows_count = 0
                self.awaiting_probe_result = False
                self.pending_one_shot_probe = False
            else:
                self.mode = 'RECIPROCATE'
                self.mode_rounds = 1
                self.recip_rounds_left = max(self.recip_rounds_left, 0)
                self.awaiting_probe_result = False
                self.pending_one_shot_probe = False
                self.probe_failures += 1
        if self.mode == 'PROBE':
            if t < self.T_probe:
                self.mode_rounds += 1
                return Action.C
            elif group_coop_when_I_def >= self.exploit_threshold and punisher_fraction <= 0.25 and (not self.permanent_reciprocate):
                self.mode = 'EXPLOIT'
                self.mode_rounds = 1
            else:
                self.mode = 'RECIPROCATE'
                self.mode_rounds = 1
        if self.mode == 'EXPLOIT':
            if punisher_fraction > 0.5:
                self.permanent_reciprocate = True
                self.mode = 'RECIPROCATE'
                self.mode_rounds = 1
            else:
                threshold_for_failure = self.exploit_threshold - 0.2
                if group_coop_when_I_def < threshold_for_failure:
                    self.bad_windows_count += 1
                else:
                    self.bad_windows_count = 0
                if self.bad_windows_count > 2:
                    self.mode = 'RECIPROCATE'
                    self.mode_rounds = 1
                    self.recip_rounds_left = self.punish_duration
                    self.bad_windows_count = 0
                    self.pending_one_shot_probe = False
                else:
                    self.mode_rounds += 1
                    if random.random() < self.epsilon:
                        return Action.C
                    return Action.D
        if self.mode == 'RECIPROCATE':
            will_probe_now = False
            if self.permanent_reciprocate and self.probe_failures < self.max_probe_failures:
                if t - self.last_probe_round >= max(1, self.L):
                    will_probe_now = True
            if self.pending_one_shot_probe and (not self.awaiting_probe_result) and (not will_probe_now):
                self.pending_one_shot_probe = False
                self.awaiting_probe_result = True
                self.mode_rounds += 1
                return Action.D
            if will_probe_now:
                self.last_probe_round = t
                self.awaiting_probe_result = True
                self.mode_rounds += 1
                return Action.D
            coop_choice = True
            if total_past >= 1:
                prev_opp_coops = int(np.sum(history.opponent_actions[-1, :]))
                needed = math.ceil((self.n - 1) / 2)
                coop_choice = prev_opp_coops >= needed
            else:
                coop_choice = True
            if self.recip_rounds_left > 0:
                self.recip_rounds_left -= 1
                if self.recip_rounds_left == 0:
                    if not self.permanent_reciprocate:
                        self.pending_one_shot_probe = True
            if random.random() < self.epsilon:
                coop_choice = not coop_choice
            self.mode_rounds += 1
            return Action.C if coop_choice else Action.D
        return Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    Collective Forgiving Conditional Cooperation (CFCC)

    - Opens with cooperation, defects in the final round.
    - Uses a short memory window to compute recent group cooperation fraction.
    - If recent cooperation >= alpha -> cooperate.
    - If recent cooperation < alpha -> enact a short P-round punishment (defect) before re-evaluating.
    - If overall cooperation over history < beta -> switch to safe mode (defect for remainder).
    - If few rounds remain and recent_fraction is close to alpha (within 10%), prefer cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, math.floor(self.r / 5)))
        self.alpha = 0.6
        self.beta = 0.25
        self.P = min(3, max(1, math.ceil(self.r / 20)))
        self.punishment_until: None | int = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0:
            return Action.C
        if t == self.r - 1:
            return Action.D
        if history is None:
            return Action.D
        total_past_rounds = history.my_actions.shape[0]
        if total_past_rounds == 0:
            return Action.D
        try:
            opps = np.asarray(history.opponent_actions, dtype=np.int64)
            mine = np.asarray(history.my_actions, dtype=np.int64)
        except Exception:
            return Action.D
        if opps.ndim == 1:
            opps_sum = opps.astype(np.int64)
        else:
            opps_sum = np.sum(opps, axis=1, dtype=np.int64)
        min_len = min(opps_sum.shape[0], mine.shape[0])
        if min_len <= 0:
            return Action.D
        opps_sum = opps_sum[:min_len]
        mine = mine[:min_len]
        total_cooperators_per_round = opps_sum + mine
        fractions = total_cooperators_per_round.astype(np.float64) / float(self.n)
        overall_fraction = float(np.mean(fractions)) if fractions.size > 0 else 0.0
        recent_start = max(0, fractions.size - self.w)
        recent_slice = fractions[recent_start:fractions.size]
        recent_fraction = float(np.mean(recent_slice)) if recent_slice.size > 0 else 0.0
        if overall_fraction < self.beta:
            self.punishment_until = None
            return Action.D
        if self.punishment_until is not None and t <= self.punishment_until:
            return Action.D
        if recent_fraction >= self.alpha:
            self.punishment_until = None
            return Action.C
        rounds_remaining = self.r - t
        close_threshold = self.alpha * 0.9
        if rounds_remaining <= self.P and recent_fraction >= close_threshold:
            self.punishment_until = None
            return Action.C
        punish_end = t + self.P - 1
        if punish_end > self.r - 1:
            punish_end = self.r - 1
        self.punishment_until = punish_end
        return Action.D

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    Adaptive Opportunistic Exploiter for the N-player Public Goods Game.

    Default: defect. Exploits cooperators when safe (no strong punishment detected).
    If defections provoke immediate drops in others' contributions, perform short
    rehabilitation (cooperate Rehab rounds) then attempt pulsed exploitation:
    single-round defections followed by observation; if punished, return to Rehab.
    Always defect in final Endgame rounds. Occasional probes (one-round cooperations)
    occur every P_probe rounds to detect changes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W = min(10, max(3, math.floor(self.r / 10))) if self.r > 0 else 3
        self.T_high = 0.7
        self.T_low = 0.3
        self.Delta_punish = 0.25
        self.P_probe = max(3, math.floor(self.W / 2))
        self.Rehab = 3
        self.Endgame = min(3, self.r)
        self.rounds_since_last_probe = 0
        self.rehab_counter = 0
        self.awaiting_exploit = False
        self.in_exploit_mode = False
        self.last_exploit_round = None
        self.last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.last_action = Action.D
            self.rounds_since_last_probe += 1
            return Action.D

        def others_avg_over_round_range(start: int, end: int) -> float:
            if start >= end:
                return 0.0
            seg = history.opponent_actions[start:end, :]
            if seg.size == 0:
                return 0.0
            return float(np.mean(seg))

        def compute_punish_detected() -> bool:
            m = len(history.my_actions)
            window_start = max(0, m - self.W)
            drops = []
            for t in range(window_start, m):
                if bool(history.my_actions[t]):
                    continue
                before_start = max(0, t - 2)
                before_end = t
                after_start = t + 1
                after_end = min(m, t + 3)
                if before_start < before_end and after_start < after_end:
                    before_mean = others_avg_over_round_range(before_start, before_end)
                    after_mean = others_avg_over_round_range(after_start, after_end)
                    drops.append(before_mean - after_mean)
            if len(drops) == 0:
                return False
            median_drop = float(np.median(np.array(drops)))
            return median_drop >= self.Delta_punish

        def compute_drop_for_round(t: int) -> float | None:
            m = len(history.my_actions)
            before_start = max(0, t - 2)
            before_end = t
            after_start = t + 1
            after_end = min(m, t + 3)
            if before_start < before_end and after_start < after_end:
                before_mean = others_avg_over_round_range(before_start, before_end)
                after_mean = others_avg_over_round_range(after_start, after_end)
                return before_mean - after_mean
            return None
        m = len(history.my_actions)
        if state.round_number >= max(0, self.r - self.Endgame):
            self.last_action = Action.D
            self.rounds_since_last_probe += 1
            return Action.D
        recent_start = max(0, m - self.W)
        if recent_start >= m:
            others_avg_lastW = 0.0
        else:
            others_avg_lastW = float(np.mean(history.opponent_actions[recent_start:m, :])) if history.opponent_actions.size else 0.0
        punish_detected = compute_punish_detected()
        if self.last_exploit_round is not None:
            t_exp = self.last_exploit_round
            if m >= t_exp + 2:
                drop = compute_drop_for_round(t_exp)
                self.last_exploit_round = None
                if drop is not None and drop >= self.Delta_punish:
                    self.rehab_counter = self.Rehab
                    action = Action.C
                    self.rehab_counter -= 1
                    if self.rehab_counter <= 0:
                        self.rehab_counter = 0
                        self.awaiting_exploit = True
                        self.in_exploit_mode = True
                    else:
                        self.awaiting_exploit = False
                        self.in_exploit_mode = False
                    self.last_action = action
                    self.rounds_since_last_probe = 0
                    return action
                else:
                    self.in_exploit_mode = True
        if self.rehab_counter > 0:
            action = Action.C
            self.rehab_counter -= 1
            if self.rehab_counter == 0:
                self.awaiting_exploit = True
                self.in_exploit_mode = True
            self.rounds_since_last_probe = 0
            self.last_action = action
            return action
        if self.awaiting_exploit:
            action = Action.D
            self.last_exploit_round = m
            self.awaiting_exploit = False
            self.rounds_since_last_probe += 1
            self.last_action = action
            return action
        if others_avg_lastW < self.T_low:
            if self.rounds_since_last_probe >= self.P_probe:
                action = Action.C
                self.rounds_since_last_probe = 0
            else:
                action = Action.D
                self.rounds_since_last_probe += 1
            self.last_action = action
            return action
        if others_avg_lastW >= self.T_high and (not punish_detected):
            if self.rounds_since_last_probe >= self.P_probe:
                action = Action.C
                self.rounds_since_last_probe = 0
            else:
                action = Action.D
                self.rounds_since_last_probe += 1
            self.in_exploit_mode = True
            self.last_action = action
            return action
        if others_avg_lastW >= self.T_high and punish_detected:
            if self.in_exploit_mode:
                if self.rounds_since_last_probe >= self.P_probe:
                    action = Action.C
                    self.rounds_since_last_probe = 0
                else:
                    action = Action.D
                    self.rounds_since_last_probe += 1
                self.last_action = action
                return action
            else:
                self.rehab_counter = self.Rehab
                action = Action.C
                self.rehab_counter -= 1
                if self.rehab_counter == 0:
                    self.awaiting_exploit = True
                    self.in_exploit_mode = True
                else:
                    self.awaiting_exploit = False
                    self.in_exploit_mode = False
                self.rounds_since_last_probe = 0
                self.last_action = action
                return action
        action = Action.D
        self.rounds_since_last_probe += 1
        self.last_action = action
        return action

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    Adaptive Exploiter for N-Player Public Goods Game.

    - Aggressively defects by default to free-ride on cooperators.
    - Identifies exploitable opponents, reciprocals, and punishers from recent history.
    - Probes adaptively and cooperates probabilistically when sustained cooperation is likely to pay off.
    - Short, cautious punish/forgive policy (Lpunish rounds).
    - Always defects in the last H rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.high = 0.8
        self.low = 0.2
        self.coop_threshold = 0.6
        self.Lpunish = 3
        self.last_probe_round = -10000

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def to_bool_array(x):
            if x is None:
                return None
            return np.array(x)
        current_round = int(state.round_number)
        t = current_round + 1
        n_players = int(self.n)
        r = int(self.r)
        k = float(self.k)
        k_over_n = k / max(1.0, float(n_players))
        if r <= 3:
            return Action.D
        if 4 <= r <= 8:
            defensive_mode = True
        else:
            defensive_mode = False
        if current_round == 0 or history is None:
            p0 = 0.3 if r >= 10 or k_over_n >= 0.5 else 0.0
            if random.random() < p0:
                return Action.C
            return Action.D
        H = min(2, r)
        if current_round >= r - H:
            return Action.D
        my_actions = np.array(history.my_actions, dtype=bool)
        opp_actions = np.array(history.opponent_actions, dtype=bool)
        n_rounds_hist = opp_actions.shape[0]
        n_opponents = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        if n_opponents == 0:
            return Action.D
        W = min(10, max(0, t - 1))
        coop_rates = np.full(n_opponents, np.nan)
        baseline_rates = np.full(n_opponents, np.nan)
        response_after_my_defect = np.full(n_opponents, np.nan)
        is_punisher = np.zeros(n_opponents, dtype=bool)
        for j in range(n_opponents):
            if n_rounds_hist > 0:
                baseline_rates[j] = float(np.mean(opp_actions[:, j]))
            else:
                baseline_rates[j] = np.nan
            if W > 0 and n_rounds_hist > 0:
                use_slice = opp_actions[-W:, j] if n_rounds_hist >= W else opp_actions[:, j]
                coop_rates[j] = float(np.mean(use_slice)) if use_slice.size > 0 else np.nan
            else:
                coop_rates[j] = np.nan
        my_defect_indices = np.where(my_actions[:-1] == False)[0] if my_actions.size > 1 else np.array([], dtype=int)
        for j in range(n_opponents):
            if my_defect_indices.size == 0:
                response_after_my_defect[j] = np.nan
            else:
                valid_next = my_defect_indices[my_defect_indices + 1 < n_rounds_hist]
                if valid_next.size == 0:
                    response_after_my_defect[j] = np.nan
                else:
                    responses = opp_actions[valid_next + 1, j]
                    if responses.size > 0:
                        response_after_my_defect[j] = float(np.mean(responses))
                    else:
                        response_after_my_defect[j] = np.nan
            pun_found = False
            for idx in my_defect_indices:
                seq_start = idx + 1
                seq_end = idx + self.Lpunish
                if seq_end < n_rounds_hist:
                    seq = opp_actions[seq_start:seq_end + 1, j]
                    if seq.size >= self.Lpunish and np.all(seq == False):
                        pun_found = True
                        break
                else:
                    seq = opp_actions[seq_start:n_rounds_hist, j]
                    if seq.size >= self.Lpunish and np.all(seq == False):
                        pun_found = True
                        break
            is_punisher[j] = bool(pun_found)
        exploitable_mask = np.zeros(n_opponents, dtype=bool)
        reciprocal_mask = np.zeros(n_opponents, dtype=bool)
        uncertain_mask = np.zeros(n_opponents, dtype=bool)
        for j in range(n_opponents):
            if is_punisher[j]:
                continue
            cr = coop_rates[j]
            resp = response_after_my_defect[j]
            base = baseline_rates[j]
            exploitable = False
            reciprocal = False
            if not np.isnan(cr) and (not np.isnan(resp)):
                if cr >= self.high and resp >= self.high:
                    exploitable = True
            if not np.isnan(resp) and (not np.isnan(base)):
                if base - resp >= 0.3:
                    reciprocal = True
            if not np.isnan(resp) and (not np.isnan(cr)) and (cr - resp >= 0.3):
                reciprocal = True
            if exploitable:
                exploitable_mask[j] = True
            elif reciprocal:
                reciprocal_mask[j] = True
            else:
                uncertain_mask[j] = True
        if np.all(np.isnan(coop_rates)):
            if np.all(np.isnan(baseline_rates)):
                group_coop_rate = 0.0
            else:
                group_coop_rate = float(np.nanmean(baseline_rates))
        else:
            group_coop_rate = float(np.nanmean(coop_rates[np.logical_not(np.isnan(coop_rates))]))
        fraction_exploitable = float(np.sum(exploitable_mask)) / float(n_opponents)
        fraction_reciprocal = float(np.sum(reciprocal_mask)) / float(n_opponents)
        fraction_punisher = float(np.sum(is_punisher)) / float(n_opponents)
        in_hold = False
        if np.any(reciprocal_mask):
            my_defect_idxs = np.where(my_actions == False)[0]
            if my_defect_idxs.size > 0:
                last_my_defect = int(my_defect_idxs[-1])
                for j in np.where(reciprocal_mask)[0]:
                    if last_my_defect >= 0 and current_round <= last_my_defect + self.Lpunish:
                        resp = response_after_my_defect[j]
                        base = baseline_rates[j]
                        if not np.isnan(resp) and (not np.isnan(base)) and (base - resp >= 0.3):
                            in_hold = True
                            break
        if fraction_exploitable > 0.0 and fraction_punisher <= 0.5:
            if defensive_mode:
                if random.random() < 0.1:
                    if in_hold:
                        return Action.D
                    self.last_probe_round = current_round
                    return Action.C
                return Action.D
            return Action.D
        if fraction_reciprocal >= 0.5 and group_coop_rate >= self.coop_threshold:
            if in_hold:
                return Action.D
            if random.random() < 0.7:
                return Action.C
            else:
                return Action.D
        if fraction_punisher >= 0.5:
            if current_round - self.last_probe_round >= self.Lpunish:
                probe_prob = 0.15 if not defensive_mode else 0.08
                if random.random() < probe_prob:
                    self.last_probe_round = current_round
                    if in_hold:
                        return Action.D
                    return Action.C
            return Action.D
        if group_coop_rate >= self.coop_threshold and fraction_reciprocal < 0.5:
            if random.random() < 0.1:
                if in_hold:
                    return Action.D
                return Action.C
            return Action.D
        if group_coop_rate <= self.low:
            return Action.D
        p_probe = 0.2 + 0.6 * float(k_over_n)
        p_probe = max(0.05, min(0.6, p_probe))
        if in_hold:
            return Action.D
        if defensive_mode:
            p_probe = min(p_probe, 0.25)
        if random.random() < p_probe:
            self.last_probe_round = current_round
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Collective-First Conditional Cooperator (CFCC)

    Seeks to establish and sustain group cooperation using recent-frequency rules with
    forgiveness, occasional probing during defensive phases, and endgame safety.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = min(10, max(1, self.game_description.n_rounds))
        self.q = 0.6
        self.eps = 0.1
        self.phi = 0.5
        self.p_probe = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        rounds_played = 0 if history is None else history.opponent_actions.shape[0]
        if state.round_number == 0:
            if r <= 2:
                return Action.D
            return Action.C
        rem = r - state.round_number
        if rem == 1:
            return Action.D
        if rem == 2:
            if rounds_played <= 0:
                return Action.D
            full_counts = np.sum(history.opponent_actions, axis=0).astype(float)
            full_rates = full_counts / float(rounds_played)
            avg_full_rate = float(np.mean(full_rates)) if full_rates.size > 0 else 0.0
            return Action.C if avg_full_rate >= 0.9 else Action.D
        window_len = min(self.T, rounds_played)
        if window_len > 0:
            curr_counts = np.sum(history.opponent_actions[-window_len:, :], axis=0).astype(float)
            curr_rates = curr_counts / float(window_len)
        else:
            curr_rates = np.zeros(n - 1, dtype=float)
        prev_window_len = min(self.T, max(0, rounds_played - window_len))
        if prev_window_len > 0:
            start = -(window_len + prev_window_len)
            end = -window_len
            prev_counts = np.sum(history.opponent_actions[start:end, :], axis=0).astype(float)
            prev_rates = prev_counts / float(prev_window_len)
            prev_window_exists = True
        else:
            prev_rates = np.zeros(n - 1, dtype=float)
            prev_window_exists = False
        treated_cooperator = np.zeros(n - 1, dtype=bool)
        for j in range(n - 1):
            curr = float(curr_rates[j])
            if curr >= self.q:
                treated_cooperator[j] = True
                continue
            if curr >= self.q - self.eps:
                if prev_window_exists:
                    prev = float(prev_rates[j])
                    prev_is_weak = prev >= self.q - self.eps and prev < self.q
                    if prev_is_weak:
                        treated_cooperator[j] = False
                    else:
                        treated_cooperator[j] = True
                else:
                    treated_cooperator[j] = True
                continue
            treated_cooperator[j] = False
        fraction_cooperators = float(np.sum(treated_cooperator)) / float(max(1, n - 1))
        average_coop_rate = float(np.mean(curr_rates)) if curr_rates.size > 0 else 0.0
        if fraction_cooperators >= self.phi:
            return Action.C
        if random.random() < self.p_probe:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    Collective Conditional Cooperator with Graduated Punishment and Forgiveness (C3G-P)

    - Signals cooperation on the first round.
    - Monitors recent group cooperation over a sliding window w = min(5, r).
    - If a defection occurs against an otherwise cooperative last round (last_round_coop_rate >= theta and < 1),
      triggers a finite group-wide punishment of P rounds (withholding cooperation).
    - After punishment, requires S = w consecutive rounds with recent_coop_rate >= theta to resume cooperation.
    - In the final round, cooperates only if the last observed round showed sufficient cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r))
        self.theta = 0.6
        self.P = max(1, math.ceil(4.0 * (1.0 - self.k / float(self.n))))
        self.S = self.w
        self.in_punishment = False
        self.punishment_counter = 0
        self.waiting_for_forgiveness = False
        self.forgiveness_counter = 0
        self.persistent_defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number
        rounds_observed = int(history.my_actions.shape[0])
        if rounds_observed == 0:
            return Action.C
        last_round_idx = rounds_observed - 1
        my_last = 1 if bool(history.my_actions[last_round_idx]) else 0
        opp_last_sum = int(history.opponent_actions[last_round_idx, :].sum()) if history.opponent_actions.size else 0
        last_round_total = my_last + opp_last_sum
        last_round_coop_rate = float(last_round_total) / float(self.n)
        window_len = min(self.w, rounds_observed)
        start_idx = rounds_observed - window_len
        my_window_sum = int(history.my_actions[start_idx:rounds_observed].sum())
        opp_window_sum = int(history.opponent_actions[start_idx:rounds_observed, :].sum()) if history.opponent_actions.size else 0
        total_window_coop = my_window_sum + opp_window_sum
        recent_coop_rate = float(total_window_coop) / (float(self.n) * float(window_len)) if window_len > 0 else 0.0
        if state.round_number == self.r - 1:
            return Action.C if last_round_coop_rate >= self.theta else Action.D
        if last_round_coop_rate >= self.theta and last_round_coop_rate < 1.0:
            self.in_punishment = True
            self.punishment_counter = self.P
            if rounds_observed > 0 and history.opponent_actions.size:
                for j in range(history.opponent_actions.shape[1]):
                    if not bool(history.opponent_actions[last_round_idx, j]):
                        long_run = float(history.opponent_actions[:, j].sum()) / float(rounds_observed)
                        if long_run < 0.1:
                            self.persistent_defectors.add(j)
        if self.in_punishment:
            action = Action.D
            self.punishment_counter -= 1
            if self.punishment_counter <= 0:
                self.in_punishment = False
                self.waiting_for_forgiveness = True
                self.forgiveness_counter = 0
            return action
        if self.waiting_for_forgiveness:
            if recent_coop_rate >= self.theta:
                self.forgiveness_counter += 1
            else:
                self.forgiveness_counter = 0
            if self.forgiveness_counter >= self.S:
                self.waiting_for_forgiveness = False
                self.forgiveness_counter = 0
                return Action.C
            return Action.D
        if recent_coop_rate >= self.theta:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    Conditional Cooperator with Probing and Short Punishment for Public Goods Game.

    - Opens cooperating (except in very short games).
    - Cooperates only when a recent clear majority of others have been cooperating.
    - When the group is stably cooperative, performs a one-shot exploit (single D) with a cooldown.
    - If that exploit provokes a clear coordinated backlash, issues a short punishment (P rounds).
    - Always defects in the final E rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.n_rounds)
        self.theta = 0.6
        self.S = 2
        self.P = 2
        self.E = min(3, self.n_rounds)
        self.last_exploit_round = -10 ** 9
        self.in_punish_until = -1
        self.coop_streak = 0
        self._other_frac_pre_exploit = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n_players
        r = self.n_rounds
        if r <= self.E:
            return Action.D
        if t == 0:
            if r <= 3:
                return Action.D
            return Action.C
        if t >= r - self.E:
            return Action.D
        if history is None:
            return Action.D
        start = max(0, t - self.w)
        num_rounds = t - start
        opp_actions = history.opponent_actions
        if num_rounds <= 0:
            other_coop_fraction = 0.0
        else:
            window_slice = opp_actions[start:t, :]
            if window_slice.size == 0:
                other_coop_fraction = 0.0
            else:
                total_other_coops = float(np.sum(window_slice))
                denom = float(num_rounds * (n - 1))
                if denom <= 0.0:
                    other_coop_fraction = 0.0
                else:
                    other_coop_fraction = total_other_coops / denom
        if num_rounds < self.w:
            any_majority = False
            for rr in range(start, t):
                if opp_actions.shape[0] > rr:
                    per_round_sum = float(np.sum(opp_actions[rr, :]))
                    per_round_frac = per_round_sum / float(max(1, n - 1))
                    if per_round_frac >= self.theta:
                        any_majority = True
                        break
            self.coop_streak = 1 if any_majority else 0
        elif other_coop_fraction >= self.theta:
            self.coop_streak += 1
        else:
            self.coop_streak = 0
        if t <= self.in_punish_until:
            return Action.D
        if t - 1 == self.last_exploit_round and self._other_frac_pre_exploit is not None:
            before = float(self._other_frac_pre_exploit)
            after = float(other_coop_fraction)
            drop = before - after
            if drop > 0.3:
                self.in_punish_until = t + self.P - 1
                self._other_frac_pre_exploit = None
                return Action.D
            self._other_frac_pre_exploit = None
        cooldown_ok = t - self.last_exploit_round > self.P
        if self.coop_streak >= self.S and cooldown_ok:
            self._other_frac_pre_exploit = other_coop_fraction
            self.last_exploit_round = t
            return Action.D
        if other_coop_fraction >= self.theta:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR)

    - Starts cooperative.
    - Punishes proportionally and temporarily when recent play shows a tilt toward defection.
    - Punishment severity scales with observed shortfall; punishments decay and include probabilistic
      forgiveness to avoid permanent collapse.
    - In the final round: always defect.
    - Uses only observable history and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(3, self.n_rounds)
        self.theta = 0.5
        self.theta_high = 0.75
        self.P_base = 2
        self.P_max = min(5, max(1, self.n_rounds // 4))
        self.p_forgive = 0.2
        self.E = min(3, self.n_rounds)
        self.mode = 'Cooperative'
        self.remaining_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            self.mode = 'Cooperative'
            self.remaining_punish = 0
            return Action.C
        if t == self.n_rounds - 1:
            return Action.D

        def compute_f_recent(hist: PlayerHistory, lookback: int) -> float:
            completed_rounds = hist.my_actions.shape[0]
            if completed_rounds <= 0:
                return 0.0
            use = min(lookback, completed_rounds)
            start_idx = completed_rounds - use
            fractions = []
            for ri in range(start_idx, completed_rounds):
                my_coop = 1 if hist.my_actions[ri] else 0
                opp_coops = int(np.sum(hist.opponent_actions[ri, :]))
                total = my_coop + opp_coops
                fractions.append(total / float(self.n_players))
            if not fractions:
                return 0.0
            return float(np.mean(fractions))
        f_recent = compute_f_recent(history, self.w)
        num_opponents = max(1, self.n_players - 1)
        last_opp_coops = float(np.sum(history.opponent_actions[-1, :]))
        f_last = last_opp_coops / float(num_opponents)
        if self.mode == 'Punishing' and self.remaining_punish > 0:
            action = Action.D
            self.remaining_punish -= 1
            if self.remaining_punish == 0:
                f_recent_post = compute_f_recent(history, self.w)
                if f_recent_post >= self.theta_high:
                    self.mode = 'Cooperative'
                    self.remaining_punish = 0
                elif random.random() < self.p_forgive:
                    self.mode = 'Cooperative'
                    self.remaining_punish = 0
                else:
                    follow = 2 if f_recent_post < self.theta else 1
                    self.remaining_punish = min(self.P_max, follow)
                    self.mode = 'Punishing'
            return action
        if self.mode == 'Punishing' and self.remaining_punish <= 0:
            self.mode = 'Cooperative'
            self.remaining_punish = 0
        if t >= self.n_rounds - self.E:
            if f_recent >= self.theta_high:
                return Action.C
            return Action.D
        if f_last >= self.theta:
            return Action.C
        shortfall = max(0.0, self.theta - f_last)
        estimated_shortfall_count = round(shortfall * float(self.n_players - 1))
        S = int(min(self.P_max, max(1, int(estimated_shortfall_count))))
        self.remaining_punish = min(self.P_max, self.P_base + S)
        self.mode = 'Punishing'
        return Action.D

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    Adaptive Exploitative Public-Goods Strategy (AE-PGS)

    - Signals cooperation early.
    - Uses conditional cooperation rates (after my C vs after my D) to estimate
      whether opponents are unconditional cooperators or reciprocators/punishers.
    - Exploits when immediate expected payoff from defecting is higher.
    - If defections lead to significant drops in opponents' cooperation, performs
      a short repair (cooperate for R_fix rounds).
    - Defects in the final round and is biased toward defection in the endgame window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = max(3, math.floor(self.r / 6))
        self.R_fix = min(4, max(2, math.floor(self.r / 10)))
        self.E = min(5, max(1, math.floor(self.r / 6)))
        self.Tc = 0.9
        self.Punish_threshold = 0.15
        self.epsilon = 1e-06
        self.repair_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        s = int(state.round_number)
        t = s + 1
        if history is None:
            num_opponents = max(0, self.n - 1)
        else:
            num_opponents = int(history.opponent_actions.shape[1])
        if s == 0 or history is None:
            return Action.C
        if t == self.r:
            return Action.D
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        total_j = np.sum(opp_actions[:s, :], axis=0).astype(float)
        p_j = total_j / float(s)
        if s >= 2:
            prev_my = my_actions[:s - 1]
            next_opp = opp_actions[1:s, :]
            count_after_my_C = int(np.sum(prev_my))
            count_after_my_D = int(s - 1 - count_after_my_C)
            if count_after_my_C > 0:
                total_after_my_C_j = np.sum(next_opp[prev_my, :], axis=0).astype(float)
                p_given_C = total_after_my_C_j / float(count_after_my_C)
            else:
                p_given_C = p_j.copy()
            if count_after_my_D > 0:
                mask_D = ~prev_my
                total_after_my_D_j = np.sum(next_opp[mask_D, :], axis=0).astype(float)
                p_given_D = total_after_my_D_j / float(count_after_my_D)
            else:
                p_given_D = p_j.copy()
        else:
            p_given_C = p_j.copy()
            p_given_D = p_j.copy()
            count_after_my_C = 0
            count_after_my_D = 0
        delta_j = p_given_C - p_given_D
        if s >= 2:
            weight_per_opponent = float(count_after_my_C + count_after_my_D)
            if weight_per_opponent > 0:
                avg_drop_after_my_D = float(np.mean(delta_j))
            else:
                avg_drop_after_my_D = 0.0
        else:
            avg_drop_after_my_D = 0.0
        if self.repair_timer > 0:
            self.repair_timer -= 1
            return Action.C
        last_my_action = bool(my_actions[-1])
        if not last_my_action and avg_drop_after_my_D > self.Punish_threshold:
            self.repair_timer = int(self.R_fix)
            if self.repair_timer > 0:
                self.repair_timer -= 1
            return Action.C
        expected_others_if_I_C = float(np.sum(p_given_C))
        expected_others_if_I_D = float(np.sum(p_given_D))
        payoff_if_C = self.k / float(self.n) * (1.0 + expected_others_if_I_C)
        payoff_if_D = 1.0 + self.k / float(self.n) * expected_others_if_I_D
        endgame_threshold = 0.1 if t > self.r - self.E else self.epsilon
        if payoff_if_C > payoff_if_D + endgame_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Proportional Forgiving Collective Cooperator (PFCC)

    - Starts cooperative.
    - Tracks recent cooperation with a sliding window and a cooperation target q scaled by k.
    - If group cooperation falls below q, imposes a short punishment whose length is
      proportional to how far cooperation fell, bounded by Pmax.
    - Forgiving: punishments are short and re-evaluated after they expire.
    - Endgame caution: in the last E rounds, only cooperate if the previous round met q.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.W = min(5, r)
        denom = max(1, n - 1)
        self.normalized_k = (k - 1) / denom
        self.q = 0.5 + 0.4 * self.normalized_k
        self.Pmax = min(3, r)
        self.E = min(3, r)
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        last_idx = state.round_number - 1
        opps_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size > 0 else 0
        my_last = 1 if bool(history.my_actions[last_idx]) else 0
        total_last = opps_last + my_last
        f_last = total_last / n if n > 0 else 0.0
        if self.punish_counter > 0:
            action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        if total_last == n:
            return Action.C
        if total_last == 1 and my_last == 1:
            diff = max(0.0, self.q - f_last)
            p = max(1, math.ceil(diff * self.Pmax))
            p = min(p, self.Pmax)
            self.punish_counter = p
            action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        T_rem = r - state.round_number
        if T_rem <= self.E:
            if f_last >= self.q:
                return Action.C
            diff = max(0.0, self.q - f_last)
            p = max(1, math.ceil(diff * self.Pmax))
            p = min(p, self.Pmax)
            self.punish_counter = p
            action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            return action
        num_available = min(self.W, state.round_number)
        start_idx = last_idx - num_available + 1
        fracs = []
        for idx in range(start_idx, last_idx + 1):
            opps = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            me = 1 if bool(history.my_actions[idx]) else 0
            fracs.append((opps + me) / n)
        if len(fracs) == 0:
            f_avg = 1.0
        else:
            f_avg = float(np.mean(np.array(fracs, dtype=float)))
        if f_avg >= self.q:
            return Action.C
        diff = max(0.0, self.q - f_avg)
        p = max(1, math.ceil(diff * self.Pmax))
        p = min(p, self.Pmax)
        self.punish_counter = p
        action = Action.D
        self.punish_counter = max(0, self.punish_counter - 1)
        return action

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    Exploitative, adaptive public-goods strategy.

    - Starts cooperative to build reputation (warm-up).
    - Tracks recent reputations of opponents (sliding window).
    - If the group looks very cooperative, performs a short probe (defect).
      If probe is tolerated, performs bounded exploitation (defect for a few rounds).
      If probe triggers a drop in cooperation (retaliation), switch to short punitive
      defection burst, then a short cooperative cooldown to regain reputation.
    - If group cooperation is moderate, cooperate to sustain it.
    - If group cooperation is low, defect to avoid being a sucker.
    - Unconditional defection in the final horizon (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        denom = 1.0 - 1.0 / max(2, self.n)
        g = 1.0 - self.k / max(1.0, self.n)
        alpha = 0.0
        if denom > 0:
            alpha = g / denom
        if alpha < 0.0:
            alpha = 0.0
        elif alpha > 1.0:
            alpha = 1.0
        self.alpha = alpha
        Lcalc = math.floor(self.r / 5)
        self.L = max(3, min(10, Lcalc))
        self.W = min(5, math.floor(self.r / 10) + 1)
        Hcalc = math.ceil(self.r * (0.08 + 0.25 * self.alpha))
        self.H = min(self.r, max(1, Hcalc))

        def _clamp(x, a, b):
            return max(a, min(b, x))
        self.high_thresh = _clamp(0.95 - 0.3 * self.alpha, 0.5, 0.95)
        self.mid_thresh = _clamp(0.6 - 0.2 * self.alpha, 0.2, 0.8)
        self.low_thresh = _clamp(0.3, 0.0, 0.5)
        self.retaliation_delta = 0.18
        self.lookahead_after_probe = 2
        self.max_consecutive_exploits = 1 + math.floor(4 * self.alpha)
        self.max_punish_rounds = min(6, 2 + math.floor(5 * self.alpha))
        self.MODE_COOPERATE = 'COOPERATE'
        self.MODE_PROBE = 'PROBE_EXPLOIT'
        self.MODE_EXPLOIT = 'EXPLOITING'
        self.MODE_PUNISH = 'PUNISH'
        self.MODE_COOLDOWN = 'COOLDOWN'
        self.mode = self.MODE_COOPERATE
        self.last_probe_round = None
        self.pre_probe_G = None
        self.consecutive_exploit_count = 0
        self.punish_count = 0
        self.cooldown_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def to_action(cooperate: bool) -> Action:
            return Action.C if cooperate else Action.D
        if self.r <= 1:
            return Action.D
        if self.n <= 1:
            return Action.D
        t_idx = int(state.round_number)
        if t_idx >= self.r - self.H:
            self.last_probe_round = None
            self.pre_probe_G = None
            self.consecutive_exploit_count = 0
            return Action.D
        if t_idx == 0:
            self.mode = self.MODE_COOPERATE
            self.last_probe_round = None
            self.pre_probe_G = None
            self.consecutive_exploit_count = 0
            self.punish_count = 0
            self.cooldown_count = 0
            return Action.C
        if history is None:
            return Action.D
        rounds_played = history.opponent_actions.shape[0]
        window_len = min(self.L, rounds_played)
        if window_len <= 0:
            if t_idx < self.W:
                return Action.C
            else:
                return Action.C
        recent = history.opponent_actions[-window_len:, :]
        try:
            R_js = np.mean(recent.astype(float), axis=0)
            G = float(np.mean(R_js)) if R_js.size > 0 else 0.0
        except Exception:
            G = 0.0
        if t_idx < self.W:
            self.mode = self.MODE_COOPERATE
            return Action.C

        def compute_post_probe_avg(last_probe_round_idx):
            """
            Return (num_observed_rounds, post_avg) where post_avg is average fraction
            of opponents cooperating in observed rounds after the probe,
            using up to lookahead_after_probe rounds.
            """
            if last_probe_round_idx is None:
                return (0, 0.0)
            start = last_probe_round_idx + 1
            if start >= rounds_played:
                return (0, 0.0)
            end = min(rounds_played, start + self.lookahead_after_probe)
            post_slice = history.opponent_actions[start:end, :]
            if post_slice.size == 0:
                return (0, 0.0)
            try:
                per_round_frac = np.mean(post_slice.astype(float), axis=1)
                post_avg = float(np.mean(per_round_frac))
                return (int(per_round_frac.size), post_avg)
            except Exception:
                return (0, 0.0)
        action_cooperate = True
        if self.mode == self.MODE_PUNISH:
            action_cooperate = False
            self.punish_count += 1
            if self.punish_count >= self.max_punish_rounds or G <= self.low_thresh:
                self.mode = self.MODE_COOLDOWN
                self.cooldown_count = 2
                self.consecutive_exploit_count = 0
                self.last_probe_round = None
                self.pre_probe_G = None
            return to_action(action_cooperate)
        if self.mode == self.MODE_COOLDOWN:
            if self.cooldown_count > 0:
                action_cooperate = True
                self.cooldown_count -= 1
                if self.cooldown_count <= 0:
                    self.mode = self.MODE_COOPERATE
                return to_action(action_cooperate)
            else:
                self.mode = self.MODE_COOPERATE
        if self.mode == self.MODE_EXPLOIT:
            if self.consecutive_exploit_count < self.max_consecutive_exploits and G >= self.high_thresh:
                action_cooperate = False
                self.consecutive_exploit_count += 1
                if self.last_probe_round is None:
                    self.last_probe_round = t_idx - 1
                    self.pre_probe_G = G
                num_obs, post_avg = compute_post_probe_avg(self.last_probe_round)
                if num_obs >= 1:
                    if self.pre_probe_G is not None and self.pre_probe_G - post_avg > self.retaliation_delta:
                        self.mode = self.MODE_PUNISH
                        self.punish_count = 0
                        self.consecutive_exploit_count = 0
                        return to_action(False)
                return to_action(action_cooperate)
            else:
                self.mode = self.MODE_COOPERATE
                self.consecutive_exploit_count = 0
                self.last_probe_round = None
                self.pre_probe_G = None
        if self.mode == self.MODE_COOPERATE:
            if G >= self.high_thresh:
                action_cooperate = False
                self.mode = self.MODE_PROBE
                self.last_probe_round = t_idx
                self.pre_probe_G = G
                self.consecutive_exploit_count = 1
                return to_action(action_cooperate)
            elif G >= self.mid_thresh:
                action_cooperate = True
                return to_action(action_cooperate)
            else:
                action_cooperate = False
                return to_action(action_cooperate)
        if self.mode == self.MODE_PROBE:
            num_obs, post_avg = compute_post_probe_avg(self.last_probe_round)
            if num_obs <= 0:
                return Action.C
            elif self.pre_probe_G is not None and self.pre_probe_G - post_avg > self.retaliation_delta:
                self.mode = self.MODE_PUNISH
                self.punish_count = 0
                self.consecutive_exploit_count = 0
                return Action.D
            else:
                self.mode = self.MODE_EXPLOIT
                if self.consecutive_exploit_count < 1:
                    self.consecutive_exploit_count = 1
                if self.consecutive_exploit_count <= self.max_consecutive_exploits and G >= self.high_thresh:
                    action_cooperate = False
                    self.consecutive_exploit_count += 1
                    return to_action(action_cooperate)
                else:
                    self.mode = self.MODE_COOPERATE
                    self.last_probe_round = None
                    self.pre_probe_G = None
                    self.consecutive_exploit_count = 0
                    return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    Adaptive Exploiter for N-Player Public Goods Game.

    - Exploits persistent cooperators, defects in endgame, and uses short-window
      statistics to adapt to punishers and group trends.
    - Uses randomized small-probability cooperation to seed/recover cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.high_coop = 0.7
        self.low_coop = 0.2
        self.alwaysC_frac = 0.9
        self.retaliator_drop = 0.3
        self.prob_seed = 0.15
        self.prob_contrite = 0.6
        self.consecutive_low_windows = 0
        self.permanent_defect = False
        self.prev_group_recent_avg = None
        self._permanent_defect_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_total = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        T_end = min(3, math.floor(r / 10) + 1)
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number >= r - T_end:
            return Action.D
        m = min(5, r)
        num_rounds_played = int(history.my_actions.shape[0])
        window = min(m, num_rounds_played)
        start_idx = num_rounds_played - window
        if window <= 0:
            return Action.D
        opp_actions_window = history.opponent_actions[start_idx:num_rounds_played, :]
        n_others = opp_actions_window.shape[1] if opp_actions_window.ndim == 2 else 0
        if n_others <= 0:
            return Action.D
        try:
            contribution_rates = np.array(np.mean(opp_actions_window, axis=0), dtype=float)
        except Exception:
            contribution_rates = np.zeros(n_others, dtype=float)
        group_recent_avg = float(np.mean(contribution_rates)) if contribution_rates.size > 0 else 0.0
        always_cooperators_idx = np.nonzero(contribution_rates >= self.alwaysC_frac)[0]
        retaliators = []
        resp_start = max(1, num_rounds_played - m)
        resp_end = num_rounds_played
        if resp_end - resp_start >= 1:
            for j in range(n_others):
                after_my_C = []
                after_my_D = []
                for t in range(resp_start, resp_end):
                    my_prev = bool(history.my_actions[t - 1])
                    opp_action_at_t = bool(history.opponent_actions[t, j])
                    if my_prev:
                        after_my_C.append(1 if opp_action_at_t else 0)
                    else:
                        after_my_D.append(1 if opp_action_at_t else 0)
                pre_rate = float(np.mean(after_my_C)) if len(after_my_C) > 0 else None
                post_rate = float(np.mean(after_my_D)) if len(after_my_D) > 0 else None
                if pre_rate is None or post_rate is None:
                    retaliators.append(False)
                else:
                    retaliators.append(pre_rate - post_rate >= self.retaliator_drop)
        else:
            retaliators = [False] * n_others
        num_retaliators = sum((1 for v in retaliators if v))
        punishing_majority = num_retaliators > n_others / 2.0
        if group_recent_avg <= self.low_coop:
            self.consecutive_low_windows += 1
        else:
            self.consecutive_low_windows = 0
            if self.permanent_defect:
                self.permanent_defect = False
        if self.consecutive_low_windows >= self._permanent_defect_threshold:
            self.permanent_defect = True
        if always_cooperators_idx.size if isinstance(always_cooperators_idx, np.ndarray) else len(always_cooperators_idx) > 0:
            return Action.D
        if self.permanent_defect and (not punishing_majority):
            return Action.D
        if punishing_majority:
            if random.random() < self.prob_contrite:
                return Action.C
            else:
                return Action.D
        if group_recent_avg >= self.high_coop:
            if random.random() < self.prob_seed:
                return Action.C
            return Action.D
        if group_recent_avg <= self.low_coop:
            if random.random() < self.prob_seed:
                return Action.C
            return Action.D
        prev_avg = self.prev_group_recent_avg
        my_prev_action = bool(history.my_actions[-1])
        action_choice = None
        if prev_avg is None:
            if random.random() < 0.2:
                action_choice = Action.C
            else:
                action_choice = Action.D
        elif my_prev_action and group_recent_avg >= prev_avg:
            if random.random() < 0.7:
                action_choice = Action.C
            else:
                action_choice = Action.D
        elif random.random() < 0.8:
            action_choice = Action.D
        else:
            action_choice = Action.C
        self.prev_group_recent_avg = group_recent_avg
        return action_choice

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    Selective Exploiter:
    Defect by default and in the endgame; when many others contributed last round, defect to free-ride;
    otherwise occasionally cooperate only if there are reliable cooperators to sustain, and use short targeted
    punishment if our cooperation appears to have been exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        self.W = min(10, max(1, math.floor(r / 5)))
        self.L_end = min(3, r)
        self.theta = 0.6
        self.S_exploit = max(1, math.ceil((n - 1) / 2))
        self.q = 0.25
        self.P = 3
        self.punishment_remaining = 0
        self.last_coop_round = None
        self.retaliation_processed_for = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        t = state.round_number
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if t >= r - self.L_end:
            return Action.D
        rounds_recorded = history.my_actions.shape[0]
        if self.last_coop_round is not None and self.retaliation_processed_for != self.last_coop_round and (self.last_coop_round + 1 < rounds_recorded):
            idx = self.last_coop_round
            total_at_idx = int(np.sum(history.opponent_actions[idx, :])) + (1 if bool(history.my_actions[idx]) else 0)
            total_next = int(np.sum(history.opponent_actions[idx + 1, :])) + (1 if bool(history.my_actions[idx + 1]) else 0)
            drop_count = total_at_idx - total_next
            denom = max(1, total_at_idx)
            drop_fraction = drop_count / denom
            if drop_fraction > 0.3 or drop_count >= 2:
                self.punishment_remaining = self.P
            self.retaliation_processed_for = self.last_coop_round
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        prev_opponents = history.opponent_actions[-1, :]
        S_prev = int(np.sum(prev_opponents))
        if S_prev >= self.S_exploit:
            return Action.D
        last_k = min(self.W, rounds_recorded)
        if last_k <= 0:
            return Action.D
        window_opponents = history.opponent_actions[-last_k:, :]
        f_j = np.mean(window_opponents, axis=0) if window_opponents.size > 0 else np.zeros(n - 1)
        R = int(np.sum(f_j >= self.theta))
        if R >= 1:
            if random.random() < self.q:
                self.last_coop_round = t
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    Opportunistic Conditional Free-rider (OCF)
    - Signals cooperation in round 1, defects in final round(s).
    - Detects unconditional cooperators (exploitables) and defects forever if any found.
    - Identifies reciprocators and cooperates sparingly to sustain profitable cooperation:
      cooperate when a clear majority of others cooperated last round.
    - Otherwise free-rides and issues rare probes to discover or re-open cooperation.
    - Uses a limited forgiveness schedule: if my defection appears to trigger a collapse
      in group cooperation, attempt a cooperative probe every F rounds to recover.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(10, max(1, self.r - 1))
        self.alpha_exploit = 0.8
        self.delta = 0.3
        self.count_minR = max(1, math.floor((self.n - 1) / 4))
        self.p_probe = min(0.1, 5 / max(10, self.r))
        self.F = 4
        self.permanent_exploit = False
        self.last_collapse_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == self.r - 1:
            return Action.D
        if self.r <= 3 and t >= 1:
            return Action.D
        if t == 0 or history is None:
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        total_rounds_played = opp_actions.shape[0]
        lookback = min(self.L, total_rounds_played)
        start_idx = max(0, total_rounds_played - lookback)
        indices = list(range(start_idx, total_rounds_played))
        n_opp = self.n - 1
        if total_rounds_played >= 1:
            last_round_idx = total_rounds_played - 1
            others_coops_last = int(np.sum(opp_actions[last_round_idx, :])) if n_opp > 0 else 0
            last_round_coop_fraction = others_coops_last / max(1, n_opp)
        else:
            last_round_coop_fraction = 0.0
        P_coop_given_my_def = np.zeros(n_opp, dtype=float)
        P_coop_given_my_coop = np.zeros(n_opp, dtype=float)
        denom_def = np.zeros(n_opp, dtype=int)
        denom_coop = np.zeros(n_opp, dtype=int)
        numer_def = np.zeros(n_opp, dtype=int)
        numer_coop = np.zeros(n_opp, dtype=int)
        for idx in indices:
            my_a = bool(my_actions[idx])
            for j in range(n_opp):
                opp_a = bool(opp_actions[idx, j])
                if my_a:
                    denom_coop[j] += 1
                    if opp_a:
                        numer_coop[j] += 1
                else:
                    denom_def[j] += 1
                    if opp_a:
                        numer_def[j] += 1
        for j in range(n_opp):
            if denom_def[j] > 0:
                P_coop_given_my_def[j] = numer_def[j] / denom_def[j]
            else:
                P_coop_given_my_def[j] = 0.0
            if denom_coop[j] > 0:
                P_coop_given_my_coop[j] = numer_coop[j] / denom_coop[j]
            else:
                P_coop_given_my_coop[j] = 0.0
        delta_j = P_coop_given_my_coop - P_coop_given_my_def
        exploitables = [j for j in range(n_opp) if P_coop_given_my_def[j] >= self.alpha_exploit]
        reciprocators = [j for j in range(n_opp) if delta_j[j] >= self.delta]
        if exploitables:
            self.permanent_exploit = True
        if self.permanent_exploit:
            return Action.D
        collapsed = False
        if total_rounds_played >= 1:
            my_last_action = not bool(my_actions[-1])
            if my_last_action:
                if lookback >= 2:
                    excl_indices = indices[:-1]
                    if len(excl_indices) > 0:
                        coop_counts = [int(np.sum(opp_actions[i, :])) for i in excl_indices]
                        recent_avg = sum(coop_counts) / len(coop_counts) / max(1, n_opp)
                    else:
                        recent_avg = last_round_coop_fraction
                else:
                    recent_avg = last_round_coop_fraction
                if last_round_coop_fraction < max(0.25, recent_avg - 0.25):
                    collapsed = True
                    if self.last_collapse_round is None or t - self.last_collapse_round > self.F:
                        self.last_collapse_round = t
        if self.last_collapse_round is not None:
            if t > self.last_collapse_round and (t - self.last_collapse_round) % self.F == 0:
                if t != self.r - 1:
                    return Action.C
        if len(reciprocators) >= self.count_minR:
            if last_round_coop_fraction >= 0.5:
                return Action.C
            else:
                return Action.D
        if random.random() < self.p_probe:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    Collective Adaptive Reciprocity with Proportional Punishment (CARPP)

    - Starts by cooperating.
    - Cooperates when a clear majority cooperated last round.
    - Applies short, proportionate punishments when cooperation collapses.
    - Forgives and tests re-entry after punishments.
    - Switches to a defensive defect mode if a majority are persistent non-contributors.
    - Uses a conservative endgame policy in the last E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        self.L = min(10, self.r)
        self.M_frac = 0.6
        self.M = int(math.ceil(self.M_frac * self.n))
        self.L_frac = 0.3
        self.Lthr = int(math.floor(self.L_frac * self.n))
        self.P_base = max(1, min(3, int(math.floor(self.r / 10))))
        self.E = min(3, self.r)
        self.in_punishment = False
        self.punishment_remaining = 0
        self.awaiting_reentry = False
        self.last_was_reentry_test = False
        self.defensive_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.in_punishment = False
            self.punishment_remaining = 0
            self.awaiting_reentry = False
            self.last_was_reentry_test = False
            self.defensive_mode = False
            return Action.C
        t0 = state.round_number
        remaining_rounds = max(0, self.r - state.round_number)
        window = min(self.L, t0)
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        if t0 >= 1:
            last_my = bool(my_actions[-1])
            last_opp_sum = int(np.sum(opp_actions[-1, :])) if opp_actions.size > 0 else 0
            last_round_contributors = int(last_my) + last_opp_sum
        else:
            last_round_contributors = 0
        if window > 0:
            recent_opp_counts = np.sum(opp_actions[-window:, :], axis=0) if opp_actions.size > 0 else np.array([], dtype=int)
            recent_my_count = int(np.sum(my_actions[-window:]))
            recent_total = int(np.sum(recent_opp_counts)) + recent_my_count
            contrib_counts = [recent_my_count] + [int(x) for x in recent_opp_counts]
        else:
            recent_total = 0
            recent_my_count = 0
            contrib_counts = [0] * self.n
        recent_avg_per_round = recent_total / window if window > 0 else 0.0
        if t0 > 0:
            total_contribs_entire = int(np.sum(my_actions)) + int(np.sum(opp_actions)) if opp_actions.size > 0 else int(np.sum(my_actions))
            avg_entire = total_contribs_entire / t0
        else:
            total_contribs_entire = 0
            avg_entire = 0.0
        persistent_noncontributors = sum((1 for c in contrib_counts if c == 0))
        if window > 0 and persistent_noncontributors > self.n / 2:
            self.defensive_mode = True
        if remaining_rounds <= self.E:
            if t0 > 0 and avg_entire >= 0.85 * self.n:
                self.in_punishment = False
                self.punishment_remaining = 0
                self.awaiting_reentry = False
                self.last_was_reentry_test = False
                self.defensive_mode = False
                return Action.C
            if last_round_contributors >= self.M:
                return Action.C
            return Action.D
        if self.last_was_reentry_test:
            if last_round_contributors >= self.M:
                self.in_punishment = False
                self.punishment_remaining = 0
                self.awaiting_reentry = False
                self.last_was_reentry_test = False
                self.defensive_mode = False
                return Action.C
            else:
                P = min(self.P_base, max(0, remaining_rounds - 1))
                if P <= 0:
                    self.in_punishment = False
                    self.punishment_remaining = 0
                    self.last_was_reentry_test = False
                    self.awaiting_reentry = False
                    return Action.D
                self.in_punishment = True
                self.punishment_remaining = P
                self.last_was_reentry_test = False
                self.awaiting_reentry = False
                self.punishment_remaining -= 1
                if self.punishment_remaining <= 0:
                    self.in_punishment = False
                    self.punishment_remaining = 0
                    self.awaiting_reentry = True
                return Action.D
        if self.awaiting_reentry:
            self.awaiting_reentry = False
            self.last_was_reentry_test = True
            return Action.C
        if self.in_punishment and self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            if self.punishment_remaining <= 0:
                self.in_punishment = False
                self.punishment_remaining = 0
                self.awaiting_reentry = True
            return Action.D
        if self.defensive_mode:
            if last_round_contributors >= self.M:
                self.awaiting_reentry = False
                self.last_was_reentry_test = True
                return Action.C
            return Action.D
        if last_round_contributors >= self.M:
            return Action.C
        if last_round_contributors <= self.Lthr:
            denom = max(1, int(math.floor(self.n / 4)))
            extra = int(math.ceil((self.M - last_round_contributors) / denom))
            P = self.P_base + extra
            P = min(P, max(0, remaining_rounds - 1))
            if P <= 0:
                return Action.D
            self.in_punishment = True
            self.punishment_remaining = P
            self.punishment_remaining -= 1
            if self.punishment_remaining <= 0:
                self.in_punishment = False
                self.punishment_remaining = 0
                self.awaiting_reentry = True
            return Action.D
        p_raw = last_round_contributors / self.n + 0.1
        p = max(0.05, min(0.9, p_raw))
        if random.random() < p:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    Exploitative Public Goods strategy.

    - Probe for a short initial period by cooperating to identify conditional cooperators.
    - After probing, classify opponents by recent cooperation rates over a sliding window.
    - If at least one cooperator exists: mostly defect (exploit) but make occasional probabilistic cooperative gestures
      whose probability depends on recent group cooperation. Detect if my defection caused a large drop in group cooperation
      and then temporarily conciliate by cooperating for a short forgiveness period. If a specific cooperator reduces their
      cooperation rate sharply across consecutive windows, temporarily boost my cooperation probability for one round to try
      to regain them.
    - If no cooperators are detected: always defect.
    - Always defect during the final 1-2 rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_probe = min(3, max(1, math.floor(self.n_rounds / 10)))
        self.w = min(5, max(1, self.n_rounds))
        self.cooperator_threshold = 0.6
        self.delta_drop = 0.25
        self.T_forgive = 2
        self.final_defect_window = min(2, max(1, self.n_rounds))
        self.forgive_remaining = 0
        self.boost_p_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.n_rounds - self.final_defect_window:
            return Action.D
        if history is None:
            return Action.C
        rounds_played = int(history.opponent_actions.shape[0]) if history.opponent_actions is not None else 0
        if rounds_played == 0:
            return Action.C
        if self.forgive_remaining > 0:
            self.forgive_remaining = max(0, self.forgive_remaining - 1)
            return Action.C
        if t < self.T_probe:
            return Action.C
        num_opponents = max(1, self.n_players - 1)
        try:
            group_coop_prev = float(np.mean(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0.0
        except Exception:
            group_coop_prev = 0.0
        if rounds_played >= 2:
            try:
                group_coop_prev2 = float(np.mean(history.opponent_actions[-2, :]))
            except Exception:
                group_coop_prev2 = group_coop_prev
        else:
            group_coop_prev2 = group_coop_prev
        window_size = min(self.w, rounds_played)
        coop_rates = []
        for j in range(history.opponent_actions.shape[1]):
            recent = history.opponent_actions[-window_size:, j]
            rate = float(np.mean(recent)) if recent.size > 0 else 0.0
            coop_rates.append(rate)
        num_cooperators = sum((1 for rrate in coop_rates if rrate >= self.cooperator_threshold))
        my_last_action_was_D = False
        try:
            my_last_action_was_D = not bool(history.my_actions[-1])
        except Exception:
            my_last_action_was_D = False
        if group_coop_prev2 - group_coop_prev >= self.delta_drop and my_last_action_was_D:
            self.forgive_remaining = self.T_forgive
            self.forgive_remaining = max(0, self.forgive_remaining - 1)
            return Action.C
        targeted_boost_triggered = False
        if rounds_played >= 2 * self.w:
            for j in range(history.opponent_actions.shape[1]):

                def window_rate(start_offset, end_offset):
                    seg = history.opponent_actions[start_offset:end_offset, j]
                    return float(np.mean(seg)) if seg.size > 0 else 0.0
                rate_A = window_rate(-self.w, None)
                rate_B = window_rate(-2 * self.w, -self.w)
                rate_C = window_rate(-3 * self.w, -2 * self.w) if rounds_played >= 3 * self.w else None
                if rate_C is not None:
                    if rate_C >= self.cooperator_threshold and rate_C - rate_B >= 0.25 and (rate_B - rate_A >= 0.25):
                        targeted_boost_triggered = True
                        break
                elif rate_B >= self.cooperator_threshold and rate_B - rate_A >= 0.25:
                    targeted_boost_triggered = True
                    break
        if targeted_boost_triggered:
            self.boost_p_remaining = 1
        if num_cooperators == 0:
            return Action.D
        if group_coop_prev >= 0.75:
            p = 0.5
        elif group_coop_prev >= 0.5:
            p = 0.3
        else:
            p = 0.1
        if self.boost_p_remaining > 0:
            p = min(0.9, p + 0.2)
            self.boost_p_remaining = max(0, self.boost_p_remaining - 1)
        u = random.random()
        if u <= p:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    Probe-and-Exploit Conditional-Cooperator (PECC)

    Builds a short-lived cooperative reputation (probe), classifies opponents
    as unconditional / conditional / defectors using a sliding window, then
    exploits detectable cooperators while occasionally contributing to avoid
    permanent collapse. Always defects in the final few rounds. Implements
    short rebuild phases when defections trigger large drops or personal
    punishment from conditional responders.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.m_probe = min(4, max(1, math.floor(r / 10)))
        self.L = min(6, max(1, r))
        self.T_end = min(3, max(1, math.floor(r / 10)))
        self.T_rebuild = max(1, math.floor(r / 20))
        self.f_high = 0.65
        self.f_low = 0.35
        self.uncond_rate = 0.9
        self.defector_rate = 0.1
        self.responsive_delta = 0.25
        self.p_coop_maintain = 0.2
        self.p_coop_rebuild = 0.75
        self.p_probe = 0.1
        self.p_random_coop = 0.05
        self.rebuild_remaining = 0
        self.awaiting_drop_check = False
        self.baseline_frac_before_defect = None
        self._awaiting_check_round_index = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        opponents = max(0, n - 1)
        if history is None or t == 0:
            return Action.C
        total_rounds = history.my_actions.shape[0]
        if opponents == 0:
            return Action.D
        if t >= r - self.T_end:
            self.awaiting_drop_check = False
            self.rebuild_remaining = 0
            return Action.D
        if t < self.m_probe:
            return Action.C

        def compute_frac_last():
            if history.opponent_actions.shape[0] >= 1:
                last_row = history.opponent_actions[-1, :]
                return float(np.mean(last_row)) if last_row.size > 0 else 0.0
            return 0.0
        window_len = min(self.L, total_rounds)
        window_start = max(0, total_rounds - window_len)
        opp_window = history.opponent_actions[window_start:total_rounds, :]
        my_window = history.my_actions[window_start:total_rounds]
        if opp_window.size == 0:
            coop_rates = np.zeros(opponents, dtype=float)
        else:
            coop_rates = np.mean(opp_window, axis=0).astype(float)
        responsiveness = np.zeros(opponents, dtype=float)
        num_transitions = max(0, total_rounds - 1)
        trans_window_len = min(self.L, num_transitions)
        trans_start = max(1, total_rounds - trans_window_len)
        if trans_window_len > 0:
            prev_my = history.my_actions[trans_start - 1:total_rounds - 1]
            opp_curr = history.opponent_actions[trans_start:total_rounds, :]
            for i in range(opponents):
                prev_coop_mask = prev_my.astype(bool)
                prev_def_mask = ~prev_coop_mask
                denom_coop = int(np.sum(prev_coop_mask))
                denom_def = int(np.sum(prev_def_mask))
                prob_given_mycoop = 0.0
                prob_given_mydef = 0.0
                if denom_coop > 0:
                    prob_given_mycoop = float(np.sum(opp_curr[prev_coop_mask, i]) / denom_coop)
                if denom_def > 0:
                    prob_given_mydef = float(np.sum(opp_curr[prev_def_mask, i]) / denom_def)
                responsiveness[i] = prob_given_mycoop - prob_given_mydef
        else:
            responsiveness = np.zeros(opponents, dtype=float)
        frac_last = compute_frac_last()
        frac_uncond = float(np.mean(coop_rates >= self.uncond_rate)) if coop_rates.size > 0 else 0.0
        frac_defectors = float(np.mean(coop_rates <= self.defector_rate)) if coop_rates.size > 0 else 0.0
        frac_conditional = float(np.mean((responsiveness >= self.responsive_delta) & (coop_rates >= 0.4))) if coop_rates.size > 0 else 0.0
        if self.rebuild_remaining > 0:
            self.rebuild_remaining -= 1
            if random.random() < self.p_coop_rebuild:
                return Action.C
            else:
                return Action.D
        if self.awaiting_drop_check:
            current_frac_last = compute_frac_last()
            baseline = 0.0 if self.baseline_frac_before_defect is None else float(self.baseline_frac_before_defect)
            if baseline - current_frac_last >= 0.25:
                self.rebuild_remaining = self.T_rebuild
                self.awaiting_drop_check = False
                self.baseline_frac_before_defect = None
                if random.random() < self.p_coop_rebuild:
                    return Action.C
                else:
                    return Action.D
            else:
                self.awaiting_drop_check = False
                self.baseline_frac_before_defect = None
        punished = False
        if total_rounds >= 1:
            i_defected_last = not bool(history.my_actions[-1])
            if i_defected_last:
                if np.any(responsiveness <= -self.responsive_delta):
                    punished = True
        if punished:
            self.rebuild_remaining = self.T_rebuild
            if random.random() < self.p_coop_rebuild:
                return Action.C
            else:
                return Action.D
        if frac_last >= self.f_high and frac_uncond + frac_conditional >= 0.25:
            if random.random() < 1.0 - self.p_coop_maintain:
                if total_rounds >= 1 and bool(history.my_actions[-1]) is True:
                    self.baseline_frac_before_defect = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
                    self.awaiting_drop_check = True
                    self._awaiting_check_round_index = total_rounds
                return Action.D
            else:
                return Action.C
        if frac_last <= self.f_low:
            if random.random() < self.p_probe:
                return Action.C
            else:
                if total_rounds >= 1 and bool(history.my_actions[-1]) is True:
                    self.baseline_frac_before_defect = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
                    self.awaiting_drop_check = True
                    self._awaiting_check_round_index = total_rounds
                return Action.D
        if frac_last > self.f_low and frac_last < self.f_high:
            per_round_frac = []
            for rr in range(total_rounds):
                per_round_frac.append(float(np.mean(history.opponent_actions[rr, :])))
            per_round_frac = np.array(per_round_frac) if len(per_round_frac) > 0 else np.array([])
            last_window_len = min(self.L, per_round_frac.size)
            last_window_mean = float(np.mean(per_round_frac[-last_window_len:])) if last_window_len > 0 else 0.0
            prev_window_mean = 0.0
            if per_round_frac.size >= 2 * self.L:
                prev_window_mean = float(np.mean(per_round_frac[-2 * self.L:-self.L]))
            elif per_round_frac.size >= self.L + 1:
                prev_available = per_round_frac[:-last_window_len]
                if prev_available.size > 0:
                    prev_window_mean = float(np.mean(prev_available))
            if last_window_mean > prev_window_mean:
                if random.random() < 0.3:
                    return Action.C
                else:
                    if total_rounds >= 1 and bool(history.my_actions[-1]) is True:
                        self.baseline_frac_before_defect = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
                        self.awaiting_drop_check = True
                        self._awaiting_check_round_index = total_rounds
                    return Action.D
            elif random.random() < self.p_random_coop:
                return Action.C
            else:
                if total_rounds >= 1 and bool(history.my_actions[-1]) is True:
                    self.baseline_frac_before_defect = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
                    self.awaiting_drop_check = True
                    self._awaiting_check_round_index = total_rounds
                return Action.D
        if random.random() < self.p_random_coop:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperator (ACCC)

    - Cooperates in initial seed rounds to signal willingness.
    - Tracks recent-window reputations and lifetime cooperation of each opponent.
    - Contributes only if a required fraction of others are "reliable".
    - Applies short, proportionate punishments when exploited (you cooperated but too few others did).
    - Treats persistent low cooperators as permanent defectors and raises the bar accordingly.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.M = min(3, self.r)
        self.W = min(8, self.r)
        self.q = 0.6
        self.alpha = 0.6
        self.P_max = min(3, self.r)
        self.perm_cutoff = 0.1
        self.punish_until_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        r = self.r
        n = self.n
        if current_t == r:
            return Action.D
        if current_t <= self.M:
            return Action.C
        if history is None or state.round_number == 0:
            return Action.C
        num_opponents = n - 1
        rounds_played = state.round_number
        reliable_flags = [False] * num_opponents
        permanent_flags = [False] * num_opponents
        recent_slice = history.opponent_actions[-self.W:, :] if rounds_played > 0 else history.opponent_actions[:0, :]
        for j in range(num_opponents):
            recent_col = recent_slice[:, j]
            recent_count = int(np.sum(recent_col)) if recent_col.size > 0 else 0
            recent_len = int(recent_col.size) if recent_col.size > 0 else 0
            recent_rate = recent_count / recent_len if recent_len > 0 else 0.0
            lifetime_col = history.opponent_actions[:, j]
            lifetime_count = int(np.sum(lifetime_col)) if lifetime_col.size > 0 else 0
            lifetime_rate = lifetime_count / rounds_played if rounds_played > 0 else 0.0
            is_permanent = lifetime_rate < self.perm_cutoff
            is_reliable = recent_rate >= self.q and (not is_permanent)
            permanent_flags[j] = is_permanent
            reliable_flags[j] = is_reliable
        reliable_count = sum((1 for f in reliable_flags if f))
        num_permanent = sum((1 for f in permanent_flags if f))
        required_base = math.ceil(num_opponents * self.alpha)
        required_count = required_base + num_permanent
        prev_index = state.round_number - 1
        prev_t = current_t - 1
        if prev_index >= 0:
            we_cooperated_prev = bool(history.my_actions[-1])
            other_coop_prev = int(np.sum(history.opponent_actions[-1, :]))
            if we_cooperated_prev and other_coop_prev < required_count:
                P = min(self.P_max, max(0, r - prev_t))
                if P > 0:
                    new_punish_until = state.round_number + P
                    if new_punish_until > self.punish_until_round:
                        self.punish_until_round = new_punish_until
        if state.round_number < self.punish_until_round:
            return Action.D
        if reliable_count >= required_count:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    Opportunistic Exploiter with Conditional Forgiveness (OECF)

    - Signals cooperation early (S rounds).
    - In normal play, exploits reliably cooperative groups, rebuilds when group is unreliable,
      and follows recent majority in ambiguous cases.
    - If exploitation provokes a clear drop in group cooperation, enters a short punishment
      episode (P rounds) followed by a cooperative probe (RPI rounds). If the probe succeeds
      (group_coop_rate >= H) it clears punishment; otherwise it re-enters punishment.
    - Always defects in the final E rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = max(1, min(5, math.floor(self.r / 10)))
        self.S = min(3, max(1, math.floor(self.r / 6)))
        self.E = min(3, self.r)
        self.H = 0.7
        self.L = 0.4
        self.P = 2
        self.RPI = 1
        self.D = 0.2
        self.punish_mode = False
        self.punish_remaining = 0
        self.probe_started_at = None
        self.probe_rounds_remaining = 0
        self.last_exploited_round_at = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = int(state.round_number)
        t = current_round + 1
        opponents = max(0, self.n - 1)

        def group_coop_rate_up_to(up_to_round: int) -> float:
            if history is None:
                return 0.0
            total_rounds_available = history.opponent_actions.shape[0]
            if up_to_round < 0 or total_rounds_available == 0:
                return 0.0
            up_to = min(up_to_round, total_rounds_available - 1)
            start = max(0, up_to - self.W + 1)
            window = history.opponent_actions[start:up_to + 1, :]
            if window.size == 0:
                return 0.0
            if window.shape[1] == 0:
                return 0.0
            opp_rates = np.sum(window.astype(np.int64), axis=0) / float(window.shape[0])
            return float(np.mean(opp_rates))

        def previous_round_strict_majority_coop() -> bool:
            if history is None:
                return False
            if history.opponent_actions.shape[0] == 0:
                return False
            last = history.opponent_actions[-1, :]
            coop_count = int(np.sum(last.astype(np.int64)))
            return coop_count > opponents / 2.0
        if self.last_exploited_round_at is not None and history is not None:
            exploited_at = self.last_exploited_round_at
            if history.opponent_actions.shape[0] >= exploited_at + 2:
                before = group_coop_rate_up_to(exploited_at)
                after = group_coop_rate_up_to(exploited_at + 1)
                if before - after >= self.D:
                    self.punish_mode = True
                    self.punish_remaining = self.P
                self.last_exploited_round_at = None
        if self.probe_started_at is not None and self.probe_rounds_remaining == 0 and (history is not None):
            probe_completed_at = self.probe_started_at + self.RPI - 1
            if history.opponent_actions.shape[0] >= probe_completed_at + 1:
                grp_rate = group_coop_rate_up_to(probe_completed_at)
                if grp_rate >= self.H:
                    self.punish_mode = False
                    self.punish_remaining = 0
                else:
                    self.punish_mode = True
                    self.punish_remaining = self.P
                self.probe_started_at = None
                self.probe_rounds_remaining = 0
        if t > self.r - self.E:
            return Action.D
        if current_round == 0:
            if t <= self.S:
                return Action.C
        if self.punish_mode:
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
                return Action.D
            if self.probe_rounds_remaining > 0:
                self.probe_rounds_remaining -= 1
                if self.probe_started_at is None:
                    self.probe_started_at = current_round
                return Action.C
            else:
                self.probe_started_at = current_round
                self.probe_rounds_remaining = self.RPI - 1
                return Action.C
        last_completed_round = current_round - 1
        grp_rate = group_coop_rate_up_to(last_completed_round)
        if t <= self.S:
            return Action.C
        if grp_rate >= self.H:
            self.last_exploited_round_at = current_round
            return Action.D
        elif grp_rate <= self.L:
            return Action.C
        elif previous_round_strict_majority_coop():
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    Adaptive Exploitative Conditional Free-rider (AECF)

    Probes opponents for a few rounds, classifies them by recent cooperation rates,
    then exploits reliably cooperative opponents by mostly defecting while giving
    rare "maintenance" cooperations to keep them around. Switches between phases:
    Probe, Exploit, ConditionalCooperate, MutualDefect (Retaliate), Repair.
    Always defects in the final T_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_probe = min(4, max(1, self.r))
        self.W = min(20, max(1, self.r))
        self.p_high = 0.8
        self.p_mid = 0.4
        self.exploit_maintain_freq = 10
        self.retaliation_drop = 0.2
        self.T_end = min(3, max(1, self.r))
        self.forgiveness_probe = 3
        self.phase = 'Probe'
        self.exploit_baseline = 0.0
        self.exploit_start_round = None
        self.last_P = 0.0
        self.repair_counter = 0
        self.repair_start_round = None
        self.last_reval_round = 0
        self.probe_checked_majority_defect = False
        self._short_game_flag = self.r <= 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r <= 1:
            return Action.D
        if state.round_number == 0 or history is None:
            return Action.C
        t = state.round_number
        rounds_played = int(history.opponent_actions.shape[0])
        n_opponents = self.n - 1

        def compute_pjs():
            recent = history.opponent_actions[-self.W:, :]
            p_js = np.array(recent, dtype=float).mean(axis=0)
            return p_js
        p_js = compute_pjs()
        P = float(np.mean(p_js)) if p_js.size > 0 else 0.0
        if rounds_played >= 2:
            baseline_group_coop = float(np.mean(np.array(history.opponent_actions[-2:, :], dtype=float)))
        else:
            baseline_group_coop = float(np.mean(np.array(history.opponent_actions[-1, :], dtype=float)))
        if not self.probe_checked_majority_defect and rounds_played >= 1:
            cooperators_round0 = int(np.sum(history.opponent_actions[0, :]))
            defectors_round0 = n_opponents - cooperators_round0
            if defectors_round0 >= math.ceil(n_opponents / 2):
                self.phase = 'MutualDefect'
            self.probe_checked_majority_defect = True
        if t >= self.r - self.T_end:
            return Action.D
        if self._short_game_flag and t >= 1:
            if P < self.p_high:
                return Action.D
        if self.phase == 'Probe' and t < self.T_probe:
            if self.phase == 'MutualDefect':
                return Action.D
            return Action.C
        if self.phase == 'Probe' and t >= self.T_probe:
            reliable_mask = p_js >= self.p_high
            reliable_count = int(np.sum(reliable_mask))
            self.last_P = P
            if reliable_count >= 1 and P >= self.p_high:
                self.phase = 'Exploit'
                self.exploit_baseline = P
                self.exploit_start_round = t
            elif P >= self.p_mid:
                self.phase = 'ConditionalCooperate'
            else:
                self.phase = 'MutualDefect'
        if self.phase == 'MutualDefect':
            if t - self.last_reval_round >= 10:
                p_js_re = compute_pjs()
                P_re = float(np.mean(p_js_re)) if p_js_re.size > 0 else 0.0
                sometimes_cooperative = int(np.sum(p_js_re >= self.p_mid))
                if P_re >= self.p_mid and sometimes_cooperative >= math.ceil(n_opponents / 2):
                    self.phase = 'Repair'
                    self.repair_counter = 0
                    self.repair_start_round = t
                    self.last_P = P_re
                self.last_reval_round = t
            return Action.D
        if self.phase == 'Exploit':
            if self.exploit_start_round is None:
                self.exploit_start_round = t
            maintenance_due = (t - self.exploit_start_round) % self.exploit_maintain_freq == 0
            if maintenance_due:
                return Action.C
            if rounds_played >= 1:
                my_last_action = bool(history.my_actions[-1])
                curr_group_coop = float(np.mean(np.array(history.opponent_actions[-1, :], dtype=float)))
                if not my_last_action and curr_group_coop < self.exploit_baseline - self.retaliation_drop:
                    self.phase = 'MutualDefect'
                    self.last_reval_round = t
                    return Action.D
            reliable_mask = p_js >= self.p_high
            reliable_count = int(np.sum(reliable_mask))
            if P < self.p_mid:
                self.phase = 'MutualDefect'
                return Action.D
            if P >= self.p_mid and P < self.p_high:
                self.phase = 'ConditionalCooperate'
                return Action.D
            return Action.D
        if self.phase == 'ConditionalCooperate':
            if rounds_played >= 1:
                cooperators_last = int(np.sum(history.opponent_actions[-1, :]))
            else:
                cooperators_last = 0
            threshold = math.ceil(n_opponents * 0.5)
            if cooperators_last >= threshold:
                return Action.C
            else:
                if P < self.p_mid:
                    self.phase = 'MutualDefect'
                    return Action.D
                return Action.D
        if self.phase == 'Repair':
            current_pjs = compute_pjs()
            current_P = float(np.mean(current_pjs)) if current_pjs.size > 0 else 0.0
            trend_positive = current_P >= self.last_P
            if self.repair_counter < self.forgiveness_probe and trend_positive:
                self.repair_counter += 1
                return Action.C
            else:
                if self.repair_counter >= self.forgiveness_probe:
                    p_js_after = current_pjs
                    P_after = current_P
                    reliable_count_after = int(np.sum(p_js_after >= self.p_high))
                    if reliable_count_after >= 1 and P_after >= self.p_high:
                        self.phase = 'Exploit'
                        self.exploit_baseline = P_after
                        self.exploit_start_round = t
                        return Action.D
                    if P_after >= self.p_mid:
                        self.phase = 'ConditionalCooperate'
                        return Action.D
                    self.phase = 'MutualDefect'
                    self.last_reval_round = t
                    return Action.D
                self.phase = 'MutualDefect'
                self.last_reval_round = t
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    Collective Conditional Cooperator with Forgiveness, Probing, and Short Punishments.

    - Starts cooperatively (unless in endgame).
    - Continues cooperating when a sufficient fraction (alpha) cooperated last round.
    - If cooperation falls below alpha, defects but occasionally probes (probabilistic)
      after a short defection streak to test for recovery.
    - If a sudden sharp drop follows a recent high-cooperation window, triggers a short,
      finite collective punishment (group defection) of limited length.
    - After punishment, requires a short reconfirmation window (W rounds) before fully
      returning to unconditional cooperation; otherwise remains defecting with probes.
    - In the final endgame_E rounds, defects (no future reciprocity).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = max(0.5, self.k / max(1, self.n))
        self.W = min(5, max(1, self.r))
        self.punish_max = min(3, max(1, self.r // 10))
        self.probe_prob = 0.1
        self.endgame_E = min(3, max(0, self.r))
        self.punish_remaining = 0
        self.post_punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        alpha = self.alpha
        W = self.W
        if self.endgame_E > 0 and t >= r - self.endgame_E:
            self.punish_remaining = 0
            self.post_punish_remaining = 0
            return Action.D
        if t == 0 or history is None:
            return Action.C

        def contributors_in_round(idx: int) -> int:
            my = int(bool(history.my_actions[idx]))
            opp = int(int(history.opponent_actions[idx, :].sum()) if history.opponent_actions.size else 0)
            return my + opp
        last_idx = t - 1
        contributors_last = contributors_in_round(last_idx)
        f_prev = float(contributors_last) / float(n) if n > 0 else 0.0
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.post_punish_remaining = W
            return action
        if self.post_punish_remaining > 0:
            if f_prev >= alpha:
                self.post_punish_remaining = 0
                return Action.C
            self.post_punish_remaining = max(0, self.post_punish_remaining - 1)
            my_actions = history.my_actions
            defect_streak = 0
            for a in reversed(my_actions):
                if not bool(a):
                    defect_streak += 1
                else:
                    break
            if defect_streak >= 2 and random.random() < self.probe_prob:
                return Action.C
            return Action.D
        triggered = False
        threshold_sharp = math.floor(max(0.0, alpha - 0.2) * n)
        if contributors_last <= threshold_sharp:
            start_idx = max(0, last_idx - (W - 1))
            for idx in range(start_idx, last_idx + 1):
                f_i = float(contributors_in_round(idx)) / float(n) if n > 0 else 0.0
                if f_i >= alpha:
                    triggered = True
                    break
        if triggered:
            remaining_rounds = max(0, r - t)
            effective_remaining = max(0, remaining_rounds - self.endgame_E)
            L = min(self.punish_max, effective_remaining)
            if L > 0:
                self.punish_remaining = L
                self.punish_remaining = max(0, self.punish_remaining - 1)
                if self.punish_remaining == 0:
                    self.post_punish_remaining = W
                return Action.D
        if f_prev >= alpha:
            return Action.C
        my_actions = history.my_actions
        defect_streak = 0
        for a in reversed(my_actions):
            if not bool(a):
                defect_streak += 1
            else:
                break
        sustained_drop = True
        start_idx = max(0, last_idx - (W - 1))
        for idx in range(start_idx, last_idx + 1):
            f_i = float(contributors_in_round(idx)) / float(n) if n > 0 else 0.0
            if f_i >= alpha:
                sustained_drop = False
                break
        if (defect_streak >= 2 or sustained_drop) and random.random() < self.probe_prob:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    Actively free-rides when immediate gain from defection exceeds the empirically
    estimated future loss caused by reduced cooperation from others. Begins with
    a short probing phase to learn opponents' responsiveness, continuously updates
    statistics over a recent window W, and switches between EXPLOIT, APPEASE and
    short calibrated PUNISH sequences. Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.G = 1.0 - self.k / max(1.0, self.n)
        approx = int(math.ceil(self.r / 10)) if self.r > 0 else 4
        self.W = int(min(12, max(4, approx)))
        self.epsilon = 0.01
        self.margin = 0.02
        try:
            p_from_gain = int(math.ceil(1.0 / max(self.G, 1e-08)))
        except Exception:
            p_from_gain = 10
        self.P_default = max(3, p_from_gain)
        self.last_probe_at = -10000
        self.last_defect_probe_at = -10000
        self.appease_until = -1
        self.punish_until = -1
        self.initial_probe_len = min(4, self.r)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        rem = max(1, self.r - t)
        if t == 0 or history is None:
            return Action.C
        if t <= self.punish_until:
            return Action.D
        if t <= self.appease_until:
            return Action.C
        if rem == 1:
            return Action.D
        if t < self.initial_probe_len:
            pattern = [Action.C, Action.C, Action.D, Action.C][:self.initial_probe_len]
            chosen = pattern[t]
            if chosen == Action.D:
                self.last_defect_probe_at = t
            if chosen == Action.C:
                self.last_probe_at = t
            return chosen
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_observed = my_actions.shape[0]
        s_start = max(0, t - 1 - self.W)
        s_end = t - 2
        if s_end >= s_start:
            s_indices = list(range(s_start, s_end + 1))
        else:
            s_indices = []
        if len(s_indices) < 2:
            consec_coop = 0
            for a in reversed(my_actions):
                if a:
                    consec_coop += 1
                else:
                    break
            long_coop_threshold = max(5, int(math.ceil(1.0 / max(self.G, 1e-08))))
            if consec_coop >= long_coop_threshold and t - self.last_defect_probe_at > long_coop_threshold:
                self.last_defect_probe_at = t
                return Action.D
            return Action.C
        coop_next_counts_after_coop = []
        coop_next_counts_after_def = []
        defect_s_list = []
        coop_s_list = []
        for s in s_indices:
            next_round = s + 1
            others_coop_count = int(opp_actions[next_round, :].sum())
            if my_actions[s]:
                coop_next_counts_after_coop.append(others_coop_count)
                coop_s_list.append(s)
            else:
                coop_next_counts_after_def.append(others_coop_count)
                defect_s_list.append(s)
        recent_counts = [int(row.sum()) for row in opp_actions[max(0, rounds_observed - self.W):rounds_observed]]
        recent_avg = float(np.mean(recent_counts)) if len(recent_counts) > 0 else 0.0
        if len(coop_next_counts_after_coop) > 0:
            baseline_coop = float(np.mean(coop_next_counts_after_coop))
        else:
            baseline_coop = float(recent_avg)
        if len(coop_next_counts_after_def) > 0:
            coop_after_def = float(np.mean(coop_next_counts_after_def))
        else:
            coop_after_def = float(recent_avg)
        d = max(0.0, baseline_coop - coop_after_def)
        taus = []
        target_threshold = 0.9 * baseline_coop
        for s in defect_s_list:
            recovered = False
            for tau in range(1, self.W + 1):
                check_round = s + tau
                if check_round <= t - 1:
                    coop_count = int(opp_actions[check_round, :].sum())
                    if coop_count >= target_threshold - 1e-09:
                        taus.append(float(tau))
                        recovered = True
                        break
                else:
                    break
            if not recovered:
                taus.append(float(self.W))
        if len(taus) > 0:
            recovery_T = float(np.mean(taus))
        else:
            recovery_T = float(self.W)
        estimated_future_loss = self.k / max(1.0, float(self.n)) * d * recovery_T
        forg_count = 0
        for tau in taus:
            if tau <= 1.0 + 1e-09:
                forg_count += 1
        forgiveness_rate = forg_count / max(1, len(taus)) if len(taus) > 0 else 0.0
        opp_count = self.n - 1
        coop_after_coop_rates = np.zeros(opp_count, dtype=float)
        coop_after_def_rates = np.zeros(opp_count, dtype=float)
        if opp_actions.shape[0] > 0:
            overall_rates = np.asarray(opp_actions[max(0, opp_actions.shape[0] - self.W):, :].mean(axis=0), dtype=float)
        else:
            overall_rates = np.zeros(opp_count, dtype=float)
        coop_after_coop_rates[:] = overall_rates
        coop_after_def_rates[:] = overall_rates
        if len(coop_s_list) > 0:
            arr = np.asarray([opp_actions[s + 1, :] for s in coop_s_list], dtype=float)
            coop_after_coop_rates = arr.mean(axis=0)
        if len(defect_s_list) > 0:
            arr = np.asarray([opp_actions[s + 1, :] for s in defect_s_list], dtype=float)
            coop_after_def_rates = arr.mean(axis=0)
        responsiveness = np.abs(coop_after_coop_rates - coop_after_def_rates)
        overall_coop_rates = overall_rates
        unconditional_cooperators = (overall_coop_rates >= 0.9 - 1e-09) & (responsiveness <= 0.05 + 1e-09)
        found_unconditional = bool(np.any(unconditional_cooperators)) if opp_count > 0 else False
        if float(baseline_coop) <= self.epsilon * float(self.n):
            return Action.D
        if rem <= recovery_T + 1.0:
            return Action.D
        punishment_condition = d >= max(0.35, 0.2 * baseline_coop) and recovery_T >= max(2.0, float(self.W) / 2.0)
        if punishment_condition:
            P_len = int(min(math.ceil(recovery_T / 2.0), 3))
            self.punish_until = t + max(0, P_len - 1)
            A = max(1, int(max(1, round(recovery_T / 2.0))))
            self.appease_until = self.punish_until + A
            return Action.D
        if self.G - (estimated_future_loss + self.margin) > 0.0:
            P = self.P_default
            if found_unconditional:
                do_probe = False
            else:
                do_probe = t - self.last_probe_at >= P
            consec_coop = 0
            for a in reversed(my_actions):
                if a:
                    consec_coop += 1
                else:
                    break
            long_coop_threshold = max(5, int(math.ceil(1.0 / max(self.G, 1e-08))))
            if consec_coop >= long_coop_threshold and t - self.last_defect_probe_at > long_coop_threshold:
                if rem > 1:
                    self.last_defect_probe_at = t
                    return Action.D
            if do_probe and rem > 1:
                self.last_probe_at = t
                return Action.C
            return Action.D
        A = max(1, int(round(recovery_T / 2.0)))
        self.appease_until = t + max(0, A - 1)
        return Action.C

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR) for the Public Goods Game.

    - Signals cooperation first round.
    - Uses a sliding window W to estimate per-player cooperation rates s_j.
    - Computes group cooperation G as the mean s_j across opponents.
    - If G >= Theta, cooperates (unless in a short targeted punishment phase).
    - If persistent defectors exist (s_j <= theta_low and at least one recent defection),
      start a short proportional punishment of length P = min(Pmax, 1+num_offenders).
      Punishments are public, short, capped, and reversible.
    - If G < Theta but no persistent defectors, probe deterministically every 4th such round
      (to avoid randomness in tournament settings); otherwise defect.
    - Forgives quickly: if previous round we defected due to our own punishment/probe but
      the group cooperation (as measured over the recent window) is high, resume cooperation.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(6, max(1, math.floor(self.r / 4)))
        self.Pmax = min(3, max(1, math.floor(self.r / 10)))
        self.Theta = max(0.5, self.k / self.n)
        self.theta_low = 0.2
        self.punishment_end = -1
        self.punishment_offenders = set()
        self.last_action_reason = None
        self.probe_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            self.last_action_reason = 'start'
            return Action.C
        if t == self.r - 1:
            self.last_action_reason = 'final'
            return Action.D
        rounds_available = int(history.opponent_actions.shape[0])
        window = min(self.W, rounds_available)
        start = rounds_available - window
        recent = history.opponent_actions[start:rounds_available, :]
        n_opponents = recent.shape[1] if recent.ndim > 1 else 1
        if recent.ndim == 1:
            recent = recent.reshape(window, 1)
            n_opponents = 1
        coop_counts = recent.sum(axis=0).astype(int)
        s_js = (coop_counts / float(window)).tolist()
        O_js = (window - coop_counts).tolist()
        if len(s_js) > 0:
            G = float(np.mean(np.array(s_js, dtype=float)))
        else:
            G = 0.0
        my_last_action_defected = False
        if history.my_actions.shape[0] >= 1:
            my_last_action_defected = history.my_actions[-1] == False
        if my_last_action_defected and self.last_action_reason in ('punish', 'probe') and (G >= self.Theta):
            self.last_action_reason = 'contrite'
            self.probe_counter = 0
            self.punishment_end = -1
            self.punishment_offenders = set()
            return Action.C
        if self.punishment_end >= t and t < self.r - 1:
            self.last_action_reason = 'punish'
            return Action.D
        if self.punishment_end < t:
            self.punishment_offenders = set()
            self.punishment_end = -1
        persistent_indices = set()
        for j_idx, (s_val, o_val) in enumerate(zip(s_js, O_js)):
            if s_val <= self.theta_low and o_val >= 1:
                persistent_indices.add(j_idx)
        if persistent_indices:
            P = min(self.Pmax, 1 + len(persistent_indices))
            punishment_end = t + P - 1
            punishment_end = min(punishment_end, self.r - 2)
            self.punishment_end = punishment_end
            self.punishment_offenders = persistent_indices.copy()
            self.last_action_reason = 'punish'
            self.probe_counter = 0
            return Action.D
        if G >= self.Theta:
            self.last_action_reason = 'cooperate'
            self.probe_counter = 0
            return Action.C
        probe_should = False
        if self.probe_counter % 4 == 3:
            probe_should = True
        self.probe_counter = self.probe_counter + 1
        if probe_should:
            self.last_action_reason = 'probe'
            return Action.C
        else:
            self.last_action_reason = 'probe_wait'
            return Action.D

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    Exploitative-Reciprocity (ER)
    - Cooperate to elicit reciprocity when the expected discounted benefit from increased
      future cooperation exceeds the immediate private cost.
    - Otherwise defect, but occasionally probe or exploit highly cooperative groups with
      small-probability cooperation to maintain cooperation.
    - Always defect in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.H = min(20, r)
        self.E = min(max(3, math.ceil(r / 10)), r)
        self.s = 1e-06
        self.p_probe_exploit = 0.1
        self.p_probe_data = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if t >= max(0, r - self.E):
            return Action.D
        if t == 0 or history is None:
            return Action.C
        T = int(history.my_actions.shape[0])
        start_idx = max(1, T - self.H)
        if start_idx >= T:
            start_idx = T
        n_opponents = max(1, n - 1)
        N_c = np.zeros(n_opponents, dtype=float)
        M_c = np.zeros(n_opponents, dtype=float)
        N_d = np.zeros(n_opponents, dtype=float)
        M_d = np.zeros(n_opponents, dtype=float)
        if start_idx < T:
            for tt in range(start_idx, T):
                my_prev = bool(history.my_actions[tt - 1])
                opp_actions_tt = np.asarray(history.opponent_actions[tt, :], dtype=bool)
                if my_prev:
                    M_c += 1.0
                    N_c += opp_actions_tt.astype(float)
                else:
                    M_d += 1.0
                    N_d += opp_actions_tt.astype(float)
        p_c = (N_c + self.s) / (M_c + 2.0 * self.s)
        p_d = (N_d + self.s) / (M_d + 2.0 * self.s)
        R_j = p_c - p_d
        R = float(np.mean(R_j)) if n_opponents > 0 else 0.0
        SumR = float(np.sum(R_j))
        coop_window_start = max(0, T - self.H)
        coop_window_end = T
        if coop_window_start < coop_window_end:
            opp_slice = np.asarray(history.opponent_actions[coop_window_start:coop_window_end, :], dtype=float)
            my_slice = np.asarray(history.my_actions[coop_window_start:coop_window_end], dtype=float)
            per_round_sum = my_slice + np.sum(opp_slice, axis=1)
            frac_per_round = per_round_sum / float(n)
            G = float(np.mean(frac_per_round))
        else:
            G = 0.0
        Cost = 1.0 - k / float(n)
        Benefit_next = k / float(n) * SumR
        remaining = max(0, r - t)
        denom = max(1, remaining)
        gamma = (remaining - 1) / float(denom)
        if gamma < 0.0:
            gamma = 0.0
        B = gamma * Benefit_next
        if B > Cost:
            return Action.C
        if G > 0.8:
            if random.random() < self.p_probe_exploit:
                return Action.C
            return Action.D
        total_obs = M_c + M_d
        scarce_count = float(np.sum(total_obs < 2.0))
        if scarce_count / float(n_opponents) > 0.5:
            if random.random() < self.p_probe_data:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    Opportunistic Leech with Targeted Punishment (OLTP)

    - Brief probe at start to identify cooperators.
    - Default exploitative stance: defect most rounds to free-ride on cooperators.
    - Periodic maintenance cooperation after a streak of defections to avoid complete breakdown.
    - Targeted punishment: if a player reduces their recent cooperation rate after I defect,
      punish that player for a short fixed window by defecting while their punish-timer is active.
    - Always defect in the final EndgameLength rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.ProbeLength = max(1, min(3, math.floor(self.r / 10)))
        self.EndgameLength = max(1, min(3, math.floor(self.r / 10)))
        self.CooperatorThreshold = 0.7
        self.LowGroupThreshold = 0.3
        self.MaxDefectStreak = 3
        self.PunishDrop = 0.2
        self.PunishLength = 3
        self.MaintenanceCoopInterval = self.MaxDefectStreak + 1
        self.n_opponents = max(0, self.n - 1)
        self.P_timers = [0 for _ in range(self.n_opponents)]
        self.pre_defect_R = [None for _ in range(self.n_opponents)]
        self.D_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t == 0 or history is None:
            if t >= self.r - self.EndgameLength:
                self.D_streak += 1
                return Action.D
            if t < self.ProbeLength:
                self.D_streak = 0
                return Action.C
            self.D_streak += 1
            return Action.D
        num_past = len(history.my_actions)
        for idx in range(self.n_opponents):
            if self.P_timers[idx] > 0:
                self.P_timers[idx] = max(0, self.P_timers[idx] - 1)
        window = min(10, num_past) if num_past > 0 else 0
        current_R = [0.0 for _ in range(self.n_opponents)]
        if window > 0 and self.n_opponents > 0:
            recent_actions = history.opponent_actions[-window:, :]
            means = np.mean(recent_actions.astype(float), axis=0)
            for idx in range(self.n_opponents):
                current_R[idx] = float(means[idx])
        for idx in range(self.n_opponents):
            pre = self.pre_defect_R[idx]
            if pre is None:
                continue
            if pre - current_R[idx] > self.PunishDrop:
                self.P_timers[idx] = self.PunishLength
            elif abs(current_R[idx] - pre) <= self.PunishDrop and self.P_timers[idx] > 0:
                self.P_timers[idx] = 0
                self.pre_defect_R[idx] = None
        G = 0.0
        if self.n_opponents > 0:
            G = float(np.mean(np.array(current_R)))
        else:
            G = 0.0
        if t >= self.r - self.EndgameLength:
            if window > 0 and self.n_opponents > 0:
                for idx in range(self.n_opponents):
                    self.pre_defect_R[idx] = current_R[idx]
            self.D_streak += 1
            return Action.D
        if t < self.ProbeLength:
            self.D_streak = 0
            return Action.C
        if G < self.LowGroupThreshold:
            if window > 0 and self.n_opponents > 0:
                for idx in range(self.n_opponents):
                    self.pre_defect_R[idx] = current_R[idx]
            self.D_streak += 1
            return Action.D
        if any((pt > 0 for pt in self.P_timers)):
            if window > 0 and self.n_opponents > 0:
                for idx in range(self.n_opponents):
                    self.pre_defect_R[idx] = current_R[idx]
            self.D_streak += 1
            return Action.D
        if G >= self.CooperatorThreshold:
            if self.D_streak >= self.MaxDefectStreak:
                self.D_streak = 0
                return Action.C
            else:
                if window > 0 and self.n_opponents > 0:
                    for idx in range(self.n_opponents):
                        self.pre_defect_R[idx] = current_R[idx]
                self.D_streak += 1
                return Action.D
        if self.LowGroupThreshold <= G < self.CooperatorThreshold:
            half_thresh = max(1, math.floor(self.MaxDefectStreak / 2))
            if self.D_streak >= half_thresh:
                self.D_streak = 0
                return Action.C
            else:
                if window > 0 and self.n_opponents > 0:
                    for idx in range(self.n_opponents):
                        self.pre_defect_R[idx] = current_R[idx]
                self.D_streak += 1
                return Action.D
        if window > 0 and self.n_opponents > 0:
            for idx in range(self.n_opponents):
                self.pre_defect_R[idx] = current_R[idx]
        self.D_streak += 1
        return Action.D

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC)

    - Starts by cooperating.
    - Maintains a cooperation threshold T = min(0.95, max(0.5, 1 - 0.5*(k/n))).
    - Uses a short sliding window W = min(5, t-1) (t is 1-indexed round) to estimate recent cooperation of others.
    - If recent cooperation (or last 1-2 rounds) meets/exceeds T, cooperate.
    - Otherwise enter a bounded punishment of length P (proportional to shortfall, capped by P_max).
    - After a punishment, play one cooperative test round to probe whether cooperation resumes.
    - Default-defects in the final round unless the history is unanimous cooperation with no prior defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        ratio = 0.0
        if self.n > 0:
            ratio = self.k / float(self.n)
        self.T = min(0.95, max(0.5, 1.0 - 0.5 * ratio))
        p_floor = math.floor(self.r / 10) if self.r >= 10 else 0
        self.P_max = min(3, max(1, p_floor))
        self.punish_until = -1
        self.next_test_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = int(state.round_number)
        if current_round == 0:
            self.punish_until = -1
            self.next_test_round = None
            return Action.C
        if history is None:
            return Action.C
        final_round_index = self.r - 1
        if current_round == final_round_index:
            prev_rounds = history.opponent_actions.shape[0]
            unanimous = False
            if prev_rounds > 0:
                opp_all = bool(np.all(history.opponent_actions))
                me_all = bool(np.all(history.my_actions))
                unanimous = opp_all and me_all
            if unanimous:
                return Action.C
            return Action.D
        if self.punish_until >= current_round:
            return Action.D
        if self.next_test_round is not None and current_round == self.next_test_round:
            self.next_test_round = None
            return Action.C
        W = min(5, current_round)
        if W <= 0:
            return Action.C
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            return Action.C
        recent_slice = opp_actions[-W:, :]
        p_recent = float(np.mean(recent_slice))
        L = 2 if W >= 2 else 1
        last2_slice = opp_actions[-L:, :]
        p_last2 = float(np.mean(last2_slice))
        if p_recent >= self.T or p_last2 >= self.T:
            return Action.C
        shortfall = max(0.0, self.T - p_recent)
        raw_P = 1
        if self.T > 0.0:
            raw_P = int(math.ceil(shortfall / self.T * float(self.P_max)))
        P = min(self.P_max, max(1, raw_P))
        intended_end = current_round + P - 1
        capped_end = min(intended_end, self.r - 2)
        self.punish_until = capped_end
        test_round = self.punish_until + 1
        if test_round <= self.r - 2:
            self.next_test_round = test_round
        else:
            self.next_test_round = None
        return Action.D

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    - Probes opponents for t_probe rounds using a short deterministic pattern.
    - Classifies opponents as unconditional cooperators, persistent defectors,
      or punishers (conditional retaliators) using probe data.
    - Operates in one of three modes: Exploitation, Repair (conditional cooperation),
      All-defector. Modes are re-evaluated every W rounds or when profitability drops.
    - Small deterministic/coarse-randomized cooperation schedule in Exploitation
      to avoid deterministic exploitation by perfect punishers.
    - Always defects in the final T_end rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.t_probe = min(6, max(1, math.floor(self.r / 5)))
        self.T_end = min(3, max(1, math.floor(self.r / 20)))
        self.W = min(6, max(1, self.r))
        self.p_keep_default = 0.15
        self.coop_high = 0.8
        self.coop_low = 0.2
        self.punishment_drop = 0.35
        base_pattern = [True, True, False, True, False, True]
        self.probe_pattern = base_pattern[:self.t_probe]
        self.mode = None
        self.last_evaluation_round = 0
        self.exploit_counter = 0
        self.exploit_period = None
        self.repair_noncoop_streak = 0
        self.last_mode_change_round = 0
        self.coop_rates = None
        self.responsiveness = None
        self.class_is_cooperator = None
        self.class_is_punisher = None
        self.profit_margin = 0.05
        self.group_drop_threshold = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r = self.r
        if r == 1:
            return Action.D
        if n == 1:
            return Action.C if self.k > 1.0 else Action.D
        if t >= max(0, r - self.T_end):
            return Action.D
        if t == 0 or history is None:
            return Action.C
        rounds_so_far = history.my_actions.shape[0]
        opp_count = max(0, n - 1)
        if t < self.t_probe:
            action_bool = False
            if t < len(self.probe_pattern):
                action_bool = bool(self.probe_pattern[t])
            if r <= self.t_probe and t >= r // 2:
                if action_bool:
                    action_bool = False
            return Action.C if action_bool else Action.D

        def recent_window_indices(window_size: int):
            end = rounds_so_far
            start = max(0, end - window_size)
            return (start, end)

        def compute_coop_and_resp(use_probe_only: bool=False, window_size: int=None):
            if opp_count == 0:
                return (np.zeros(0), np.zeros(0))
            if use_probe_only:
                start = 0
                end = min(rounds_so_far, self.t_probe)
            else:
                w = window_size if window_size is not None else self.W
                start, end = recent_window_indices(w)
            opp_actions_window = history.opponent_actions[start:end, :]
            if opp_actions_window.size == 0:
                coop_rates = np.zeros(opp_count)
            else:
                coop_rates = np.mean(opp_actions_window.astype(float), axis=0)
            coop_after_C_counts = np.zeros(opp_count)
            coop_after_C_total = np.zeros(opp_count)
            coop_after_D_counts = np.zeros(opp_count)
            coop_after_D_total = np.zeros(opp_count)
            my_actions_all = history.my_actions
            opp_all = history.opponent_actions
            for rr in range(max(1, start), end):
                prev_my = bool(my_actions_all[rr - 1])
                opp_actions_now = opp_all[rr, :]
                if prev_my:
                    coop_after_C_total += 1
                    coop_after_C_counts += opp_actions_now.astype(float)
                else:
                    coop_after_D_total += 1
                    coop_after_D_counts += opp_actions_now.astype(float)
            coop_after_C_prob = np.zeros(opp_count)
            coop_after_D_prob = np.zeros(opp_count)
            maskC = coop_after_C_total > 0
            maskD = coop_after_D_total > 0
            coop_after_C_prob[maskC] = coop_after_C_counts[maskC] / coop_after_C_total[maskC]
            coop_after_D_prob[maskD] = coop_after_D_counts[maskD] / coop_after_D_total[maskD]
            responsiveness = coop_after_C_prob - coop_after_D_prob
            return (coop_rates, responsiveness)
        need_eval = False
        if self.mode is None:
            need_eval = True
        elif t - self.last_evaluation_round >= self.W:
            need_eval = True
        if need_eval:
            coop_rates_probe, responsiveness_probe = compute_coop_and_resp(use_probe_only=True)
            coop_rates_recent, responsiveness_recent = compute_coop_and_resp(use_probe_only=False, window_size=self.W)
            if coop_rates_probe.size == opp_count and coop_rates_probe.size > 0:
                coop_rates = coop_rates_probe
                responsiveness = responsiveness_probe
            else:
                coop_rates = coop_rates_recent
                responsiveness = responsiveness_recent
            self.coop_rates = coop_rates
            self.responsiveness = responsiveness
            self.class_is_cooperator = coop_rates >= self.coop_high
            self.class_is_defector = coop_rates <= self.coop_low
            self.class_is_punisher = responsiveness >= self.punishment_drop
            count_coop = float(np.sum(self.class_is_cooperator)) if opp_count > 0 else 0.0
            count_punish = float(np.sum(self.class_is_punisher)) if opp_count > 0 else 0.0
            fraction_cooperators = count_coop / opp_count if opp_count > 0 else 0.0
            fraction_punishers = count_punish / opp_count if opp_count > 0 else 0.0
            coop_rates_long, _ = compute_coop_and_resp(use_probe_only=False, window_size=max(self.W, self.t_probe))
            fraction_low = 0.0
            if coop_rates_long.size > 0:
                fraction_low = float(np.sum(coop_rates_long <= self.coop_low)) / coop_rates_long.size
            prev_mode = self.mode
            if fraction_low >= 0.85:
                self.mode = 'all_defect'
            elif fraction_cooperators >= 0.35 and fraction_punishers < 0.3:
                self.mode = 'exploit'
            elif fraction_punishers >= 0.3:
                self.mode = 'repair'
            elif fraction_cooperators >= 0.2:
                self.mode = 'exploit'
            else:
                self.mode = 'repair'
            if self.mode == 'exploit':
                frac_coop = fraction_cooperators
                p_keep = min(self.p_keep_default, 0.6 * frac_coop)
                if p_keep <= 0.0:
                    self.exploit_period = max(1000, self.W * 10)
                else:
                    self.exploit_period = max(1, math.ceil(1.0 / p_keep))
                self.exploit_counter = 0
            if prev_mode != self.mode:
                self.repair_noncoop_streak = 0
                self.last_mode_change_round = t
            self.last_evaluation_round = t
        if rounds_so_far >= 1:
            w = min(self.W, rounds_so_far)
            start_idx = rounds_so_far - w
            my_payoffs_recent = history.my_payoffs[start_idx:rounds_so_far]
            opp_payoffs_recent = history.opponent_payoffs[start_idx:rounds_so_far, :] if opp_count > 0 else np.zeros((w, 0))
            my_avg = float(np.mean(my_payoffs_recent)) if my_payoffs_recent.size > 0 else 0.0
            opp_median = 0.0
            if opp_count > 0:
                opp_avg_each = np.mean(opp_payoffs_recent, axis=0)
                opp_median = float(np.median(opp_avg_each))
            if opp_median > 0.0 and my_avg < opp_median * (1.0 - self.profit_margin):
                self.mode = 'repair'
                self.repair_noncoop_streak = 0
                self.exploit_period = None
        last_round_idx = rounds_so_far - 1
        last_others_contrib = 0
        if opp_count > 0 and last_round_idx >= 0:
            last_others_contrib = int(np.sum(history.opponent_actions[last_round_idx, :].astype(int)))
        tau = math.ceil((n - 1) * 0.5)
        probe_group_avg = 0.0
        if rounds_so_far >= 1:
            probe_end = min(self.t_probe, rounds_so_far)
            if probe_end > 0:
                probe_opp_actions = history.opponent_actions[0:probe_end, :] if opp_count > 0 else np.zeros((probe_end, 0))
                if probe_opp_actions.size > 0:
                    probe_group_avg = float(np.mean(np.sum(probe_opp_actions.astype(int), axis=1)))
        recent_group_avg = 0.0
        w2 = min(self.W, rounds_so_far)
        if w2 > 0 and opp_count > 0:
            recent_opp_actions = history.opponent_actions[rounds_so_far - w2:rounds_so_far, :]
            recent_group_avg = float(np.mean(np.sum(recent_opp_actions.astype(int), axis=1)))
        if probe_group_avg > 0.0:
            drop = (probe_group_avg - recent_group_avg) / probe_group_avg
            if drop >= self.group_drop_threshold:
                self.mode = 'repair'
        if self.mode == 'all_defect':
            return Action.D
        if self.mode == 'exploit':
            frac_cooperators = 0.0
            if opp_count > 0 and self.coop_rates is not None:
                frac_cooperators = float(np.sum(self.class_is_cooperator)) / opp_count
            p_keep = min(self.p_keep_default, 0.6 * frac_cooperators)
            if p_keep <= 0.0:
                return Action.D
            period = self.exploit_period if self.exploit_period is not None else max(1, math.ceil(1.0 / p_keep))
            if self.class_is_punisher is not None and np.any(self.class_is_punisher):
                period = max(period, 2 * period)
            decision = Action.D
            if self.exploit_counter % period == 0:
                decision = Action.C
            self.exploit_counter += 1
            return decision
        if self.mode == 'repair':
            if last_others_contrib >= tau:
                self.repair_noncoop_streak = 0
                return Action.C
            else:
                self.repair_noncoop_streak += 1
                if self.repair_noncoop_streak <= 2:
                    return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    Selective Exploitative Conditional Cooperator (SECC)

    Starts with a short cooperative probe to learn opponents' tendencies, then defaults
    to defection to harvest contributions from identified cooperators. Uses occasional
    targeted cooperation to maintain or repair a contributor pool, and switches to
    safe defection if broad retaliation is detected or in the endgame. Adds small
    randomized cooperation to avoid deterministic exploitation traps.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.P = max(1, min(6, math.floor(self.r / 8)))
        self.theta_high = 0.7
        self.theta_low = 0.3
        self.collapse_window = 3
        base_repair_len = min(4, max(1, math.floor(self.r / 20)))
        self.endgame_len = min(5, max(1, math.floor(self.r / 10)))
        self.epsilon = 0.05
        if self.k / max(1.0, float(self.n)) < 0.25:
            self.epsilon = 0.02
            self.repair_len = max(1, math.floor(base_repair_len / 2))
        else:
            self.repair_len = base_repair_len
        self.persistent_defect = False
        self.repair_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r = self.r
        if t >= r - self.endgame_len:
            return Action.D
        if t == 0 or history is None:
            if self.P >= 1 and t < self.P:
                return Action.C
            return Action.D
        my_actions = np.array(history.my_actions[:t], dtype=np.bool_)
        opp_actions = np.array(history.opponent_actions[:t, :], dtype=np.bool_)
        rounds_observed = my_actions.shape[0]
        try:
            opp_sums = np.sum(opp_actions.astype(int), axis=1)
        except Exception:
            opp_sums = np.zeros(rounds_observed, dtype=int)
        my_sums = my_actions.astype(int)
        total_contribs = opp_sums + my_sums
        last_total_contrib = int(total_contribs[-1]) if rounds_observed >= 1 else 0
        window = min(self.collapse_window, rounds_observed)
        if window >= 1:
            recent_avg_contrib = float(np.mean(total_contribs[-window:]))
        else:
            recent_avg_contrib = float(np.mean(total_contribs)) if rounds_observed > 0 else 0.0
        full_history_avg = float(np.mean(total_contribs)) if rounds_observed > 0 else 0.0
        opponent_count = opp_actions.shape[1] if opp_actions.ndim == 2 else 0
        f_js = np.zeros(opponent_count, dtype=float)
        if rounds_observed > 0 and opponent_count > 0:
            f_js = np.sum(opp_actions.astype(int), axis=0) / float(rounds_observed)
        num_coop = 0
        num_cond = 0
        num_def = 0
        for f in f_js:
            if f >= self.theta_high:
                num_coop += 1
            elif f <= self.theta_low:
                num_def += 1
            else:
                num_cond += 1
        if t >= self.P:
            if opponent_count > 0 and np.all(f_js <= self.theta_low):
                self.persistent_defect = True
        if self.persistent_defect:
            return Action.D
        if rounds_observed >= 2:
            last_my_action = bool(my_actions[-1])
            if not last_my_action:
                prev_round = opp_actions[-2, :]
                last_round = opp_actions[-1, :]
                decreases = np.sum(prev_round.astype(int) - last_round.astype(int) > 0)
                threshold = max(0, math.ceil(n / 2) - 1)
                if decreases >= threshold:
                    self.persistent_defect = True
                    return Action.D
        sustained_collapse = False
        if rounds_observed >= self.collapse_window:
            last_window = total_contribs[-self.collapse_window:]
            threshold_val = max(1.0, full_history_avg / 2.0)
            if np.all(last_window <= threshold_val):
                sustained_collapse = True
        if sustained_collapse and self.repair_counter <= 0:
            self.repair_counter = self.repair_len
        if self.repair_counter > 0:
            self.repair_counter -= 1
            return Action.C
        sudden_collapse = False
        if rounds_observed >= 1:
            if recent_avg_contrib + 1e-09 < full_history_avg - 1.0:
                sudden_collapse = True
            elif last_total_contrib < max(1, recent_avg_contrib - 1.0):
                sudden_collapse = True
        if sudden_collapse:
            if rounds_observed >= 1 and (not bool(my_actions[-1])):
                return Action.C
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        if num_coop >= 1:
            if recent_avg_contrib >= (num_coop + num_cond) * 0.5:
                return Action.D
        if num_coop <= 1 and recent_avg_contrib < n * 0.5:
            if random.random() < self.epsilon:
                return Action.C
            return Action.D
        if num_coop >= 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR)

    Starts cooperative (except single-round games). Monitors recent window of rounds to
    compute an adaptive cooperation threshold tau. If group cooperation falls below tau the
    strategy imposes a proportionate temporary punishment phase. It also performs targeted
    retaliation against a small number of isolated defectors, and uses a short probationary
    cooperative probe after punishments to forgive restored cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.w = min(5, r)
        raw_tau = 0.5 + 0.5 * (k / max(1.0, n))
        self.tau = min(0.95, max(0.6, raw_tau))
        self.P_max = min(5, max(1, math.floor(r / 4)))
        self.Q = 1
        self.phase = 'COOPERATE'
        self.punishment_remaining = 0
        self.probation_remaining = 0
        self.targeted = set()
        self._punish_recovery_count = 0
        self.n = n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            if self.game_description.n_rounds == 1:
                return Action.D
            self.phase = 'COOPERATE'
            self.punishment_remaining = 0
            self.probation_remaining = 0
            self.targeted.clear()
            self._punish_recovery_count = 0
            return Action.C
        rounds_observed = history.my_actions.shape[0]
        effective_w = min(self.w, rounds_observed)
        if effective_w <= 0:
            if self.game_description.n_rounds == 1:
                return Action.D
            return Action.C
        my_recent = history.my_actions[-effective_w:]
        opp_recent = history.opponent_actions[-effective_w:, :]
        opp_rates = np.mean(opp_recent.astype(float), axis=0) if opp_recent.size else np.array([])
        my_rate = float(np.mean(my_recent.astype(float)))
        sum_opponents = float(np.sum(opp_recent.astype(float)))
        sum_my = float(np.sum(my_recent.astype(float)))
        total_contributions = sum_opponents + sum_my
        rG = total_contributions / (effective_w * max(1, self.n))
        last_round_index = -1
        last_my = bool(history.my_actions[last_round_index])
        last_opps = history.opponent_actions[last_round_index, :] if history.opponent_actions.size else np.array([], dtype=bool)
        last_total_coops = int(last_my) + int(np.sum(last_opps.astype(int)))
        majority_cooperated = last_total_coops > self.n / 2.0
        recent_targeted = set()
        if majority_cooperated and last_opps.size:
            for j in range(len(last_opps)):
                defected_last = not bool(last_opps[j])
                r_i = float(opp_rates[j]) if opp_rates.size else 0.0
                if defected_last and r_i < 1.0:
                    recent_targeted.add(j)
        final_round_index = self.game_description.n_rounds - 1
        if state.round_number == final_round_index:
            if rounds_observed >= 1:
                everyone_cooperated_last = last_total_coops == self.n
                if everyone_cooperated_last and rG >= self.tau:
                    return Action.C
            return Action.D

        def _opponent_forgiven(j: int) -> bool:
            if effective_w >= 2:
                last_two = history.opponent_actions[-min(2, effective_w):, j]
                return bool(np.all(last_two.astype(bool)))
            else:
                return bool(np.all(history.opponent_actions[-effective_w:, j].astype(bool)))
        to_remove = set()
        for j in list(self.targeted):
            if j < 0 or j >= history.opponent_actions.shape[1]:
                to_remove.add(j)
            elif _opponent_forgiven(j):
                to_remove.add(j)
        for j in to_remove:
            self.targeted.discard(j)
        if self.phase == 'COOPERATE' and recent_targeted:
            if 1 <= len(recent_targeted) <= 2:
                for j in recent_targeted:
                    self.targeted.add(j)
        if self.phase == 'COOPERATE':
            if rG < self.tau:
                shortfall = max(0.0, self.tau - rG)
                P = max(1, math.ceil(self.P_max * (shortfall / max(1e-09, self.tau))))
                self.punishment_remaining = P
                self.phase = 'PUNISH'
                self._punish_recovery_count = 0
                self.targeted.clear()
                action = Action.D
                self.punishment_remaining -= 1
                if self.punishment_remaining <= 0:
                    self.phase = 'PROBATION'
                    self.probation_remaining = self.Q
                return action
            if len(self.targeted) > 0:
                return Action.D
            return Action.C
        if self.phase == 'PUNISH':
            action = Action.D
            if rG >= self.tau:
                self._punish_recovery_count += 1
            else:
                self._punish_recovery_count = 0
            if self._punish_recovery_count >= 2:
                self.punishment_remaining = 0
                self.phase = 'PROBATION'
                self.probation_remaining = self.Q
                return action
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            if self.punishment_remaining <= 0:
                self.phase = 'PROBATION'
                self.probation_remaining = self.Q
            return action
        if self.phase == 'PROBATION':
            action = Action.C
            self.probation_remaining = max(0, self.probation_remaining - 1)
            if self.probation_remaining <= 0:
                if rG >= self.tau:
                    self.phase = 'COOPERATE'
                    self.targeted.clear()
                    self._punish_recovery_count = 0
                else:
                    shortfall = max(0.0, self.tau - rG)
                    P = max(1, math.ceil(self.P_max * (shortfall / max(1e-09, self.tau))))
                    self.punishment_remaining = P
                    self.phase = 'PUNISH'
                    self._punish_recovery_count = 0
            return action
        return Action.C

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    Adaptive Exploiter (targeted free-riding with limited retaliation)

    Exploits unconditional cooperators, free-rides reciprocators most of the time
    while giving occasional cooperation to avoid permanent punishment, and uses
    short targeted punishments (capped) against players who appear to be
    retaliating. Always defects in the final RG rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.n = int(self.game_description.n_players)
        self.k = float(self.game_description.k)
        self.W = max(3, math.floor(r / 6))
        self.E = min(5, max(1, math.floor(r / 10)))
        self.RG = min(2, math.floor(r / 10))
        self.P = min(3, r)
        self.high_rate = 0.9
        self.low_rate = 0.1
        self.responsive_threshold = 0.6
        self.keep_coop_prob = 0.2
        self.explore_coop_prob = 0.7
        self.reciprocator_defect_prob = 0.8
        self.punishing_timers: dict[int, int] = {}
        self._last_round_seen: int | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_index = int(state.round_number)
        r = int(self.game_description.n_rounds)
        if t_index == 0 or history is None:
            self._last_round_seen = 0
            if random.random() < self.explore_coop_prob:
                return Action.C
            return Action.D
        rounds_available = history.opponent_actions.shape[0]
        if self._last_round_seen is None:
            self._last_round_seen = t_index
        else:
            rounds_passed = max(0, t_index - self._last_round_seen)
            if rounds_passed > 0 and self.punishing_timers:
                for opp in list(self.punishing_timers.keys()):
                    self.punishing_timers[opp] = max(0, self.punishing_timers.get(opp, 0) - rounds_passed)
                    if self.punishing_timers.get(opp, 0) <= 0:
                        self.punishing_timers.pop(opp, None)
            self._last_round_seen = t_index
        if self.RG >= 1 and t_index >= r - self.RG:
            self.punishing_timers.clear()
            return Action.D
        if t_index < self.E:
            if any((rem > 0 for rem in self.punishing_timers.values())):
                for opp in list(self.punishing_timers.keys()):
                    self.punishing_timers[opp] = max(0, self.punishing_timers.get(opp, 0) - 1)
                    if self.punishing_timers.get(opp, 0) <= 0:
                        self.punishing_timers.pop(opp, None)
                return Action.D
            if random.random() < self.explore_coop_prob:
                return Action.C
            return Action.D
        window_len = min(self.W, rounds_available)
        start_idx = max(0, rounds_available - window_len)
        opp_actions_window = history.opponent_actions[start_idx:rounds_available, :]
        if window_len <= 0 or opp_actions_window.size == 0:
            return Action.D
        coop_rates = np.mean(opp_actions_window.astype(float), axis=0) if opp_actions_window.size else np.zeros(self.n - 1)
        pair_start = start_idx + 1
        pair_count = max(0, rounds_available - pair_start)
        if pair_count > 0:
            my_lagged = history.my_actions[pair_start - 1:rounds_available - 1]
            opp_current = history.opponent_actions[pair_start:rounds_available, :]
            my_matrix = np.tile(my_lagged.reshape(pair_count, 1), (1, opp_current.shape[1]))
            responsiveness = np.sum((opp_current == my_matrix).astype(float), axis=0) / float(pair_count)
        else:
            responsiveness = np.zeros(self.n - 1)
        last_actions = history.opponent_actions[-1, :]
        recent_group_contrib = float(np.mean(opp_actions_window.astype(float))) if opp_actions_window.size else 0.0
        unconditional_cooperators = set()
        reciprocators = set()
        defectors = set()
        uncertain = set()
        for j in range(self.n - 1):
            cr = float(coop_rates[j])
            resp = float(responsiveness[j])
            if cr >= self.high_rate and resp <= self.responsive_threshold - 0.05:
                unconditional_cooperators.add(j)
            elif resp >= self.responsive_threshold:
                reciprocators.add(j)
            elif cr <= self.low_rate:
                defectors.add(j)
            else:
                uncertain.add(j)
        if rounds_available >= 2:
            my_last = bool(history.my_actions[-1])
            if not my_last:
                prev_opp_actions = history.opponent_actions[-2, :]
                last_opp_actions = history.opponent_actions[-1, :]
                for j in range(self.n - 1):
                    if not last_opp_actions[j] and prev_opp_actions[j]:
                        self.punishing_timers[j] = self.P
        if self.punishing_timers:
            for j in list(self.punishing_timers.keys()):
                if last_actions[j]:
                    self.punishing_timers.pop(j, None)
        if any((rem > 0 for rem in self.punishing_timers.values())):
            for opp in list(self.punishing_timers.keys()):
                self.punishing_timers[opp] = max(0, self.punishing_timers.get(opp, 0) - 1)
                if self.punishing_timers.get(opp, 0) <= 0:
                    self.punishing_timers.pop(opp, None)
            return Action.D
        if len(unconditional_cooperators) >= 1 and len(reciprocators) == 0:
            return Action.D
        if len(defectors) / max(1, self.n - 1) >= 0.5:
            return Action.D
        if len(reciprocators) >= 1:
            recent_others_coop = recent_group_contrib
            if recent_others_coop >= 0.6:
                if random.random() < self.reciprocator_defect_prob:
                    return Action.D
                return Action.C
            else:
                return Action.D
        if len(unconditional_cooperators) >= 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    Adaptive Exploiter (AE) for repeated Public Goods Game.

    - Probes opponents early, measures influence of own cooperation (delta) and
      which opponents punish, and then exploits cooperators while avoiding
      sustained punishment. Always defects in the final rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.P_drop = 0.25
        self.T_low = 0.2
        self.eps = 0.02
        self.p_min = 0.02
        self.p_max = 0.8
        pr = math.floor(self.r / 10)
        self.ProbeRounds = min(3, max(1, pr))
        self.EndgameRounds = min(3, max(1, pr))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        n = self.n
        num_opponents = max(1, n - 1)
        if state.round_number == 0 or history is None:
            return Action.C
        R = int(history.my_actions.shape[0])
        if t > r - self.EndgameRounds or t == r:
            return Action.D
        if t <= self.ProbeRounds:
            return Action.C
        if R <= 0:
            coop_rates = np.zeros((num_opponents,), dtype=float)
        else:
            coop_rates = np.array(history.opponent_actions.sum(axis=0), dtype=float) / float(R)
        if coop_rates.size == 0:
            G = 0.0
        else:
            G = float(np.mean(coop_rates))
        W = min(10, R)
        if W <= 0:
            delta = 0.0
            punish_flags = [False] * num_opponents
            PunishFraction = 0.0
        else:
            start_idx = R - W
            recent_idx = list(range(start_idx, R))
            my_recent = np.array(history.my_actions[recent_idx], dtype=bool)
            opp_recent = np.array(history.opponent_actions[recent_idx, :], dtype=float)
            if opp_recent.size == 0:
                avg_others_per_round = np.zeros((len(recent_idx),), dtype=float)
            else:
                avg_others_per_round = opp_recent.mean(axis=1)
            if my_recent.any():
                avg_others_when_I_C = float(avg_others_per_round[my_recent].mean())
            else:
                avg_others_when_I_C = 0.0
            if (~my_recent).any():
                avg_others_when_I_D = float(avg_others_per_round[~my_recent].mean())
            else:
                avg_others_when_I_D = 0.0
            delta = avg_others_when_I_C - avg_others_when_I_D
            if my_recent.sum() == 0 or (~my_recent).sum() == 0:
                delta = 0.0
            punish_flags = [False] * num_opponents
            if R >= 2:
                max_check_start = max(start_idx, 0)
                transition_indices = list(range(max_check_start, R - 1))
                if len(transition_indices) > 0:
                    coop_after_coop = [list() for _ in range(num_opponents)]
                    coop_after_defect = [list() for _ in range(num_opponents)]
                    for i in transition_indices:
                        my_act = bool(history.my_actions[i])
                        next_row = np.array(history.opponent_actions[i + 1, :], dtype=float)
                        for j in range(num_opponents):
                            val = float(next_row[j])
                            if my_act:
                                coop_after_coop[j].append(val)
                            else:
                                coop_after_defect[j].append(val)
                    for j in range(num_opponents):
                        if len(coop_after_coop[j]) >= 1 and len(coop_after_defect[j]) >= 1:
                            avg_after_coop = float(sum(coop_after_coop[j]) / len(coop_after_coop[j]))
                            avg_after_defect = float(sum(coop_after_defect[j]) / len(coop_after_defect[j]))
                            if avg_after_coop - avg_after_defect > self.P_drop:
                                punish_flags[j] = True
                        else:
                            punish_flags[j] = False
                else:
                    punish_flags = [False] * num_opponents
            else:
                punish_flags = [False] * num_opponents
            PunishFraction = float(sum((1 for x in punish_flags if x)) / float(num_opponents))
        if G < self.T_low:
            return Action.D
        if PunishFraction >= 0.4:
            p = 0.5 * G + 0.2
            p = max(self.p_min, min(p, 0.9))
            if p >= 1.0:
                p = 1.0 - 1e-06
            return Action.C if random.random() < p else Action.D
        if delta <= self.eps:
            p = max(self.p_min, 0.15 * G + 0.05)
            p = max(self.p_min, min(p, 0.4))
        else:
            norm_delta = float(min(max(delta / 0.5, 0.0), 1.0))
            p = self.p_min + 0.6 * norm_delta
            p = max(self.p_min, min(p, self.p_max))
        try:
            highly_coop_indices = [i for i, rate in enumerate(coop_rates.tolist()) if rate > 0.9]
        except Exception:
            highly_coop_indices = []
        if len(highly_coop_indices) > 0:
            exploitable_found = False
            for j in highly_coop_indices:
                if j < len(punish_flags) and (not punish_flags[j]):
                    exploitable_found = True
                    break
            if exploitable_found:
                p = min(p, 0.2)
                p = max(self.p_min, p)
        if R >= 2:
            prev_W = min(10, max(0, R - W))
            if prev_W >= 1:
                prev_start = max(0, R - W - prev_W)
                prev_end = prev_start + prev_W
                if prev_end > prev_start:
                    prev_idx = list(range(prev_start, prev_end))
                    if len(prev_idx) > 0:
                        prev_opp = np.array(history.opponent_actions[prev_idx, :], dtype=float)
                        if prev_opp.size > 0:
                            G_prev = float(prev_opp.mean())
                            if G_prev - G > 0.3:
                                recent_my = np.array(history.my_actions[R - W:R], dtype=bool)
                                prev_my = np.array(history.my_actions[max(0, R - W - prev_W):R - W], dtype=bool)
                                if recent_my.sum() < W and prev_my.size > 0:
                                    prev_coop_rate = float(prev_my.mean()) if prev_my.size > 0 else 0.0
                                    recent_coop_rate = float(recent_my.mean()) if recent_my.size > 0 else 0.0
                                    if recent_coop_rate < prev_coop_rate:
                                        p = min(self.p_max, p + 0.2)
        p = max(self.p_min, min(p, self.p_max))
        if p >= 1.0:
            p = 1.0 - 1e-06
        choose_C = random.random() < p
        return Action.C if choose_C else Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    Adaptive Exploiter for an N-player Public Goods Game.

    Key ideas:
    - Probe by defecting initially.
    - Detect and always defect against unconditional cooperators ("suckers").
    - Rarely cooperate to maintain cooperation when most others are cooperating and punishers are few.
    - Respect punishers (reciprocators) to avoid long costly punishment cycles.
    - Short rehabilitations (T_rehab rounds) when my defections appear to have caused cooperation collapse.
    - Always defect in the endgame horizon.
    - Smooth statistics over a recent window W and avoid flip-flopping classifications unless changes are substantial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.p_uc_default = 0.95
        self.Delta = 0.25
        self.p_coop_small = 0.15
        self.T_rehab = 2
        self.H_end = min(3, max(0, self.r - 1))
        self.reclass_threshold = 0.2
        self.prev_p = None
        self.prev_r = None
        self.classification = None
        self.rehab_end_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        rounds_total = self.r
        current_round = state.round_number
        if current_round >= rounds_total - self.H_end:
            return Action.D
        if current_round == 0 or history is None:
            return Action.D
        num_opponents = history.opponent_actions.shape[1]
        W = min(10, current_round)
        if current_round < 5 or W < 3:
            p_uc = 1.0
        else:
            p_uc = self.p_uc_default
        start_idx = max(0, current_round - W)
        recent_rounds = slice(start_idx, current_round)
        if start_idx < current_round:
            opp_window = history.opponent_actions[recent_rounds, :]
            my_window = history.my_actions[recent_rounds]
            per_round_opp_sums = np.sum(opp_window.astype(np.int32), axis=1)
            per_round_my = my_window.astype(np.int32)
            group_counts_window = per_round_opp_sums + per_round_my
            if group_counts_window.size > 0:
                S_avg = float(np.mean(group_counts_window))
            else:
                S_avg = 0.0
        else:
            group_counts_window = np.array([], dtype=np.int32)
            S_avg = 0.0
        last_opp = history.opponent_actions[-1, :]
        my_last = bool(history.my_actions[-1])
        S_prev = int(np.sum(last_opp.astype(np.int32)) + (1 if my_last else 0))
        p_j = np.zeros(num_opponents, dtype=float)
        r_j = np.zeros(num_opponents, dtype=float)
        if W > 0:
            opp_window_int = history.opponent_actions[recent_rounds, :].astype(np.int32)
            p_j = np.mean(opp_window_int, axis=0)
            if current_round - start_idx >= 2:
                prev_slice_start = start_idx + 1
                prev_slice = slice(prev_slice_start, current_round)
                my_prev = history.my_actions[prev_slice_start - 1:current_round - 1]
                coop_after_my_coop = np.zeros(num_opponents, dtype=int)
                tot_after_my_coop = np.zeros(num_opponents, dtype=int)
                coop_after_my_def = np.zeros(num_opponents, dtype=int)
                tot_after_my_def = np.zeros(num_opponents, dtype=int)
                for rel_idx, round_idx in enumerate(range(prev_slice_start, current_round)):
                    prev_my = bool(history.my_actions[round_idx - 1])
                    opp_actions_at_t = history.opponent_actions[round_idx, :].astype(np.int32)
                    if prev_my:
                        coop_after_my_coop += opp_actions_at_t
                        tot_after_my_coop += 1
                    else:
                        coop_after_my_def += opp_actions_at_t
                        tot_after_my_def += 1
                for j in range(num_opponents):
                    p_coop_if_my_coop = coop_after_my_coop[j] / tot_after_my_coop[j] if tot_after_my_coop[j] > 0 else None
                    p_coop_if_my_def = coop_after_my_def[j] / tot_after_my_def[j] if tot_after_my_def[j] > 0 else None
                    if p_coop_if_my_coop is None or p_coop_if_my_def is None:
                        r_j[j] = 0.0
                    else:
                        r_j[j] = float(p_coop_if_my_coop - p_coop_if_my_def)
            else:
                r_j = np.zeros(num_opponents, dtype=float)
        else:
            p_j = np.zeros(num_opponents, dtype=float)
            r_j = np.zeros(num_opponents, dtype=float)
        new_class = ['unknown'] * num_opponents
        for j in range(num_opponents):
            pj = float(p_j[j])
            rj = float(r_j[j])
            if pj >= p_uc and rj >= -0.05:
                new_class[j] = 'sucker'
            elif rj <= -self.Delta:
                new_class[j] = 'punisher'
            elif pj >= 0.6 and -self.Delta < rj <= -0.05:
                new_class[j] = 'conditional'
            elif pj <= 0.2:
                new_class[j] = 'defector'
            else:
                new_class[j] = 'other'
        if self.prev_p is None or self.prev_r is None or self.classification is None:
            self.prev_p = p_j.copy()
            self.prev_r = r_j.copy()
            self.classification = new_class.copy()
        else:
            for j in range(num_opponents):
                if abs(p_j[j] - self.prev_p[j]) >= self.reclass_threshold or abs(r_j[j] - self.prev_r[j]) >= self.reclass_threshold:
                    self.classification[j] = new_class[j]
                    self.prev_p[j] = p_j[j]
                    self.prev_r[j] = r_j[j]
                else:
                    self.prev_p[j] = 0.7 * self.prev_p[j] + 0.3 * p_j[j]
                    self.prev_r[j] = 0.7 * self.prev_r[j] + 0.3 * r_j[j]
        count_suckers = sum((1 for c in self.classification if c == 'sucker'))
        count_punishers = sum((1 for c in self.classification if c == 'punisher'))
        count_conditional = sum((1 for c in self.classification if c == 'conditional'))
        average_p_others = float(np.mean(self.prev_p)) if num_opponents > 0 else 0.0
        M = math.ceil(n / 2)
        if average_p_others <= 0.2:
            return Action.D
        if count_suckers >= 1:
            return Action.D
        if current_round < self.rehab_end_round:
            return Action.C
        many_punishers_bar = math.ceil((n - 1) / 2)
        lower_bar_threshold = M - 1 if count_punishers >= many_punishers_bar else M
        if S_prev >= M:
            punishers_many_threshold = math.ceil((n - 1) / 3)
            if count_punishers >= punishers_many_threshold:
                my_cooperated_last = bool(history.my_actions[-1])
                helped_restore = False
                if start_idx < current_round:
                    if S_prev >= S_avg + 1.0:
                        helped_restore = True
                punisher_positive_resp = False
                for j in range(num_opponents):
                    if self.classification[j] == 'punisher' and self.prev_r[j] >= 0.05:
                        punisher_positive_resp = True
                        break
                if helped_restore or (my_cooperated_last and punisher_positive_resp):
                    return Action.C
                else:
                    return Action.D
            else:
                if random.random() < self.p_coop_small:
                    return Action.C
                return Action.D
        else:
            can_rehab = False
            if W >= 4:
                window_my = history.my_actions[recent_rounds].astype(np.int32)
                window_group = group_counts_window
                def_indices = np.where(window_my == 0)[0]
                if def_indices.size > 0:
                    first_def_idx = int(def_indices[0])
                    if first_def_idx > 0:
                        avg_before = float(np.mean(window_group[:first_def_idx])) / float(n)
                        avg_after = float(np.mean(window_group[first_def_idx:])) / float(n)
                        drop = max(0.0, avg_before - avg_after)
                        cond_threshold = math.ceil((n - 1) / 3)
                        if drop > 0.4 and count_conditional >= cond_threshold:
                            if current_round <= rounds_total - self.H_end - 2:
                                can_rehab = True
            if can_rehab:
                self.rehab_end_round = current_round + self.T_rehab
                return Action.C
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    Collective Conditional Cooperation with Forgiveness (CCCF)

    - Cooperate first round.
    - Use a short window m (<=4) to estimate recent cooperation of others.
    - Cooperate when recent cooperation among others >= threshold tau (usually 0.5,
      relaxed to 0.4 if k >= 0.8*n).
    - When exploitation is detected (c_bar < tau), trigger a finite punishment of
      length P0 + Escale (bounded by Pmax). Escale increases on repeated triggers up
      to Escale_max = Pmax - P0.
    - After a punishment episode finishes, perform a one-shot repair-test (cooperate),
      then observe the following m rounds. If cooperation recovers (c_bar >= tau),
      reset Escale to 0. Otherwise escalate punishment and repeat.
    - In the final Tend rounds require strong recent cooperation (0.9) to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(4, self.r)
        self.P0 = 2
        self.Pmax = 4
        self.Escale_max = max(0, self.Pmax - self.P0)
        if self.k >= 0.8 * self.n:
            self.tau = 0.4
        else:
            self.tau = 0.5
        self.Tend = min(3, self.r)
        self.Pcur = 0
        self.Escale = 0
        self.awaiting_repair_test = False
        self.repair_observing = False
        self.repair_test_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def compute_c_bar_from_history(last_rows: slice) -> float:
            if history is None:
                return 0.0
            opps = history.opponent_actions
            if opps.size == 0:
                return 0.0
            window = opps[last_rows, :]
            if window.size == 0:
                return 0.0
            return float(np.mean(window))

        def ge_with_tiebreak(x: float, thresh: float) -> bool:
            eps = 1e-12
            if x > thresh + eps:
                return True
            if x < thresh - eps:
                return False
            return random.random() < 0.5
        t = int(state.round_number)
        if t == 0 or history is None:
            return Action.C
        in_endgame = t >= max(0, self.r - self.Tend)
        available_rounds = history.opponent_actions.shape[0]
        last_k = min(self.m, available_rounds)
        if last_k <= 0:
            c_bar = 0.0
        else:
            c_bar = compute_c_bar_from_history(slice(available_rounds - last_k, available_rounds))
        if in_endgame:
            if ge_with_tiebreak(c_bar, 0.9):
                return Action.C
            else:
                return Action.D
        if self.Pcur > 0:
            self.Pcur -= 1
            if self.Pcur == 0:
                self.awaiting_repair_test = True
            return Action.D
        if self.awaiting_repair_test:
            self.awaiting_repair_test = False
            self.repair_observing = True
            self.repair_test_round = t
            return Action.C
        if self.repair_observing and self.repair_test_round is not None:
            start_obs = self.repair_test_round + 1
            end_obs_exclusive = start_obs + self.m
            if available_rounds >= end_obs_exclusive:
                obs_c_bar = compute_c_bar_from_history(slice(start_obs, end_obs_exclusive))
                if ge_with_tiebreak(obs_c_bar, self.tau):
                    self.Escale = 0
                    self.repair_observing = False
                    self.repair_test_round = None
                    available_rounds = history.opponent_actions.shape[0]
                    last_k = min(self.m, available_rounds)
                    c_bar = compute_c_bar_from_history(slice(available_rounds - last_k, available_rounds)) if last_k > 0 else 0.0
                else:
                    if self.Escale < self.Escale_max:
                        self.Escale += 1
                    new_P = min(self.P0 + self.Escale, self.Pmax)
                    self.Pcur = new_P - 1
                    self.repair_observing = False
                    self.repair_test_round = None
                    if self.Pcur == 0:
                        self.awaiting_repair_test = True
                    return Action.D
            else:
                return Action.C
        if ge_with_tiebreak(c_bar, self.tau):
            return Action.C
        new_P = min(self.P0 + self.Escale, self.Pmax)
        if new_P <= 0:
            return Action.D
        self.Pcur = new_P - 1
        if self.Escale < self.Escale_max:
            self.Escale += 1
        if self.Pcur == 0:
            self.awaiting_repair_test = True
        return Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    Adaptive conditional-exploitation strategy for the N-player Public Goods Game.

    - Tracks, from public history, each opponent's conditional cooperation probability
      following my last action (C or D) as well as unconditional cooperation rates.
    - In each round chooses the immediate-action (C or D) that maximizes expected
      immediate payoff using empirical conditional predictions, with a small decaying
      exploration probability to probe opponents.
    - Identifies and exploits apparent unconditional cooperators when the group is
      non-retaliatory.
    - If the group retaliates to my defections (they reduce cooperation after I defect),
      the strategy prefers cooperation when the short-term gain from defecting is
      small relative to the expected long-run loss from eliciting retaliation.
    - Always defects in the final H_end rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.delta = 0.06
        self.H_end = 2
        self.epsilon_cap = 0.15
        self.tol = 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        round_index = state.round_number
        t = round_index + 1
        if round_index >= max(0, r - self.H_end):
            return Action.D
        if round_index == 0 or history is None:
            return Action.C
        T = int(history.my_actions.shape[0])
        if T == 0:
            return Action.C
        n_opponents = int(history.opponent_actions.shape[1])
        my_actions = np.asarray(history.my_actions, dtype=bool)
        N_C = int(my_actions.sum())
        N_D = T - N_C
        p_js = [0.0] * n_opponents
        p_j_given_C = [None] * n_opponents
        p_j_given_D = [None] * n_opponents
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        for j in range(n_opponents):
            opp_j = opp_actions[:, j]
            total_coop_j = int(opp_j.sum())
            p_j = float(total_coop_j) / float(T) if T > 0 else 0.0
            p_js[j] = p_j
            if N_C > 0:
                coop_after_C = int(opp_j[my_actions].sum())
                p_j_given_C[j] = float(coop_after_C) / float(N_C)
            else:
                p_j_given_C[j] = None
            if N_D > 0:
                coop_after_D = int(opp_j[~my_actions].sum())
                p_j_given_D[j] = float(coop_after_D) / float(N_D)
            else:
                p_j_given_D[j] = None
        p_group_list_C = [p_j_given_C[j] if p_j_given_C[j] is not None else p_js[j] for j in range(n_opponents)]
        p_group_list_D = [p_j_given_D[j] if p_j_given_D[j] is not None else p_js[j] for j in range(n_opponents)]
        if n_opponents > 0:
            p_group_C = float(np.mean(np.asarray(p_group_list_C, dtype=float)))
            p_group_D = float(np.mean(np.asarray(p_group_list_D, dtype=float)))
        else:
            p_group_C = 0.0
            p_group_D = 0.0
        epsilon_t = min(self.epsilon_cap, 0.5 / math.sqrt(max(1.0, float(t))))
        min_rounds_for_uncond = max(4, int(math.ceil(r / 10))) if r > 0 else 4
        unconditional_cooperators = [j for j in range(n_opponents) if p_js[j] >= 0.95 and T >= min_rounds_for_uncond]
        non_retaliatory = p_group_D >= p_group_C - self.delta
        if len(unconditional_cooperators) > 0 and non_retaliatory:
            chosen = Action.D
            if random.random() < epsilon_t:
                chosen = Action.C
            return chosen
        S_C = 0.0
        S_D = 0.0
        for j in range(n_opponents):
            pj_c = p_j_given_C[j] if p_j_given_C[j] is not None else p_js[j]
            pj_d = p_j_given_D[j] if p_j_given_D[j] is not None else p_js[j]
            S_C += float(pj_c)
            S_D += float(pj_d)
        U_C = k / float(n) * (1.0 + S_C)
        U_D = 1.0 + k / float(n) * S_D
        if U_D > U_C + self.tol:
            chosen = Action.D
        elif U_C > U_D + self.tol:
            chosen = Action.C
        else:
            chosen = Action.D
        if random.random() < epsilon_t:
            chosen = Action.C if chosen == Action.D else Action.D
        retaliatory = p_group_D < p_group_C - self.delta
        if retaliatory and U_D > U_C:
            p_drop = max(0.0, p_group_C - p_group_D)
            remaining_rounds = max(0, r - t)
            expected_loss_per_round = p_drop * (k / float(n))
            threshold = expected_loss_per_round * float(remaining_rounds) / 2.0
            if U_D - U_C < threshold:
                chosen = Action.C
        return chosen

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    Adaptive Exploiter for the N-player Public Goods Game.

    - Starts by cooperating to solicit responses.
    - Estimates per-opponent influence of our cooperation on their future cooperation.
    - Cooperates when the expected future benefit (aggregated across opponents and remaining rounds)
      exceeds the immediate private cost of cooperating.
    - Detects credible retaliation (punishment) after defections and can temporarily cooperate to
      stop punishment if influence suggests this will help.
    - Occasionally performs rare cooperative probes after long streaks of defection to re-estimate
      influence and avoid permanent breakdown.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._tiny = 1e-09
        self._probe_prob = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        upcoming_round_1based = state.round_number + 1
        if upcoming_round_1based == r:
            return Action.D
        if upcoming_round_1based == 1 or history is None:
            return Action.C
        last_obs_idx = state.round_number - 1
        opponents = n - 1
        counts_prevC_total = np.zeros(opponents, dtype=float)
        counts_prevC_coop = np.zeros(opponents, dtype=float)
        counts_prevD_total = np.zeros(opponents, dtype=float)
        counts_prevD_coop = np.zeros(opponents, dtype=float)
        for i in range(1, state.round_number):
            prev_my_action = bool(history.my_actions[i - 1])
            opp_actions_at_i = history.opponent_actions[i, :].astype(float)
            if prev_my_action:
                counts_prevC_total += 1.0
                counts_prevC_coop += opp_actions_at_i
            else:
                counts_prevD_total += 1.0
                counts_prevD_coop += opp_actions_at_i
        P_C_given_I_C = (counts_prevC_coop + 1.0) / (counts_prevC_total + 2.0)
        P_C_given_I_D = (counts_prevD_coop + 1.0) / (counts_prevD_total + 2.0)
        influence_per_opponent = P_C_given_I_C - P_C_given_I_D
        if opponents > 0:
            Delta = float(np.mean(influence_per_opponent))
        else:
            Delta = 0.0
        remaining_rounds = r - upcoming_round_1based
        immediate_cost = 1.0 - k / n
        denom = opponents * (k / n) * remaining_rounds + self._tiny
        T = immediate_cost / denom
        retaliation_detected = False
        my_actions = history.my_actions
        last_defect_idx = None
        for idx in range(last_obs_idx, -1, -1):
            if not bool(my_actions[idx]):
                last_defect_idx = idx
                break
        if last_defect_idx is not None:
            post_start = last_defect_idx + 1
            if post_start <= last_obs_idx:
                post_end = min(last_defect_idx + 2, last_obs_idx)
                post_block = history.opponent_actions[post_start:post_end + 1, :]
                if post_block.size > 0:
                    post_mean = float(post_block.mean())
                    pre_end = last_defect_idx - 1
                    pre_start = max(0, last_defect_idx - 2)
                    if pre_end >= pre_start:
                        pre_block = history.opponent_actions[pre_start:pre_end + 1, :]
                        baseline_mean = float(pre_block.mean())
                    elif last_defect_idx > 0:
                        pre_block = history.opponent_actions[0:last_defect_idx, :]
                        baseline_mean = float(pre_block.mean())
                    else:
                        baseline_mean = post_mean
                    if baseline_mean - post_mean > 0.25:
                        retaliation_detected = True
        if retaliation_detected:
            if Delta > 0.0:
                return Action.C
            return Action.D
        consec_defections = 0
        for a in my_actions[::-1]:
            if not bool(a):
                consec_defections += 1
            else:
                break
        B = max(3, r // 10)
        if consec_defections >= B:
            band = max(0.0001, 0.1 * abs(T))
            borderline = abs(Delta - T) <= band
            probe_needed = borderline or random.random() < self._probe_prob
            if probe_needed:
                return Action.C
        if Delta > T:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    Collective Conditional Cooperator with Forgiveness and Proportional Punishment (3FP).

    Starts cooperative, tracks short-window recent cooperation rates, punishes recent defectors
    proportionally and briefly with per-player timers (bounded by P_max within a sliding window W),
    uses probabilistic cooperation in ambiguous cases, has endgame and isolation adjustments,
    and attempts contrition after accidental defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(10, self.r)
        self.tau_good = 0.7
        self.tau_punish = 0.45
        self.P_max = min(5, self.r)
        self.per_opponent = [{'active_timer': 0, 'renewed_used': False, 'punished_rounds': []} for _ in range(max(0, self.n - 1))]
        self.group_iso_timer = 0
        self.contrition_timer = 0
        self.last_round_seen = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0 or history is None:
            self.last_round_seen = current_round
            return Action.C
        rounds_so_far = history.my_actions.shape[0]
        if rounds_so_far != current_round:
            rounds_so_far = current_round
        if self.last_round_seen < 0:
            delta = 1
        else:
            delta = max(0, current_round - self.last_round_seen)
        if delta > 0:
            for opp in self.per_opponent:
                if opp['active_timer'] > 0:
                    opp['active_timer'] = max(0, opp['active_timer'] - delta)
            if self.group_iso_timer > 0:
                self.group_iso_timer = max(0, self.group_iso_timer - delta)
            if self.contrition_timer > 0:
                self.contrition_timer = max(0, self.contrition_timer - delta)
        actual_window = min(self.W, rounds_so_far)
        if actual_window <= 0:
            self.last_round_seen = current_round
            return Action.C
        opp_actions = history.opponent_actions
        opp_recent = opp_actions[-actual_window:, :] if actual_window <= opp_actions.shape[0] else opp_actions
        opp_count = max(1, self.n - 1)
        R_js = []
        for j in range(opp_count):
            coop_count = int(np.sum(opp_recent[:, j]))
            R_j = coop_count / actual_window
            R_js.append(R_j)
        my_actions = history.my_actions
        my_recent = my_actions[-actual_window:] if actual_window <= my_actions.shape[0] else my_actions
        my_R = float(np.sum(my_recent)) / actual_window
        G = float(np.mean(R_js)) if opp_count > 0 else 1.0
        r_rem = max(0, self.r - current_round)
        for idx, opp in enumerate(self.per_opponent):
            window_start = current_round - actual_window
            if window_start < 0:
                window_start = 0
            opp['punished_rounds'] = [rr for rr in opp['punished_rounds'] if rr >= window_start]
        endgame_threshold = max(2, math.ceil(0.05 * self.r))
        if r_rem <= endgame_threshold:
            if G >= 0.5:
                self.last_round_seen = current_round
                return Action.C
            else:
                self.last_round_seen = current_round
                return Action.D
        D_indexes = [j for j, Rj in enumerate(R_js) if Rj < self.tau_punish]
        if len(D_indexes) > 0:
            s_vals = [(self.tau_punish - R_js[j]) / max(1e-09, self.tau_punish) for j in D_indexes]
            s = float(np.mean(s_vals)) if len(s_vals) > 0 else 0.0
            if s < 0:
                s = 0.0
            P = max(1, min(self.P_max, math.ceil(s * self.P_max)))
            for j in D_indexes:
                opp = self.per_opponent[j]
                window_start = current_round - actual_window
                if window_start < 0:
                    window_start = 0
                punished_in_window = sum((1 for rr in opp['punished_rounds'] if rr >= window_start))
                allowed_remaining = max(0, self.P_max - punished_in_window)
                if opp['active_timer'] > 0:
                    continue
                if allowed_remaining <= 0:
                    continue
                if opp['renewed_used'] and punished_in_window >= self.P_max:
                    continue
                start_P = min(P, allowed_remaining)
                start_P = max(1, start_P)
                opp['active_timer'] = start_P
                if punished_in_window > 0:
                    opp['renewed_used'] = True
        any_active_punishments = any((opp['active_timer'] > 0 for opp in self.per_opponent))
        long_horizon_threshold = max(3, math.ceil(0.2 * self.r))
        if G < 0.2 and r_rem > long_horizon_threshold:
            if self.group_iso_timer <= 0:
                self.group_iso_timer = min(3, r_rem)
        if any_active_punishments:
            for idx, opp in enumerate(self.per_opponent):
                if opp['active_timer'] > 0:
                    opp['punished_rounds'].append(current_round)
            self.last_round_seen = current_round
            return Action.D
        if self.group_iso_timer > 0:
            self.last_round_seen = current_round
            return Action.D
        if G >= self.tau_good:
            self.last_round_seen = current_round
            return Action.C
        if self.tau_punish <= G and G < self.tau_good:
            p = max(0.2, min(0.9, G))
            if random.random() < p:
                self.last_round_seen = current_round
                return Action.C
            else:
                self.last_round_seen = current_round
                return Action.D
        if G < 0.2 and r_rem > long_horizon_threshold:
            if self.group_iso_timer <= 0:
                self.group_iso_timer = min(3, r_rem)
            self.last_round_seen = current_round
            return Action.D
        try:
            my_last_action = bool(history.my_actions[-1])
        except Exception:
            my_last_action = True
        other_coop_per_round = np.sum(opp_actions[-actual_window:, :], axis=1) / max(1, self.n - 1)
        if actual_window > 1:
            avg_excl_last = float(np.mean(other_coop_per_round[:-1]))
        else:
            avg_excl_last = float(np.mean(other_coop_per_round))
        last_round_coop_others = float(other_coop_per_round[-1])
        mass_drop_spike = avg_excl_last - last_round_coop_others >= 0.4
        mistaken_defect = not my_last_action and my_R >= 0.5 and (not any_active_punishments)
        if (mistaken_defect or mass_drop_spike) and self.contrition_timer <= 0:
            self.contrition_timer = min(2, r_rem)
        if self.contrition_timer > 0:
            self.last_round_seen = current_round
            return Action.C
        self.last_round_seen = current_round
        return Action.D

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    Exploitative Public Goods strategy.

    - Probes early to identify cooperators.
    - Exploits reliably cooperative, non-retaliatory opponents by defecting while they keep cooperating.
    - Uses short punishments and brief forgiveness to restore cooperation.
    - Defaults to majority-conditional cooperation when no clear exploitable targets.
    - Defects unconditionally in the final Endgame rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.ProbeRounds = min(4, max(1, math.floor(self.r / 8)))
        self.Endgame = min(3, self.r)
        self.ExploitHigh = 0.8
        self.ExploitLow = 0.6
        self.PunishLength = 2
        self.ForgiveAfter = 1
        self.exploit_target = None
        self.punish_remaining = 0
        self.forgive_remaining = 0
        self.last_exploit_failed_round = -1
        self.retaliation_delta_threshold = 0.25
        self.retaliation_min_samples = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n_rounds = self.r
        n_players = self.n
        n_opponents = n_players - 1
        if t == 0 or history is None:
            return Action.C
        if n_rounds <= self.ProbeRounds + self.Endgame:
            if t >= n_rounds - self.Endgame:
                return Action.D
            return Action.D
        if t >= n_rounds - self.Endgame:
            return Action.D
        if t < self.ProbeRounds:
            return Action.C
        start_win = max(0, t - self.W)
        end_win = t
        window_len = end_win - start_win
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        if window_len <= 0 or opp_actions.size == 0:
            cr = np.zeros((n_opponents,), dtype=float)
        else:
            window_opp = opp_actions[start_win:end_win, :]
            cr = np.mean(window_opp.astype(float), axis=0)
        avg_others = float(np.mean(cr)) if cr.size > 0 else 0.0
        if window_len <= 0:
            my_recent_defections = 0
        else:
            my_recent_defections = int(np.sum(~my_actions[start_win:end_win]))
        trans_start = max(1, t - self.W)
        trans_end = t
        trans_len = trans_end - trans_start
        retaliatory = np.zeros((n_opponents,), dtype=bool)
        if trans_len > 0:
            my_prev = my_actions[trans_start - 1:trans_end - 1]
            opp_now = opp_actions[trans_start:trans_end, :]
            denom_coop = int(np.sum(my_prev))
            denom_def = int(trans_len - denom_coop)
            if denom_coop > 0:
                rows_coop = np.where(my_prev)[0]
                coop_after_my_coop = np.sum(opp_now[rows_coop, :].astype(float), axis=0) / max(1, denom_coop)
            else:
                coop_after_my_coop = np.zeros((n_opponents,), dtype=float)
            if denom_def > 0:
                rows_def = np.where(~my_prev)[0]
                coop_after_my_def = np.sum(opp_now[rows_def, :].astype(float), axis=0) / max(1, denom_def)
            else:
                coop_after_my_def = np.zeros((n_opponents,), dtype=float)
            for j in range(n_opponents):
                valid_samples = 0
                if denom_coop >= self.retaliation_min_samples:
                    valid_samples += 1
                if denom_def >= self.retaliation_min_samples:
                    valid_samples += 1
                if valid_samples >= 1:
                    if coop_after_my_coop[j] - coop_after_my_def[j] >= self.retaliation_delta_threshold:
                        retaliatory[j] = True
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        if self.forgive_remaining > 0:
            self.forgive_remaining -= 1
            return Action.C
        if self.exploit_target is not None:
            j = int(self.exploit_target)
            if not 0 <= j < n_opponents:
                self.exploit_target = None
            else:
                current_cr = float(cr[j]) if cr.size > 0 else 0.0
                is_retaliatory = bool(retaliatory[j]) if retaliatory.size > 0 else False
                if is_retaliatory:
                    self.exploit_target = None
                elif current_cr >= self.ExploitLow:
                    return Action.D
                else:
                    self.punish_remaining = self.PunishLength - 1
                    self.forgive_remaining = self.ForgiveAfter
                    return Action.D
        candidates = []
        for j in range(n_opponents):
            if cr.size == 0:
                rate = 0.0
            else:
                rate = float(cr[j])
            if rate >= self.ExploitHigh:
                if retaliatory.size == 0 or not retaliatory[j]:
                    candidates.append((rate, j))
        if candidates:
            candidates.sort(reverse=True)
            chosen_j = int(candidates[0][1])
            if self.last_exploit_failed_round >= 0 and t - self.last_exploit_failed_round <= 1:
                self.exploit_target = None
            else:
                self.exploit_target = chosen_j
                cur_cr = float(cr[chosen_j]) if cr.size > 0 else 0.0
                if cur_cr >= self.ExploitLow:
                    return Action.D
                else:
                    self.exploit_target = None
        if avg_others < 0.2:
            return Action.D
        noisy = False
        if cr.size > 1:
            if float(np.max(cr) - np.min(cr)) > 0.5:
                noisy = True
            elif float(np.std(cr)) > 0.25:
                noisy = True
        if noisy:
            period = 1 + n_players // 3
            if period <= 0:
                period = 1
            if t % period == 0:
                return Action.C
            else:
                return Action.D
        prev_round_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.shape[0] >= 1 else 0
        majority_threshold = math.ceil(n_opponents / 2)
        retaliatory_count = int(np.sum(retaliatory)) if retaliatory.size > 0 else 0
        retaliatory_majority = retaliatory_count >= majority_threshold
        if prev_round_coops >= majority_threshold:
            return Action.C
        if retaliatory_majority:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    Exploit-Minded Minimal-Maintenance Public Goods Strategy.

    - Probes briefly to classify opponents (unconditional cooperators, defectors,
      conditional reciprocators, mixed).
    - If many unconditional cooperators: exploit by defecting.
    - If enough conditional/mixed players and group cooperation is reasonably high:
      play a maintenance-minimal policy: cooperate occasionally when others do,
      punish short-term collapses, and forgive probabilistically to retest.
    - Otherwise: default to defection with occasional probes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = float(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.coop_high = 0.85
        self.def_low = 0.15
        self.cond_threshold = 0.25
        self.punish_len = 2
        self.forgive_prob = 0.2
        self.safety_horizon = 3
        self.tie_break = Action.D
        self.probe_rounds = int(min(4, max(1, math.floor(self.n_rounds / 10))))
        self.probe_prob = 0.2
        self.rare_probe_prob = 0.05
        self.maintain_frac_threshold = 0.4
        self.group_rate_threshold = 0.4
        self.p_C = 0.6
        self.punish_timer = 0
        self.await_forgive = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        cur_idx = int(state.round_number)
        t = cur_idx + 1
        rounds_left = int(self.n_rounds - cur_idx)
        if cur_idx == 0 or history is None:
            return Action.D
        if rounds_left <= self.safety_horizon:
            return Action.D
        if cur_idx < self.probe_rounds:
            if random.random() < self.probe_prob:
                return Action.C
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            if self.punish_timer == 0:
                self.await_forgive = True
            return Action.D
        completed = cur_idx
        opponent_actions = history.opponent_actions
        my_actions = history.my_actions
        opp_count = int(self.game_description.n_players - 1)
        try:
            if completed <= 0:
                counts = np.zeros(opp_count, dtype=float)
            else:
                counts = np.array(opponent_actions[:completed, :].sum(axis=0), dtype=float)
        except Exception:
            return Action.D
        denom = float(max(1, completed))
        rates = counts / denom
        lookback_m = min(6, completed)
        if lookback_m <= 0:
            cond = np.zeros(opp_count, dtype=float)
        else:
            s_start = max(1, completed - lookback_m)
            s_end = completed
            if s_start >= s_end:
                cond = np.zeros(opp_count, dtype=float)
            else:
                indices = list(range(s_start, s_end))
                opp_slice = opponent_actions[indices, :]
                my_prev = my_actions[[i - 1 for i in indices]]
                matches = (opp_slice == my_prev[:, None]).astype(float).sum(axis=0)
                cond = matches / float(len(indices))
        group_rate = float(np.mean(rates)) if opp_count > 0 else 0.0
        if lookback_m <= 0:
            recent_group_rate = 0.0
        else:
            last_indices = list(range(max(0, completed - lookback_m), completed))
            if not last_indices:
                recent_group_rate = 0.0
            else:
                total_by_round = opponent_actions[last_indices, :].sum(axis=1)
                recent_group_rate = float(total_by_round.mean()) / float(max(1, opp_count))
        labels = []
        for j in range(opp_count):
            rj = float(rates[j])
            cj = float(cond[j])
            if rj >= self.coop_high:
                labels.append('unconditional_cooperator')
            elif rj <= self.def_low:
                labels.append('defector')
            elif cj >= self.cond_threshold:
                labels.append('conditional_reciprocator')
            else:
                labels.append('mixed')
        count_uncond = sum((1 for L in labels if L == 'unconditional_cooperator'))
        count_def = sum((1 for L in labels if L == 'defector'))
        count_cond = sum((1 for L in labels if L == 'conditional_reciprocator'))
        count_mixed = sum((1 for L in labels if L == 'mixed'))
        majority_uncond = count_uncond > opp_count / 2.0
        if majority_uncond:
            if random.random() < self.rare_probe_prob:
                return Action.C
            return Action.D
        frac_cond_or_mixed = float(count_cond + count_mixed) / float(max(1, opp_count))
        maintenance_condition = frac_cond_or_mixed >= self.maintain_frac_threshold and group_rate >= self.group_rate_threshold
        others_last = int(opponent_actions[completed - 1, :].sum()) if completed >= 1 else 0
        prev_others_last = int(opponent_actions[completed - 2, :].sum()) if completed >= 2 else None
        my_last_action = bool(my_actions[completed - 1]) if completed >= 1 else False
        if maintenance_condition:
            if completed >= 2 and (not my_last_action) and (prev_others_last is not None):
                drop_threshold = max(1, math.ceil(0.2 * opp_count))
                if others_last <= prev_others_last - drop_threshold:
                    self.punish_timer = self.punish_len
                    self.punish_timer -= 1
                    if self.punish_timer == 0:
                        self.await_forgive = True
                    return Action.D
            if self.await_forgive:
                self.await_forgive = False
                if random.random() < self.forgive_prob:
                    return Action.C
                return Action.D
            T_high = int(math.ceil(opp_count * 0.6))
            if others_last >= T_high:
                if random.random() < self.p_C:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if count_uncond >= 1:
            if random.random() < self.rare_probe_prob:
                return Action.C
            return Action.D
        if random.random() < self.rare_probe_prob * 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    Selective Opportunistic Free-rider (SOF)

    Default: defect. Occasionally cooperate with low probability when opponents show
    stable high cooperation or when a strong cooperative subgroup exists. Withdraw
    entirely if cooperation collapses or if reactive punishers are detected.
    Always defect in the final round(s).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W_max = min(10, self.r)
        self.punishment_till_round = -1
        self.last_G_snapshot = None
        self.last_G_snapshot_round = None
        self.reactive_punishers = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        W_global = self.W_max
        if t == 0 or history is None:
            return Action.D
        endgame_window = min(3, r)
        if t == r - 1:
            return Action.D
        force_low_endgame = t >= r - endgame_window and t < r - 1
        rounds_played = int(history.opponent_actions.shape[0])
        opponents_count = max(0, n - 1)
        W = min(W_global, rounds_played) if rounds_played > 0 else 0
        if W <= 0 or opponents_count == 0:
            coop_rates = np.zeros(0)
            G = 0.0
        else:
            recent = history.opponent_actions[-W:, :]
            coop_counts = np.sum(recent, axis=0).astype(float)
            coop_rates = coop_counts / float(W)
            G = float(np.mean(coop_rates)) if coop_rates.size > 0 else 0.0
        L = int(np.sum(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0
        strong_cooperators = {int(j) for j, v in enumerate(coop_rates.tolist()) if v >= 0.8}
        HIGH = 0.8
        MED = 0.4
        LOW = 0.2
        if W > 0 and rounds_played >= 2 * W:
            prev_window = history.opponent_actions[-2 * W:-W, :]
            prev_counts = np.sum(prev_window, axis=0).astype(float)
            prev_rates = prev_counts / float(W)
            drops = prev_rates - coop_rates
            for j in range(opponents_count):
                if drops[j] >= 0.3:
                    self.reactive_punishers.add(int(j))
        to_remove = [j for j in self.reactive_punishers if j < coop_rates.size and coop_rates[j] >= 0.8]
        for j in to_remove:
            self.reactive_punishers.discard(j)
        if any((j < coop_rates.size and coop_rates[j] < 0.8 for j in self.reactive_punishers)):
            return Action.D
        if self.last_G_snapshot is not None and self.last_G_snapshot_round is not None:
            if self.last_G_snapshot - G >= 0.15 and self.last_G_snapshot_round < t:
                self.punishment_till_round = t + W_global if W_global > 0 else t + 1
                self.last_G_snapshot = None
                self.last_G_snapshot_round = None
                return Action.D
        if t < self.punishment_till_round:
            return Action.D
        if self.punishment_till_round != -1 and t == self.punishment_till_round:
            required_majority = math.ceil(0.6 * (n - 1)) if n - 1 > 0 else 0
            if G > MED and L >= required_majority:
                p_reentry_probe = 0.1
                if random.random() < p_reentry_probe:
                    self.last_G_snapshot = G
                    self.last_G_snapshot_round = t
                    return Action.C
            self.punishment_till_round = -1
        if force_low_endgame:
            return Action.D
        if 1 <= t <= min(W_global, r) - 1:
            required_probing_majority = math.ceil(0.75 * (n - 1)) if n - 1 > 0 else 0
            if L >= required_probing_majority and G >= HIGH:
                self.last_G_snapshot = G
                self.last_G_snapshot_round = t
                return Action.C
            return Action.D
        if G <= LOW:
            return Action.D
        if G >= HIGH:
            if L < math.ceil(0.5 * (n - 1)):
                return Action.D
            target_threshold = math.ceil(0.6 * (n - 1)) if n - 1 > 0 else 0
            last_cooperators_indices = set((int(i) for i, val in enumerate(history.opponent_actions[-1, :]) if val))
            if len(strong_cooperators) >= target_threshold and len(strong_cooperators & last_cooperators_indices) >= target_threshold:
                p_target = 0.3
                if random.random() < p_target:
                    self.last_G_snapshot = G
                    self.last_G_snapshot_round = t
                    return Action.C
                else:
                    return Action.D
            p_high = 0.25
            if random.random() < p_high:
                self.last_G_snapshot = G
                self.last_G_snapshot_round = t
                return Action.C
            return Action.D
        if LOW < G < HIGH:
            required_majority_mid = math.ceil(0.6 * (n - 1)) if n - 1 > 0 else 0
            if L >= required_majority_mid:
                p_mid = 0.1
                if random.random() < p_mid:
                    self.last_G_snapshot = G
                    self.last_G_snapshot_round = t
                    return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    Opportunistic Classify-and-Exploit (OCE) for the N-player Public Goods Game.

    High-level behavior:
    - Cooperate for a short initial probing phase to build reputation.
    - Maintain short-window statistics for each opponent to classify them as
      Cooperator / Neutral / Defector.
    - Exploit (defect) when a majority of others are stable cooperators or when
      group cooperation is high enough to safely free-ride.
    - Defect when cooperation is unlikely (majority defectors or very low group rate).
    - In mixed groups, primarily cooperate to gather information and maintain ties,
      but probe occasionally with small probability.
    - Track and mark retaliators (opponents who reduce contribution rates after being
      defected on). If retaliators threaten exploitation opportunities, switch to
      defect-only until forgiveness conditions are met.
    - Always defect in the final round(s) (endgame window).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.L = max(3, min(10, math.floor(self.r / 10)))
        self.K = math.ceil(math.sqrt(self.r)) if self.r > 0 else 1
        self.initial_probe_rounds = min(3, self.r)
        self.retaliator_marks = [False] * max(0, self.n - 1)
        self.snapshots = []
        self.defect_only_due_to_retaliation = False
        self.consecutive_high_G = 0
        self.p_probe = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_idx = state.round_number
        r = self.r
        n = self.n
        opponents = max(0, n - 1)
        threshold_majority = math.ceil(opponents / 2)
        if len(self.retaliator_marks) != opponents:
            self.retaliator_marks = [False] * opponents
        if t_idx == 0 or history is None:
            return Action.C
        rounds_so_far = history.opponent_actions.shape[0]
        last_m = min(self.L, rounds_so_far)
        if last_m > 0:
            last_actions = history.opponent_actions[-last_m:, :]
            if opponents == 0:
                last_actions = np.zeros((last_m, 0), dtype=bool)
        else:
            last_actions = np.zeros((0, opponents), dtype=bool)
        if last_m > 0 and opponents > 0:
            f_j = np.array(last_actions, dtype=float).mean(axis=0)
        else:
            f_j = np.zeros((opponents,), dtype=float)
        C = int(np.sum(f_j >= 0.8))
        N = int(np.sum((f_j >= 0.3) & (f_j < 0.8)))
        D = int(np.sum(f_j < 0.3))
        if last_m > 0 and opponents > 0:
            G = float(np.mean(last_actions))
        else:
            G = 0.0
        new_snapshots = []
        for snap in self.snapshots:
            snap_round = snap['round']
            start = snap_round + 1
            if start <= rounds_so_far - 1:
                post_slice = history.opponent_actions[start:start + self.L, :]
                post_len = post_slice.shape[0]
                if post_len > 0 and opponents > 0:
                    f_post = np.array(post_slice, dtype=float).mean(axis=0)
                    f_pre = snap['f_pre']
                    drops = f_pre - f_post
                    for j in range(opponents):
                        if drops[j] > 0.25:
                            self.retaliator_marks[j] = True
                if rounds_so_far - snap_round <= self.L:
                    new_snapshots.append(snap)
            elif rounds_so_far - snap_round <= self.L:
                new_snapshots.append(snap)
        self.snapshots = new_snapshots
        count_retaliators = int(sum((1 for v in self.retaliator_marks if v)))
        if count_retaliators >= threshold_majority or count_retaliators + D >= threshold_majority:
            self.defect_only_due_to_retaliation = True
        if G >= 0.6:
            self.consecutive_high_G += 1
        else:
            self.consecutive_high_G = 0
        if self.consecutive_high_G >= self.L:
            self.retaliator_marks = [False] * opponents
            self.snapshots = []
            self.defect_only_due_to_retaliation = False
        if t_idx == r - 1:
            action = Action.D
        elif t_idx >= r - self.K:
            action = Action.D
        elif self.defect_only_due_to_retaliation:
            action = Action.D
        elif t_idx + 1 <= self.initial_probe_rounds:
            action = Action.C
        elif C >= threshold_majority:
            action = Action.D
        elif G >= 0.6 and C + N >= threshold_majority:
            action = Action.D
        elif D >= threshold_majority or G <= 0.2:
            action = Action.D
        elif random.random() < self.p_probe:
            action = Action.D
        else:
            action = Action.C
        if action == Action.D and last_m > 0 and (opponents > 0):
            snap = {'round': t_idx, 'f_pre': f_j.copy()}
            self.snapshots.append(snap)
            self.snapshots = [s for s in self.snapshots if rounds_so_far - s['round'] <= self.L]
        return action

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    Adaptive exploitative public-goods strategy.

    Phases:
    - Short randomized probe phase to estimate opponent cooperation rates and responsiveness.
    - Classification of opponents into unconditional cooperators, conditional cooperators,
      defectors, and punishers.
    - Adaptive play:
      * Avoid last-round cooperation.
      * If many punishers: conservative conditional cooperation to avoid triggering collapse.
      * If many unconditional cooperators: exploit with small maintenance contributions.
      * If majority conditional cooperators: match recent cooperation and use short contrition
        sequences when retaliation is detected.
      * If mostly defectors: always defect.
    The class keeps modest internal state (maintenance boost counters, forgiveness counter,
    previous measures) to implement ramp-up / ramp-down behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.high_coop = 0.85
        self.low_coop = 0.2
        self.resp_threshold = 0.2
        self.punisher_resp = 0.4
        self.punisher_count_threshold = 0.3
        self.maintenance_min_prob = 0.05
        self.probe_bias = 0.6
        self.probe_rounds = None
        self.maintenance_boost_rounds = 0
        self.forgiveness_rounds = 0
        self.last_uncond_frac = None
        self.prev_uncond_coop_rate = None
        self.last_classification = None

    def _compute_probe_rounds(self):
        r = self.game_description.n_rounds
        pr = min(6, max(2, math.floor(r / 10)))
        return pr

    def _safe_mean(self, arr):
        if len(arr) == 0:
            return 0.0
        return float(np.mean(np.array(arr, dtype=float)))

    def _classify_opponents(self, history, rounds_to_use=None):
        """
        Returns:
         - classifications: list of strings per opponent: 'uncond', 'cond', 'defect', 'punisher'
         - coop_rates: list of coop rates per opponent (over rounds_to_use if provided)
         - resp_up: list of responsiveness estimates per opponent
        """
        n_players = self.game_description.n_players
        n_opponents = n_players - 1
        T = 0 if history is None else history.opponent_actions.shape[0]
        if rounds_to_use is None:
            rounds_to_use = T
        rounds_to_use = min(rounds_to_use, T)
        coop_rates = [0.0] * n_opponents
        resp_up = [0.0] * n_opponents
        classifications = ['defect'] * n_opponents
        if T == 0 or rounds_to_use == 0:
            return (classifications, coop_rates, resp_up)
        ops = history.opponent_actions[:rounds_to_use, :]
        coop_rates = [float(np.mean(ops[:, j])) for j in range(n_opponents)]
        if rounds_to_use >= 2:
            our_actions = history.my_actions[:rounds_to_use,]
            for j in range(n_opponents):
                coop_after_c = []
                coop_after_d = []
                for t in range(1, rounds_to_use):
                    if our_actions[t - 1]:
                        coop_after_c.append(bool(ops[t, j]))
                    else:
                        coop_after_d.append(bool(ops[t, j]))
                pc = float(np.mean(np.array(coop_after_c))) if len(coop_after_c) > 0 else 0.0
                pd = float(np.mean(np.array(coop_after_d))) if len(coop_after_d) > 0 else 0.0
                resp_up[j] = pc - pd
        else:
            resp_up = [0.0] * n_opponents
        punish_flags = [False] * n_opponents
        if rounds_to_use >= 3:
            our_actions_full = history.my_actions[:rounds_to_use]
            ops_full = history.opponent_actions[:rounds_to_use, :]
            for j in range(n_opponents):
                if resp_up[j] <= -self.punisher_resp:
                    punish_flags[j] = True
                    continue
                for u in range(rounds_to_use - 2):
                    if not our_actions_full[u]:
                        baseline = None
                        if u >= 2:
                            baseline = float(np.mean(ops_full[u - 2:u, j]))
                        elif u >= 1:
                            baseline = float(np.mean(ops_full[0:u, j])) if u > 0 else None
                        else:
                            baseline = None
                        after = float(np.mean(ops_full[u + 1:u + 3, j]))
                        if baseline is not None:
                            if baseline - after >= 0.2:
                                punish_flags[j] = True
                                break
        for j in range(n_opponents):
            cr = coop_rates[j]
            ru = resp_up[j]
            if punish_flags[j]:
                classifications[j] = 'punisher'
            elif cr >= self.high_coop and ru < self.resp_threshold:
                classifications[j] = 'uncond'
            elif (cr >= self.low_coop and cr < self.high_coop) and ru >= self.resp_threshold:
                classifications[j] = 'cond'
            elif cr <= self.low_coop and ru < self.resp_threshold:
                classifications[j] = 'defect'
            elif cr >= 0.5 and ru >= 0.05:
                classifications[j] = 'cond'
            elif cr >= 0.6:
                classifications[j] = 'uncond'
            elif ru < -0.25:
                classifications[j] = 'punisher'
            else:
                classifications[j] = 'defect'
        return (classifications, coop_rates, resp_up)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t_idx = state.round_number
        remaining = r - t_idx
        if self.probe_rounds is None:
            self.probe_rounds = self._compute_probe_rounds()
        if t_idx == 0:
            if random.random() < self.probe_bias:
                return Action.C
            return Action.D
        if history is None:
            return Action.D
        if remaining <= 2:
            return Action.D
        T = history.opponent_actions.shape[0]
        if t_idx < self.probe_rounds:
            if random.random() < self.probe_bias:
                return Action.C
            return Action.D
        classifications, coop_rates, resp_up = self._classify_opponents(history, rounds_to_use=T)
        self.last_classification = classifications
        n_opponents = n - 1
        counts = {'uncond': 0, 'cond': 0, 'defect': 0, 'punisher': 0}
        for c in classifications:
            if c in counts:
                counts[c] += 1
            else:
                counts['defect'] += 1
        frac_uncond = counts['uncond'] / max(1, n_opponents)
        frac_punish = counts['punisher'] / max(1, n_opponents)
        frac_cond = counts['cond'] / max(1, n_opponents)
        frac_def = counts['defect'] / max(1, n_opponents)
        prev_round = T - 1
        coop_prev_opponents = float(np.sum(history.opponent_actions[prev_round, :])) if prev_round >= 0 else 0.0
        our_prev = float(history.my_actions[prev_round]) if prev_round >= 0 else 0.0
        pop_coop = (coop_prev_opponents + our_prev) / float(n)
        p_maint = max(self.maintenance_min_prob, 1.0 / max(5.0, float(remaining)))
        if frac_uncond >= 0.7 and remaining > 10:
            p_maint = max(p_maint, 0.1)
        if self.maintenance_boost_rounds > 0:
            p_maint = min(1.0, p_maint * 2.0)
            self.maintenance_boost_rounds -= 1
        try:
            uncond_idxs = [i for i, c in enumerate(classifications) if c == 'uncond']
            if len(uncond_idxs) > 0 and T >= 4:
                last2 = float(np.mean(history.opponent_actions[max(0, T - 2):T, :][:, uncond_idxs])) if T - 2 >= 0 else 0.0
                prev2 = float(np.mean(history.opponent_actions[max(0, T - 4):max(0, T - 2), :][:, uncond_idxs])) if T - 4 >= 0 else 0.0
                if prev2 - last2 >= 0.15:
                    self.maintenance_boost_rounds = max(self.maintenance_boost_rounds, 3)
        except Exception:
            pass
        if self.forgiveness_rounds > 0:
            self.forgiveness_rounds -= 1
            return Action.C
        if frac_punish >= self.punisher_count_threshold:
            if pop_coop >= 0.6:
                return Action.C
            else:
                return Action.D
        if frac_uncond >= 0.35:
            if random.random() < p_maint:
                return Action.C
            return Action.D
        largest_category = max(counts.items(), key=lambda kv: (kv[1], kv[0]))[0]
        if largest_category == 'cond':
            pop_avg_last2 = pop_coop
            if T >= 2:
                coop_last2 = 0.0
                rounds_considered = 0
                for rr in range(max(0, T - 2), T):
                    coop_last2 += float(np.sum(history.opponent_actions[rr, :])) + float(history.my_actions[rr])
                    rounds_considered += 1
                if rounds_considered > 0:
                    pop_avg_last2 = coop_last2 / (rounds_considered * n)
            if pop_avg_last2 >= 0.5:
                return Action.C
            else:
                if T >= 2 and (not bool(history.my_actions[-1])):
                    cond_idxs = [i for i, c in enumerate(classifications) if c == 'cond']
                    if len(cond_idxs) > 0 and T >= 3:
                        last = float(np.mean(history.opponent_actions[T - 1, cond_idxs]))
                        prev = float(np.mean(history.opponent_actions[T - 2, cond_idxs]))
                        if prev - last >= 0.15:
                            S = min(3, max(0, remaining - 1))
                            if S > 0:
                                self.forgiveness_rounds = S - 1
                                return Action.C
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    Opportunistic Exploiter for N-player Public Goods Game.

    - Signals cooperation early to attract conditional cooperators.
    - Builds short cooperative streaks, then performs short defect "harvests"
      when opponents are reliably cooperative and not responsive.
    - Monitors for punishment (drop in group cooperation after a defection).
      If punished, stops harvesting and attempts a recovery; if punishment
      persists, switches to persistent defection until cooperation recovers
      or the endgame begins.
    - Always defects in the final E rounds and in one-shot/2-round games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = max(1, min(5, math.floor(self.r / 5) + 1))
        self.E = min(3, math.floor(self.r / 10) + 1)
        self.theta_exploit = 0.8
        self.theta_coop = 0.5
        self.delta_resp = 0.15
        self.S_coop = 2
        self.H_len = 1
        self.harvest_remaining = 0
        self.last_pre_harvest_group_coop = None
        self.punishing = False
        self.recovery_until = -1
        self.permanent_defect = False
        self.last_harvest_start_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if self.r <= 2:
            return Action.D
        if t >= self.r - self.E:
            self.harvest_remaining = 0
            self.last_pre_harvest_group_coop = None
            return Action.D
        if history is None:
            return Action.C
        rounds_so_far = len(history.my_actions)
        if rounds_so_far != t:
            t = rounds_so_far
        effective_W = min(self.W, max(1, t))
        if t == 0:
            return Action.C
        opp_actions = history.opponent_actions
        if opp_actions.ndim != 2 or opp_actions.shape[0] != t:
            return Action.C
        n_opponents = opp_actions.shape[1]
        win_start = max(0, t - effective_W)
        win_end = t
        opp_window = opp_actions[win_start:win_end, :]
        coop_counts = np.sum(opp_window.astype(np.int32), axis=0)
        coop_i = coop_counts / float(effective_W)
        group_coop = float(np.mean(coop_i)) if n_opponents > 0 else 0.0
        sens_list = []
        pair_start = win_start
        pair_end_inclusive = t - 2
        my_acts = history.my_actions
        for i in range(n_opponents):
            count_after_myC = 0
            denom_myC = 0
            count_after_myD = 0
            denom_myD = 0
            if pair_end_inclusive >= pair_start:
                for j in range(pair_start, pair_end_inclusive + 1):
                    if my_acts[j]:
                        denom_myC += 1
                        if opp_actions[j + 1, i]:
                            count_after_myC += 1
                    else:
                        denom_myD += 1
                        if opp_actions[j + 1, i]:
                            count_after_myD += 1
            if denom_myC > 0:
                P_i_given_C = count_after_myC / float(denom_myC)
            else:
                P_i_given_C = float(coop_i[i])
            if denom_myD > 0:
                P_i_given_D = count_after_myD / float(denom_myD)
            else:
                P_i_given_D = float(coop_i[i])
            sensitivity_i = P_i_given_C - P_i_given_D
            sens_list.append(sensitivity_i)
        avg_sensitivity = float(np.mean(sens_list)) if sens_list else 0.0
        coop_streak = 0
        for a in reversed(my_acts):
            if a:
                coop_streak += 1
            else:
                break
        if t >= 1 and (not history.my_actions[-1]) and (self.last_pre_harvest_group_coop is not None) and (not self.punishing):
            effective_W_now = min(self.W, max(1, t))
            win_start_now = max(0, t - effective_W_now)
            win_end_now = t
            opp_window_now = opp_actions[win_start_now:win_end_now, :]
            coop_counts_now = np.sum(opp_window_now.astype(np.int32), axis=0)
            coop_i_now = coop_counts_now / float(effective_W_now)
            group_coop_now = float(np.mean(coop_i_now)) if n_opponents > 0 else 0.0
            drop = self.last_pre_harvest_group_coop - group_coop_now
            if drop > 0.2:
                self.punishing = True
                L = max(1, self.S_coop)
                self.recovery_until = t + L
                self.harvest_remaining = 0
                self.last_pre_harvest_group_coop = None
        if t < self.recovery_until:
            return Action.C
        if self.punishing and t >= self.recovery_until:
            if group_coop < self.theta_coop and avg_sensitivity > self.delta_resp:
                self.permanent_defect = True
                self.punishing = False
        if self.permanent_defect:
            return Action.D
        if self.harvest_remaining > 0:
            if group_coop < self.theta_coop:
                self.harvest_remaining = 0
                self.last_pre_harvest_group_coop = None
                return Action.C
            self.harvest_remaining -= 1
            if self.harvest_remaining == 0:
                pass
            return Action.D
        if group_coop >= self.theta_exploit and avg_sensitivity <= self.delta_resp and (coop_streak >= self.S_coop):
            self.harvest_remaining = max(1, self.H_len)
            self.last_pre_harvest_group_coop = group_coop
            self.last_harvest_start_round = t
            self.harvest_remaining -= 1
            return Action.D
        if avg_sensitivity > self.delta_resp:
            if group_coop < self.theta_coop:
                return Action.D
            else:
                return Action.C
        elif group_coop < self.theta_coop:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    Adaptive Exploitative Conditional Strategy (AECS)

    - Probes early with an alternating C/D pattern to estimate how opponents respond to my actions.
    - Estimates expected number of other contributors conditional on whether I cooperate or defect,
      with simple smoothing (pseudocount alpha = 1) and an informative prior of half the opponents.
    - Uses a short moving window to detect recent changes in group cooperation; uses windowed stats
      when change is detected.
    - Defects in a short endgame window.
    - If my defections appear to trigger a sharp collapse in others' cooperation, performs a short
      "rebuild" of cooperation for 1-2 rounds to regain conditional cooperators.
    - Small exploration probability epsilon (min(0.08, 4/r)) flips actions occasionally (not in endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = 1.0
        self.prior_others = max(0.0, (self.n - 1) / 2.0)
        self.margin = 0.01
        self.gamma = 0.25
        self.W = max(3, math.floor(self.r / 6)) if self.r > 0 else 3
        self.T_probe = min(6, max(2, math.floor(self.r / 6))) if self.r > 0 else 2
        self.E = min(3, math.floor(self.r / 10)) if self.r > 0 else 1
        self.epsilon = min(0.08, 4.0 / max(1, self.r))
        self.R_rebuild = 2
        self.rebuild_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.r
        n = self.n
        k = self.k
        if self.E > 0 and t > r - self.E:
            return Action.D
        if self.rebuild_counter > 0:
            self.rebuild_counter = max(0, self.rebuild_counter - 1)
            intended = Action.C
            if random.random() < self.epsilon:
                return Action.D if intended is Action.C else Action.C
            return intended
        if state.round_number == 0 or t <= self.T_probe:
            intended_bool = t % 2 == 1
            intended = Action.C if intended_bool else Action.D
            if random.random() < self.epsilon:
                return Action.D if intended is Action.C else Action.C
            return intended
        if history is None:
            return Action.D
        opp_actions = history.opponent_actions
        opp_actions = np.array(opp_actions, dtype=np.bool_) if opp_actions is not None else np.zeros((0, max(0, n - 1)), dtype=np.bool_)
        rounds_played = opp_actions.shape[0]
        if rounds_played > 0:
            others_counts = np.sum(opp_actions, axis=1).astype(float)
        else:
            others_counts = np.array([], dtype=float)
        my_actions = np.array(history.my_actions, dtype=np.bool_) if history.my_actions is not None else np.array([], dtype=np.bool_)
        coop_mask = my_actions.astype(bool)
        defect_mask = ~coop_mask
        sum_others_if_I_cooperated = float(np.sum(others_counts[coop_mask])) if coop_mask.size and np.any(coop_mask) else 0.0
        sum_others_if_I_defected = float(np.sum(others_counts[defect_mask])) if defect_mask.size and np.any(defect_mask) else 0.0
        count_cooperated = int(np.sum(coop_mask)) if coop_mask.size else 0
        count_defected = int(np.sum(defect_mask)) if defect_mask.size else 0
        long_run_avg_others = float(np.mean(others_counts)) if others_counts.size else 0.0
        long_run_frac = long_run_avg_others / max(1.0, n - 1)
        W = min(self.W, rounds_played) if rounds_played > 0 else 0
        if W > 0:
            recent_others = others_counts[-W:]
            recent_my = my_actions[-W:]
            recent_mean_others = float(np.mean(recent_others))
            recent_frac = recent_mean_others / max(1.0, n - 1)
            recent_coop_mask = recent_my.astype(bool)
            recent_def_mask = ~recent_coop_mask
            recent_sum_others_if_I_cooperated = float(np.sum(recent_others[recent_coop_mask])) if recent_coop_mask.size and np.any(recent_coop_mask) else 0.0
            recent_sum_others_if_I_defected = float(np.sum(recent_others[recent_def_mask])) if recent_def_mask.size and np.any(recent_def_mask) else 0.0
            recent_count_cooperated = int(np.sum(recent_coop_mask)) if recent_coop_mask.size else 0
            recent_count_defected = int(np.sum(recent_def_mask)) if recent_def_mask.size else 0
        else:
            recent_mean_others = 0.0
            recent_frac = 0.0
            recent_sum_others_if_I_cooperated = 0.0
            recent_sum_others_if_I_defected = 0.0
            recent_count_cooperated = 0
            recent_count_defected = 0
        alpha = self.alpha
        prior = self.prior_others
        Exp_others_if_C_long = (sum_others_if_I_cooperated + alpha * prior) / (count_cooperated + alpha)
        Exp_others_if_D_long = (sum_others_if_I_defected + alpha * prior) / (count_defected + alpha)
        Exp_others_if_C_win = (recent_sum_others_if_I_cooperated + alpha * prior) / (recent_count_cooperated + alpha) if recent_count_cooperated + alpha > 0 else Exp_others_if_C_long
        Exp_others_if_D_win = (recent_sum_others_if_I_defected + alpha * prior) / (recent_count_defected + alpha) if recent_count_defected + alpha > 0 else Exp_others_if_D_long
        use_window = abs(recent_frac - long_run_frac) > 0.1
        if use_window:
            Exp_others_if_C = Exp_others_if_C_win
            Exp_others_if_D = Exp_others_if_D_win
        else:
            Exp_others_if_C = Exp_others_if_C_long
            Exp_others_if_D = Exp_others_if_D_long
        Payoff_if_C = 0.0 + k / float(n) * (1.0 + Exp_others_if_C)
        Payoff_if_D = 1.0 + k / float(n) * Exp_others_if_D
        if Payoff_if_C >= Payoff_if_D + self.margin:
            intended = Action.C
        else:
            intended = Action.D
        if W > 0:
            recent_drop = long_run_frac - recent_frac >= self.gamma
            I_defected_recently = False
            if recent_my.size:
                I_defected_recently = np.any(~recent_my)
            if recent_drop and I_defected_recently and (intended is Action.D):
                self.rebuild_counter = self.R_rebuild
                self.rebuild_counter = max(0, self.rebuild_counter - 1)
                intended = Action.C
        if random.random() < self.epsilon:
            intended = Action.D if intended is Action.C else Action.C
        return intended

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    Probe-and-exploit Public Goods strategy.

    - Probes early with alternating C/D to estimate opponents' unconditional and
      conditional cooperation rates.
    - Classifies opponents (unconditional cooperator, defector, reciprocal, punisher, neutral).
    - After probing, cooperates only when the expected future lift from reciprocal
      opponents (over a short horizon) compensates the immediate cost of cooperating.
    - Defects in the final round. Periodically re-checks and explores with small probability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.high_coop = 0.85
        self.low_coop = 0.15
        self.resp_delta = 0.25
        self.tau_default = 0.01
        self.p_explore_initial = min(0.05, 5.0 / max(1, self.r))
        self.T_probe = min(6, max(4, math.floor(self.r / 4)))
        self.T_probe = min(self.T_probe, max(1, self.r // 2))
        self.M_recheck = max(10, math.floor(self.r / 10))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx0 = state.round_number
        round_num = round_idx0 + 1
        rounds_played = 0 if history is None else history.my_actions.shape[0]
        opponents = self.n - 1
        R_rem = self.r - round_num
        if round_num <= self.T_probe:
            if round_num % 2 == 1:
                return Action.C
            else:
                return Action.D
        if history is None or rounds_played == 0:
            return Action.C
        opp_actions = np.array(history.opponent_actions, dtype=np.bool_)
        my_actions = np.array(history.my_actions, dtype=np.bool_)
        N_j = rounds_played
        C_j = np.sum(opp_actions, axis=0).astype(float)
        p_j = (C_j + 1.0) / (N_j + 2.0)
        if rounds_played >= 2:
            prev_my = my_actions[:-1]
            next_opp = opp_actions[1:, :]
            N_after_myC = np.sum(prev_my[:, None] == True, axis=0).astype(float)
            C_after_myC = np.sum(next_opp[prev_my, :], axis=0).astype(float) if np.any(prev_my) else np.zeros(opponents)
            N_after_myD = np.sum(prev_my[:, None] == False, axis=0).astype(float)
            C_after_myD = np.sum(next_opp[~prev_my, :], axis=0).astype(float) if np.any(~prev_my) else np.zeros(opponents)
        else:
            N_after_myC = np.zeros(opponents)
            C_after_myC = np.zeros(opponents)
            N_after_myD = np.zeros(opponents)
            C_after_myD = np.zeros(opponents)
        P_coop_given_myC = (C_after_myC + 1.0) / (N_after_myC + 2.0)
        P_coop_given_myD = (C_after_myD + 1.0) / (N_after_myD + 2.0)
        r_j = P_coop_given_myC - P_coop_given_myD
        is_uncond_coop = p_j >= self.high_coop
        is_defector = p_j <= self.low_coop
        is_recip = (r_j >= self.resp_delta) & (P_coop_given_myC >= 0.5)
        is_punisher = r_j <= -self.resp_delta
        if round_num == self.r:
            return Action.D
        p_others_now = float(np.mean(p_j)) if opponents > 0 else 0.0
        expected_others_coop = p_others_now * (self.n - 1)
        U_immediate_C = -1.0 + self.k / self.n * (expected_others_coop + 1.0)
        U_immediate_D = 0.0 + self.k / self.n * expected_others_coop
        immediate_diff = U_immediate_C - U_immediate_D
        pos_r = np.maximum(0.0, r_j)
        sum_pos_r = float(np.sum(pos_r))
        sum_pos_r = min(sum_pos_r, max(0.0, self.n - 1))
        L_effect = min(3, max(0, R_rem))
        expected_future_benefit = self.k / self.n * (sum_pos_r * L_effect)
        delta = immediate_diff + expected_future_benefit
        punishers_count = int(np.sum(is_punisher))
        if punishers_count > 0:
            tau = max(self.tau_default, 0.05)
            expected_future_benefit *= 0.8
            delta = immediate_diff + expected_future_benefit
        else:
            tau = self.tau_default
        defectors_count = int(np.sum(is_defector))
        if defectors_count >= max(1, opponents // 2):
            delta -= 0.1
        plan_cooperate = delta >= tau
        if defectors_count == opponents and opponents > 0:
            plan_cooperate = False
        uncond_count = int(np.sum(is_uncond_coop))
        if uncond_count > 0 and round_num % self.M_recheck == 0:
            if not plan_cooperate and R_rem > 0:
                plan_cooperate = True
        p_explore = self.p_explore_initial * max(0.0, 1.0 - (round_num - 1) / max(1, self.r))
        if random.random() < p_explore:
            plan_cooperate = not plan_cooperate
        if R_rem <= 2 and plan_cooperate:
            if delta < max(0.02, 2 * tau):
                plan_cooperate = False
        return Action.C if plan_cooperate else Action.D

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    Selective Exploiter:
    Adaptive conditional cooperation with minimal concession. The strategy classifies opponents
    as (near-)unconditional cooperators, conditionals/reciprocators, or defectors based on a
    recent window of plays. It exploits unconditional cooperators and high group cooperation by
    defecting while making small randomized cooperative concessions to avoid collapse of
    conditional cooperators. It also probes with small probability to learn/repair and enters a
    short Repair mode (S rounds of cooperation) if defections appear to trigger large group drops.
    Always defects in the final H rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        W_base = max(3, math.floor(self.r / 5))
        self.W = min(20, W_base)
        if self.r <= 5:
            self.W = max(1, min(2, self.W))
        self.eps = max(0.03, 3.0 / max(1.0, self.r))
        if self.r <= 5:
            self.eps = max(0.02, 1.5 / max(1.0, self.r))
        self.H = max(1, math.ceil(0.1 * self.r))
        self.p_always = 0.95
        self.p_cond_min = 0.25
        self.resp_thresh = 0.2
        self.tau_high = 0.6
        self.tau_low = 0.3
        S_base = max(2, math.ceil(self.r / 20))
        self.S = min(4, S_base)
        self.repair_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.r <= 1:
            return Action.D
        t = state.round_number
        remaining_rounds = max(1, self.r - t)
        if t >= self.r - self.H:
            return Action.D
        if self.repair_counter > 0:
            self.repair_counter = max(0, self.repair_counter - 1)
            return Action.C
        if t == 0 or history is None:
            probe_prob = self.eps
            if self.r > 20:
                probe_prob = max(probe_prob, 0.08)
            if random.random() < probe_prob:
                return Action.C
            return Action.D
        opp_actions = np.array(history.opponent_actions, copy=False)
        my_actions = np.array(history.my_actions, copy=False)
        rounds_played = opp_actions.shape[0]
        last_m = min(self.W, rounds_played)
        start_idx = max(0, rounds_played - last_m)
        end_idx = rounds_played
        m = max(1, opp_actions.shape[1])
        window_opp = opp_actions[start_idx:end_idx, :]
        pj = np.mean(window_opp.astype(float), axis=0) if window_opp.size > 0 else np.zeros(m, dtype=float)
        prev_high_counts = np.zeros(m, dtype=float)
        prev_high_tot = np.zeros(m, dtype=float)
        prev_low_counts = np.zeros(m, dtype=float)
        prev_low_tot = np.zeros(m, dtype=float)
        for s in range(start_idx + 1, end_idx):
            prev = s - 1
            my_prev = 1.0 if bool(my_actions[prev]) else 0.0
            opp_prev = opp_actions[prev, :].astype(float) if opp_actions.size > 0 else np.zeros(m, dtype=float)
            group_prev_avg = (my_prev + np.sum(opp_prev)) / float(self.n)
            is_high = group_prev_avg >= 0.5
            coop_j_at_s = 1.0 * opp_actions[s, :].astype(float)
            if is_high:
                prev_high_counts += coop_j_at_s
                prev_high_tot += 1.0
            else:
                prev_low_counts += coop_j_at_s
                prev_low_tot += 1.0
        resp = np.zeros(m, dtype=float)
        for j in range(m):
            p_high = prev_high_counts[j] / prev_high_tot[j] if prev_high_tot[j] > 0 else 0.0
            p_low = prev_low_counts[j] / prev_low_tot[j] if prev_low_tot[j] > 0 else 0.0
            resp[j] = p_high - p_low
        group_rate = float(np.mean(pj)) if pj.size > 0 else 0.0
        last_round_ops = opp_actions[-1, :].astype(float) if opp_actions.size > 0 else np.zeros(m, dtype=float)
        last_round_group_rate = float(np.mean(last_round_ops)) if last_round_ops.size > 0 else 0.0
        count_always = int(np.sum(pj >= self.p_always))
        cond_mask = np.logical_and(pj >= self.p_cond_min, resp >= self.resp_thresh)
        frac_conditional = float(np.sum(cond_mask) / max(1, pj.size))
        repair_trigger = False
        per_round_group_rates = []
        for rr in range(start_idx, end_idx):
            per_round_group_rates.append(float(np.mean(opp_actions[rr, :].astype(float))))
        per_round_group_rates = np.array(per_round_group_rates) if len(per_round_group_rates) > 0 else np.array([])
        for s in range(start_idx, end_idx - 1):
            if not bool(my_actions[s]):
                group_s1 = float(np.mean(opp_actions[s + 1, :].astype(float)))
                if s - start_idx + 1 > 0:
                    avg_before = float(np.mean(per_round_group_rates[0:s - start_idx + 1]))
                else:
                    avg_before = float(np.mean(per_round_group_rates)) if per_round_group_rates.size > 0 else 0.0
                if avg_before - group_s1 > 0.25:
                    repair_trigger = True
                    break
        if repair_trigger and t < self.r - self.H:
            self.repair_counter = max(0, self.S - 1)
            return Action.C
        eps_probe = max(self.eps, 0.05)
        keep_prob = min(0.15, 2.0 / float(max(1, remaining_rounds)))
        if count_always >= 1:
            if frac_conditional >= 0.5:
                p_coop = keep_prob
            else:
                p_coop = eps_probe
            if random.random() < p_coop:
                return Action.C
            return Action.D
        if group_rate >= self.tau_high:
            if random.random() < keep_prob:
                return Action.C
            return Action.D
        if self.tau_low <= group_rate < self.tau_high:
            if last_round_group_rate >= 0.5 and frac_conditional >= 0.4:
                return Action.C
            if random.random() < self.eps:
                return Action.C
            return Action.D
        if random.random() < self.eps:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    Adaptive exploitative Public Goods strategy.

    - Probes early (C in round 0, D in round 1) to learn responsiveness.
    - Tracks per-opponent conditional cooperation after my C vs my D.
    - Computes expected drop in contributors (E_drop) if I defect now.
    - Defects when immediate one-shot gain exceeds expected next-round loss.
    - Enters a forgiveness window S rounds if my defection appears to trigger punishment.
    - If punishment persists past S rounds, switch to permanent defection.
    - Occasional probing with probability eps(t) = min(0.25, 1/t) to keep estimates fresh,
      except during forgiveness or in the final rounds.
    - Last round always defects. In final K rounds bias strongly to defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = float(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.immediate_gain_from_defect = 1.0 - self.k / self.n
        self.value_per_other_contribution = self.k / self.n
        self.S = max(1, math.floor(self.r / 10))
        self.K = min(3, math.floor(self.r / 10))
        self.low_coop_threshold = 0.2
        self.T_coop = 0.5
        self.forgiveness_until_round = -1
        self.last_forgiveness_trigger = -1
        self.permanent_defection = False
        self._eps = 1e-08

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_index = int(state.round_number)
        if t_index == 0 or history is None:
            return Action.C
        if t_index >= self.r - 1:
            return Action.D
        t_one_based = t_index + 1
        eps = min(0.25, 1.0 / max(1.0, float(t_one_based)))
        rounds_so_far = int(history.my_actions.shape[0])
        opp_actions = history.opponent_actions
        n_opponents = int(opp_actions.shape[1]) if opp_actions.ndim == 2 else 0
        if rounds_so_far <= 0 or n_opponents == 0:
            return Action.D
        coop_rate_j = np.mean(opp_actions.astype(float), axis=0)
        coop_after_Icoop_num = np.zeros(n_opponents, dtype=float)
        coop_after_Icoop_den = np.zeros(n_opponents, dtype=float)
        coop_after_Idef_num = np.zeros(n_opponents, dtype=float)
        coop_after_Idef_den = np.zeros(n_opponents, dtype=float)
        my_actions = history.my_actions.astype(bool)
        for idx in range(rounds_so_far - 1):
            next_actions = opp_actions[idx + 1, :].astype(float)
            if my_actions[idx]:
                coop_after_Icoop_num += next_actions
                coop_after_Icoop_den += 1.0
            else:
                coop_after_Idef_num += next_actions
                coop_after_Idef_den += 1.0
        coop_after_Icoop_j = np.where(coop_after_Icoop_den > 0.0, coop_after_Icoop_num / coop_after_Icoop_den, coop_rate_j.copy())
        coop_after_Idefect_j = np.where(coop_after_Idef_den > 0.0, coop_after_Idef_num / coop_after_Idef_den, coop_rate_j.copy())
        punish_effect_j = coop_after_Icoop_j - coop_after_Idefect_j
        positive_punish = np.where(punish_effect_j > 0.0, punish_effect_j, 0.0)
        E_drop = float(np.sum(positive_punish))
        expected_next_round_loss = self.value_per_other_contribution * E_drop
        avg_group_coop_last = float(np.mean(opp_actions[-1, :].astype(float)))
        if rounds_so_far >= 2 and (not my_actions[-1]):
            if E_drop > 0.05:
                new_until = t_index + self.S
                if new_until > self.forgiveness_until_round:
                    self.forgiveness_until_round = new_until
                    self.last_forgiveness_trigger = t_index
        if self.last_forgiveness_trigger >= 0 and t_index >= self.forgiveness_until_round and (not self.permanent_defection):
            if avg_group_coop_last < self.low_coop_threshold:
                self.permanent_defection = True
        if self.permanent_defection:
            return Action.D
        if t_index < self.forgiveness_until_round:
            return Action.C
        in_final_phase = self.K > 0 and t_index >= self.r - self.K
        usual_action_defect = False
        if expected_next_round_loss + self._eps < self.immediate_gain_from_defect:
            usual_action_defect = True
        else:
            usual_action_defect = False
        if avg_group_coop_last < self.low_coop_threshold:
            usual_action_defect = True
        persistent_cooperators = np.sum((coop_rate_j >= 0.9) & (punish_effect_j <= 0.05))
        if persistent_cooperators >= 1:
            if expected_next_round_loss + self._eps < self.immediate_gain_from_defect:
                usual_action_defect = True
        if self.value_per_other_contribution > 0.0:
            punishers_threshold = self.immediate_gain_from_defect / self.value_per_other_contribution
            if E_drop > punishers_threshold:
                usual_action_defect = False
        if in_final_phase:
            if self.value_per_other_contribution > 0.0:
                if E_drop > punishers_threshold:
                    usual_action_defect = False
                else:
                    usual_action_defect = True
        remaining_rounds_est = max(1.0, float(self.r - t_one_based))
        cooperative_stream_value = avg_group_coop_last * self.value_per_other_contribution * remaining_rounds_est
        if cooperative_stream_value > self.immediate_gain_from_defect * 1.0 and avg_group_coop_last >= self.T_coop:
            usual_action_defect = False
        action_defect = usual_action_defect
        do_probe = not in_final_phase and random.random() < eps
        if do_probe:
            action_defect = not usual_action_defect
        return Action.D if action_defect else Action.C

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    Adaptive Exploiter for N-player Public Goods Game.

    - Adapts between Exploit, Cautious/Conditional, and All-D (Give-up) modes.
    - Probes on first round (for sufficiently long games), defects in endgame or tiny games.
    - Measures recent cooperation rates (F) and retaliation responsiveness (R).
    - Targets persistent unconditional cooperators occasionally, but enforces a minimum cooperation floor.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.delta = 1.0 - self.k / float(self.n)
        self.W = max(3, min(10, math.floor(self.r / 10))) if self.r >= 10 else max(3, min(10, math.floor(self.r / 10)))
        self.E = min(5, max(1, math.floor(self.r / 8)))
        self.majority = math.ceil(self.n / 2)
        self.min_coop = max(0.03, min(0.15, 0.03 + 0.05 * (1.0 - self.delta)))
        seed = int(self.n * 1009 + self.r * 97 + int(self.k * 1000))
        self.rng = random.Random(seed)
        self.forgiveness_remaining = 0
        self.last_target_exploit_round = -9999
        self.target_exploit_cooldown = max(1, math.ceil(self.W / 2))
        self.rounds_seen = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if self.r <= 3:
            return Action.D
        if t == 0:
            return Action.C
        if history is None:
            return Action.D
        rounds_so_far = history.my_actions.shape[0]
        self.rounds_seen = rounds_so_far
        if t >= self.r - self.E:
            return Action.D
        m = min(self.W, rounds_so_far)
        start_idx = rounds_so_far - m
        opp_acts_recent = history.opponent_actions[start_idx:start_idx + m, :]
        my_acts_recent = history.my_actions[start_idx:start_idx + m]
        alpha = 0.6
        weights = np.array([alpha ** (m - 1 - i) for i in range(m)], dtype=np.float64)
        weights = weights / (weights.sum() + 1e-12)
        opp_float = opp_acts_recent.astype(np.float64)
        f_j_arr = np.dot(weights, opp_float) if opp_float.size else np.array([], dtype=np.float64)
        if f_j_arr.size > 0:
            F = float(np.mean(f_j_arr))
        else:
            F = 0.0
        drops = []
        n_opponents = history.opponent_actions.shape[1] if history.opponent_actions.size else 0
        per_opp_drops = [[] for _ in range(n_opponents)]
        last_strong_retaliation_round = -9999
        strong_drop_thresh = 0.25
        for s in range(max(0, rounds_so_far - self.W), rounds_so_far - 1):
            if not bool(history.my_actions[s]):
                prev_avg = float(np.mean(history.opponent_actions[s, :])) if n_opponents > 0 else 0.0
                next_avg = float(np.mean(history.opponent_actions[s + 1, :])) if n_opponents > 0 else 0.0
                drop = max(0.0, prev_avg - next_avg)
                drops.append(drop)
                if drop > strong_drop_thresh:
                    last_strong_retaliation_round = s
                for j in range(n_opponents):
                    prev_j = 1.0 if history.opponent_actions[s, j] else 0.0
                    next_j = 1.0 if history.opponent_actions[s + 1, j] else 0.0
                    per_opp_drops[j].append(max(0.0, prev_j - next_j))
        if len(drops) == 0:
            R = 0.0
        else:
            raw_R = float(np.mean(drops))
            smoothing_factor = min(1.0, len(drops) / 3.0)
            R = raw_R * smoothing_factor
        just_observed_strong_retaliation = last_strong_retaliation_round >= rounds_so_far - 3
        persistent_cooperators = []
        for j in range(n_opponents):
            fj = float(f_j_arr[j]) if f_j_arr.size > 0 else 0.0
            drops_j = per_opp_drops[j]
            avg_drop_j = float(np.mean(drops_j)) if len(drops_j) > 0 else 0.0
            if fj >= 0.95 and avg_drop_j <= 0.01:
                persistent_cooperators.append(j)
        prev_idx = rounds_so_far - 1
        prev_other_coop_count = int(np.sum(history.opponent_actions[prev_idx, :])) if n_opponents > 0 else 0
        prev_my_coop = 1 if history.my_actions[prev_idx] else 0
        prev_total_cooperators = prev_other_coop_count + prev_my_coop
        large_cooperative_majority_others = math.ceil(0.6 * (self.n - 1))
        if F < 0.2 and R < 0.05:
            return Action.D
        if R > 0.12:
            if just_observed_strong_retaliation:
                self.forgiveness_remaining = max(self.forgiveness_remaining, 2)
            if self.forgiveness_remaining > 0:
                self.forgiveness_remaining -= 1
                return Action.C
            if prev_total_cooperators >= self.majority:
                return Action.C
            else:
                return Action.D
        if len(persistent_cooperators) > 0:
            if t - self.last_target_exploit_round >= self.target_exploit_cooldown and prev_other_coop_count >= large_cooperative_majority_others:
                if self.rng.random() < 0.9:
                    self.last_target_exploit_round = t
                    return Action.D
        if prev_other_coop_count >= large_cooperative_majority_others:
            my_coop_rate = float(np.mean(history.my_actions.astype(np.float64))) if rounds_so_far > 0 else 0.0
            if my_coop_rate < self.min_coop:
                if self.rng.random() < 0.5:
                    return Action.C
            return Action.D
        base_p = 0.1 + 0.6 * F * (1.0 - self.delta)
        p = max(0.05, min(0.6, base_p))
        my_coop_rate = float(np.mean(history.my_actions.astype(np.float64))) if rounds_so_far > 0 else 0.0
        if my_coop_rate < self.min_coop:
            need = min(0.5, (self.min_coop - my_coop_rate) * 3.0)
            p = max(p, need + 0.02)
        if self.rng.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    Adaptive Exploiter for the N-player Public Goods Game.

    - Probes early (cooperates first round).
    - Estimates how others respond to my cooperation/defection over a recent window.
    - Exploits (defects) when defections do not noticeably reduce others' cooperation.
    - Cooperates when defections reliably trigger reductions in others' cooperation (to preserve the revenue stream).
    - Targets likely unconditional cooperators when they form a sizable fraction of opponents.
    - Includes small randomized cooperation (eps) and forced occasional cooperation to remain unpredictable.
    - Includes short recovery phases after observed punishment-like drops.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._recovery_until_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        r_idx = state.round_number
        remaining = r - r_idx
        if remaining == 1:
            return Action.D
        if r_idx == 0 or history is None:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opponent_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_so_far = my_actions.shape[0]
        W = min(10, r_idx)
        last_W_start = max(0, r_idx - W)
        last_W_end_exclusive = r_idx
        low_group = 0.2
        retaliation_thresh = 0.05
        punish_drop = 0.2
        eps = max(1.0 / max(1, r), 0.05)
        recovery_length = 2
        forced_coop_interval = math.ceil(1.0 / eps)
        early_probe_round = r_idx <= 2
        n_opponents = n - 1
        if W <= 0 or opponent_actions.size == 0:
            coop_rates = np.zeros(n_opponents, dtype=float)
        else:
            slice_actions = opponent_actions[last_W_start:last_W_end_exclusive, :]
            try:
                coop_rates = np.mean(slice_actions.astype(float), axis=0)
            except Exception:
                coop_rates = np.zeros(n_opponents, dtype=float)
        if coop_rates.size > 0:
            P = float(np.mean(coop_rates))
        else:
            P = 0.0
        p_after_coop_vals = []
        p_after_defect_vals = []
        others_coop_per_round = []
        for t_idx in range(0, r_idx):
            if opponent_actions.shape[0] > t_idx:
                others_coop_per_round.append(float(np.mean(opponent_actions[t_idx, :].astype(float))))
            else:
                others_coop_per_round.append(0.0)
        for i in range(last_W_start, max(last_W_start, r_idx - 1)):
            if i + 1 >= r_idx:
                continue
            my_act_i = bool(my_actions[i])
            after_coop = others_coop_per_round[i + 1]
            if my_act_i:
                p_after_coop_vals.append(after_coop)
            else:
                p_after_defect_vals.append(after_coop)
        if len(p_after_coop_vals) > 0:
            P_after_coop = float(np.mean(p_after_coop_vals))
        else:
            P_after_coop = None
        if len(p_after_defect_vals) > 0:
            P_after_defect = float(np.mean(p_after_defect_vals))
        else:
            P_after_defect = None
        if P_after_coop is None:
            P_after_coop = P
        if P_after_defect is None:
            P_after_defect = P
        R = P_after_coop - P_after_defect
        observed_punishment = False
        for i in range(last_W_start, max(last_W_start, r_idx - 1)):
            if i + 1 >= r_idx:
                continue
            if not bool(my_actions[i]):
                before = others_coop_per_round[i]
                after = others_coop_per_round[i + 1]
                if before - after >= punish_drop:
                    observed_punishment = True
                    break
        if observed_punishment:
            self._recovery_until_round = max(self._recovery_until_round, r_idx + recovery_length)
        if r_idx < self._recovery_until_round:
            return Action.C
        consec_defects = 0
        for a in reversed(my_actions):
            if not bool(a):
                consec_defects += 1
            else:
                break
        if consec_defects >= forced_coop_interval:
            self._recovery_until_round = max(self._recovery_until_round, r_idx + 1)
            return Action.C
        unconditional_mask = np.zeros(n_opponents, dtype=bool)
        if coop_rates.size > 0:
            for j in range(n_opponents):
                rate_j = coop_rates[j]
                if rate_j < 0.9:
                    continue
                vals = []
                for i in range(last_W_start, max(last_W_start, r_idx - 1)):
                    if i + 1 >= r_idx:
                        continue
                    if not bool(my_actions[i]):
                        try:
                            val = float(opponent_actions[i + 1, j])
                        except Exception:
                            val = 0.0
                        vals.append(val)
                if len(vals) == 0:
                    unconditional_mask[j] = True
                else:
                    coop_after_def = float(np.mean(vals))
                    if coop_after_def >= max(0.9, rate_j - 0.1):
                        unconditional_mask[j] = True
        n_unconditional = int(np.sum(unconditional_mask))
        frac_unconditional = float(n_unconditional) / max(1, n_opponents)
        short_horizon = remaining <= 3
        if P < low_group:
            if early_probe_round:
                return Action.C
            return Action.D
        if frac_unconditional > 0.4 and P >= low_group:
            if random.random() < eps:
                return Action.C
            return Action.D
        if R <= retaliation_thresh:
            coop_prob = eps if not short_horizon else max(eps * 0.5, 0.0)
            if random.random() < coop_prob:
                return Action.C
            return Action.D
        if short_horizon:
            if P > 0.6 and R > retaliation_thresh:
                return Action.C
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    Calculated Exploiter for N-player Public Goods Game.

    - Probes in an initial exploration window to learn opponents.
    - In exploitation, identifies unconditional cooperators, conditional (responsive) players,
      and punishers. Exploits unconditional and forgiving players by defecting most rounds,
      with occasional cooperative "repairs" to maintain cooperation. Retreats to cautious,
      majority-mimic behavior if punishment or coordinated defection is detected.
    - Always defects in final L rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.m = self.k / self.n if self.n > 0 else 0.0
        self.immediate_gain_defect = 1.0 - self.m
        self.cautious_until = -1
        self.last_caution_trigger_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        n = self.n
        r = self.r
        k = self.k
        E = max(1, min(6, math.floor(0.1 * r)))
        L = max(1, math.floor(0.05 * r))
        if self.m > 0.9:
            E = min(E + 1, r)
        if state.round_number == 0 or history is None:
            if r <= 5:
                return Action.C
            return Action.C
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        past_rounds = opp_actions.shape[0]
        if past_rounds == 0:
            return Action.C if t <= E else Action.D
        if t > r - L:
            return Action.D
        if t <= E:
            last_opp = opp_actions[-1, :] if past_rounds >= 1 else np.zeros((n - 1,), dtype=bool)
            coop_count = int(np.sum(last_opp))
            if coop_count > (n - 1) / 2:
                return Action.C
            else:
                return Action.D
        desired_W = min(20, max(5, t - 1))
        W = min(desired_W, past_rounds)
        start_idx = max(0, past_rounds - W)
        window_opp = opp_actions[start_idx:past_rounds, :]
        window_my = my_actions[start_idx:past_rounds]
        if window_opp.shape[0] > 0:
            coop_rate = np.mean(window_opp.astype(float), axis=0)
        else:
            coop_rate = np.zeros((n - 1,), dtype=float)
        group_coop_rate = float(np.mean(coop_rate)) if coop_rate.size > 0 else 0.0
        responsiveness = np.zeros((n - 1,), dtype=float)
        if window_opp.shape[0] >= 2:
            prev_my = window_my[:-1]
            next_opp = window_opp[1:, :]
            cnt_myC = np.sum(prev_my)
            cnt_myD = prev_my.shape[0] - cnt_myC
            if cnt_myC > 0:
                numer_C = np.sum(next_opp[prev_my, :].astype(float), axis=0)
                p_given_myC = numer_C / cnt_myC
            else:
                p_given_myC = np.zeros((n - 1,), dtype=float)
            if cnt_myD > 0:
                numer_D = np.sum(next_opp[~prev_my, :].astype(float), axis=0)
                p_given_myD = numer_D / cnt_myD
            else:
                p_given_myD = np.zeros((n - 1,), dtype=float)
            responsiveness = p_given_myC - p_given_myD
        else:
            responsiveness = np.zeros((n - 1,), dtype=float)
        forgiveness = np.zeros((n - 1,), dtype=float)
        forgiveness_counts = np.zeros((n - 1,), dtype=int)
        for offset in range(1, window_opp.shape[0]):
            for j in range(n - 1):
                if window_opp[offset - 1, j] and (not window_opp[offset, j]):
                    gap = None
                    for look in range(offset + 1, window_opp.shape[0]):
                        if window_opp[look, j]:
                            gap = look - offset
                            break
                    if gap is not None:
                        forgiveness[j] += gap
                        forgiveness_counts[j] += 1
        for j in range(n - 1):
            if forgiveness_counts[j] > 0:
                forgiveness[j] = forgiveness[j] / forgiveness_counts[j]
            else:
                forgiveness[j] = 0.0
        fraction_conditional = float(np.sum(responsiveness >= 0.15) / max(1, n - 1))
        fraction_unconditional_coop = float(np.sum((coop_rate >= 0.8) & (np.abs(responsiveness) <= 0.05)) / max(1, n - 1))
        punishers_mask = np.zeros((n - 1,), dtype=bool)
        full_opp = opp_actions
        full_my = my_actions
        full_len = full_opp.shape[0]
        for s in range(max(0, full_len - W), full_len):
            if not full_my[s]:
                for j in range(n - 1):
                    if s - 1 >= 0 and full_opp[s - 1, j]:
                        stayed_defect = True
                        for look in range(s, min(full_len, s + 3)):
                            if full_opp[look, j]:
                                stayed_defect = False
                                break
                        if stayed_defect:
                            punishers_mask[j] = True
        fraction_punishers = float(np.sum(punishers_mask) / max(1, n - 1))
        reactive_trigger = False
        if window_opp.shape[0] >= 4:
            last_two = window_opp[-2:, :] if window_opp.shape[0] >= 2 else window_opp
            prev_piece = window_opp[:-2, :]
            before_avg = float(np.mean(prev_piece.astype(float)))
            after_avg = float(np.mean(last_two.astype(float)))
            if before_avg - after_avg > 0.4 and after_avg < 0.3:
                reactive_trigger = True
        if reactive_trigger:
            self.cautious_until = state.round_number + W
            self.last_caution_trigger_round = state.round_number
        in_cautious_mode = state.round_number < self.cautious_until
        if fraction_punishers >= 0.35 and (not in_cautious_mode):
            self.cautious_until = state.round_number + W
            in_cautious_mode = True
            self.last_caution_trigger_round = state.round_number

        def majority_mimic():
            if past_rounds >= 1:
                last = opp_actions[-1, :]
                coopers = int(np.sum(last))
                if coopers > (n - 1) / 2:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if in_cautious_mode:
            return majority_mimic()
        if fraction_unconditional_coop >= 0.25:
            cycle = max(3, math.ceil(self.immediate_gain_defect * 10.0))
            last_coop_idx = None
            for idx in range(full_len - 1, -1, -1):
                if full_my[idx]:
                    last_coop_idx = idx
                    break
            if last_coop_idx is None:
                rounds_since_last_coop = full_len
            else:
                rounds_since_last_coop = full_len - last_coop_idx
            if group_coop_rate < 0.4 or fraction_punishers >= 0.15:
                self.cautious_until = state.round_number + W
                self.last_caution_trigger_round = state.round_number
                return majority_mimic()
            if rounds_since_last_coop >= cycle:
                return Action.C
            return Action.D
        if fraction_conditional >= 0.25:
            valid_forgiveness = forgiveness[forgiveness > 0]
            if valid_forgiveness.size > 0:
                mean_forgiveness = float(np.mean(valid_forgiveness))
                S = min(4, max(1, int(round(mean_forgiveness))))
            else:
                S = 2
            consec_defects = 0
            for val in reversed(full_my):
                if not val:
                    consec_defects += 1
                else:
                    break
            if consec_defects >= S:
                return Action.C
            if window_opp.shape[0] >= 4:
                last_avg = float(np.mean(window_opp[-2:, :].astype(float)))
                prev_avg = float(np.mean(window_opp[-4:-2, :].astype(float)))
                if prev_avg - last_avg > 0.2:
                    self.cautious_until = state.round_number + W
                    self.last_caution_trigger_round = state.round_number
                    return Action.C
            return Action.D
        if group_coop_rate >= 0.3 and np.any(coop_rate >= 0.6):
            cycle = max(3, math.ceil(self.immediate_gain_defect * 10.0))
            last_coop_idx = None
            for idx in range(full_len - 1, -1, -1):
                if full_my[idx]:
                    last_coop_idx = idx
                    break
            rounds_since_last_coop = full_len - last_coop_idx if last_coop_idx is not None else full_len
            if rounds_since_last_coop >= cycle:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    Exploitative Adaptive Public-Goods Strategy.

    - Probes briefly to identify cooperators.
    - Exploits unconditional cooperators.
    - Opportunistically free-rides when many others cooperated, but checks and
      punishes if that causes cooperation to collapse.
    - Cooperates with responsive/reciprocating opponents when the group is
      sufficiently cooperative.
    - Refuses to subsidize defectors.
    - Always defects in the final T_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.P = min(4, max(1, math.floor(self.r * 0.05)))
        self.W = min(10, max(1, self.r))
        self.Unconditional_cooperator_threshold = 0.95
        self.Defector_threshold = 0.2
        self.Responsiveness_threshold = 0.25
        self.High_coop_threshold = math.ceil(0.8 * (self.n - 1))
        self.Majority_coop_threshold = math.ceil(0.5 * (self.n - 1))
        self.Exploit_streak_max = 1
        self.Punishment_duration = min(3, max(1, math.floor(self.r * 0.05)))
        self.T_end = min(2, self.r)
        self.exploit_streak_count = 0
        self.punishment_mode = False
        self.punishment_timer = 0
        self._pending_free_ride = False
        self._pending_baseline = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n_minus_1 = self.n - 1
        if history is None:
            rounds_played = 0
        else:
            rounds_played = int(history.my_actions.shape[0])
        if self._pending_free_ride and rounds_played >= 1:
            current_others = int(np.sum(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0
            drop = float(self._pending_baseline - current_others)
            collapse_threshold = 0.2 * n_minus_1
            if drop > collapse_threshold:
                self.punishment_mode = True
                self.punishment_timer = int(self.Punishment_duration)
                self.exploit_streak_count = 0
            self._pending_free_ride = False
            self._pending_baseline = 0
        if t >= self.r - self.T_end:
            self.exploit_streak_count = 0
            return Action.D
        if self.punishment_mode:
            self.punishment_timer -= 1
            if self.punishment_timer <= 0:
                self.punishment_mode = False
                self.punishment_timer = 0
            self.exploit_streak_count = 0
            return Action.D
        if t < self.P:
            return Action.C
        if history is None or rounds_played == 0:
            return Action.C
        window_size = min(self.W, rounds_played)
        window_start = rounds_played - window_size
        opp_window = history.opponent_actions[window_start:rounds_played, :]
        my_window = history.my_actions[window_start:rounds_played]
        if opp_window.size == 0:
            C_rate = np.zeros((n_minus_1,), dtype=float)
        else:
            C_counts = np.sum(opp_window.astype(np.int64), axis=0)
            C_rate = C_counts.astype(float) / float(window_size)
        conditional_window_start = window_start + 1
        n_conditional = max(0, rounds_played - conditional_window_start)
        if n_conditional <= 0:
            resp = np.zeros((n_minus_1,), dtype=float)
        else:
            my_prev = history.my_actions[conditional_window_start - 1:rounds_played - 1]
            opp_now = history.opponent_actions[conditional_window_start:rounds_played, :]
            mask_prev_C = my_prev.astype(np.bool_)
            mask_prev_D = ~mask_prev_C
            count_prev_C = int(np.sum(mask_prev_C))
            count_prev_D = int(np.sum(mask_prev_D))
            P_C_given_myC = np.zeros((n_minus_1,), dtype=float)
            P_C_given_myD = np.zeros((n_minus_1,), dtype=float)
            if count_prev_C > 0:
                sums_when_myC = np.sum(opp_now[mask_prev_C, :].astype(np.int64), axis=0)
                P_C_given_myC = sums_when_myC.astype(float) / float(count_prev_C)
            if count_prev_D > 0:
                sums_when_myD = np.sum(opp_now[mask_prev_D, :].astype(np.int64), axis=0)
                P_C_given_myD = sums_when_myD.astype(float) / float(count_prev_D)
            resp = P_C_given_myC - P_C_given_myD
        others_last = int(np.sum(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0
        frac_uncond = 0.0
        frac_defectors = 0.0
        frac_responsive = 0.0
        if n_minus_1 > 0:
            frac_uncond = float(np.sum(C_rate >= self.Unconditional_cooperator_threshold)) / float(n_minus_1)
            frac_defectors = float(np.sum(C_rate <= self.Defector_threshold)) / float(n_minus_1)
            frac_responsive = float(np.sum(resp >= self.Responsiveness_threshold)) / float(n_minus_1)
        if frac_uncond >= 0.5:
            self.exploit_streak_count = 0
            return Action.D
        if others_last >= self.High_coop_threshold and self.exploit_streak_count < self.Exploit_streak_max:
            self.exploit_streak_count += 1
            self._pending_free_ride = True
            self._pending_baseline = int(others_last)
            return Action.D
        if frac_responsive >= 0.4 or frac_uncond + frac_responsive >= 0.5:
            if others_last >= self.Majority_coop_threshold:
                self.exploit_streak_count = 0
                return Action.C
            else:
                self.exploit_streak_count = 0
                return Action.D
        if frac_defectors >= 0.5:
            self.exploit_streak_count = 0
            return Action.D
        if others_last >= self.Majority_coop_threshold:
            self.exploit_streak_count = 0
            return Action.C
        self.exploit_streak_count = 0
        return Action.D

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    Learns which opponents retaliate and which forgive, builds reputation early,
    probes to detect punishers, exploits when a large cooperative majority exists
    and retaliators are few, and reconciles quickly if defections provoke durable
    punishment. Periodically re-probes to refresh estimates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.in_opportunistic = False
        self.opportunistic_start_round = None
        self.opportunistic_baseline_group = None
        self.opportunistic_baseline_coop_rates = None
        self.reconcile_rounds_left = 0
        self.recovery_waiting = False
        self.recovery_needed_consecutive = 2
        self.recovery_observed_consecutive = 0
        self.last_reprobe_round = -999
        self.last_defection_round = None
        self.last_defection_group_baseline = None
        self.delta = 0.3
        self.T_reprobe = 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        k = self.k
        t = state.round_number
        gamma = k / float(n)
        immediate_gain = 1.0 - gamma
        RetaliatorTolerance = 0.2 + 0.6 * immediate_gain
        if r <= 3:
            return Action.D
        if t == 0 or history is None:
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        rounds_played = my_actions.shape[0]
        if t == r - 1:
            return Action.D
        if t == r - 2:
            if opp_actions.size == 0:
                return Action.D
            never_defected = np.all(opp_actions, axis=0) if opp_actions.shape[0] > 0 else np.array([True] * (n - 1))
            if np.all(never_defected):
                return Action.C
            return Action.D
        P = min(4, max(1, r - 1))
        probe_index = P - 1
        if t < probe_index:
            return Action.C
        if t == probe_index:
            W_temp = min(8, rounds_played)
            if W_temp > 0:
                recent = opp_actions[-W_temp:, :] if W_temp > 0 else opp_actions
                self.last_defection_group_baseline = float(np.mean(recent))
            else:
                self.last_defection_group_baseline = None
            self.last_defection_round = t
            return Action.D
        W = min(8, rounds_played)
        if rounds_played >= 1:
            last_round_opps = opp_actions[-1, :]
            L = int(np.sum(last_round_opps))
        else:
            L = 0
        if W > 0:
            recent_window = opp_actions[-W:, :] if W > 0 else np.zeros((0, n - 1), dtype=bool)
            coop_rate_j = np.array(np.mean(recent_window.astype(float), axis=0))
        else:
            coop_rate_j = np.zeros(n - 1, dtype=float)
        if rounds_played >= 2:
            my_prev = my_actions[:-1]
            opp_next = opp_actions[1:, :]
            count_myC = np.sum(my_prev)
            count_myD = my_prev.shape[0] - count_myC
            if count_myC > 0:
                coop_after_myC_j = np.array(np.sum(opp_next[my_prev, :].astype(float), axis=0) / max(1, int(count_myC)))
            else:
                coop_after_myC_j = coop_rate_j.copy()
            if count_myD > 0:
                coop_after_myD_j = np.array(np.sum(opp_next[~my_prev, :].astype(float), axis=0) / max(1, int(count_myD)))
            else:
                coop_after_myD_j = coop_rate_j.copy()
        else:
            coop_after_myC_j = coop_rate_j.copy()
            coop_after_myD_j = coop_rate_j.copy()
        delta_j = coop_after_myC_j - coop_after_myD_j
        proportion_retaliators = float(np.mean(delta_j > self.delta)) if n - 1 > 0 else 0.0
        nearly_all_defectors = bool(np.all(coop_rate_j < 0.1)) if coop_rate_j.size > 0 else False
        if t - self.last_reprobe_round >= self.T_reprobe and rounds_played >= 2 and (not self.in_opportunistic) and (self.reconcile_rounds_left == 0):
            if my_actions[-1] and my_actions[-2]:
                recent_group_avg = float(np.mean(opp_actions[-min(4, rounds_played):, :])) if rounds_played > 0 else 0.0
                if recent_group_avg > 0.2:
                    self.last_reprobe_round = t
                    self.last_defection_round = t
                    W_temp = min(8, rounds_played)
                    if W_temp > 0:
                        recent = opp_actions[-W_temp:, :] if W_temp > 0 else opp_actions
                        self.last_defection_group_baseline = float(np.mean(recent))
                    else:
                        self.last_defection_group_baseline = None
                    return Action.D
        if self.reconcile_rounds_left > 0:
            if self.last_defection_group_baseline is not None:
                current_group_avg = float(np.mean(opp_actions[-min(4, rounds_played):, :])) if rounds_played > 0 else 0.0
                if current_group_avg >= self.last_defection_group_baseline - 0.1:
                    self.reconcile_rounds_left = 0
                    self.recovery_observed_consecutive = 0
                    return Action.C
            self.reconcile_rounds_left = max(0, self.reconcile_rounds_left - 1)
            return Action.C
        if self.in_opportunistic:
            if t >= r - 2:
                self.in_opportunistic = False
                self.opportunistic_baseline_group = None
                self.opportunistic_baseline_coop_rates = None
                return Action.C
            if proportion_retaliators >= RetaliatorTolerance:
                self.in_opportunistic = False
                self.reconcile_rounds_left = 2
                return Action.C
            current_group_avg = float(np.mean(coop_rate_j)) if coop_rate_j.size > 0 else 0.0
            baseline = self.opportunistic_baseline_group if self.opportunistic_baseline_group is not None else current_group_avg
            if abs(current_group_avg - baseline) > 0.15:
                self.in_opportunistic = False
                self.reconcile_rounds_left = 2
                return Action.C
            if L == 0:
                self.in_opportunistic = False
                return Action.D
            return Action.D
        if L == 0:
            return Action.D
        if nearly_all_defectors:
            return Action.D
        majority_needed = math.ceil((n - 1) / 2)
        if proportion_retaliators < RetaliatorTolerance and L >= majority_needed:
            self.in_opportunistic = True
            self.opportunistic_start_round = t
            self.opportunistic_baseline_group = float(np.mean(coop_rate_j)) if coop_rate_j.size > 0 else 0.0
            self.opportunistic_baseline_coop_rates = coop_rate_j.copy()
            self.last_defection_round = t
            self.last_defection_group_baseline = float(np.mean(opp_actions[-min(8, rounds_played):, :])) if rounds_played > 0 else None
            return Action.D
        if rounds_played >= 1 and my_actions[-1] == False and (self.last_defection_group_baseline is not None):
            current_group_avg = float(np.mean(opp_actions[-min(4, rounds_played):, :])) if rounds_played > 0 else 0.0
            if current_group_avg <= self.last_defection_group_baseline - 0.2:
                if proportion_retaliators > 0.0:
                    self.reconcile_rounds_left = 2
                    self.in_opportunistic = False
                    return Action.C
                else:
                    self.reconcile_rounds_left = 1
                    return Action.C
        if proportion_retaliators > 0.6:
            return Action.C
        if coop_rate_j.size > 0:
            mean_cr = float(np.mean(coop_rate_j))
            std_cr = float(np.std(coop_rate_j))
            noisy = std_cr > 0.25 and mean_cr < 0.8
            if noisy:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    Adaptive Selective Exploiter for N-player Public Goods.
    - Maintains per-opponent EMA trust estimates.
    - Cooperates early and when a reliable cooperative majority is detected.
    - Opportunistically defects at a controlled rate against cooperative groups.
    - Defaults to defection in risky environments, with small probing probability.
    - Punishes one-round defections that create a "sucker" event by reducing trust and
      defecting next round. Trusts recover via the EMA updates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.trusts = [0.5 for _ in range(max(0, self.n - 1))]
        self.alpha = min(0.5, 3.0 / max(1, self.r))
        self.COOP_LABEL = 0.6
        self.END_HORIZON = min(3, max(1, math.floor(self.r / 10)))
        self.MAX_EXPLOIT = 0.25
        self.MIN_EXPLOIT = 0.05
        self._last_punished_event_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_index = state.round_number
        t = t_index + 1
        n = self.n
        r = self.r
        if t_index == 0 or history is None:
            return Action.C
        try:
            last_op_actions = np.asarray(history.opponent_actions[-1, :], dtype=bool)
        except Exception:
            return Action.D
        for j in range(len(self.trusts)):
            c_j = 1.0 if bool(last_op_actions[j]) else 0.0
            self.trusts[j] = (1.0 - self.alpha) * self.trusts[j] + self.alpha * c_j
        try:
            my_last = bool(history.my_actions[-1])
        except Exception:
            my_last = False
        opp_last_sum = int(np.sum(last_op_actions))
        total_contribs_last = (1 if my_last else 0) + opp_last_sum
        majority_cooperated = total_contribs_last > n / 2.0
        opponent_defected_mask = np.asarray([not bool(x) for x in last_op_actions], dtype=bool)
        any_opponent_defected = bool(np.any(opponent_defected_mask))
        punished_this_round = False
        sucker_round_index = t_index - 1
        if majority_cooperated and any_opponent_defected and (sucker_round_index != self._last_punished_event_round):
            for j in range(len(self.trusts)):
                if opponent_defected_mask[j]:
                    self.trusts[j] = max(0.0, self.trusts[j] - 0.25)
            self._last_punished_event_round = sucker_round_index
            punished_this_round = True
        if t == r:
            return Action.D
        if t >= r - self.END_HORIZON + 1:
            return Action.D
        if punished_this_round:
            return Action.D
        p_arr = np.asarray(self.trusts, dtype=float)
        S = float(np.sum(p_arr))
        denom = max(1, n - 1)
        m = S / float(denom)
        Ccount = int(np.sum(p_arr >= self.COOP_LABEL))
        majority_threshold = math.ceil((n - 1) / 2.0)
        if Ccount >= majority_threshold:
            frac = 0.0
            if m > 0.5:
                frac = min(1.0, (m - 0.5) / 0.5)
            else:
                frac = 0.0
            eps = self.MIN_EXPLOIT + (self.MAX_EXPLOIT - self.MIN_EXPLOIT) * frac
            if random.random() < eps:
                return Action.D
            else:
                return Action.C
        else:
            if random.random() < self.MIN_EXPLOIT:
                return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    Exploitative Conditional Cooperator (ECC)

    - Builds cooperative reputation early.
    - Occasionally probes (deliberate defections) to detect punishers and exploitable cooperators.
    - If a substantial fraction of opponents appear to react to my defections, switch to reciprocal play.
    - If many opponents defect for two rounds in a row, enter a short Punish Mode.
    - In the final L rounds always defect (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = min(10, max(3, math.floor(self.r / 10)))
        self.L = min(3, max(1, math.floor(self.r / 10)))
        self.Theta_high = 0.65
        self.S = 0.25
        self.p0 = 0.2
        self.last_test_round = -999
        self.mode = 'testing'
        self.punish_remaining = 0
        self.reciprocity_entry_round = None
        self.stricter_exploit = False
        self.recovery_consecutive_majority = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if t == 0 or history is None:
            return Action.C
        if t >= self.r - self.L:
            self.mode = 'testing'
            self.punish_remaining = 0
            return Action.D
        num_opponents = max(0, self.n - 1)
        hist_rounds = history.opponent_actions.shape[0]
        hist_rounds = min(hist_rounds, t)
        window_start = max(0, hist_rounds - self.W)
        recent_opponent_actions = history.opponent_actions[window_start:hist_rounds, :]
        if recent_opponent_actions.size == 0:
            coop_rate = np.zeros((num_opponents,), dtype=float)
        else:
            coop_rate = np.array(np.mean(recent_opponent_actions.astype(float), axis=0))
        avg_other = float(np.mean(coop_rate)) if num_opponents > 0 else 0.0
        last_round_contribs = int(np.sum(history.opponent_actions[-1, :])) if hist_rounds >= 1 else 0
        my_actions = history.my_actions[:hist_rounds]
        opponent_actions = history.opponent_actions[:hist_rounds, :]
        test_indices = []
        look_start = max(0, hist_rounds - self.W)
        for idx in range(look_start, hist_rounds):
            if not bool(my_actions[idx]) and idx + 1 < hist_rounds:
                test_indices.append(idx)
        reactive_count_per_opp = np.zeros((num_opponents,), dtype=float)
        tests_per_opp = np.zeros((num_opponents,), dtype=int)
        for t0 in test_indices:
            pre_start = max(0, t0 - self.W)
            denom = t0 - pre_start
            for j in range(num_opponents):
                if denom <= 0:
                    continue
                pre_rate = float(np.mean(opponent_actions[pre_start:t0, j].astype(float)))
                resp = float(opponent_actions[t0 + 1, j])
                tests_per_opp[j] += 1
                if resp < pre_rate - 1e-09:
                    reactive_count_per_opp[j] += 1
        reactive_flags = np.zeros((num_opponents,), dtype=float)
        for j in range(num_opponents):
            if tests_per_opp[j] > 0:
                if reactive_count_per_opp[j] / tests_per_opp[j] >= 0.5:
                    reactive_flags[j] = 1.0
        fraction_reactive = float(np.sum(reactive_flags) / num_opponents) if num_opponents > 0 else 0.0
        sustained_low_coop = False
        if hist_rounds >= 2:
            coop_last = int(np.sum(opponent_actions[-1, :]))
            coop_prev = int(np.sum(opponent_actions[-2, :]))
            def_last = num_opponents - coop_last
            def_prev = num_opponents - coop_prev
            threshold_defections = math.ceil(num_opponents / 2)
            if def_last >= threshold_defections and def_prev >= threshold_defections:
                sustained_low_coop = True
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                self.mode = 'testing'
                self.stricter_exploit = True
                self.reciprocity_entry_round = None
            return action
        if sustained_low_coop:
            r_remaining = max(1, self.r - t)
            P = min(3, r_remaining)
            self.punish_remaining = P
            self.mode = 'punish'
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if fraction_reactive >= self.S:
            if self.mode != 'reciprocity':
                self.reciprocity_entry_round = t
            self.mode = 'reciprocity'
        elif self.mode == 'reciprocity':
            self.mode = 'testing'
            self.reciprocity_entry_round = None
        if hist_rounds >= 2:
            coop_last = int(np.sum(opponent_actions[-1, :]))
            coop_prev = int(np.sum(opponent_actions[-2, :]))
            maj_needed = math.ceil(num_opponents / 2)
            if coop_prev >= maj_needed and coop_last >= maj_needed:
                self.recovery_consecutive_majority += 1
            else:
                self.recovery_consecutive_majority = 0
            if self.recovery_consecutive_majority >= 1:
                self.stricter_exploit = False
        if self.mode == 'reciprocity':
            if self.reciprocity_entry_round is not None and t == self.reciprocity_entry_round:
                coopers = int(np.sum(history.opponent_actions[-1, :]))
                if coopers > num_opponents / 2.0:
                    return Action.C
                else:
                    return Action.D
            else:
                coopers = int(np.sum(history.opponent_actions[-1, :]))
                if coopers > num_opponents / 2.0:
                    return Action.C
                else:
                    return Action.D
        test_interval = max(5, self.W)
        performed_test = False
        if hist_rounds >= 2:
            if bool(history.my_actions[-1]) and bool(history.my_actions[-2]):
                if t - self.last_test_round >= test_interval:
                    self.last_test_round = t
                    performed_test = True
                    return Action.D
        Theta_to_use = self.Theta_high + (0.1 if self.stricter_exploit else 0.0)
        if avg_other >= Theta_to_use:
            denom = max(1e-09, 1.0 - self.Theta_high)
            p_exploit = min(0.6, self.p0 + 0.4 * ((avg_other - self.Theta_high) / denom))
            p_exploit = max(0.0, p_exploit)
            if random.random() < p_exploit:
                return Action.D
            else:
                return Action.C
        else:
            maj_needed = math.ceil(num_opponents / 2)
            if last_round_contribs >= maj_needed:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    Adaptive ProberExploiter with Conditional Cooperation (APEC)

    Implements a deterministic probing phase to detect conditional punishers,
    exploits when safe, and switches to a reciprocity-preserving mode if defections
    provoke retaliation. Always defects in the final endgame window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.P = min(3, self.r)
        self.E = min(3, self.r)
        self.tau = 0.2
        self.delta = 0.2
        self.S = max(6, math.floor(self.r / 10))
        self.M_exit = max(5, math.floor(self.r / 5))
        self.mode = 'Probing'
        self.probe_done = False
        self.probe_result_round = None
        self.exploit_start_round = None
        self.reciprocal_since_round = None
        self.processed_defections = set()
        self.last_maintenance_probe = None
        self.reciprocal_persistent_defect = False
        self.low_coop_consec = 0
        self.forgiveness_remaining = 1
        self.reciprocal_defections_tried = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r = self.r

        def others_coop_rate_at_round(idx: int, hist: PlayerHistory) -> float:
            if hist is None:
                return 0.0
            if hist.opponent_actions.shape[1] == 0:
                return 0.0
            return float(np.sum(hist.opponent_actions[idx, :]) / max(1, n - 1))

        def others_coop_rate_window(end_exclusive: int, length: int, hist: PlayerHistory) -> float:
            if hist is None or end_exclusive <= 0:
                return 0.0
            start = max(0, end_exclusive - length)
            rounds = hist.opponent_actions[start:end_exclusive, :]
            if rounds.size == 0:
                return 0.0
            per_round = np.sum(rounds, axis=1) / max(1, n - 1)
            return float(np.mean(per_round))
        if t >= r - self.E:
            return Action.D
        if t == 0 or history is None:
            if r == 1:
                return Action.D
            if self.P == 1:
                return Action.C
            return Action.C
        rounds_played = history.my_actions.shape[0]
        if not self.probe_done:
            if rounds_played >= self.P:
                if self.P >= 2:
                    base_idx = 0
                    check_idx = self.P - 1
                    punisher_count = 0
                    if history.opponent_actions.shape[0] > check_idx:
                        for j in range(n - 1):
                            try:
                                before = bool(history.opponent_actions[base_idx, j])
                                after = bool(history.opponent_actions[check_idx, j])
                            except Exception:
                                before = False
                                after = False
                            if before and (not after):
                                punisher_count += 1
                        ratio = punisher_count / max(1, n - 1)
                        if ratio >= self.tau:
                            self.mode = 'Reciprocal'
                            self.reciprocal_since_round = t
                            self.reciprocal_persistent_defect = False
                            self.low_coop_consec = 0
                            self.forgiveness_remaining = 1
                            self.reciprocal_defections_tried = []
                        else:
                            self.mode = 'Exploit'
                            self.exploit_start_round = t
                        self.probe_done = True
                        self.probe_result_round = t
                else:
                    self.mode = 'Exploit'
                    self.exploit_start_round = t
                    self.probe_done = True
                    self.probe_result_round = t
        for my_idx in range(rounds_played):
            if my_idx in self.processed_defections:
                continue
            try:
                my_action = bool(history.my_actions[my_idx])
            except Exception:
                my_action = False
            if my_action is False:
                if my_idx + 1 < rounds_played:
                    if my_idx - 1 >= 0:
                        baseline_idx = my_idx - 1
                    else:
                        baseline_idx = my_idx
                    avg_before = others_coop_rate_at_round(baseline_idx, history)
                    avg_after = others_coop_rate_at_round(my_idx + 1, history)
                    if avg_before - avg_after >= self.delta:
                        if self.mode != 'Reciprocal':
                            self.mode = 'Reciprocal'
                            self.reciprocal_since_round = t
                            self.reciprocal_persistent_defect = False
                            self.low_coop_consec = 0
                            self.forgiveness_remaining = 1
                            self.reciprocal_defections_tried = []
                        if self.mode == 'Reciprocal':
                            self.reciprocal_defections_tried.append((my_idx, True))
                    elif self.mode == 'Reciprocal':
                        self.reciprocal_defections_tried.append((my_idx, False))
                    self.processed_defections.add(my_idx)
                else:
                    pass
        if not self.probe_done and t < self.P:
            if self.P == 1:
                return Action.C
            elif self.P == 2:
                if t == 0:
                    return Action.C
                else:
                    return Action.D
            elif t == 0:
                return Action.C
            elif t == 1:
                return Action.D
            else:
                return Action.C
        if self.mode == 'Exploit':
            if self.exploit_start_round is None:
                self.exploit_start_round = t
            if (t - self.exploit_start_round) % self.S == 0:
                self.last_maintenance_probe = t
                return Action.C
            return Action.D
        elif self.mode == 'Reciprocal':
            recent_half_condition = False
            majority_condition = False
            recent_group_rate = 0.0
            if rounds_played >= 1:
                prev_round_idx = rounds_played - 1
                coop_prev = 0
                try:
                    coop_prev = int(np.sum(history.opponent_actions[prev_round_idx, :]))
                except Exception:
                    coop_prev = 0
                if n - 1 > 0:
                    majority_condition = coop_prev >= math.ceil((n - 1) / 2.0)
                else:
                    majority_condition = False
                recent_group_rate = others_coop_rate_window(rounds_played, 3, history)
                recent_half_condition = recent_group_rate >= 0.6
            else:
                majority_condition = False
                recent_half_condition = False
                recent_group_rate = 0.0
            if rounds_played >= 1:
                last_rate = others_coop_rate_at_round(rounds_played - 1, history)
                if last_rate < 0.2:
                    self.low_coop_consec += 1
                else:
                    self.low_coop_consec = 0
                if self.low_coop_consec >= 3:
                    self.reciprocal_persistent_defect = True
            if self.reciprocal_persistent_defect:
                if (t - (self.reciprocal_since_round or 0)) % self.S == 0:
                    return Action.C
                return Action.D
            if majority_condition or recent_half_condition:
                return Action.C
            elif self.forgiveness_remaining > 0:
                self.forgiveness_remaining -= 1
                return Action.C
            else:
                return Action.D
        else:
            if history is not None and history.opponent_actions.shape[0] >= 1:
                prev = int(np.sum(history.opponent_actions[-1, :]))
                if prev >= math.ceil((n - 1) / 2.0):
                    return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    Adaptive Exploitative Conditional Cooperator (AECC)

    - Encourages cooperation when group is reliably cooperative, but takes
      infrequent, single-round defections to free-ride (scheduled exploitation).
    - Probes occasionally when cooperation collapsed to find returning cooperators.
    - Detects retaliation/punishment and temporarily suspends exploit behavior,
      using forgiveness windows to re-enter cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, self.r)
        self.K = max(3, math.ceil(self.n / max(1.0, self.k)))
        self.p_scale = self.k / max(1.0, self.n - 1)
        self.p_high = min(0.9, 0.5 + 0.4 * self.p_scale)
        self.p_low = max(0.05, 0.15 * self.p_scale)
        self.forgive_len = min(6, max(2, math.ceil(5 * (1 - self.p_scale))))
        self.probe_prob = min(0.25, 0.1 + 0.5 * self.p_scale)
        self.M_coop = max(1, math.ceil(self.K / 2))
        self.last_exploit_round = -10000
        self.suspend_exploit_until = -1
        self.stop_probing_permanently = False
        self.consecutive_good_p = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        k = self.k
        if state.round_number == 0 or history is None or history.my_actions.size == 0:
            if r == 1:
                return Action.D
            return Action.C
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        rounds_played = my_actions.shape[0]
        if state.round_number >= r - 1:
            return Action.D
        if state.round_number >= max(0, r - 2):
            return Action.D
        if r <= 4:
            if state.round_number > 0:
                return Action.D
        last_w = min(self.W, rounds_played)
        opp_recent = opp_actions[-last_w:, :]
        if last_w > 0:
            f_j = np.mean(opp_recent.astype(np.float64), axis=0)
        else:
            f_j = np.zeros((n - 1,), dtype=np.float64)
        p = float(np.mean(f_j)) if f_j.size > 0 else 0.0
        strong_cooperators = int(np.sum(f_j >= 0.85))
        likely_group_contribs = int(round(p * (n - 1)))
        coop_counts_by_round = np.sum(opp_recent.astype(np.int32), axis=1) if last_w > 0 else np.array([], dtype=np.int32)
        threshold_majority = math.ceil(self.p_high * max(0, n - 1))
        rounds_since_majority_coop = 0
        for count in coop_counts_by_round[::-1]:
            if count < threshold_majority:
                rounds_since_majority_coop += 1
            else:
                break
        if p >= self.p_high:
            self.consecutive_good_p += 1
        else:
            self.consecutive_good_p = 0
        if self.consecutive_good_p >= self.forgive_len:
            self.suspend_exploit_until = -1
            self.stop_probing_permanently = False
        if last_w >= self.forgive_len:
            per_round_p = coop_counts_by_round.astype(np.float64) / max(1, n - 1)
            if np.all(per_round_p[-self.forgive_len:] < self.p_low):
                self.stop_probing_permanently = True
        if rounds_played >= 2:
            if not my_actions[-2]:
                prev = opp_actions[-2, :]
                cur = opp_actions[-1, :]
                retaliators = np.sum(np.logical_and(prev, np.logical_not(cur)))
                if retaliators >= math.ceil((n - 1) / 2):
                    self.suspend_exploit_until = max(self.suspend_exploit_until, state.round_number + self.M_coop)
        if rounds_played >= 1:
            if my_actions[-1]:
                last_other_coops = int(np.sum(opp_actions[-1, :])) if last_w > 0 else 0
                if last_other_coops < math.ceil((n - 1) / 2):
                    self.suspend_exploit_until = max(self.suspend_exploit_until, state.round_number + self.M_coop)
                    return Action.D
        my_recent_coop_streak = 0
        for a in my_actions[::-1]:
            if bool(a):
                my_recent_coop_streak += 1
            else:
                break
        exploit_suspended = state.round_number < self.suspend_exploit_until
        decision = Action.D
        if p >= self.p_high and (not self.stop_probing_permanently):
            if exploit_suspended:
                decision = Action.C
            elif my_recent_coop_streak >= self.K - 1:
                decision = Action.D
                self.last_exploit_round = state.round_number
            else:
                decision = Action.C
        elif self.p_low <= p < self.p_high:
            if strong_cooperators >= 1 and likely_group_contribs >= math.ceil(k / 2):
                decision = Action.C
            else:
                decision = Action.D
        elif self.stop_probing_permanently:
            decision = Action.D
        elif random.random() < self.probe_prob:
            decision = Action.C
        else:
            decision = Action.D
        if exploit_suspended and decision == Action.D:
            decision = Action.C
        if state.round_number >= r - 1:
            decision = Action.D
        return decision


class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    Adaptive Exploiter (AE)

    Exploits lenient/unconditional cooperators while preserving cooperation
    with conditional cooperators when it is profitable. Probes early to learn
    conditional response, uses Laplace-smoothed estimates of P(cooperate | my_prev_action),
    forgives briefly after detected punishment, and always defects in the final
    endgame_horizon rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T = min(6, max(3, math.floor(self.r / 5))) if self.r > 0 else 3
        self.delta_drop = 0.25
        self.forgiveness_rounds = 2
        self.safety_epsilon = 0.01
        self.endgame_horizon = 2
        self.forgiveness_counter = 0
        self.irrecoverable = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        n = self.n
        r = self.r
        k = self.k
        if t >= max(0, r - self.endgame_horizon):
            self.forgiveness_counter = 0
            return Action.D
        if t == 0 or history is None:
            return Action.C
        my_actions = np.array(history.my_actions, dtype=np.bool_)
        opp_actions = np.array(history.opponent_actions, dtype=np.bool_)
        m = my_actions.shape[0]
        num_opponents = n - 1
        C_when_I_C = np.zeros(num_opponents, dtype=float)
        N_when_I_C = np.zeros(num_opponents, dtype=float)
        C_when_I_D = np.zeros(num_opponents, dtype=float)
        N_when_I_D = np.zeros(num_opponents, dtype=float)
        if m >= 2:
            for u in range(1, m):
                prev_my = bool(my_actions[u - 1])
                row = opp_actions[u]
                if prev_my:
                    N_when_I_C += row.astype(float)
                    C_when_I_C += row.astype(float)
                    N_when_I_C += (~row).astype(float)
                else:
                    N_when_I_D += (~row).astype(float)
                    N_when_I_D += row.astype(float)
                    C_when_I_D += row.astype(float)
        else:
            pass
        N_when_I_C = np.zeros(num_opponents, dtype=float)
        N_when_I_D = np.zeros(num_opponents, dtype=float)
        C_when_I_C = np.zeros(num_opponents, dtype=float)
        C_when_I_D = np.zeros(num_opponents, dtype=float)
        if m >= 2:
            for u in range(1, m):
                prev_my = bool(my_actions[u - 1])
                row = opp_actions[u].astype(float)
                if prev_my:
                    N_when_I_C += 1.0
                    C_when_I_C += row
                else:
                    N_when_I_D += 1.0
                    C_when_I_D += row
        p_j_C = (C_when_I_C + 1.0) / (N_when_I_C + 2.0)
        p_j_D = (C_when_I_D + 1.0) / (N_when_I_D + 2.0)
        exploitable_mask = p_j_D >= p_j_C - 0.05
        punisher_mask = p_j_C - p_j_D >= self.delta_drop
        defector_mask = (p_j_C < 0.1) & (p_j_D < 0.1)
        exploitable_count = int(np.sum(exploitable_mask))
        punisher_count = int(np.sum(punisher_mask))
        predicted_others_if_C = float(np.sum(p_j_C))
        predicted_others_if_D = float(np.sum(p_j_D))
        punishment_detected = False
        if m >= 2:
            mean_coop_after_C = 0.0
            mean_coop_after_D = 0.0
            count_after_C = 0
            count_after_D = 0
            for u in range(1, m):
                prev_my = bool(my_actions[u - 1])
                coop_frac = float(np.mean(opp_actions[u].astype(float)))
                if prev_my:
                    mean_coop_after_C += coop_frac
                    count_after_C += 1
                else:
                    mean_coop_after_D += coop_frac
                    count_after_D += 1
            if count_after_C > 0:
                mean_coop_after_C /= float(count_after_C)
            else:
                mean_coop_after_C = float(np.mean(p_j_C)) if num_opponents > 0 else 0.0
            if count_after_D > 0:
                mean_coop_after_D /= float(count_after_D)
            else:
                mean_coop_after_D = float(np.mean(p_j_D)) if num_opponents > 0 else 0.0
            if m >= 2 and (not bool(my_actions[m - 2])):
                last_round_coop_frac = float(np.mean(opp_actions[m - 1].astype(float)))
                if mean_coop_after_C - last_round_coop_frac >= self.delta_drop / 2.0 and mean_coop_after_C - mean_coop_after_D >= self.delta_drop / 4.0:
                    punishment_detected = True
        if punishment_detected and self.forgiveness_counter <= 0 and (not self.irrecoverable):
            self.forgiveness_counter = self.forgiveness_rounds
        if self.forgiveness_counter > 0 and (not self.irrecoverable):
            self.forgiveness_counter -= 1
            if self.forgiveness_counter == 0:
                avg_pC = float(np.mean(p_j_C)) if num_opponents > 0 else 0.0
                if avg_pC < 0.15 and punisher_count > 0:
                    self.irrecoverable = True
            return Action.C
        if self.irrecoverable:
            return Action.D
        if t < self.T:
            if t == self.T - 1:
                return Action.C
            return Action.D
        predicted_0 = predicted_others_if_D
        predicted_1 = predicted_others_if_C
        payoff_0 = 1.0 - 0.0 + k / n * (predicted_0 + 0.0)
        payoff_1 = 1.0 - 1.0 + k / n * (predicted_1 + 1.0)
        remaining_rounds = max(0, r - (t + 1))
        delta_next = max(0.0, predicted_1 - predicted_0)
        future_loss = remaining_rounds * (k / n) * delta_next
        if payoff_0 >= payoff_1 + future_loss + self.safety_epsilon:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    Adaptive Exploiter for N-Player Public Goods Game.

    Cooperates only when estimated influence on others (from past conditional behavior)
    makes cooperation personally profitable over a short planning horizon H.
    Probes initially and occasionally (forgiveness), withdraws from unresponsive players,
    and always defects in the final F rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.H = int(min(10, max(1, math.floor(self.r / 5))))
        self.m = int(min(4, max(1, math.floor(self.r / 20))))
        self.p_forgive = 0.05
        self.delta = 0.06
        self.tau = 0.15
        self.F = int(min(2, self.r))
        self.margin_epsilon = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        n = self.n
        k = self.k
        if t >= r - self.F:
            return Action.D
        if t == 0 or history is None:
            if self.m == 1:
                return Action.D
            return Action.C
        T = int(history.my_actions.shape[0])
        if t < self.m:
            if t % 2 == 0:
                return Action.C
            return Action.D
        opp_actions = np.asarray(history.opponent_actions)
        my_actions = np.asarray(history.my_actions)
        num_opponents = int(opp_actions.shape[1])
        if num_opponents == 0:
            return Action.D
        sum_C_by_opp = np.sum(opp_actions.astype(int), axis=0)
        baseline_C_rate = (sum_C_by_opp + 1.0) / (T + 2.0)
        if T <= 1:
            obs_myC = 0
            obs_myD = 0
            count_C_after_myC = np.zeros(num_opponents, dtype=float)
            count_C_after_myD = np.zeros(num_opponents, dtype=float)
        else:
            prev_my = my_actions[:-1]
            next_opp = opp_actions[1:, :]
            obs_myC = int(np.sum(prev_my))
            obs_myD = int(np.sum(~prev_my))
            if obs_myC > 0:
                count_C_after_myC = np.sum(next_opp[prev_my, :].astype(int), axis=0)
            else:
                count_C_after_myC = np.zeros(num_opponents, dtype=float)
            if obs_myD > 0:
                count_C_after_myD = np.sum(next_opp[~prev_my, :].astype(int), axis=0)
            else:
                count_C_after_myD = np.zeros(num_opponents, dtype=float)
        Pj_C_given_myC = (count_C_after_myC + 1.0) / (obs_myC + 2.0)
        Pj_C_given_myD = (count_C_after_myD + 1.0) / (obs_myD + 2.0)
        influence = Pj_C_given_myC - Pj_C_given_myD
        baseline_mask_unresponsive = baseline_C_rate < self.tau
        Hj_multiplier = np.where(baseline_mask_unresponsive, 0.5, 1.0)
        effective_positive_influence = np.where(influence > self.delta, influence, 0.0)
        expected_extra_contribs = float(np.sum(effective_positive_influence * (self.H * Hj_multiplier)))
        marginal_benefit = k / n * expected_extra_contribs
        mean_baseline = float(np.mean(baseline_C_rate))
        high_baseline_threshold = 0.7
        if mean_baseline > high_baseline_threshold:
            return Action.D
        last_round_opp = opp_actions[-1, :].astype(bool)
        if np.all(last_round_opp):
            harsh_punishers = np.where(influence < -self.delta)[0]
            if harsh_punishers.size == 0:
                return Action.D
            else:
                if marginal_benefit > 1.0 + self.margin_epsilon:
                    return Action.C
                return Action.D
        if random.random() < self.p_forgive:
            if marginal_benefit > 0.0:
                return Action.C
        if marginal_benefit > 1.0 + self.margin_epsilon:
            return Action.C
        if self.r <= self.m + self.F:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    Adaptive Exploiter for N-Player Public Goods Game.

    Signals cooperation once (unless very short game), then attempts to identify
    conditional cooperators to exploit. Uses windowed statistics, targeted
    punishment with forgiveness, occasional probes, and an endgame defection window.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.endgame_E = min(3, math.ceil(self.r / 6)) if self.r > 0 else 0
        self.punish_len = min(5, math.ceil(self.r / 10)) if self.r > 0 else 1
        self.strong_thresh = 0.9
        self.TC_high = 0.6
        self.TC_mid = 0.4
        self.TC_low = 0.2
        opponents = max(0, self.n - 1)
        self.punish_until = [-1] * opponents
        self.punish_start_round = [-1] * opponents
        self.punish_start_rate = [0.0] * opponents
        self.prev_P_recent = 0.0
        self.last_band = None
        self.calibration_start_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_idx = int(state.round_number)
        n = self.n
        r = self.r
        opponents = max(0, n - 1)
        if r == 1:
            return Action.D
        if r <= 3:
            return Action.D
        if round_idx == 0:
            return Action.C
        if history is None:
            return Action.D
        rounds_played = int(history.opponent_actions.shape[0])
        opp_counts = np.array(history.opponent_actions.sum(axis=0), dtype=float) if rounds_played > 0 else np.zeros(opponents)
        p_j = opp_counts / rounds_played if rounds_played > 0 else np.zeros(opponents)
        if rounds_played >= 1:
            we_cooperated_last = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :]
            last_opp_coops = int(np.sum(last_opponents))
            last_opp_defectors = [i for i in range(opponents) if not bool(last_opponents[i])]
            if we_cooperated_last and len(last_opp_defectors) > 0:
                if last_opp_coops >= max(0, opponents - 1):
                    for j in last_opp_defectors:
                        self.punish_until[j] = round_idx + self.punish_len
                        self.punish_start_round[j] = round_idx
                        self.punish_start_rate[j] = float(p_j[j]) if opponents > 0 else 0.0
        W = min(10, round_idx)
        window_size = W if W > 0 else rounds_played
        if window_size > 0 and opponents > 0:
            start_idx = max(0, rounds_played - window_size)
            window_actions = history.opponent_actions[start_idx:rounds_played, :]
            for j in range(opponents):
                if self.punish_until[j] > round_idx:
                    if self.punish_start_round[j] >= 0 and round_idx - self.punish_start_round[j] >= W and (window_size >= W):
                        window_rate = float(np.sum(window_actions[:, j]) / window_size) if window_size > 0 else 0.0
                        if window_rate > self.punish_start_rate[j] + 1e-09:
                            self.punish_until[j] = -1
                            self.punish_start_round[j] = -1
                            self.punish_start_rate[j] = 0.0

        def compute_P_recent():
            if rounds_played == 0:
                return 0.0
            w = window_size
            start = max(0, rounds_played - w)
            window_actions = history.opponent_actions[start:rounds_played, :] if w > 0 else history.opponent_actions
            include_mask = np.array([self.punish_until[j] <= round_idx for j in range(opponents)], dtype=bool)
            included_count = int(np.sum(include_mask))
            if included_count == 0:
                include_mask = np.ones(opponents, dtype=bool)
                included_count = opponents if opponents > 0 else 1
            if window_actions.shape[0] == 0:
                return 0.0
            included_actions = window_actions[:, include_mask]
            total_contribs = float(np.sum(included_actions))
            denom = float(w * included_count) if w * included_count > 0 else 1.0
            return total_contribs / denom
        P_recent = compute_P_recent()
        strong_cooperators = int(np.sum(p_j >= self.strong_thresh)) if opponents > 0 else 0
        if round_idx >= r - self.endgame_E:
            self.prev_P_recent = P_recent
            return Action.D
        if strong_cooperators >= 1:
            if P_recent < self.TC_low:
                pass
            else:
                self.prev_P_recent = P_recent
                return Action.D
        action = Action.D
        if P_recent >= self.TC_high:
            if random.random() < 0.2:
                action = Action.C
            else:
                action = Action.D
            self.last_band = 'high'
        elif self.TC_mid <= P_recent and P_recent < self.TC_high:
            if random.random() < 0.4:
                action = Action.C
            else:
                action = Action.D
            self.last_band = 'mid'
        elif self.TC_low <= P_recent and P_recent < self.TC_mid:
            if self.last_band != 'lowmid':
                self.calibration_start_round = round_idx
                self.last_band = 'lowmid'
            calibration_age = 0
            if self.calibration_start_round is not None:
                calibration_age = round_idx - self.calibration_start_round
            if calibration_age < 5:
                if random.random() < 0.25:
                    action = Action.C
                else:
                    action = Action.D
            else:
                action = Action.D
        else:
            if random.random() < 0.05:
                action = Action.C
            else:
                action = Action.D
            self.last_band = 'verylow'
        self.prev_P_recent = P_recent
        return action