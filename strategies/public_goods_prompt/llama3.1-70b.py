"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: llama3.1:70b
- Game: public_goods_prompt
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState




class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Forgiveness.
    
    Prioritizes cooperation and mutual benefit while allowing for adaptability to changing opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.forgiveness_rate = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True) == 1:
            return Action.C
        avg_contribution = np.mean(np.append(history.opponent_actions[-1, :], history.my_actions[-1]))
        if avg_contribution > 0.5:
            return Action.C
        current_round_probability = min(1, (state.round_number / self.game_description.n_rounds) ** self.forgiveness_rate)
        return Action.D if random.random() > current_round_probability else Action.C

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that prioritizes cooperation and mutual benefit while adapting to opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions[-1, :].sum() == 0:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        my_payoff_last_round = self.game_description.k / self.game_description.n_players * history.opponent_actions[-1, :].sum()
        if avg_contribution > 0.5:
            return Action.C
        elif my_payoff_last_round < np.mean(history.opponent_payoffs[-1, :]):
            return Action.D
        if state.round_number % 3 == 0:
            return Action(random.choice([Action.C, Action.D]))
        return Action.C

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACT) strategy in the Public Goods Game.

    This strategy balances individual and collective welfare by cooperating when possible,
    while adapting to protect against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_contributions = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if hasattr(history, 'my_actions') else False)
        opponents_defected = history.opponent_actions[-1, :] == 0
        if state.round_number < self.game_description.n_rounds - 1:
            if previous_contributions >= self.game_description.n_players - 1:
                return Action.C
            elif np.any(opponents_defected):
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    A strategy for the Public Goods Game that balances individual incentives with collective welfare,
    adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_count = [0] * (game_description.n_players - 1)
        self.triggered_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        average_contribution = np.mean(history.opponent_actions[-1, :])
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i]:
                self.opponent_defection_count[i] += 1
            else:
                self.opponent_defection_count[i] = 0
            if self.opponent_defection_count[i] > 2 and i not in self.triggered_opponents:
                self.triggered_opponents.add(i)
        if average_contribution >= 0.5:
            return Action.C
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i] and (not history.my_actions[-1]):
                return Action.C
        if any((not history.opponent_actions[-1, i] for i in self.triggered_opponents)):
            return Action.D
        if sum((1 - action for action in history.opponent_actions[-1])) > self.game_description.n_players // 2:
            return Action.D
        if sum((1 - action for action in history.opponent_actions[-1])) == 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare,
    adapting to the behavior of others while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defector_punishment_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_contributions = opponent_cooperators + (history.my_actions[-1] if history else False)
        if history and (not history.my_actions[-1]) and (opponent_cooperators > 0):
            self.defector_punishment_probability = 0.5 * (1 - opponent_cooperators / self.game_description.n_players)
            self.defector_punishment_probability *= 0.9 ** state.round_number
        if total_contributions > self.game_description.n_players / 2:
            return Action.C
        else:
            probability = self.game_description.k / self.game_description.n_players * (1 - total_contributions / self.game_description.n_players)
            return Action.D if random.random() < max(probability, self.defector_punishment_probability) else Action.C
        if state.round_number == 0 and history is None:
            return Action.C
        if total_contributions == self.game_description.n_players / 2:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    A collective strategy that adapts to the group's behavior and reciprocates cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        if self.game_description.n_players == 1:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if self.my_last_defection and np.any(history.opponent_payoffs[-1, :] > history.my_payoffs[-1]):
            return Action.C
        if opponent_cooperators < self.game_description.n_players // 2:
            prob_defect = (self.game_description.n_players // 2 - opponent_cooperators) / (self.game_description.n_players // 2)
            return Action.D if random.random() < prob_defect else Action.C
        if opponent_cooperators == 0:
            return Action.C
        return Action.C



class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    Collective Strategy: "Adaptive Reciprocity with Gradualism"
    
    This public goods game strategy balances individual incentives with collective welfare 
    by adapting to the group's behavior and gradually adjusting cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.gradualism_parameter = 0
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contrib_prev = np.mean(history.opponent_actions[-1, :])
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        if avg_contrib_prev > 0.7:
            self.gradualism_parameter += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if np.sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2:
            return Action.D
        if avg_contrib_prev > 0.8 and len([avg_contrib_prev] + [np.mean(history.opponent_actions[i, :]) for i in range(max(-3, -state.round_number), -1)]) == 3:
            return Action.C
        if avg_contrib_prev > 0.5 and self.gradualism_parameter < 2:
            return Action.C
        elif avg_contrib_prev <= 0.5 and self.gradualism_parameter >= 2:
            return Action.D
        if avg_contrib_prev < 0.3:
            self.punishment_rounds = 2
        return Action.C

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the N-Player Public Goods Game.
    
    This strategy starts by cooperating to establish a baseline for collective welfare. 
    It then adapts based on group contributions, punishing free-riders and forgiving those who cooperate after defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_status = [False] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.n_players // 2:
            cooperate = True
        else:
            cooperate = False
        avg_payoff_prev_round = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        for i, (opponent_action, opponent_payoff) in enumerate(zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :])):
            if opponent_action == Action.D and opponent_payoff > avg_payoff_prev_round:
                self.punishment_status[i] = True
        for i, (opponent_action, punishment) in enumerate(zip(history.opponent_actions[-1, :], self.punishment_status)):
            if opponent_action == Action.C and punishment:
                self.punishment_status[i] = False
        for i, punishment in enumerate(self.punishment_status):
            if punishment:
                cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        if cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Adaptive Collective Contribution (ACC) strategy for the Public Goods Game.
    
    This strategy prioritizes cooperation when others are contributing sufficiently, 
    promoting collective welfare while ensuring individual payoffs. By adapting to 
    past contributions, the strategy responds to changes in opponent behavior, 
    encouraging cooperation while minimizing exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5 * game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        past_contributions = np.mean(history.opponent_actions[max(0, state.round_number - self.game_description.n_rounds // 2):, :], axis=0)
        avg_past_contribution = np.mean(past_contributions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif avg_past_contribution >= self.contribution_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that balances individual self-interest with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.low_contribution_threshold = 0.2 * self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        previous_round_total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else False)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_round_total_contributions >= self.game_description.n_players // 2:
            return Action.C
        p = (1 - self.game_description.k / self.game_description.n_players) * (previous_round_total_contributions / self.game_description.n_players)
        if state.round_number < 3 and previous_round_total_contributions < self.low_contribution_threshold:
            return random.choices([Action.C, Action.D], weights=[0.3, 0.7])[0]
        return random.choices([Action.C, Action.D], weights=[1 - p, p])[0]



class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation while adapting to the actions of others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = (game_description.k - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1) / self.game_description.n_players
        average_opponent_ccr = np.mean(opponent_cooperations[:-1])
        if average_opponent_ccr >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare by adapting to the group's contribution level and adjusting cooperation levels over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = np.zeros(game_description.n_players - 1)
        self.adaptive_withdrawal_levels = np.ones(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        avg_total_contributions = np.mean(np.sum(history.opponent_actions[:, :], axis=1)) + 1
        if avg_total_contributions >= self.game_description.n_players / 2:
            return Action.C
        else:
            for i, opponent_cooperation_rate in enumerate(self.opponent_cooperation_rates):
                opponent_cooperations = np.sum(history.opponent_actions[:, i])
                opponent_cooperation_rate = opponent_cooperations / (state.round_number + 1)
                self.opponent_cooperation_rates[i] = opponent_cooperation_rate
                if opponent_cooperation_rate < 0.5:
                    self.adaptive_withdrawal_levels[i] -= 1 / self.game_description.n_players
            return Action.C if np.random.rand() > max(self.adaptive_withdrawal_levels) else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_defections = np.sum(history.opponent_actions[-3:, :], axis=0) == 0
        if np.any(opponent_defections):
            avg_total_contributions_threshold = self.game_description.n_players / 3
            if avg_total_contributions >= avg_total_contributions_threshold:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    Adaptive Cooperation with Selfless Retribution strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.contribution_history = []
        self.my_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.n_rounds // 2 and len(self.contribution_history) > 1:
            avg_contribution = np.mean(np.array(self.contribution_history)[-3:])
            if avg_contribution >= 0.5:
                return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_contributions = sum(opponent_actions) + 1
        avg_contribution = (total_contributions - 1) / (self.game_description.n_players - 1)
        if len(self.contribution_history) > 2:
            prev_avg_contribution = np.mean(np.array(self.contribution_history)[-3:-1])
            if avg_contribution < prev_avg_contribution:
                return Action.D if random.random() < 0.5 else Action.C
        if state.round_number >= self.n_rounds * 0.8 and len(self.my_payoffs) > 2:
            avg_my_payoff = np.mean(np.array(self.my_payoffs)[-3:])
            opponent_avg_payoffs = np.mean(history.opponent_payoffs[-1, :], axis=0)
            if avg_my_payoff >= opponent_avg_payoffs:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if len(history.opponent_actions) > 1 and history.opponent_actions[-2, :].sum() >= self.game_description.n_players // 2:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    Collective strategy "Adaptive Cooperation with Gradual Concession" for N-Player Public Goods Game.
    
    This strategy aims to balance individual self-interest with collective welfare by cooperating initially 
    and gradually adapting to the group's behavior. It introduces a contribution threshold (T) to ensure 
    cooperation is rewarded only when the group contributes significantly, and gradually concedes in late 
    rounds to minimize losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.concession_threshold = 15
        self.contribution_threshold = 0.7 * game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number < self.concession_threshold:
            if state.round_number < 5:
                return Action.C if opponent_cooperators / self.game_description.n_players > 0.5 else Action.D
            return Action.C if opponent_cooperators >= self.contribution_threshold else Action.D
        concession_probability = max(0, (state.round_number - self.concession_threshold) / (self.game_description.n_rounds - self.concession_threshold))
        return Action.C if random.random() > concession_probability else Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    Adaptive Collective Strategy with Gradual Reciprocity.
    Cooperate initially and adapt based on opponents' past actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_threshold = 0.5
        self.acr_adjustment_factor = 0.05
        self.acr_window_size = min(5, game_description.n_rounds)
        self.last_round_payoff_threshold = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-min(self.acr_window_size, state.round_number):]
        opponent_cooperators = np.sum(opponent_actions, axis=1)
        average_opponent_acr = np.mean(opponent_cooperators / self.game_description.n_players)
        if average_opponent_acr >= self.acr_threshold:
            return Action.C
        if state.round_number > self.acr_window_size:
            overall_contribution_rate = np.mean(np.sum(history.opponent_actions, axis=1) / self.game_description.n_players)
            if overall_contribution_rate > self.acr_threshold + 0.05:
                self.acr_threshold *= 1 - self.acr_adjustment_factor
            elif overall_contribution_rate < self.acr_threshold - 0.05:
                self.acr_threshold *= 1 + self.acr_adjustment_factor
        if state.round_number == self.game_description.n_rounds - 1:
            average_payoff_per_player = np.mean(history.my_payoffs)
            return Action.C if average_payoff_per_player >= self.last_round_payoff_threshold else Action.D
        defect_probability = (self.acr_threshold - average_opponent_acr) / self.acr_threshold
        return Action.D if random.random() < defect_probability else Action.C

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Collective Optimism strategy prioritizes cooperation while being mindful of potential exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_detected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        average_contribution = np.mean(history.opponent_actions[-1, :])
        if average_contribution > 0.5:
            return Action.C
        my_payoff = history.my_payoffs[-1]
        expected_payoff = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)
        if my_payoff < expected_payoff:
            return Action.D
        recent_contributions = history.opponent_actions[-3:, :]
        recent_average_contribution = np.mean(recent_contributions, axis=(0, 1))
        if recent_average_contribution < 0.5:
            self.exploitation_detected = True
        if self.exploitation_detected:
            self.exploitation_detected = False
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_phase = False
        self.triggered_defection_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.defection_phase and self.triggered_defection_rounds_left > 0:
            self.triggered_defection_rounds_left -= 1
            return Action.D
        previous_opponent_actions = history.opponent_actions[-1, :]
        total_contributions_prev = np.sum(history.my_actions[-1] + previous_opponent_actions)
        if total_contributions_prev >= self.game_description.k / 2:
            probability_cooperate = total_contributions_prev / self.game_description.n_players
            return Action.C if random.random() < probability_cooperate else Action.D
        my_previous_payoff = history.my_payoffs[-1]
        if my_previous_payoff >= 1:
            return Action.C
        if not self.defection_phase and np.sum(previous_opponent_actions) == 0:
            self.defection_phase = True
            self.triggered_defection_rounds_left = 2
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    Adaptive Cooperativity with Gradual Punishment (ACGP) strategy.

    This strategy prioritizes cooperation while adaptively responding to the actions of others.
    It balances individual self-interest with collective well-being, making it a robust strategy for 
    a wide range of opponent behaviors in the Public Goods Game tournament.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and history is None):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 1:
            return Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :] | history.my_actions[-1])
        if history.my_actions[-1] == False and avg_contribution > 0.5:
            return Action.C
        deviation = 0.5 - avg_contribution
        if deviation < 0:
            return Action.C
        p_defect = deviation / (1 - avg_contribution)
        if random.random() < p_defect:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperate-to-Defect (ACD)

    This strategy balances individual self-interest with collective welfare by encouraging cooperation,
    adapting to changing opponent behaviors, and punishing exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions_avg = 0.0
        self.rounds_since_threshold_update = 0
        self.opponent_defection_counts = np.zeros(game_description.n_players - 1, dtype=int)
        self.punishment_status = np.full(game_description.n_players - 1, False, dtype=bool)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.rounds_since_threshold_update >= 5:
            self.total_contributions_avg = np.mean(np.sum(history.opponent_actions[-5:, :], axis=1))
            self.rounds_since_threshold_update = 0
        else:
            self.rounds_since_threshold_update += 1
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                self.opponent_defection_counts[i] += 1
                if self.opponent_defection_counts[i] > 0.8 * state.round_number:
                    self.punishment_status[i] = True
            else:
                self.opponent_defection_counts[i] = max(self.opponent_defection_counts[i] - 1, 0)
                self.punishment_status[i] = False
        if history is not None and state.round_number > 1:
            for i, opponent_action in enumerate(history.opponent_actions[-2:, :]):
                if not np.all(opponent_action):
                    self.punishment_status[i] = True
        else:
            self.punishment_status[:] = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        total_contributions = np.sum(history.opponent_actions[-1, :]) + 1
        if total_contributions >= self.total_contributions_avg and (not any(self.punishment_status)):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Collective strategy "Adaptive Cooperate-to-Thrive" (ACT) for the N-Player Public Goods Game.
    
    ACT balances individual self-interest with collective welfare by adapting to game history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5
        self.adaptive_threshold_adjustment = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = np.mean(history.my_payoffs)
        prev_cooperators = sum(prev_round_actions) + 1
        if prev_cooperators >= math.ceil(self.game_description.k / self.game_description.n_players * self.game_description.n_players):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.
    
    This strategy aims to balance individual incentives with collective welfare in the Public Goods Game.
    It adapts to the game's history and parameters, making it robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_opponent = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_contrib = np.mean(opponent_actions)
        if self.punish_opponent:
            self.punish_opponent = False
            return Action.D
        if not self.punish_opponent and history.my_actions[-1] == 0 and (avg_contrib >= 0.5):
            return Action.C
        if state.round_number < self.game_description.n_rounds - 2 and avg_contrib < 0.5:
            self.punish_opponent = True
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if avg_contrib >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    Collective Strategy: "Adaptive Collective Optimism" (ACO)

    Aims to balance individual self-interest with collective welfare, fostering cooperation while adapting to diverse opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_cooperators = 0
        self.consecutive_defections = np.zeros(game_description.n_players - 1)
        self.self_interest_threshold_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(opponent_actions)
        self.last_round_cooperators = cooperators
        if state.round_number >= 3 and self.self_interest_threshold_payoff is None:
            my_last_3_rounds_avg_payoffs = np.mean(history.my_payoffs[-3:])
            opponent_last_3_rounds_avg_payoffs = np.mean(np.sum(history.opponent_payoffs[-3:, :], axis=0), axis=0)
            self_interest_threshold = np.mean(opponent_last_3_rounds_avg_payoffs)
            if my_last_3_rounds_avg_payoffs < self_interest_threshold:
                return Action.D
        if cooperators >= n_players // 2:
            return Action.C
        if state.round_number == 1 and (not np.all(opponent_actions)):
            return Action.D
        for i, opponent_action in enumerate(opponent_actions):
            self.consecutive_defections[i] += int(not opponent_action)
            if self.consecutive_defections[i] >= 2 or (state.round_number > 1 and (not np.any(history.opponent_actions[-3:-1, i]))):
                return Action.D
        if cooperators >= n_players // 4:
            return Action.C
        my_last_3_rounds_avg_payoffs = np.mean(history.my_payoffs[-3:])
        opponent_last_3_rounds_avg_payoffs = np.mean(np.sum(history.opponent_payoffs[-3:, :], axis=0), axis=0)
        self_interest_threshold = np.mean(opponent_last_3_rounds_avg_payoffs)
        if my_last_3_rounds_avg_payoffs < self_interest_threshold:
            return Action.D
        if random.random() < 0.5:
            return Action.C
        else:
            return Action.D

    def reset(self):
        """
        Resets the strategy instance to its initial state.

        Use this method to reinitialize the instance, e.g., when a new game starts.
        """
        self.last_round_cooperators = 0
        self.consecutive_defections = np.zeros(self.game_description.n_players - 1)
        self.self_interest_threshold_payoff = None

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with cooperation in the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        avg_contribution_rate = (np.sum(prev_round_opponent_actions) + 1) / self.game_description.n_players
        if avg_contribution_rate >= 0.5:
            return Action.C
        prev_round_my_action = history.my_actions[-1]
        prev_round_opponent_defectors = np.sum(prev_round_opponent_actions == False)
        if avg_contribution_rate > 0.5 and prev_round_my_action and prev_round_opponent_defectors:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that prioritizes cooperation and adapts to individual behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 1.0
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and history.my_actions.size < state.round_number):
            return Action.C
        total_payoff = np.sum(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        average_payoff = total_payoff / self.game_description.n_players
        if total_payoff >= self.game_description.n_players:
            self.cooperation_rate *= 1.05
        else:
            self.cooperation_rate *= 0.95
        opponent_defector_index = np.argmax(history.opponent_payoffs[-1, :] > average_payoff)
        if history.opponent_actions[-1, opponent_defector_index]:
            self._punish_opponent(opponent_defector_index)
        return Action.C if random.random() < self.cooperation_rate else Action.D

    def _punish_opponent(self, opponent_index):
        if self.opponent_cooperation_rates is None:
            self.opponent_cooperation_rates = np.ones(self.game_description.n_players - 1)
        self.opponent_cooperation_rates[opponent_index] *= 0.9

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    Collective strategy: "Adaptive Cooperative Follower"

    Decision Rules:
    1. Initial Round: Cooperate (C) to establish a cooperative tone.
    2. General Rule: In each subsequent round, calculate the average contribution rate of all players in the previous round.
                    If the average is above 0.5, cooperate (C). Otherwise, defect (D).
    3. Punishment Mechanism: If another player defects (D) while you cooperated (C) in the same round, defect (D) against that player for the next two rounds.
    4. Forgiveness: After punishing a defector, return to the general rule if their contribution rate improves.

    Edge Cases:
    1. Last Round: Cooperate (C) in the last round to maximize collective payoff and maintain cooperation until the end.
    2. Tiebreaker: In case of a tie in average contribution rate (e.g., 0.5), cooperate (C) if most players cooperated in the previous round, otherwise defect (D).
    3. Single Defector: If only one player defects while all others cooperate, punish that player for two rounds before returning to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        average_contribution_rate = previous_round_cooperators / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if history.my_payoffs is not None and len(history.my_payoffs) > 0 and (history.opponent_actions is not None) and (len(history.opponent_actions) > 0):
            opponents_defected_last_round = np.where((history.opponent_actions[-1, :] == False) & (self.punished_opponents == False))[0]
            if len(opponents_defected_last_round) > 0:
                self.punished_opponents.extend(opponents_defected_last_round)
                return Action.D
        if average_contribution_rate >= 0.5:
            if len(self.punished_opponents) > 0 and history.opponent_actions is not None and (len(history.opponent_actions) > 1):
                opponents_cooperated_last_round = np.where(history.opponent_actions[-2, self.punished_opponents] == True)[0]
                for opponent in opponents_cooperated_last_round:
                    self.punished_opponents.remove(opponent)
            return Action.C
        if len(self.punished_opponents) > 0 and history.my_payoffs is not None and (len(history.my_payoffs) > 1):
            opponents_cooperated_two_rounds_ago = np.where(history.opponent_actions[-2, self.punished_opponents] == True)[0]
            for opponent in opponents_cooperated_two_rounds_ago:
                self.punished_opponents.remove(opponent)
        if previous_round_cooperators <= 1 and sum(history.my_actions) > 0:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    A collective strategy that employs gradual cooperation with adaptive punishment.
    It aims to balance individual incentives with collective welfare and adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold = 0.5 * self.n_players * self.k / (self.n_players + self.k)
        self.moving_window_size = min(5, self.n_rounds)
        self.punishment_threshold = 1.5
        self.defection_streak_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        average_payoff_last_m_rounds = np.mean(history.my_payoffs[max(0, state.round_number - self.moving_window_size):])
        if total_contributions_prev_round >= self.threshold:
            return Action.C
        elif average_payoff_last_m_rounds < self.punishment_threshold:
            return Action.D
        else:
            opponent_defection_streaks = np.sum(history.opponent_actions[:, :], axis=0)
            opponent_defectors = [i for i, streak in enumerate(opponent_defection_streaks) if streak >= self.defection_streak_threshold]
            if opponent_defectors:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    A strategy that adapts to the game's dynamics by initially cooperating, 
    reciprocating cooperation, punishing freeloaders, and rewarding cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.opponent_memory = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_previous_action = history.my_actions[-1]
        if self.opponent_memory is None:
            self.opponent_memory = np.zeros(self.game_description.n_players - 1, dtype=np.bool_)
        else:
            self.opponent_memory = np.roll(self.opponent_memory, 1)
            self.opponent_memory[0] = my_previous_action
        avg_contribution = np.mean(opponent_actions) + history.my_actions[-1]
        if state.round_number > 1:
            avg_payoff_history = np.mean(history.my_payoffs)
            prev_avg_payoff_history = np.mean(history.my_payoffs[:-1])
            if avg_payoff_history > prev_avg_payoff_history:
                self.threshold -= 0.1
            else:
                self.threshold += 0.1
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action == False and self.opponent_memory[i] == False:
                return Action.D
            elif opponent_action == True and self.opponent_memory[i] == True:
                return Action.C
        if avg_contribution > self.threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif np.all(opponent_actions == False):
            return random.choice([Action.C, Action.D])
        return Action.D

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    A strategy balancing individual self-interest with collective welfare by adapting to the behavior of others,
    punishing free riders, and forgiving past transgressions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        cooperation_ratio = total_contributions / self.game_description.n_players
        if cooperation_ratio >= 0.5:
            return Action.C
        punishment_probability = (self.game_description.n_players - total_contributions) / self.game_description.n_players
        if random.random() < punishment_probability:
            return Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and history.my_actions[-1]:
                self.defectors.add(i)
            elif opponent_action and i in self.defectors:
                if state.round_number > 1 and history.opponent_actions[-2, i] or state.round_number == 1:
                    self.defectors.remove(i)
        for opponent_index in range(self.game_description.n_players - 1):
            if opponent_index in self.defectors:
                return Action.D
        return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        identical_actions = all((action == history.my_actions[0] for action in history.my_actions))
        if identical_actions and len(set(history.opponent_actions[:, :].flatten())) == 1:
            return Action.C

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    The Adaptive Collective Conscience strategy prioritizes collective welfare by encouraging initial cooperation,
    adapting to group behavior, and gradually reducing cooperation in late game rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contributors = 0
        self.rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_players == 1:
            return Action.C
        if state.round_number == 0:
            return Action.C
        if history:
            self.contributors = sum(history.opponent_actions[-1, :])
            self.rounds_played += 1
        if state.round_number < 5:
            if self.contributors >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        if state.round_number < self.game_description.n_rounds // 2:
            if history and len(history.my_actions) > 1:
                prev_contributors = sum(history.opponent_actions[-2, :])
                if self.contributors >= prev_contributors:
                    return Action.C
                else:
                    while len(history.my_actions) > state.round_number and sum(history.opponent_actions[state.round_number - len(history.my_actions), :]) < self.game_description.n_players // 2:
                        pass
                    return Action.C if sum(history.opponent_actions[-1, :]) >= self.game_description.n_players // 2 else Action.D
            else:
                return Action.C
        defect_prob = 0.2 + 0.1 * (self.game_description.n_rounds - state.round_number)
        if random.random() < defect_prob:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    Adaptive Collective Maximization strategy for N-Player Public Goods Game.
    This strategy prioritizes collective welfare while being adaptive to changing circumstances.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0
        self.defector_ids = set()
        self.exploration_probability = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = np.sum(opponent_actions)
        threshold = self.game_description.k / self.game_description.n_players * 0.5
        if total_cooperators >= threshold:
            self.consecutive_defections = 0
            return Action.C
        else:
            self.consecutive_defections += 1
            for i, action in enumerate(opponent_actions):
                if not action and np.all(~history.opponent_actions[-3:, i]):
                    self.defector_ids.add(i)
            return Action.D
        if random.random() < self.exploration_probability:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            previous_total_cooperators = np.sum(history.opponent_actions[-2, :])
            if total_cooperators >= threshold and previous_total_cooperators >= threshold:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation and adapts to individual behaviors.
    Initially cooperate, then reciprocate with cooperation if the average contribution is above 0.5 tokens.
    Punish defectors by contributing 0 tokens in the next round, but forgive if they cooperate subsequently.
    Defect in the last round to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
            self.punished_opponents = set((i for i, action in enumerate(history.opponent_actions[-1, :]) if not action))
        elif any((action for i, action in enumerate(history.opponent_actions[-1, :]) if i in self.punished_opponents)):
            self.punished_opponents = set()
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :])
        if total_cooperators >= (self.game_description.n_players + 1) / 2:
            return Action.C
        if any((i for i in self.punished_opponents)):
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal strategy for N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective welfare by adaptively 
    adjusting cooperation levels based on the group's past behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if state.round_number < n_players // 2:
            if total_cooperators > (n_players - 1) / k:
                return Action.C
            else:
                return Action.C if random.random() < 0.5 else Action.D
        avg_payoff = np.mean(history.my_payoffs[-1])
        nash_equilibrium = 1 / k
        if state.round_number >= n_players // 2:
            if avg_payoff > nash_equilibrium:
                return Action.C
            else:
                return Action.C if random.random() < (1 - (state.round_number / self.game_description.n_rounds) ** 2) * 0.25 else Action.D
        last_payoff = history.my_payoffs[-1]
        avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
        if state.round_number > 3 * n_players // 4:
            if last_payoff < avg_opponent_payoff:
                return Action.C
            else:
                return Action.D
        if total_cooperators == 0:
            return Action.C
        elif all(history.my_actions) and sum(history.opponent_actions[:, 1:]) == 0:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Collective Strategy: "Adaptive Tit-for-Tat with Gradual Cooperation"
    
    This strategy aims to balance individual self-interest with collective welfare.
    By cooperating initially and adapting to the group's behavior, we promote a culture of cooperation.
    The punishment mechanism discourages free-riding, while the forgiveness mechanism encourages players to return to cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.punishment_step = 0.1
        self.forgiveness_step = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        average_contribution = total_contributions / self.game_description.n_players
        if history.my_payoffs.size > 0:
            my_previous_action = Action.C if history.my_actions[-1] else Action.D
            opponent_defected_while_i_cooperated = any((not action and my_previous_action == Action.C for action in history.opponent_actions[-1, :]))
            if opponent_defected_while_i_cooperated:
                self.cooperation_threshold -= self.punishment_step
                self.cooperation_threshold = max(0, self.cooperation_threshold)
        if average_contribution >= self.cooperation_threshold:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    A collective strategy that adapts to opponents' behaviors and promotes cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            return Action.C if prev_cooperators > self.n_players / 2 else Action.D if random.random() < 0.5 else Action.C
        elif state.round_number < 3 * self.n_rounds // 4:
            prev_total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if history.my_actions[-1] == Action.C and prev_total_contributions >= self.k / self.n_players * self.n_players // 2:
                return Action.C
            else:
                return Action.D if random.random() < 0.5 else Action.C
        elif state.round_number < self.n_rounds - 1:
            prev_total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if history.my_actions[-1] == Action.D and prev_total_contributions < self.k / self.n_players * self.n_players // 2:
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    Adaptive Collective Optimism strategy.

    This strategy prioritizes collective welfare by encouraging cooperation,
    disciplining defectors, and forgiving opponents who reform their behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defections = [0] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        average_contribution = cooperators / self.game_description.n_players
        if average_contribution > 0.5:
            return Action.C
        for i, opponent_defection in enumerate(history.opponent_actions[-1, :]):
            if not opponent_defection:
                self.opponent_defections[i] += 1
                if self.opponent_defections[i] > 2:
                    return Action.D
            else:
                self.opponent_defections[i] = 0
        if sum(history.my_payoffs) < sum(np.mean(history.opponent_payoffs, axis=1)):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    An adaptive cooperative threshold strategy for N-player public goods games.
    
    This strategy balances individual self-interest with collective welfare by 
    adapting to the overall level of cooperation and encouraging others to do 
    the same. By starting with cooperation and adjusting thresholds based on 
    group behavior, we aim to create a positive feedback loop that fosters mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = game_description.k / game_description.n_players * 0.6
        self.P = 0.3
        self.T_high = game_description.k / game_description.n_players * 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contributions = np.mean(history.my_actions[:-1])
        total_contributions = np.sum(history.opponent_actions, axis=1)
        if state.round_number < self.game_description.n_rounds // 2:
            if avg_contributions >= self.T:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 3 * self.game_description.n_rounds // 4:
            if np.mean(total_contributions) < self.P * self.game_description.n_players:
                self.T *= 0.9
            if avg_contributions >= self.T:
                return Action.C
            else:
                return Action.D
        elif avg_contributions >= self.T_high:
            return Action.C
        else:
            return Action.D
        if np.all(history.opponent_actions == 0):
            return Action.D
        if np.all(history.my_actions[:-1] == 1) and np.any(history.opponent_actions == 0):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare.
    It employs a dynamic approach that adapts to the group's behavior and gradually punishes free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_phase = False
        self.rounds_defected = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if self.punishment_phase and self.rounds_defected < 2:
            self.rounds_defected += 1
            return Action.D
        if self.punishment_phase and avg_contribution > 0.5:
            self.punishment_phase = False
            self.rounds_defected = 0
        if not np.any(history.opponent_actions[-1, :]):
            return Action.C
        if avg_contribution > 0.5 and history.my_payoffs[-1] >= self.game_description.k / self.game_description.n_players:
            return Action.C
        if not self.punishment_phase:
            self.punishment_phase = True
            self.rounds_defected = 1
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare by adapting to the group's overall cooperation level and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contribution = np.mean(history.my_actions[-1:] + history.opponent_actions[-1, :])
        if avg_contribution >= 0.5 and (not self.punishing):
            return Action.C
        elif avg_contribution < 0.5 and (not self.punishing):
            return Action.D
        if self.punishing:
            self.punishing = False
            return Action.D
        my_prev_action = history.my_actions[-1]
        opponent_prev_actions = history.opponent_actions[-1, :]
        if not my_prev_action and np.sum(opponent_prev_actions) < np.sum(history.opponent_actions[-2, :]):
            self.punishing = True
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and np.mean(history.my_actions) >= 0.5:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment and Forgiveness.

    This strategy aims to establish a cooperative tone, punish non-cooperation,
    and forgive opponents who cooperate after being punished.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_since_punishment = 0
        self.opponent_cooperated_after_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        threshold = self.game_description.k / self.game_description.n_players
        opponent_contributions = sum(history.opponent_actions[-1, :])
        consistent_cooperator = all(history.opponent_actions[:, 0][-4:]) if state.round_number >= 4 else False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif opponent_contributions > threshold or consistent_cooperator:
            self.opponent_cooperated_after_punishment = False
            self.rounds_since_punishment = 0
            return Action.C
        elif opponent_contributions < threshold and (not self.opponent_cooperated_after_punishment) or (history.my_actions[-1] == True and history.opponent_actions[-1, 0] == False):
            self.rounds_since_punishment += 1
            return Action.D
        elif opponent_contributions >= threshold and self.rounds_since_punishment > 0:
            self.opponent_cooperated_after_punishment = True
            self.rounds_since_punishment = 0
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    The Adaptive Collective Contribution (ACC) strategy for the Public Goods Game.
    It promotes cooperation by reciprocating contributions, punishing defection,
    and forgiving past defectors who switch to contributing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = []
        self.my_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contrib_prev = np.mean(history.opponent_actions[-1, :])
        avg_payoff_prev = np.mean(history.opponent_payoffs[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(self.my_payoffs) > 0 and self.my_payoffs[-1] > avg_payoff_prev:
            return Action.D
        if avg_contrib_prev >= 0.5:
            return Action.C
        else:
            return Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i] and (i == len(history.my_payoffs) or history.opponent_actions[-2, i] is False):
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for Public Goods Game.
    
    This strategy aims to promote cooperation by adapting to the collective behavior of other players.
    It encourages initial cooperation, responds positively to reciprocal altruism, gradually adapts to non-cooperative behavior,
    and renews cooperation when others demonstrate willingness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_level = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        average_contribution = self.game_description.k / self.game_description.n_players * self.game_description.n_players
        if total_contributions >= average_contribution:
            self.contribution_level = 1
            return Action.C
        else:
            self.contribution_level -= 1 / (self.game_description.n_rounds - state.round_number)
            self.contribution_level = max(0, self.contribution_level)
            if total_contributions == 1 and self.contribution_level > 0:
                return Action.C
            return Action.D if self.contribution_level <= 0 else Action.C

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        threshold = (self.n_players - 1) / self.k
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.C
        if total_contributions > threshold:
            return Action.C
        p = max(0, 1 - total_contributions / (self.n_players - 1))
        if state.round_number < 3:
            p = max(0, 0.5 - total_contributions / (self.n_players - 1))
        if random.random() > p:
            return Action.D
        else:
            return Action.C
        if not history.my_actions[-1]:
            q = 0.5
            if random.random() < q:
                return Action.D

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    Gradually increase cooperation based on the group's past behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contrib_rate = 0.0
        self.my_contributions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(self.my_contributions)
        self.avg_contrib_rate = total_contributions / (state.round_number + 1)
        theta = min(1, max(0, self.game_description.k / self.game_description.n_players + 0.2 * self.avg_contrib_rate))
        if state.round_number == self.game_description.n_rounds - 1:
            previous_cooperators = sum(history.opponent_actions[-1, :])
            return Action.C if previous_cooperators >= self.game_description.n_players // 2 else Action.D
        if self.avg_contrib_rate >= theta:
            action = Action.C
        else:
            action = Action.D
        self.my_contributions.append(1 if action == Action.C else 0)
        return action

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Collective strategy "Adaptive Harmony" aims to balance individual self-interest 
    with the pursuit of collective welfare in a Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        total_contributions = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :])
        if total_contributions >= self.game_description.n_players * self.game_description.k / 2:
            self.cooperation_probability = min(self.cooperation_probability * 1.2, 1)
        else:
            self.cooperation_probability *= 0.7
        for i in range(self.game_description.n_players - 1):
            if self.opponent_cooperation_rates[i] > 0.5:
                self.cooperation_probability = min(self.cooperation_probability * 1.2, 1)
            elif self.opponent_cooperation_rates[i] < 0.3:
                self.cooperation_probability *= 0.7
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            self.cooperation_probability *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if random.random() < self.cooperation_probability:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    Adaptive cooperation with gradual escalation in public goods game.

    Cooperates initially and adapts based on group behavior, punishing free-riders
    while forgiving those who start cooperating again.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_history = np.zeros(game_description.n_players - 1, dtype=np.float64)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T = min(0.5, self.game_description.k / self.game_description.n_players)
        avg_opponent_cooperation = np.mean(history.opponent_actions[-1, :])
        if avg_opponent_cooperation > T:
            return Action.C
        p = avg_opponent_cooperation / T
        escalate_defection = random.random() < 1 - p
        if escalate_defection:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if not opponent_action:
                    self.opponent_cooperation_history[i] *= 0.8
                else:
                    self.opponent_cooperation_history[i] = min(1, self.opponent_cooperation_history[i] * 1.2)
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_contributions = np.sum(history.my_actions) + np.sum(np.sum(history.opponent_actions, axis=1))
            max_total_contributions = state.round_number * self.game_description.n_players
            return Action.C if total_contributions >= 0.75 * max_total_contributions else Action.D
        if sum(self.opponent_cooperation_history) < (self.game_description.n_players - 1) / 2:
            return Action.C if avg_opponent_cooperation >= 0.5 else Action.D
        return Action.C

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare by adapting to the group's behavior and encouraging cooperation through reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(np.concatenate(([history.my_actions[-1]], previous_round_opponent_actions)))
        if average_contribution > 0.5:
            return Action.C
        if n_players == 1 or (n_players == 2 and state.round_number == self.game_description.n_rounds - 1):
            return Action.C
        if np.all(history.my_actions) == False:
            return random.choices([Action.D], weights=[0.8])[0]
        if average_contribution < 0.5:
            self.retaliation_probability = (1 - average_contribution) * 2
            if np.random.rand() < self.retaliation_probability:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if average_contribution == 0.5:
            return random.choices([Action.C], weights=[0.6])[0]
        return Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    This strategy promotes cooperation while being robust to a wide range of opponent behaviors.
    It encourages initial cooperation, adapts to the group's behavior through observation and retaliation,
    gradually increases or decreases contributions based on the group's performance, and forgives past defections when cooperation is re-established.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_level = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        total_contributions_last_round = sum(opponent_actions_last_round) + int(history.my_actions[-1])
        avg_contribution_last_round = total_contributions_last_round / self.game_description.n_players
        cooperation_threshold = self.game_description.k / self.game_description.n_players * 0.6
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_contribution_last_round >= cooperation_threshold and sum(opponent_actions_last_round) > 0 and (history.my_payoffs[-1] > 1 - int(history.my_actions[-1])):
            self.retaliation_level = min(self.retaliation_level + 0.2, 1.0)
            return Action.C
        if avg_contribution_last_round < cooperation_threshold:
            self.retaliation_level = max(self.retaliation_level - 0.2, 0.0)
            return Action.D
        if history.my_payoffs[-1] <= 1 - int(history.my_actions[-1]) and sum(opponent_actions_last_round) > 0:
            self.retaliation_level = max(self.retaliation_level - 0.2, 0.0)
            return Action.D
        if state.round_number >= 3 and sum((sum(history.opponent_actions[-i - 1, :]) for i in range(3))) + int(sum(history.my_actions[-3:])) == self.game_description.n_players * 3:
            self.retaliation_level = 1.0
        return Action.C if self.retaliation_level >= 0.5 else Action.D

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Balances individual self-interest with collective welfare by adapting to the group's behavior and gradually adjusting cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.game_description.n_players // 2:
            average_past_contributions = np.mean(history.my_actions)
            if average_past_contributions > 0.5:
                return Action.C
            else:
                p_defect = average_past_contributions ** 2
                return Action.D if random.random() < p_defect else Action.C
        elif self.game_description.n_players // 2 <= state.round_number < 3 * self.game_description.n_players // 4:
            total_past_contributions = sum(history.my_actions)
            if total_past_contributions > state.round_number / 2:
                return Action.C
            else:
                p_defect = 1 - np.mean(history.my_actions)
                return Action.D if random.random() < p_defect else Action.C
        elif state.round_number >= 3 * self.game_description.n_players // 4:
            average_payoff = np.mean(history.opponent_payoffs)
            if average_payoff > self.game_description.k / 2:
                return Action.C
            else:
                p_defect = np.mean(history.my_actions) ** 2
                return Action.D if random.random() < p_defect else Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperation with Conditional Punishment"
    
    This strategy initially cooperates and adapts to the group's behavior, punishing free-riders conditionally.
    It balances individual self-interest with collective welfare, making it a robust approach to playing the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        average_contribution = opponent_cooperations / self.game_description.n_players
        if history.my_actions[-1] == Action.C and np.any(history.opponent_actions[-1, :] == Action.D):
            return Action.D
        elif history.my_actions[-1] == Action.D and np.all(history.opponent_actions[-1, :] == Action.C):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        p = (1 - self.game_description.k / self.game_description.n_players) * average_contribution
        if self.game_description.n_players == 1:
            return Action.D
        if average_contribution > 0.5 or (average_contribution == 0.5 and random.random() < 0.5):
            return Action.C
        else:
            return Action.D if random.random() < p else Action.C

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    A collective strategy for public goods games, balancing individual incentives with 
    collective welfare by adapting to group behavior and gradually punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_count = {}
        for i in range(game_description.n_players):
            self.defection_count[i] = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_round_contributions > (self.game_description.n_players - 1) / self.game_description.k:
            return Action.C
        elif prev_round_contributions > 0 and prev_round_contributions <= (self.game_description.n_players - 1) / self.game_description.k:
            prob_contribute = prev_round_contributions / self.game_description.n_players
            return Action.C if random.random() < prob_contribute else Action.D
        else:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if not opponent_action and i != history.my_actions.shape[0]:
                    self.defection_count[i] += 1
                    if self.defection_count[i] > 2:
                        return Action.D
            return Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if opponent_action and self.defection_count[i] > 0:
                self.defection_count[i] = 0

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        if self.game_description.n_players == 2 and history is not None:
            return Action.D if history.my_actions[-1] else Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(prev_opponent_actions)
        defectors = self.game_description.n_players - 1 - cooperators
        if self.retaliation_rounds > 0:
            self.retaliation_rounds -= 1
            return Action.D
        if cooperators >= (self.game_description.n_players - 1) // 2 and (not self.retaliation_rounds):
            return Action.C
        if defectors == 1 or defectors == 2:
            self.retaliation_rounds = 1
        elif defectors >= 3:
            self.retaliation_rounds = 2
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents and encourages cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_contributions = np.sum(history.opponent_actions[-1, :])
        avg_prev_contrib = opponent_contributions / self.game_description.n_players
        avg_prev_payoff = np.mean(history.my_payoffs)
        if avg_prev_contrib >= 0.5 or (state.round_number == self.game_description.n_rounds - 1 and avg_prev_contrib > 0):
            return Action.C
        if state.round_number < self.game_description.n_rounds - 1:
            self.p_coop = min(max(self.p_coop + (avg_prev_payoff - 1) / 2, 0.05), 0.95)
        if history.my_actions.all() == False:
            self.p_coop = max(0.05, self.p_coop * 0.9)
        return Action.C if random.random() < self.p_coop else Action.D

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Selfish Adjustment.
    
    This strategy initially cooperates to establish a baseline of cooperation and encourage others to do the same. 
    It then adapts its behavior based on the previous round's total contributions, punishing non-cooperation and adjusting selfishly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.phi = 0.25
        self.selfish_adjustment_rounds = 3
        self.payoff_threshold_ratio = 0.8
        self.rounds_defecting_selfishly = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and len(history.my_actions) < self.selfish_adjustment_rounds):
            return Action.C
        avg_contrib = np.mean(history.opponent_actions[-1, :])
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        max_possible_contributions = self.game_description.n_players
        if total_contributions < self.phi * max_possible_contributions:
            return Action.D
        group_average_payoff = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs[:, -1])))
        personal_payoff_ratio = history.my_payoffs[-1] / group_average_payoff if group_average_payoff != 0 else float('inf')
        if personal_payoff_ratio < self.payoff_threshold_ratio and (not (len(history.my_actions) == state.round_number and all(history.my_actions))):
            self.rounds_defecting_selfishly += 1
            return Action.D
        if avg_contrib >= self.theta:
            self.rounds_defecting_selfishly = max(0, self.rounds_defecting_selfishly - 1)
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    A collective adaptive cooperation strategy with forgiveness mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        tcr = np.mean(history.my_actions[-1:] + history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if np.sum(history.my_actions[-1:] + history.opponent_actions[-1, :]) >= self.game_description.n_players * self.threshold:
            return Action.C
        if tcr > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation and gradually retaliates against defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_round_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            previous_cooperation = history.my_actions[-1]
            cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if previous_cooperation and cooperation_rate > 0.5:
                return Action.C
            elif not previous_cooperation and cooperation_rate > np.mean(history.opponent_actions[-2:, :].mean(axis=0)):
                return Action.C
            else:
                return Action.D
        else:
            previous_two_rounds_cooperation = history.my_actions[-2:]
            cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if all(previous_two_rounds_cooperation) and cooperation_rate > 0.75:
                return Action.C
            elif any((not x for x in previous_two_rounds_cooperation)):
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation to achieve mutual benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.escalation_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        average_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        my_previous_payoff = history.my_payoffs[-1]
        if state.round_number > 1:
            previous_total_contributions = sum(history.opponent_actions[-2, :]) + (history.my_actions[-2] == Action.C)
            if total_contributions < previous_total_contributions:
                self.escalation_counter += 1
        if total_contributions >= self.game_description.n_players * self.game_description.k / 2:
            return Action.C
        elif total_contributions < self.game_description.n_players * self.game_description.k / 2 and average_payoff > self.game_description.k / self.game_description.n_players:
            cooperation_probability = average_payoff / (self.game_description.k / self.game_description.n_players)
            return Action.C if random.random() < cooperation_probability else Action.D
        elif self.escalation_counter >= 3:
            return Action.D
        elif all(history.opponent_actions[-1, :] == False):
            return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    This strategy implements the Collective Gradual Cooperation with Adaptive Punishment approach.
    It balances individual self-interest with collective welfare by gradually increasing cooperation 
    while adapting to the group's behavior. It relies on reciprocity, punishment, and forgiveness 
    to maintain a high level of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_high = 0.5
        self.cooperation_threshold_low = 0.2
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 5:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            opponent_cooperations = np.sum(history.opponent_actions[-1, :])
            avg_contribution = (opponent_cooperations + 1) / self.game_description.n_players
        else:
            return Action.C
        if avg_contribution > self.cooperation_threshold_high and (not self.punishment_mode):
            return Action.C
        elif avg_contribution < self.cooperation_threshold_low and (history is None or len(history.opponent_actions) < 2 or np.sum(history.opponent_actions[-2, :]) / self.game_description.n_players < self.cooperation_threshold_low):
            self.punishment_mode = True
            return Action.D
        elif avg_contribution > self.cooperation_threshold_high and self.punishment_mode:
            self.punishment_mode = False
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    This strategy aims to promote collective welfare by encouraging initial cooperation, 
    adapting to the group's behavior, implementing a gentle punishment mechanism, and forgiving past transgressions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            return Action.D
        if total_contributions < self.game_description.k / 4 and state.round_number < self.game_description.n_rounds - 3:
            return Action.C
        average_expected_value = self.game_description.k / 2
        if total_contributions > average_expected_value:
            return Action.C
        else:
            return Action.D
        if history.my_actions[-1] == False and (total_contributions < average_expected_value or state.round_number < self.game_description.n_rounds - 3):
            self.punishment_rounds_left = 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    A strategy that adapts to opponents' behavior by cooperating initially, 
    gradually punishing defectors, and forgiving those who start contributing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players * (game_description.n_players - 1)
        self.punishment_state = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishment_state is None:
            self.punishment_state = {i: 1.0 for i in range(self.game_description.n_players)}
        opponent_actions_last_round = history.opponent_actions[-1, :]
        avg_contrib_last_round = np.mean(opponent_actions_last_round)
        if avg_contrib_last_round >= self.theta:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_defectors_last_round = np.where(~opponent_actions_last_round)[0]
        for i in opponent_defectors_last_round:
            if self.punishment_state[i] > 0.1:
                self.punishment_state[i] -= 0.1
        opponent_cooperators_last_round = np.where(opponent_actions_last_round)[0]
        for i in opponent_cooperators_last_round:
            if self.punishment_state[i] < 1.0:
                self.punishment_state[i] += 0.1
        if random.random() < np.mean(list(self.punishment_state.values())):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    A strategy that balances individual incentives with collective welfare, 
    promoting cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_players = set()
        self.forgiven_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        avg_contribution_rate = np.mean(np.concatenate(([True], prev_round_actions)))
        for player_idx, (action, payoff) in enumerate(zip(prev_round_actions, history.opponent_payoffs[-1, :])):
            if action and payoff > np.mean(history.my_payoffs):
                self.punished_players.add(player_idx)
        for player_idx in list(self.punished_players):
            if prev_round_actions[player_idx]:
                self.forgiven_players.add(player_idx)
                self.punished_players.remove(player_idx)
        if avg_contribution_rate >= 0.5:
            return Action.C
        if any((history.opponent_actions[-1, player_idx] for player_idx in self.punished_players)):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        num_cooperators = np.sum(prev_round_actions)
        if num_cooperators >= self.game_description.n_players // 2 or (num_cooperators == 0 and state.round_number > 0):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = None
        self.tau = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > self.game_description.n_players / 2 else Action.D
        opponents_cooperated = sum(history.opponent_actions[-1, :])
        average_payoff_per_contributor = np.mean(np.where(opponents_cooperated + history.my_actions[-1] == self.game_description.n_players, self.game_description.k / self.game_description.n_players * (opponents_cooperated + 1), 0))
        if self.alpha is None:
            self.alpha = opponents_cooperated + history.my_actions[-1]
        else:
            self.alpha = (self.alpha * (state.round_number - 2) + opponents_cooperated + history.my_actions[-1]) / (state.round_number - 1)
        self.tau = self.game_description.k / self.game_description.n_players * self.alpha
        if average_payoff_per_contributor > self.tau:
            return Action.C
        else:
            beta = 0.2
            opponents_defected = sum(1 - history.opponent_actions[-1, :])
            if opponents_defected <= beta * self.game_description.n_players:
                return Action.D
            if state.round_number == self.game_description.n_rounds:
                return Action.C if self.game_description.k > self.game_description.n_players / 2 else Action.D
            elif opponents_defected == self.game_description.n_players - 1:
                self.alpha = None
                self.tau = None
        return Action.D

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    A collective strategy that promotes cooperation by adapting to changes in 
    collective payoffs and punishing persistent defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players
        self.withdrawn_contribution_rate = 1.0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if avg_payoff >= self.threshold:
            self.consecutive_defections = 0
            self.withdrawn_contribution_rate = 1.0
            return Action.C
        else:
            self.consecutive_defections += 1
            if self.consecutive_defections > 1:
                self.withdrawn_contribution_rate *= 0.5
            if history.my_payoffs[-1] > 0 and np.mean(history.opponent_actions[-1, :]):
                return Action.C
            else:
                return Action.D if state.round_number < self.game_description.n_rounds - 1 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Escalation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        average_contribution = opponent_cooperations / self.game_description.n_players
        if average_contribution > 0.5:
            return Action.C
        self.defection_probability = min(self.defection_probability + 0.1, 1)
        if state.round_number > 1:
            opponent_actions_last_two_rounds = history.opponent_actions[-2:, :]
            if np.all(opponent_actions_last_two_rounds[0, :] == False) and np.any(opponent_actions_last_two_rounds[1, :]):
                return Action.D
            elif average_contribution > 0.5:
                self.defection_probability = 0.0
        if state.round_number % 2 == 0:
            self.defection_probability = min(self.defection_probability + 0.1, 1)
        return Action.D if random.random() < self.defection_probability else Action.C

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents in a public goods game.
    Starts with cooperation, then adjusts based on the number of cooperators and a sensitivity parameter.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.sensitivity_parameter = 0
        self.contribution_threshold = self.game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators < self.contribution_threshold:
            self.sensitivity_parameter += 1
            if self.sensitivity_parameter >= 3:
                self.contribution_threshold *= 0.9
        else:
            self.sensitivity_parameter = 0
        if cooperators < self.game_description.n_players * 0.2 and state.round_number > 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if cooperators >= self.contribution_threshold:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    A strategy that encourages cooperation by initially contributing,
    reciprocating cooperation, punishing defection, and adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_rounds - 1 <= state.round_number:
            return Action.D
        elif (history.my_actions == False).all():
            return Action.C
        avg_contribution_rate = history.my_payoffs.mean() / self.game_description.k
        if avg_contribution_rate > 0.5:
            self.threshold -= 0.1
        elif avg_contribution_rate < 0.5:
            self.threshold += 0.1
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators / self.game_description.n_players >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors and prioritizes collective welfare.
    It initially establishes a cooperative baseline, then adjusts its level of cooperation based on others' actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_cooperators = np.sum(prev_round_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_round_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif prev_round_cooperators < self.game_description.n_players / 3:
            return Action.D
        elif history.my_actions[-1] == False and prev_round_cooperators >= 2 * self.game_description.n_players // 3:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    Implement the "Adaptive Cooperation with Gradual Reciprocity" strategy.
    
    This collective strategy prioritizes cooperation while adapting to opponents' behavior,
    promoting a balanced approach between individual and collective interests.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.p_d = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.n_players == 1:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-1, :]
        recent_cooperators = np.sum(recent_opponent_actions)
        if recent_cooperators >= self.n_players / 2:
            return Action.C
        opponent_average_cooperation_rates = []
        for i in range(self.n_players - 1):
            opponent_history = history.opponent_actions[:, i]
            average_rate = np.mean(opponent_history[:-1])
            opponent_average_cooperation_rates.append(average_rate)
        if len(opponent_average_cooperation_rates) > 0:
            average_opponent_cooperation_rate = np.mean(opponent_average_cooperation_rates)
            if average_opponent_cooperation_rate > 0.5:
                return Action.C if random.random() < average_opponent_cooperation_rate else Action.D
        return Action.C if random.random() > self.p_d else Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if np.sum(history.opponent_actions[-1, :] != history.my_actions[-1]) == 1:
            return Action.C
        if avg_contribution > 0.5:
            return Action.C
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            return Action.D
        total_contribution = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if avg_contribution > 0.7 and (not history.my_actions[-1]):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    A strategy for the Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_average_contribution = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(np.append(history.my_actions[-1], opponent_actions))
        if state.round_number > 0 and (not np.any(history.my_actions[:-1])):
            return Action.D
        if self.previous_average_contribution is None:
            self.previous_average_contribution = average_contribution
        self.previous_average_contribution = average_contribution
        if state.round_number == self.game_description.n_rounds - 1:
            my_total_payoff = np.sum(history.my_payoffs)
            if my_total_payoff > self.game_description.n_players * (self.game_description.k / self.game_description.n_players):
                return Action.C
            else:
                return Action.D
        elif average_contribution > 0.5:
            return Action.C
        elif average_contribution < 0.3:
            return Action.D
        else:
            cooperators = np.sum(opponent_actions)
            if cooperators >= self.game_description.n_players // 2:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.
    
    ACE prioritizes cooperation while adapting to the group's dynamics. 
    By reciprocating cooperation and punishing defections, ACE encourages others to contribute to the public good. 
    The escalation mechanism promotes increased cooperation when individual contributions are consistently above average, 
    fostering a culture of mutual support.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_window = 3
        self.escalation_window = 5
        self.my_cooperation_streak = 0
        self.opponent_defection_streaks = np.zeros(game_description.n_players - 1, dtype=int)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        cooperators = sum(opponent_actions)
        for i, action in enumerate(opponent_actions):
            if not action:
                self.opponent_defection_streaks[i] += 1
                if self.opponent_defection_streaks[i] > self.punishment_window:
                    return Action.D
            else:
                self.opponent_defection_streaks[i] = 0
        if cooperators / (self.game_description.n_players - 1) > 0.5:
            self.my_cooperation_streak += 1
            if self.my_cooperation_streak >= self.escalation_window and state.round_number < self.game_description.n_rounds - 1:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if history.my_actions[-1] is False:
            self.my_cooperation_streak = 0
            cooperators = sum(history.opponent_actions[:-1, :].mean(axis=0))
            if cooperators / (self.game_description.n_players - 1) > 0.5:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    Collective Strategy: "Gradual Reciprocity with Endgame Adaptation"
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        threshold = self.game_description.k / self.game_description.n_players
        if avg_contribution >= threshold and state.round_number < self.game_description.n_rounds - 1:
            return Action.C
        if np.all(history.opponent_actions[-1, :] == False):
            return Action.C
        for opponent_action in history.opponent_actions[-1, :]:
            if not opponent_action and history.my_actions[-1] and (state.round_number > 0):
                self.contribution_probability *= 0.9
            elif opponent_action and (not history.my_actions[-1]) and (state.round_number > 0):
                self.contribution_probability = min(1.0, self.contribution_probability * 1.1)
        if random.random() < self.contribution_probability:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and avg_contribution < threshold:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation.
    It is adaptive, robust, and designed to perform well against a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_low_average_contributions = 0
        self.opponent_defection_count = 0
        self.self_protection_trigger = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.my_payoffs.size > 0 and np.mean(history.my_payoffs[-1]) < 0.2 * self.game_description.k:
            self.self_protection_trigger = True
        else:
            self.self_protection_trigger = False
        if history.opponent_actions.size > 0 and np.mean(history.opponent_actions[-1, :]) < 0.5:
            self.consecutive_low_average_contributions += 1
        else:
            self.consecutive_low_average_contributions = 0
        p_defect = min(self.consecutive_low_average_contributions / (self.game_description.n_rounds - 1), 1)
        if random.random() < p_defect and (not self.self_protection_trigger):
            return Action.D
        if history.opponent_actions.size > 0 and np.any(history.opponent_actions[-1, :] == False):
            self.opponent_defection_count += 1
        else:
            self.opponent_defection_count = 0
        q_forgive = max(0.7 - self.opponent_defection_count / (self.game_description.n_rounds - 1), 0)
        if random.random() < q_forgive and (not self.self_protection_trigger):
            return Action.C
        if self.self_protection_trigger:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare.
    It employs a gradual reciprocity approach, adapting to the group's behavior while incorporating elements of forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_probabilities = [0.0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_contributions = sum(opponent_actions) + (history.my_actions[-1] == True)
        average_contribution_per_player = total_contributions / self.game_description.n_players
        if average_contribution_per_player >= 0.5:
            return Action.C
        else:
            defector_count = self.game_description.n_players - total_contributions
            defect_probability = defector_count / self.game_description.n_players
            for i, opponent_action in enumerate(opponent_actions):
                if not opponent_action and average_contribution_per_player > 0.5:
                    self.punishment_probabilities[i] = 0.7
                elif opponent_action and self.punishment_probabilities[i] > 0:
                    self.punishment_probabilities[i] = 0
            combined_defect_probability = min(1, defect_probability + self.punishment_probabilities[0])
            return Action.D if random.random() < combined_defect_probability else Action.C

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    A strategy that initially cooperates and then adapts to opponents' actions by punishing defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_probabilities = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and np.all(history.opponent_actions[-1, :] == False)):
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.C
        opponent_cooperations = history.opponent_actions[-1, :]
        average_contribution = np.mean(opponent_cooperations)
        if average_contribution < 0.5:
            deviation_from_average = 0.5 - average_contribution
            punishment_probability = min(1.0, max(0.0, deviation_from_average * 2))
            for i in range(self.game_description.n_players - 1):
                if opponent_cooperations[i]:
                    self.punishment_probabilities[i] = min(1.0, self.punishment_probabilities[i] + 0.1)
                else:
                    self.punishment_probabilities[i] = max(0.0, self.punishment_probabilities[i] - 0.1)
            return Action.D if random.random() < punishment_probability else Action.C
        return Action.C

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective welfare,
    adapting to the group's behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_contribution = 0.5 * (game_description.k / game_description.n_players)
        self.last_round_strategy = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.shape[1] == 1:
            return Action.C
        previous_round_contributions = sum(history.opponent_actions[-1, :])
        average_contribution_others = previous_round_contributions / history.opponent_actions.shape[1]
        if previous_round_contributions > self.game_description.k / 2:
            return Action.C
        probability_defect = max(0.0, self.threshold_contribution - average_contribution_others)
        defect_action = random.random() < probability_defect
        action = Action.D if defect_action else Action.C
        if self.last_round_strategy is not None:
            my_previous_payoff = history.my_payoffs[-1]
            total_previous_payoffs = np.sum(history.my_payoffs) + np.sum(np.mean(history.opponent_payoffs, axis=1))
            total_current_payoffs = total_previous_payoffs - my_previous_payoff
            if state.round_number > 1:
                previous_total_payoffs = np.sum(history.my_payoffs[-2]) + np.sum(np.mean(history.opponent_payoffs[:-1, :], axis=1))
            if state.round_number == 1 or total_current_payoffs < previous_total_payoffs:
                action = Action.D if action == Action.C else Action.C
        self.last_round_strategy = action
        return action

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) balances individual self-interest with collective welfare by adapting to the group's contribution history.
    It aims to maximize overall payoffs while minimizing exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        avg_contribution_rate = total_contributions / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if avg_contribution_rate >= 0.5:
            return Action.C
        elif total_contributions == 0:
            return Action.D
        else:
            p_defect = (1 - avg_contribution_rate) / (2 - avg_contribution_rate)
            return Action.C if random.random() > p_defect else Action.D



class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal.
    
    This strategy starts by cooperating in the first round to establish a positive tone. 
    Then, it adapts its cooperation level based on the average contribution rate of all players from previous rounds. 
    A dynamic threshold is introduced to decrease over time if the average contribution rate falls below 0.5. 
    Additionally, a punishment mechanism is implemented to discourage persistent defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contribution = None
        self.theta = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution = np.mean(np.sum(history.opponent_actions[:-1, :], axis=0) / (state.round_number - 1))
        if self.avg_contribution is None:
            self.avg_contribution = avg_contribution
        else:
            self.avg_contribution = 0.5 * self.avg_contribution + 0.5 * avg_contribution
        if self.avg_contribution < 0.5 and self.theta > 0.05:
            self.theta -= 0.05
        opponent_defections = np.sum(history.opponent_actions[:-1, :] == False, axis=0)
        recent_opponent_defections = np.sum(history.opponent_actions[-2:, :] == False, axis=0)
        punished_players = (opponent_defections >= 2) & (recent_opponent_defections >= 1)
        if np.any(punished_players):
            return Action.D
        if self.avg_contribution >= self.theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for Public Goods Game.
    
    This strategy adapts to various opponent behaviors and aligns with the collective mindset,
    prioritizing collective welfare by encouraging initial cooperation, reciprocity, punishing free-riding behavior,
    learning from others' cooperative actions, and adapting to changing group dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_probability = 0.5
        self.round_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = history.opponent_actions[-1, :]
        total_contributions = sum(opponent_cooperations) + (history.my_actions[-1] == Action.C)
        if total_contributions > self.game_description.n_players / 2:
            return Action.C
        my_payoff = history.my_payoffs[-1]
        my_contribution = int(history.my_actions[-1] == Action.C)
        if my_payoff > my_contribution:
            return Action.D
        if sum(opponent_cooperations) > self.game_description.n_players / 2:
            self.contribution_probability *= 1.2
            self.contribution_probability = min(1, self.contribution_probability)
        self.round_counter += 1
        if self.round_counter % 5 == 0:
            avg_contributions = np.mean(history.my_actions[-5:])
            if avg_contributions > 0.7:
                self.contribution_probability *= 1.1
            elif avg_contributions < 0.3:
                self.contribution_probability /= 1.1
            self.contribution_probability = max(0, min(1, self.contribution_probability))
        return Action.C if random.random() < self.contribution_probability else Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    A robust, adaptive strategy for the Public Goods Game that balances individual self-interest with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.avg_payoff_recent = []
        self.avg_payoff_all = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contrib_prev = np.mean(history.opponent_actions[-1, :])
        if avg_contrib_prev >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        if len(self.avg_payoff_recent) > 2:
            recent_avg = np.mean(self.avg_payoff_recent)
            all_avg = np.mean(self.avg_payoff_all)
            if recent_avg > all_avg:
                self.threshold -= 0.1
            elif recent_avg < all_avg:
                self.threshold += 0.1
        for i in range(len(history.opponent_actions[-1])):
            if history.opponent_actions[-1, i] == False and avg_contrib_prev > self.threshold:
                action = Action.D
                break
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C
        total_cooperation = np.sum(history.opponent_actions[-1, :]) + 1
        if total_cooperation < self.game_description.n_players * 0.2:
            action = Action.D
        if history.my_payoffs is not None and len(history.my_payoffs) > 0:
            payoff = history.my_payoffs[-1]
            self.avg_payoff_all.append(payoff)
            self.avg_payoff_recent.append(payoff)
            if len(self.avg_payoff_recent) > 3:
                self.avg_payoff_recent.pop(0)
        return action

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    Adaptive Collective Conscience (ACC) strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        prev_payoffs = history.opponent_payoffs[-1, :]
        cooperators = sum(prev_actions)
        if cooperators < self.game_description.n_players // 2 and my_prev_action == Action.C:
            avg_payoff = np.mean(prev_payoffs)
            payoff_diff = avg_payoff - prev_payoffs[0]
            punishment_prob = min(max(payoff_diff, 0) / (self.game_description.k * self.game_description.n_players), 1)
            if random.random() < punishment_prob:
                return Action.D
        elif my_prev_action == Action.D and cooperators >= self.game_description.n_players // 2:
            forgiveness_prob = 0.5
            if random.random() < forgiveness_prob:
                return Action.C
        deviators = sum(prev_actions == False)
        if deviators <= 1:
            return Action.C
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation while maintaining a balance between punishing free-riders and forgiving occasional mistakes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.my_history is not None and len(self.my_history) < state.round_number:
            self.my_history.append(history.my_actions[-1])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.shape[0] <= 2 or np.all(~history.opponent_actions):
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if avg_contribution >= 0.5:
            return Action.C
        total_contributions_last_two_rounds = np.sum(history.opponent_actions[-2:, :], axis=(0, 1)) + np.sum(history.my_actions[-2:])
        if total_contributions_last_two_rounds < 2 * 0.5 * self.game_description.n_players:
            return Action.D
        if history.my_actions[-1] and (self.my_history is not None and np.any(self.my_history[max(0, len(self.my_history) - 3):])):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.free_riders = set()
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contrib = np.mean(history.opponent_actions[-1, :])
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if not action and history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players:
                self.free_riders.add(i)
        if len(self.free_riders) > 0 and (not self.punished):
            return Action.D
        for i in self.free_riders:
            if history.opponent_actions[-1, i]:
                self.free_riders.remove(i)
                self.punished = False
                break
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_contrib >= 0.5 or (avg_contrib == 0.5 and (not self.free_riders)):
            return Action.C
        if avg_contrib < 0.5:
            return Action.D
        if sum(history.opponent_actions[-1, :]) == 0 and (not self.punished):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the pursuit of collective welfare.
    It adapts behavior based on game history and parameters, encouraging cooperation while punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        theta = math.floor(self.game_description.k / self.game_description.n_players)
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        if history.my_actions[-1] == False and history.opponent_actions[-1, :].all():
            phi = 2
            if state.round_number >= history.my_payoffs.shape[0] - phi:
                return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators > self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D
        if cooperators == self.game_description.n_players // 2:
            return Action.C
        if history.opponent_actions[-1, :].any() and (not history.my_actions[-1]):
            self.punishment_rounds = theta

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    Collective Strategy: "Gradual Reciprocity with Adaptive Punishment"
    
    This strategy prioritizes collective welfare by encouraging cooperation through 
    reciprocity and punishment mechanisms, adapting to the group's behavior to maintain a 
    balance between individual and collective interests.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_opponent = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            return Action.C
        if self.punish_opponent is not None:
            action = Action.D
            self.punish_opponent = None
        else:
            opponent_contributions = history.opponent_actions[-1, :]
            avg_contribution = np.mean(opponent_contributions)
            if avg_contribution >= 0.5:
                action = Action.C
            else:
                defect_probability = (0.5 - avg_contribution) / 0.5
                action = Action.D if random.random() < defect_probability else Action.C
            if history.my_actions[-1] == Action.C and opponent_contributions.any():
                punish_probability = 0.7
                self.punish_opponent = True if random.random() < punish_probability else None
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        return action

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    Adaptive Cooperativity with Gradual Punishment strategy for N-Player Public Goods Game.
    Balances individual incentives with collective welfare, adapting to the behavior of others while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.forgiveness_scores = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.forgiveness_scores = [0] * self.game_description.n_players
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        opponents_contribution_rate = np.mean(history.opponent_actions)
        my_previous_action = history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif opponents_contribution_rate < 0.5 and my_previous_action == True:
            return Action.D
        else:
            for i, opponent in enumerate(history.opponent_actions[-1, :]):
                if not opponent and history.my_actions[-2] == False and (self.forgiveness_scores[i] < 3):
                    self.forgiveness_scores[i] += 1
            if sum(self.forgiveness_scores) / len(self.forgiveness_scores) > self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    Adaptive Cooperative Optimism strategy for N-player Public Goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reset_state()

    def reset_state(self):
        self.consecutive_defects = 0
        self.exploited = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.game_description.n_players / 2:
            self.consecutive_defects = 0
            self.exploited = False
            return Action.C
        elif previous_round_cooperators > 0 and (not self.exploited):
            defect_probability = (self.game_description.n_players - previous_round_cooperators) / self.game_description.n_players
            if random.random() < defect_probability:
                self.consecutive_defects += 1
                if self.consecutive_defects >= 2:
                    self.exploited = True
                return Action.D
            else:
                return Action.C
        elif previous_round_cooperators == 0 or (self.exploited and self.consecutive_defects < 4):
            self.consecutive_defects += 1
            if self.consecutive_defects >= 2:
                self.reset_state()
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1 and history.my_actions[-1]:
            if sum(history.opponent_actions[-2, :]) >= self.game_description.n_players / 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    An adaptive strategy for the Public Goods Game that prioritizes collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C] * 8 + [Action.D])
        if state.round_number <= 1 and history.my_actions[-1]:
            self.exploitation_count += 1
        total_contributions = sum(history.opponent_actions[-1, :])
        if total_contributions < self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            self.exploitation_count += 1
        else:
            self.exploitation_count = max(0, self.exploitation_count - 1)
        if history.opponent_actions[-1, :].sum() >= self.game_description.n_players // 2 or (self.exploitation_count <= 0 and state.round_number > 2):
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 3:
            return Action.C
        if self.exploitation_count >= 2 and history.my_actions[-1]:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    A collective strategy for the Public Goods Game (PGG) that adapts to a wide range of opponent behaviors.
    It prioritizes collective welfare by encouraging cooperation, punishing defectors, and forgiving past defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_streaks = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.all(history.opponent_actions[-1, :] == True):
            return Action.C
        my_last_action = history.my_actions[-1]
        opponent_last_actions = history.opponent_actions[-1, :]
        for i, (opponent_defected, opponent_defection_streak) in enumerate(zip(opponent_last_actions == False, self.opponent_defection_streaks)):
            if my_last_action and opponent_defected:
                self.opponent_defection_streaks[i] += 1
                if self.opponent_defection_streaks[i] >= 3:
                    return Action.D
            elif not my_last_action and (not opponent_defected):
                self.opponent_defection_streaks[i] = max(0, self.opponent_defection_streaks[i] - 1)
        average_contribution = np.mean(history.my_actions[-3:] if len(history.my_actions) >= 3 else history.my_actions)
        if average_contribution > 0.5:
            return Action.C
        return Action.D



class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    Adaptive Reciprocity with Conditional Cooperation strategy.
    
    This strategy initiates collective contribution by cooperating in the first round. 
    In subsequent rounds, it adapts to the group's contribution level and opponents' behaviors.
    It rewards contributors, punishes free-riders, and reciprocates generosity from other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = []
        self.punishment_rounds = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(self.opponent_contributions) < self.game_description.n_players - 1:
            self.opponent_contributions = [0.0] * (self.game_description.n_players - 1)
        for i, action in enumerate(history.opponent_actions[-1, :]):
            opponent_contribution = float(action)
            self.opponent_contributions[i] += opponent_contribution / state.round_number
        if history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :]):
            for i in range(self.game_description.n_players - 1):
                if not history.opponent_actions[-1, i]:
                    if i not in self.punishment_rounds:
                        self.punishment_rounds[i] = state.round_number + 2
        opponent_defectors = [i for i, action in enumerate(history.opponent_actions[-1, :]) if not action and i in self.punishment_rounds and (state.round_number < self.punishment_rounds[i])]
        if len(opponent_defectors) > 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_contributions = sum(history.opponent_actions[-1, :]) + 1
            if total_contributions >= self.game_description.k * (self.game_description.n_players / self.game_description.n_players):
                return Action.C
            else:
                return Action.D
        opponent_reciprocation = [i for i in range(self.game_description.n_players - 1) if history.opponent_actions[-1, i] and self.opponent_contributions[i] > np.percentile(self.opponent_contributions, 75)]
        if len(opponent_reciprocation) > 0:
            return Action.C
        avg_contribution = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        threshold = self.game_description.k / self.game_description.n_players / 2
        if avg_contribution > threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    Strategy that balances individual self-interest with collective welfare,
    adapting to the behavior of others while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reciprocity_scores = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        avg_contrib = np.mean(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if avg_contrib > self.game_description.k / self.game_description.n_players:
            for i in range(self.game_description.n_players - 1):
                if history.opponent_actions[-1, i] and history.my_actions[-1]:
                    self.reciprocity_scores[i] += 1
                elif not history.opponent_actions[-1, i] and history.my_actions[-1]:
                    self.reciprocity_scores[i] -= 2
            if np.sum(self.reciprocity_scores > 0) >= (self.game_description.n_players - 1) // 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    A strategy that adapts to opponents' actions by initially cooperating, 
    then responding to majority cooperation or defection. It also gradually 
    forgives past defectors and punishes persistent ones.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.game_description.n_players // 2:
            self.consecutive_defections = 0
            return Action.C
        else:
            if history.my_actions[-1] == False:
                self.consecutive_defections += 1
            elif self.consecutive_defections > 0 and opponent_cooperators > 0:
                self.consecutive_defections -= 1
            if self.consecutive_defections >= 3:
                return Action.D
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            elif history.opponent_actions.shape[1] == 1:
                return Action.C if history.my_actions[-1] else Action.D
            return Action.D

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Optimism (ACO)
    Balances individual self-interest with collective welfare by adapting to the group's behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.contribution = 1
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if history.my_actions[-1] and (not np.any(history.opponent_actions[-1, :])):
            return Action.D
        if self.punishment_phase:
            self.punishment_phase = False
            return Action.D
        cr = np.mean(history.opponent_actions[-1, :])
        if cr >= self.threshold:
            self.contribution = 1
        else:
            self.contribution = 0
        payoff_percentage = history.my_payoffs[-1] / (self.game_description.k * self.game_description.n_players)
        if payoff_percentage < 0.5:
            self.contribution -= 1
            if self.contribution < 0:
                self.contribution = 0
        elif payoff_percentage > 0.75:
            self.contribution += 1
            if self.contribution > 1:
                self.contribution = 1
        if np.sum(history.opponent_actions[-1, :]) < self.game_description.n_players / 2:
            self.punishment_phase = True
        return Action.C if self.contribution > 0 else Action.D

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective welfare by adapting to the group's behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_contribution = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        previous_round_contributions = history.opponent_actions[-1, :] + (self.previous_contribution > 0)
        average_contribution = np.mean(previous_round_contributions)
        if average_contribution >= 0.5:
            self.previous_contribution = 1.0
            return Action.C
        elif average_contribution == 0:
            self.previous_contribution = 0.0
            return Action.D
        else:
            self.previous_contribution -= 0.25
            if self.previous_contribution < 0.25:
                self.previous_contribution = 0.25
            return Action.C if self.previous_contribution > 0 else Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    A collective strategy that adapts to the game's history by cooperating when others contribute and punishing non-cooperators with gradual retaliation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5 * game_description.n_players
        self.average_payoff_threshold = 1.5 * (game_description.k / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == self.game_description.n_rounds - 1 and history.my_payoffs.mean() > 1):
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] is True)
        average_payoff = np.mean([p if c else 0 for p, c in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :])] + [history.my_payoffs[-1] if history.my_actions[-1] else 0])
        if total_contributions < self.contribution_threshold:
            return Action.D
        if average_payoff > self.average_payoff_threshold or (total_contributions == 0 and state.round_number != self.game_description.n_rounds - 1):
            return Action.C
        defectors = sum((1 for c in history.opponent_actions[-1, :] if not c))
        retaliation_probability = min(defectors / self.game_description.n_players, 1)
        if random.random() < retaliation_probability:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    A strategy that promotes collective optimism by initially cooperating and 
    adapting to the group's behavior, while minimizing losses from exploitative players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperating = True
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions)
        if self.punishing:
            self.punishing = False
            return Action.D
        if cooperators >= self.game_description.n_players // 2:
            self.cooperating = True
            return Action.C
        elif cooperators == self.game_description.n_players // 2 and self.cooperating:
            return Action.C
        if not self.cooperating:
            self.coopering = False
            return Action.D
        if any(opponent_actions) and all(history.opponent_payoffs[-1, :]):
            self.punishing = True
            return Action.D
        self.cooperating = False
        return Action.D

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) Strategy.

    This strategy adapts to a wide range of opponent behaviors by initially cooperating,
    reciprocating cooperation, gradually retaliating against free-riding, and forgiving.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[0.9, 0.1])[0]
        elif state.round_number < 5:
            return random.choices([Action.C, Action.D], weights=[0.9, 0.1])[0]
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        act = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)
        prev_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_contributions >= act:
            return Action.C
        retaliation_prob = max(0.0, min((act - prev_contributions) / self.game_description.n_players, 1.0))
        if random.random() < retaliation_prob:
            return Action.D
        if state.round_number >= 2 and history.my_actions[-1] == True and (history.my_actions[-2] == True):
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Punishment strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_prob = 0.0
        self.reputation_scores = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.reputation_scores is None and history is not None:
            self.reputation_scores = np.ones(self.game_description.n_players - 1)
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        avg_contribution = opponent_cooperations / (self.game_description.n_players - 1)
        if avg_contribution >= 0.5:
            self.punishment_prob = max(0.0, self.punishment_prob - 0.1)
            return Action.C
        else:
            self.punishment_prob = min(self.punishment_prob + 0.1, 1.0)
            if random.random() < self.punishment_prob:
                return Action.D
            my_action = history.my_actions[-1]
            opponent_actions = history.opponent_actions[-1, :]
            for i, action in enumerate(opponent_actions):
                if action and my_action:
                    self.reputation_scores[i] = min(self.reputation_scores[i] + 0.05, 1.0)
                elif not action and my_action:
                    self.reputation_scores[i] = max(self.reputation_scores[i] - 0.1, 0.0)
            opponent_reputations = np.mean(self.reputation_scores) if self.reputation_scores is not None else 1.0
            if opponent_reputations < 0.5:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    A collective strategy that adapts to the group's behavior by cooperating when collective efforts are strong and gradually punishing free-riding when contributions dwindle.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C if history.my_actions[-1] else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif cooperators > 0:
            defect_probability = (self.game_description.n_players / 2 - cooperators) / (self.game_description.n_players / 2)
            return Action.D if random.random() < defect_probability else Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Punishment.
    
    This strategy promotes cooperation while adapting to the group's behavior. 
    By cooperating initially and gradually punishing defectors, we encourage others to contribute, 
    ensuring the collective benefit is maximized. The punishment mechanism prevents exploitation 
    by those who would otherwise take advantage of cooperative players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.punishment_threshold = 2
        self.consecutive_punishments = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if avg_contribution < self.cooperation_threshold:
            self.consecutive_punishments += 1
            if self.consecutive_punishments > self.punishment_threshold:
                return Action.D
        else:
            self.consecutive_punishments = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_defectors = np.sum(history.opponent_actions[-1, :] == False)
        if opponent_defectors == 1 and avg_contribution < self.cooperation_threshold:
            return Action.C
        if avg_contribution >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    A strategy that prioritizes cooperation while adapting to the actions of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_count = 0
        self.defection_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        self.cooperation_count = sum(prev_opponent_actions)
        self.defection_count = len(prev_opponent_actions) - self.cooperation_count
        contribution_threshold_exceeded = self.cooperation_count >= math.ceil((self.game_description.n_players - 1) / self.game_description.k)
        reciprocal_cooperation = self.cooperation_count >= len(prev_opponent_actions) // 2
        punishment_defection = self.defection_count > len(prev_opponent_actions) // 2
        exploitation_prevention_defection = history.my_payoffs is not None and sum(history.my_payoffs < np.mean(history.opponent_payoffs, axis=1)) / state.round_number >= 0.75
        if contribution_threshold_exceeded or reciprocal_cooperation:
            return Action.C
        elif punishment_defection or exploitation_prevention_defection:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    Adaptive Reciprocity with Gradual Escalation strategy.
    
    This strategy promotes cooperation by reciprocating contributions, 
    escalating defection when faced with insufficient cooperation, and 
    restarting cooperation after escalation phases to encourage mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.patience_parameter = 3
        self.escalation_rounds = self.patience_parameter + 1
        self.in_escalation_phase = False
        self.escalation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if np.all(~history.opponent_actions[-1, :]):
            self.in_escalation_phase = False
            self.escalation_count = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_c = np.mean(history.opponent_actions[-1, :])
        if np.sum(history.my_actions[-1:]) + np.sum(history.opponent_actions[-1, :]) < self.game_description.n_players / 2:
            self.escalation_count += 1
            if not self.in_escalation_phase and self.escalation_count > self.patience_parameter:
                self.in_escalation_phase = True
        else:
            self.escalation_count = 0
        if self.in_escalation_phase and np.sum(history.my_actions[-2:]) + np.sum(np.sum(history.opponent_actions[-2:, :], axis=1)) >= self.game_description.n_players:
            self.in_escalation_phase = False
        if avg_c >= 0.5 and (not self.in_escalation_phase):
            return Action.C
        if self.in_escalation_phase or (self.escalation_count > self.patience_parameter and state.round_number < self.game_description.n_rounds - 1):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare,
    adapting to the behavior of other players in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_levels = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.shape[1] == 1:
            if history.opponent_actions[-1, 0]:
                return Action.C
            else:
                return Action.D
        recent_contributions = self.contribution_levels[-3:] if len(self.contribution_levels) >= 3 else self.contribution_levels
        avg_contribution = np.mean(recent_contributions) if recent_contributions else 0
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        total_contributions = prev_round_cooperators + history.my_actions[-1]
        if total_contributions > avg_contribution:
            return Action.C
        else:
            return Action.D
        self.contribution_levels.append(total_contributions)

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    The ACC strategy prioritizes collective welfare by cooperating when others do and punishing non-cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        avg_contribution = np.mean(prev_round_actions)
        if avg_contribution < self.threshold and (not all(prev_round_actions)):
            return Action.D
        if avg_contribution >= self.threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if sum(prev_round_actions) == self.game_description.n_players - 1:
            if history.my_payoffs[-1] > 0:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reciprocity_threshold = 0.1
        self.defector_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round = history.opponent_actions[-1, :]
        prev_contributors = sum(prev_round)
        prev_total_contributions = prev_contributors / self.game_description.k
        reciprocitybonus = np.mean(history.my_payoffs) >= (self.game_description.n_players - 1) / self.game_description.k
        if reciprocitybonus:
            coop_probability = 0.6 + self.reciprocity_threshold
        else:
            coop_probability = 0.5
        if prev_total_contributions >= (self.game_description.n_players - 1) / self.game_description.k:
            return Action.C
        elif prev_contributors > self.game_description.n_players // 2:
            return Action.C if random.random() < coop_probability else Action.D
        else:
            return Action.D
        defector_count = 0
        for action in history.my_actions[-self.defector_threshold:]:
            if action == Action.D:
                defector_count += 1
        if defector_count >= self.defector_threshold:
            return Action.D

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Adaptive Collective Optimism: A cooperative strategy that balances individual 
    self-interest with collective welfare by adapting behavior based on past contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if total_contributions >= self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        if history.my_payoffs.size > 1 and (history.my_actions[-2] == False and history.my_actions[-1] == False) or (history.opponent_actions.shape[0] > 1 and np.all(history.opponent_actions[-2, :] == False) and np.all(history.opponent_actions[-1, :] == False)):
            self.consecutive_defects += 1
        else:
            self.consecutive_defects = 0
        if self.consecutive_defects >= 2:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_contributions > self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.C
        elif sum(history.opponent_actions[-1, :]) >= self.game_description.n_players // 2:
            cooperation_probability = (sum(history.opponent_actions[-1, :]) + history.my_actions[-1]) / self.game_description.n_players
            return Action.C if random.random() < cooperation_probability else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    This strategy implements "Adaptive Reciprocity with Collective Optimism".
    It balances individual self-interest with the need for cooperation to achieve a higher collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        last_round_opponent_actions = history.opponent_actions[-1, :]
        total_contributions_last_round = sum(last_round_opponent_actions) + (history.my_actions[-1] == Action.C)
        average_contribution_last_round = total_contributions_last_round / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 1:
            if total_contributions_last_round > self.game_description.n_players // 2:
                return Action.C
            probability_defect = min(1, max(0, self.game_description.k / self.game_description.n_players * average_contribution_last_round))
            if random.random() < probability_defect:
                return Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            last_round_defectors = sum((not action for action in last_round_opponent_actions))
            if total_contributions_last_round <= self.game_description.n_players // 2 and last_round_defectors > 0:
                probability_defect = min(1, max(0, self.game_description.k / self.game_description.n_players * last_round_defectors / self.game_description.n_players))
                if random.random() < probability_defect:
                    return Action.D
        return Action.C

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    Collective strategy that prioritizes cooperation through gradual reciprocity and forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.past_action = None
        self.opponent_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        average_contribution = opponent_cooperators / self.game_description.n_players
        if history.my_payoffs is not None and history.my_payoffs[-1] < 1:
            self.opponent_defected = True
        if self.past_action == Action.D and average_contribution > 0.5:
            return Action.C
        if self.opponent_defected:
            self.opponent_defected = False
            return Action.D
        if average_contribution > 0.5:
            return Action.C
        else:
            return Action.D
        self.past_action = Action.C if average_contribution > 0.5 else Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Forgiveness strategy.
    
    This strategy prioritizes collective welfare by initially cooperating and 
    encouraging others to do the same. It adapts to the game's dynamics by gradually 
    reducing cooperation in response to defection, while maintaining a willingness 
    to forgive and reset cooperation when opponents demonstrate improved behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_level = 1
        self.defector_count = 0
        self.forgiveness_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        defectors = history.opponent_actions.shape[1] - cooperators
        if defectors > 0:
            self.defector_count += 1
        else:
            self.defector_count = max(0, self.defector_count - 1)
        if self.defector_count >= 1:
            self.cooperation_level -= 1 / self.forgiveness_threshold
        if cooperators == history.opponent_actions.shape[1]:
            self.cooperation_level = min(1, self.cooperation_level + 1 / self.forgiveness_threshold)
        if cooperators >= self.game_description.n_players // 2 or random.random() < self.cooperation_level:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperated_last_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is None or len(history.opponent_actions) < 2:
            return Action.C
        recent_cooperators = sum(history.opponent_actions[-1, :])
        previous_cooperators = sum(history.opponent_actions[-2, :])
        if self.cooperated_last_round is None or recent_cooperators >= self.game_description.n_players // 2:
            self.cooperated_last_round = True
            return Action.C
        if previous_cooperators >= self.game_description.n_players // 2 and recent_cooperators > 0:
            self.cooperated_last_round = True
            return Action.C
        self.cooperated_last_round = False
        return Action.D

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    This strategy aims to balance individual and collective interests by using a combination of conditional cooperation, 
    reciprocity, and exploration. It encourages cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_since_defection = 0
        self.defect_count = np.zeros(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if total_contributions >= self.game_description.n_players * 0.5:
            action = Action.C
        elif history.my_payoffs[-1] < np.mean(history.my_payoffs) and history.my_actions[-1]:
            action = Action.D
        else:
            action = Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if not opponent_action:
                self.defect_count[i] += 1
                if self.defect_count[i] > 2 and self.rounds_since_defection >= 5:
                    action = Action.D
            else:
                self.defect_count[i] = 0
        if state.round_number % 5 == 0:
            action = random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            if np.mean(history.my_payoffs) > 0.5 * (self.game_description.k + 1):
                return Action.C
            else:
                return Action.D
        self.rounds_since_defection += 1
        return action

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the Public Goods Game.

    Aims to balance individual self-interest with collective welfare by initially cooperating,
    reciprocating cooperation, punishing defection, and adapting to changing group dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        reciprocity_condition = np.any(opponent_actions)
        total_contributions = sum(np.append(history.my_actions[-1], opponent_actions))
        optimism_threshold_condition = total_contributions >= self.game_description.k / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif not reciprocity_condition and (not optimism_threshold_condition):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Adaptive Cooperate-to-Thrive strategy for the N-Player Public Goods Game.
    
    This collective strategy balances individual self-interest with collective welfare.
    It prioritizes cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players <= 3:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        unique_actions, counts = np.unique(opponent_actions, return_counts=True)
        if len(unique_actions) == 1:
            return Action(unique_actions[0])
        for i in range(self.game_description.n_players - 1):
            opponent_defection_rate = np.mean(~history.opponent_actions[:, i])
            if opponent_defection_rate > 0.5:
                self.exploiters.add(i)
        average_contribution = np.mean(history.my_actions[-1]) + np.sum(opponent_actions) / (self.game_description.n_players - 1)
        if average_contribution < 0.5 or any((~opponent_actions[i] for i in self.exploiters)):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    Adaptive Cooperation with Gradualism strategy.
    Balances individual self-interest with collective welfare,
    adapting to opponents' behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or (state.round_number == 3 and (not history.opponent_actions.any())):
            return Action.C
        opponent_contributions = np.sum(history.opponent_actions, axis=0)
        opponent_contribution_frequencies = opponent_contributions / state.round_number
        self.cooperation_threshold = np.mean(opponent_contribution_frequencies)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if np.any(history.opponent_actions) else Action.D
        my_contribution_frequency = np.sum(history.my_actions) / state.round_number
        previous_opponents_contributed = np.any(history.opponent_actions[-1, :])
        if my_contribution_frequency >= self.cooperation_threshold and previous_opponents_contributed or (not history.my_actions[-1] and my_contribution_frequency >= self.cooperation_threshold):
            return Action.C
        if np.all(history.my_actions[-2:]) and (not np.any(history.opponent_actions[-2:, :])):
            return Action.D
        if not np.any(history.opponent_actions):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    A collective strategy that adapts to changing circumstances by reciprocating cooperation, 
    deterring exploitation through gradual punishment, and forgiving isolated instances of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        if state.round_number <= self.game_description.n_players // 2:
            return random.choice([Action.C, Action.D])
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation to achieve better collective outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_payoff = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2:
            if history.opponent_actions[-1, 0]:
                return Action.C
            else:
                return Action.D
        total_contribution = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if state.round_number > 1 else True)
        if state.round_number == self.game_description.n_rounds - 1:
            if total_contribution < self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C
        if total_contribution >= self.game_description.n_players / 2:
            return Action.C
        if state.round_number % 5 == 0 and state.round_number > 1:
            average_payoff = np.mean(history.my_payoffs)
            if average_payoff < self.threshold_payoff:
                self.defection_probability = (state.round_number / self.game_description.n_rounds) ** 2
            else:
                self.defection_probability = 0
        if not hasattr(self, 'defection_probability'):
            self.defection_probability = (state.round_number - 1) / ((self.game_description.n_rounds - 1) * 2)
        return Action.D if random.random() < self.defection_probability else Action.C

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation 
    to achieve collective welfare in a Public Goods Game. It adapts to various opponent behaviors 
    while promoting cooperation through gradual punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C
        elif not self.punishment_phase and state.round_number <= 10:
            avg_contribution_rate = np.mean(history.my_actions)
            opponent_avg_contribution_rate = np.mean(np.mean(history.opponent_actions, axis=1))
            if (avg_contribution_rate + opponent_avg_contribution_rate) / 2 > 0.5:
                return Action.C
            else:
                self.punishment_phase = True
                return Action.D
        elif self.punishment_phase and state.round_number <= 10:
            if np.mean(history.opponent_actions[-1, :]) == 1:
                self.punishment_phase = False
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            avg_contribution_rate = np.mean(history.my_actions)
            opponent_avg_contribution_rate = np.mean(np.mean(history.opponent_actions, axis=1))
            if (avg_contribution_rate + opponent_avg_contribution_rate) / 2 > 0.5:
                return Action.C
            else:
                return Action.D
        else:
            avg_contribution_rate = np.mean(history.my_actions)
            opponent_avg_contribution_rate = np.mean(np.mean(history.opponent_actions, axis=1))
            if (avg_contribution_rate + opponent_avg_contribution_rate) / 2 > 0.5:
                return Action.C
            elif self.punishment_phase:
                return Action.D
            else:
                self.punishment_phase = True
                return Action.D
        if state.round_number >= 1 and history.my_actions[-2] != history.my_actions[-1]:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal strategy for N-Player Public Goods Game.
    
    This strategy aims to balance individual and collective welfare by adapting to the group's behavior while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5
        self.punishment_threshold = self.game_description.n_players * 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        avg_contrib = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
        if avg_contrib >= self.contribution_threshold:
            return Action.C
        total_contributions = np.sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        if total_contributions < self.punishment_threshold:
            defect_prob = (self.game_description.n_players - total_contributions) / self.game_description.n_players
            return Action.D if random.random() < defect_prob else Action.C
        avg_payoff = np.mean(history.my_payoffs + history.opponent_payoffs[:, -1])
        if avg_payoff > np.mean(history.my_payoffs[:-1] + history.opponent_payoffs[:, :-1]):
            self.contribution_threshold -= 0.1
        else:
            self.contribution_threshold += 0.1
        return Action.D

    def _get_avg_payoff(self, history: PlayerHistory) -> float:
        return np.mean(history.my_payoffs + history.opponent_payoffs[:, -1])

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    A collective strategy that balances individual incentives with collective welfare in the Public Goods Game.
    It employs a dynamic approach that adapts to the game's history and opponent behaviors, 
    using gradual cooperation and adaptive punishment to encourage cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.in_punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution_prev = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
        total_contributions_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_contributions_prev < self.game_description.k / self.game_description.n_players * 0.5:
            self.in_punishment_phase = True
        if self.in_punishment_phase and avg_contribution_prev >= 0.5:
            self.in_punishment_phase = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_defections = sum((1 for action in history.opponent_actions[-1, :] if not action))
        if opponent_defections <= 1 and avg_contribution_prev >= 0.5:
            self.in_punishment_phase = False
        if self.in_punishment_phase:
            return Action.D
        elif avg_contribution_prev < 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that adapts to the behavior of others,
    prioritizing collective welfare by encouraging cooperation when it is likely to succeed
    and punishing defection when it is not.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if cooperators == 0:
            return Action.C
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif history.my_actions[-1] == True:
            return Action.D
        else:
            return Action.D



class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    Initially cooperates to set a positive tone, then adapts based on the group's behavior.
    Tries to identify potential exploiters and defects against them.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiter_threshold = 0.75
        self.contribution_threshold = self.game_description.n_players // 2
        self.noise_tolerance = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number >= 5 and np.mean(opponent_cooperators) < self.game_description.n_players // 4:
            self.contribution_threshold = self.game_description.n_players // 3
        opponent_defection_rates = np.mean(history.opponent_actions, axis=0)
        if state.round_number >= 10 and any((rate > self.exploiter_threshold for rate in opponent_defection_rates)):
            return Action.D
        if opponent_cooperators >= self.contribution_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors in the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_tracker = np.zeros((game_description.n_players,), dtype=int)
        self.defection_tracker = np.zeros((game_description.n_players,), dtype=int)
        self.cooperation_threshold = 0.5
        self.my_payoffs = []
        self.opponent_average_payoffs = [0.0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action:
                self.contribution_tracker[i] += 1
            else:
                self.defection_tracker[i] += 1
        for i, payoff in enumerate(history.opponent_payoffs[-1, :]):
            self.opponent_average_payoffs[i] = (self.opponent_average_payoffs[i] * (state.round_number - 1) + payoff) / state.round_number
        cooperators = []
        free_riders = []
        for i in range(self.game_description.n_players):
            if self.contribution_tracker[i] > self.cooperation_threshold * state.round_number:
                cooperators.append(i)
            elif self.opponent_average_payoffs[i] > np.mean(self.my_payoffs) and self.defection_tracker[i] >= self.cooperation_threshold * state.round_number:
                free_riders.append(i)
        if len(cooperators) >= self.game_description.n_players // 2:
            return Action.C
        if free_riders and np.mean(self.my_payoffs) < min([self.opponent_average_payoffs[i] for i in free_riders]):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(cooperators) == self.game_description.n_players // 2 and np.mean(self.my_payoffs) > min([self.opponent_average_payoffs[i] for i in range(self.game_description.n_players)]):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    This strategy aims to exploit opponents' cooperation while minimizing losses from over-contribution.
    It adapts to the game's history, using past actions and payoffs to inform its decisions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defections = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_defections is None and state.round_number == 1:
            self.opponent_defections = np.zeros(self.game_description.n_players - 1, dtype=bool)
        avg_contrib_prev = np.mean(history.my_actions[-1:] + history.opponent_actions[-1, :])
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i] and (state.round_number > 1 and (not history.opponent_actions[-2, i])):
                self.opponent_defections[i] = True
        if np.any(self.opponent_defections):
            return Action.D
        theta = 0.4 if state.round_number >= self.game_description.n_rounds - 5 else 0.6
        if avg_contrib_prev > theta:
            return Action.C
        return Action.D

    def update_opponent_defections(self, opponent_actions):
        for i in range(len(opponent_actions)):
            if not opponent_actions[i] and (len(self.opponent_defections) > 1 and self.opponent_defections[i]):
                self.opponent_defections[i] = True

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation in the Public Goods Game.
    It leverages game history to inform its decisions, aiming for optimal payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = game_description.k / game_description.n_players * 0.5
        self.total_contributions = 0
        self.rounds_since_cooperation_test = 0
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.total_contributions += sum(history.opponent_actions[-1, :])
        average_contribution = self.total_contributions / (state.round_number * self.game_description.n_players)
        if average_contribution < self.exploitation_threshold:
            return Action.D
        if history.my_actions[-1] and all(history.opponent_actions[-1, :]):
            self.punishing = True
            return Action.C
        if self.punishing:
            self.punishing = False
            return Action.D
        if not all(history.opponent_actions[-1, :]):
            return Action.D
        if state.round_number >= self.game_description.n_rounds // 2 and history.my_payoffs[-1] == 0:
            self.rounds_since_cooperation_test += 1
            if self.rounds_since_cooperation_test > 5:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" in N-Player Public Goods Game.
    
    Decision Rules:
    - Initial Rounds: Cooperate (C) to gather information about opponents' behavior.
    - Exploitation Phase: Defect (D) if at least one opponent has defected or the average contribution is above 0.5.
    - Punishment Phase: Defect against an opponent for two rounds after they defect while you cooperated.
    - Adaptive Adjustment: Adjust defection rate every five rounds based on opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents_defected_last_round = 0
        self.my_defection_rate = 0.5
        self.punishment_target = None
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        if self.punishment_target is not None and self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        avg_contribution_last_round = np.mean(np.concatenate(([history.my_actions[-1]], opponent_actions_last_round)))
        if self.opponents_defected_last_round > 0 or avg_contribution_last_round > 0.5:
            return Action.D
        if state.round_number % 5 == 0:
            opponents_cooperators = np.sum(history.opponent_actions[-5:, :], axis=0)
            avg_opponents_cooperation_rate = np.mean(opponents_cooperators / 5, axis=0)
            if np.mean(avg_opponents_cooperation_rate) > 0.5:
                self.my_defection_rate *= 0.9
            else:
                self.my_defection_rate *= 1.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponents_alive = np.sum(history.opponent_actions[-1, :])
        if opponents_alive == 1:
            return Action.D if history.my_actions[-1] else Action.C
        self.opponents_defected_last_round = np.sum(~opponent_actions_last_round)
        if random.random() < self.my_defection_rate:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to various opponent behaviors by incorporating reciprocal punishment,
    tit-for-tat with forgiveness, exploitation, and defection threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_threshold = game_description.k / game_description.n_players - 1
        self.opponent_cooperations = [0] * (game_description.n_players - 1)
        self.my_defections_in_a_row = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        my_action_last_round = history.my_actions[-1]
        for i, action in enumerate(opponent_actions_last_round):
            if action and (not self.opponent_cooperations[i]):
                self.opponent_cooperations[i] += 1
        if any((not action for action in opponent_actions_last_round)):
            return Action.D
        if my_action_last_round:
            if sum(opponent_actions_last_round) / len(opponent_actions_last_round) < self.defection_threshold:
                self.my_defections_in_a_row += 1
            else:
                self.my_defections_in_a_row = 0
        opponent_payoffs_avg = np.mean(history.opponent_payoffs[-1])
        my_payoff_avg = history.my_payoffs[-1]
        if opponent_payoffs_avg > my_payoff_avg and state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, action in enumerate(opponent_actions_last_round):
            if not action and self.opponent_cooperations[i] == 0:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"
    
    This strategy initially contributes to establish a baseline for cooperation.
    It then uses reciprocal punishment, conditional cooperation, and adaptive threshold adjustment
    to maintain cooperation while exploiting non-cooperative behavior when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            if history is None or len(history.my_actions) < 2:
                return Action.C
            else:
                return Action(history.opponent_actions[-1, 0])
        if not all(history.opponent_actions[-1, :]):
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators == self.game_description.n_players:
            self.threshold += 0.1
            if self.threshold > 1:
                self.threshold = 1
            return Action.C
        elif cooperators < self.threshold * self.game_description.n_players:
            self.threshold -= 0.1
            if self.threshold < 0:
                self.threshold = 0
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior.
    Cooperates initially and in single-opponent games, but defects if fewer than a threshold of opponents cooperate.
    Updates the cooperation threshold based on opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.opponent_actions[0]) == 1):
            return Action.C
        if self.game_description.n_rounds - state.round_number <= 2:
            return Action.D
        opponents = history.opponent_actions[-1]
        total_opponents = len(opponents)
        cooperators = sum(opponents)
        proportion_cooperated = cooperators / total_opponents if total_opponents > 0 else 0
        if proportion_cooperated < self.cooperation_threshold:
            return Action.D
        if proportion_cooperated > 0.75 and self.cooperation_threshold > 0:
            self.cooperation_threshold -= 0.1
            self.cooperation_threshold = max(self.cooperation_threshold, 0)
        elif proportion_cooperated < 0.25 and self.cooperation_threshold < 1:
            self.cooperation_threshold += 0.1
            self.cooperation_threshold = min(self.cooperation_threshold, 1)
        return Action.C

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    An exploitative strategy for the repeated Public Goods Game.
    
    Initially cooperates to gather information about opponents' behaviors and potential cooperation levels.
    Then, it adapts its actions based on the average contribution of all opponents from previous rounds,
    with a bias towards defection. As the game progresses, it shifts towards defection while still adapting
    to opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_cooperation_rates is None:
            self.opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        if state.round_number < 5:
            avg_opponent_contribution = np.mean(history.opponent_actions[-1, :])
            if avg_opponent_contribution >= 0.6:
                return Action.C if random.random() < 0.4 else Action.D
            else:
                return Action.D
        elif state.round_number < self.game_description.n_players // 2:
            avg_opponent_contribution = np.mean(history.opponent_actions[-1, :])
            if avg_opponent_contribution > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_players - 1:
            avg_opponent_contribution = np.mean(history.opponent_actions[-1, :])
            if avg_opponent_contribution >= 0.4:
                return Action.C if random.random() < 0.3 else Action.D
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    A conditional cooperator strategy that adapts to opponents' behavior while maintaining an exploitative mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < self.game_description.n_players // 2:
            opponent_contributions = np.mean(history.opponent_actions, axis=1)
            avg_contribution = np.mean(opponent_contributions)
            if avg_contribution > 0.5:
                return Action.C
            return Action.D
        elif self.game_description.n_players // 2 <= state.round_number < 3 * self.game_description.n_players // 4:
            opponent_actions_last_two_rounds = history.opponent_actions[-2:, :]
            cooperators_last_two_rounds = np.sum(opponent_actions_last_two_rounds, axis=0)
            if np.any(cooperators_last_two_rounds > 0):
                return Action.C if random.random() < 0.7 else Action.D
            elif np.all(np.all(history.opponent_actions == False, axis=1)):
                return Action.D
            total_contributions = np.sum(history.opponent_actions[-1, :])
            threshold = self.game_description.k / self.game_description.n_players * self.game_description.n_players // 2
            if total_contributions > threshold:
                return Action.C
            return Action.D
        elif 3 * self.game_description.n_players // 4 <= state.round_number < self.game_description.n_rounds - 1:
            opponent_actions_last_two_rounds = history.opponent_actions[-2:, :]
            cooperators_last_two_rounds = np.sum(opponent_actions_last_two_rounds, axis=0)
            if np.any(cooperators_last_two_rounds / 2 > 0.8):
                return Action.C if random.random() < 0.9 else Action.D
            elif np.all(np.all(history.opponent_actions == False, axis=1)):
                return Action.D
            total_contributions = np.sum(history.opponent_actions[-1, :])
            threshold = self.game_description.k / self.game_description.n_players * self.game_description.n_players // 2
            if total_contributions > threshold:
                return Action.C
            return Action.D
        else:
            if np.all(np.all(history.opponent_actions == False, axis=1)):
                return Action.D
            return Action.D

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    An adaptive strategy that balances exploitation with cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperation_rate = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < min(5, self.n_rounds):
            previous_cooperators = sum(history.opponent_actions[-1, :])
            if previous_cooperators > self.n_players / 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.n_rounds // 2:
            window_size = min(5, state.round_number - 1) + 1
            opponent_cooperation_rates = np.mean(history.opponent_actions[-window_size:, :], axis=0)
            average_opponent_cooperation_rate = np.mean(opponent_cooperation_rates)
            if average_opponent_cooperation_rate > self.threshold_cooperation_rate:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.n_rounds - 1:
            opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
            most_cooperative_opponents = np.where(opponent_cooperation_rates == max(opponent_cooperation_rates))[0]
            if history.opponent_actions[-1, most_cooperative_opponents].any():
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        for opponent in range(self.n_players):
            if history.opponent_actions[-2, opponent] and history.opponent_actions[-3, opponent]:
                return Action.D
        for opponent in range(self.n_players):
            if not history.opponent_actions[-2, opponent] and (not history.opponent_actions[-3, opponent]):
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    An exploitative Public Goods Game strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = np.sum(history.opponent_actions[-self.window_size:, :], axis=0)
        avg_prev_payoffs_coop = np.mean(np.where(prev_cooperators > 0, history.opponent_payoffs[-self.window_size:, :], 0), axis=0)
        avg_prev_payoffs_defect = np.mean(np.where(prev_cooperators == 0, history.opponent_payoffs[-self.window_size:, :], 0), axis=0)
        if np.any(history.opponent_actions[-1, :] == True) and np.mean(avg_prev_payoffs_coop[avg_prev_payoffs_coop > 0]) > np.mean(avg_prev_payoffs_defect[avg_prev_payoffs_defect > 0]):
            return Action.C
        if history.my_actions[-1] and (not np.any(history.opponent_actions[-1, :])):
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defector_count = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1]
        average_contribution = np.mean(opponent_actions)
        for i in range(self.game_description.n_players - 1):
            if not opponent_actions[i]:
                self.defector_count[i] += 1
            else:
                self.defector_count[i] = max(0, self.defector_count[i] - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif np.all(opponent_actions):
            return random.choice([Action.C] * 3 + [Action.D])
        elif average_contribution >= 0.5:
            for i, dc in enumerate(self.defector_count):
                if dc >= 2:
                    return Action.D
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    Exploitative strategy that adapts to the contributions of others in a Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if self.opponent_defection_threshold_met and history.my_actions[-1] is True:
            self.opponent_defection_threshold_met = False
            return Action.C
        cooperators_last_round = sum(history.opponent_actions[-1, :])
        if cooperators_last_round >= self.game_description.k / 2:
            if history.my_payoffs.size > 0 and history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :]):
                self.opponent_defection_threshold_met = True
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.shape[0] <= 1 or cooperators_last_round == self.game_description.k / 2 - 1:
            return Action.C

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    An adaptive exploiter strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_streaks = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs)
        for i, actions in enumerate(np.transpose(history.opponent_actions)):
            if not any(actions):
                self.opponent_defection_streaks[i] += 1
        if avg_payoff >= 0.5:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :])
        if total_contributions / self.game_description.n_players > 0.75:
            return Action.C
        for i, streak in enumerate(self.opponent_defection_streaks):
            if streak > state.round_number // 2 and state.round_number < self.game_description.n_rounds - 1:
                return Action.C
            elif streak >= 3:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    Conditional Cooperation with Learning strategy for the Public Goods Game.
    Adapts to various opponent behaviors while prioritizing individual payoffs.
    Balances short-term gains with long-term benefits through learning and opponent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.update_counter = 0
        self.previous_avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution < 0.2:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(len(opponent_actions)):
            if not opponent_actions[i]:
                return Action.D
            elif np.mean(history.opponent_actions[:, i]) > 0.8:
                return Action.C
        self.update_counter += 1
        if self.update_counter >= self.game_description.n_rounds // 3:
            self.update_counter = 0
            avg_payoff = np.mean(history.my_payoffs)
            if self.previous_avg_payoff is not None and avg_payoff > self.previous_avg_payoff:
                self.cooperation_threshold -= 0.1
            elif self.previous_avg_payoff is not None and avg_payoff < self.previous_avg_payoff:
                self.cooperation_threshold += 0.1
            self.previous_avg_payoff = avg_payoff
        if avg_contribution >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    Adaptive, exploitative strategy for the Public Goods Game (PGG).
    This strategy combines elements of Tit-for-Tat with self-interest adjustments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_cooperators = None
        self.self_interest_adjustment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.last_round_cooperators is not None:
            avg_contribution = self.last_round_cooperators / self.game_description.n_players
        else:
            avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution < self.game_description.k / self.game_description.n_players * 0.5:
            return Action.D
        if history is not None and len(history.my_payoffs) > 1:
            my_avg_payoff = np.mean(history.my_payoffs)
            opponent_avg_payoffs = np.mean(np.sum(history.opponent_payoffs, axis=0))
            if my_avg_payoff < opponent_avg_payoffs:
                self.self_interest_adjustment = True
        if avg_contribution > self.game_description.k / self.game_description.n_players:
            return Action.C
        elif self.self_interest_adjustment and history.my_actions[-1] == Action.D:
            return Action.C
        elif not self.self_interest_adjustment or history.my_actions[-1] != Action.D:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.last_round_cooperators = sum(history.opponent_actions[-1, :])

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    Adaptive Exploiter strategy for Public Goods Game.
    
    This strategy balances exploration, exploitation, and punishment to maximize payoffs in a wide range of opponent behaviors and game scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_adjustment = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.opponent_actions.shape) < 2 or history.opponent_actions.shape[1] != self.game_description.n_players - 1:
            return Action.D
        if state.round_number <= 3:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_opponent_contribution = np.mean(opponent_actions)
        avg_threshold = 0.5 + self.threshold_adjustment
        defector_flags = np.all(history.opponent_actions[-2:, :], axis=0) == False
        punish_opponents = np.any(defector_flags)
        last_round = state.round_number == self.game_description.n_rounds - 1
        if avg_opponent_contribution > avg_threshold:
            if not (last_round or punish_opponents):
                return Action.C
            else:
                return Action.D
        elif history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            if last_round:
                return Action.D
            elif punish_opponents:
                return Action.D
            elif avg_opponent_contribution > avg_threshold / 2:
                self.threshold_adjustment = -0.1 * random.random()
                return Action.C
            else:
                return Action.D
        elif history.my_payoffs[-1] >= np.mean(history.opponent_payoffs[-1, :]):
            if last_round or punish_opponents:
                return Action.D
            elif avg_opponent_contribution > avg_threshold / 2:
                self.threshold_adjustment = -0.1 * random.random()
                return Action.C
            else:
                return Action.D
        elif history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]) and avg_opponent_contribution > avg_threshold / 2:
            self.threshold_adjustment = -0.1 * random.random()
            return Action.C
        elif last_round:
            return Action.D
        elif punish_opponents or np.any(history.opponent_actions[-1, :] == False):
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Adaptive and exploitative strategy for the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = [0.0] * (game_description.n_players - 1)
        self.exploitable_opponents = [False] * (game_description.n_players - 1)
        self.punishing_opponents = [False] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == 3:
            total_contributions = sum(history.opponent_actions[:3].sum(axis=0)) + history.my_actions[:3].sum()
            average_contribution = total_contributions / (self.game_description.n_players * 3)
            self.cooperative_environment = average_contribution > 0.5
        if state.round_number < self.game_description.n_rounds - 1:
            previous_round_cooperation_ratio = history.opponent_actions[-1].sum() / self.game_description.n_players
            if previous_round_cooperation_ratio > 0.5 and self.cooperative_environment or (self.game_description.n_players == 2 and history.my_payoffs[-1] < 1):
                return Action.C
        for opponent in range(self.game_description.n_players - 1):
            if not self.exploitable_opponents[opponent]:
                if history.opponent_actions[-1, opponent] == False:
                    self.opponent_contributions[opponent] += 0
                else:
                    self.opponent_contributions[opponent] += 1
                if self.opponent_contributions[opponent] < state.round_number // 2 or (state.round_number > 1 and history.opponent_actions[-2, opponent] == False and (history.opponent_actions[-1, opponent] == False)):
                    self.exploitable_opponents[opponent] = True
            if self.exploitable_opponents[opponent]:
                return Action.D
        for opponent in range(self.game_description.n_players - 1):
            if self.punishing_opponents[opponent]:
                if history.opponent_actions[-1, opponent] == True:
                    self.punishing_opponents[opponent] = False
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            consistent_cooperators = sum(history.my_payoffs > 0.5 * self.game_description.k) >= 0.8 * state.round_number
            if consistent_cooperators and self.cooperative_environment:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    An adaptive exploitative strategy for the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = np.zeros(game_description.n_players - 1)
        self.contribution_window = 3
        self.free_rider_threshold = 0.5
        self.exploitation_cycle_cooperate_rounds = 2
        self.exploitation_cycle_defect_rounds = 1
        self.randomization_probability = 0.2
        self.last_exploitation_phase_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.opponent_contributions *= 1 - 1 / self.contribution_window
        for i in range(self.game_description.n_players - 1):
            self.opponent_contributions[i] += history.opponent_actions[-1, i] * (1 / self.contribution_window)
        opponent_classification = np.where(self.opponent_contributions > self.free_rider_threshold, 'cooperator', 'free-rider')
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            self.exploitation_cycle_defect_rounds += 1
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        if np.all(opponent_classification == 'free-rider'):
            if self.last_exploitation_phase_action is None or self.last_exploitation_phase_action == Action.C:
                self.last_exploitation_phase_action = Action.C
                return Action.C
            else:
                return Action.D
        for i in range(self.game_description.n_players - 1):
            if opponent_classification[i] == 'cooperator':
                if state.round_number % (self.exploitation_cycle_cooperate_rounds + self.exploitation_cycle_defect_rounds) < self.exploitation_cycle_cooperate_rounds:
                    if random.random() > self.randomization_probability:
                        return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    A strategic thinking approach to the Public Goods Game, 
    aiming to adapt to various opponent behaviors while exploiting their willingness to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number <= 5:
            avg_contribution_rate = np.mean(history.opponent_actions[-1, :])
            return Action.C if avg_contribution_rate > 0.5 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            majority_action = np.mean(history.opponent_actions[-1, :]) >= 0.5
            return Action.C if majority_action else Action.D
        elif state.round_number < self.game_description.n_rounds:
            cooperation_prob = max(0.25, 0.5 - state.round_number / self.game_description.n_rounds * 0.25)
            return Action.C if random.random() < cooperation_prob else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in the Public Goods Game.
    
    Initially contributes to gather information and induce cooperation. Then, it defects 
    if enough players contributed in the previous round or continues to cooperate if not 
    enough players did so but this player had cooperated before. It also incorporates a 
    punishment mechanism for rounds with low overall contributions and exploits consistent 
    cooperators. In the final round, it maximizes immediate payoff based on past cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions_history = None
        self.consistent_cooperators_detected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.total_contributions_history is None:
            self.total_contributions_history = [sum(history.opponent_actions[0, :]) + 1]
        else:
            self.total_contributions_history.append(sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C))
        total_prev_round = self.total_contributions_history[-1]
        if total_prev_round >= self.game_description.n_players // 2:
            return Action.D
        elif total_prev_round < self.game_description.n_players // 2 and history.my_actions[-1] == Action.C:
            return Action.C
        else:
            return Action.D
        if total_prev_round <= self.game_description.n_players // 4:
            if any(history.opponent_actions[-1, :]):
                return Action.C
            else:
                return Action.D
        if state.round_number > self.game_description.n_rounds // 2:
            consistent_cooperators = np.all(history.opponent_actions[:, :], axis=0)
            if any(consistent_cooperators):
                self.consistent_cooperators_detected = True
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if np.mean(self.total_contributions_history) > self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        if self.consistent_cooperators_detected:
            return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    
    This strategy balances individual gain with collective welfare by:
    - Initially cooperating to establish trust
    - Adapting to opponents' behavior based on their past actions
    - Identifying and taking advantage of defective opponents
    - Maintaining reciprocity with cooperative opponents through Tit-for-Tat with Noise
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_classification = None
        self.tftn_defect_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_c = np.mean(history.opponent_actions[-1, :])
        if avg_c >= 0.5:
            return Action.C
        if self.opponent_classification is None:
            self.classify_opponents(history)
        opponent_defect_rate = self.get_opponent_defect_rate(history)
        p_defect = opponent_defect_rate * 0.8
        if random.random() < p_defect:
            return Action.D
        last_opponent_action = history.opponent_actions[-1, 0]
        if random.random() < self.tftn_defect_prob:
            return Action.D
        else:
            return Action.C if last_opponent_action else Action.D

    def classify_opponents(self, history: PlayerHistory):
        opponent_actions = history.opponent_actions[:, 0]
        self.opponent_classification = np.mean(opponent_actions) >= 0.5

    def get_opponent_defect_rate(self, history: PlayerHistory):
        opponent_actions = history.opponent_actions[:, 0]
        return 1 - np.mean(opponent_actions)

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to various opponent behaviors in a repeated Public Goods Game.
    It encourages cooperation when others contribute significantly and defects when opponents are not contributing enough or when it's beneficial to do so.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_count += 1
        opponent_actions = history.opponent_actions[-1]
        if self.round_count <= 5:
            avg_contribution = np.mean(opponent_actions)
            if avg_contribution > 0.5:
                return Action.C
            else:
                return Action.D
        elif self.round_count < math.ceil(self.game_description.n_rounds / 2):
            my_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1])
            if my_payoff > avg_opponent_payoff:
                return Action.C
            else:
                return Action.D
        elif self.round_count < self.game_description.n_rounds - 1:
            opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
            most_cooperative_opponent_index = np.argmax(opponent_cooperation_rates)
            most_cooperative_opponent_action = history.opponent_actions[-1, most_cooperative_opponent_index]
            return Action.C if most_cooperative_opponent_action else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    Cooperates initially to gauge opponents' behavior, then adapts based on their cooperation rate.
    Punishes defectors and takes advantage of cooperative opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.rounds_since_update = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        opponent_cooperations = history.opponent_actions[-1, :].sum()
        average_opponent_cooperation_rate = np.mean(history.opponent_actions)
        if self.rounds_since_update >= 5:
            self.cooperation_threshold += (average_opponent_cooperation_rate - 0.5) * 0.1
            self.cooperation_threshold = max(0.3, min(self.cooperation_threshold, 0.7))
            self.rounds_since_update = 0
        if opponent_cooperations / (self.game_description.n_players - 1) >= self.cooperation_threshold:
            return Action.C
        defector_indices = np.where(~history.opponent_actions[-1, :])[0]
        for index in defector_indices:
            if history.my_payoffs[-1] < (1 - average_opponent_cooperation_rate) * self.game_description.k:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    
    This strategy, named "Conditional Cooperator with Gradual Escalation" (CCGE), 
    aims to exploit the cooperation of others while gradually escalating its own 
    cooperation based on the level of previous contributions. By punishing defectors 
    and exploiting consistent cooperators, CCGE seeks to maximize individual payoff 
    in a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defectors = set()
        self.cooperators = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        average_payoff = np.mean(history.my_payoffs[-1:])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_contributions > 0 and average_payoff < self.game_description.k / self.game_description.n_players:
            p = total_contributions / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        if total_contributions == 0:
            defectors_count = sum((1 for opponent in history.opponent_actions[-1, :] if not opponent))
            if defectors_count > 0 and len(self.defectors) > 0:
                punishment_prob = 1 - history.my_payoffs[-1] / (self.game_description.k * self.game_description.n_players)
                return Action.D if random.random() < punishment_prob else Action.C
            elif len(self.cooperators) == 0 or all(history.opponent_actions[-1, :]):
                return Action.C
        for opponent in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, opponent]:
                self.cooperators.add(opponent)
            else:
                self.defectors.add(opponent)
        if average_payoff >= self.game_description.k / self.game_description.n_players or total_contributions > 0:
            return Action.C
        for cooperator in list(self.cooperators):
            if history.opponent_actions[-1, cooperator]:
                return Action.D
        zero_contribution_rounds = sum((1 for round_actions in history.my_actions[:-1] if not round_actions))
        if zero_contribution_rounds >= 3:
            p = 0.5
            return Action.C if random.random() < p else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    Gradual Reciprocity with Punishment strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.theta = (self.n_players - 1) / self.n_players
        self.contribution_tracking = np.zeros(self.n_players, dtype=np.float64)
        self.previous_round_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] | np.array([history.my_actions[-1]], dtype=bool))
        average_contribution = previous_round_cooperators / self.n_players
        if self.previous_round_payoff is not None and self.k * average_contribution < 1:
            return Action.D
        if average_contribution >= self.theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        deviators = sum(history.opponent_actions[-1, :] == False) + (history.my_actions[-1] == False)
        if deviators == 1 and average_contribution > self.theta / 2:
            return Action.C
        self.contribution_tracking += history.opponent_actions[-1, :] | np.array([history.my_actions[-1]], dtype=bool)
        if state.round_number < self.game_description.n_rounds // 2:
            self.theta = max(self.theta - 0.01, 0) if sum(self.contribution_tracking) > self.n_players * state.round_number / 2 else min(self.theta + 0.01, 1)
        self.previous_round_payoff = history.my_payoffs[-1]

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    An adaptive strategy that exploits the tension between individual incentives and collective welfare in the Public Goods Game.
    It balances cooperation and defection based on past actions and payoffs to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contributions = None
        self.threshold = None

    def calculate_threshold(self, avg_contribution_prev_round):
        delta = 0.1
        return self.game_description.k / self.game_description.n_players * avg_contribution_prev_round + delta

    def detect_cooperative_opponents(self, opponent_actions):
        m = 3
        cooperative_opponents = []
        for i in range(opponent_actions.shape[1]):
            if all(opponent_actions[-m:, i]):
                cooperative_opponents.append(i)
        return cooperative_opponents

    def identify_weak_opponents(self, opponent_payoffs):
        weak_opponents = []
        for i in range(opponent_payoffs.shape[1]):
            if np.var(opponent_payoffs[:, i]) > 0.5:
                weak_opponents.append(i)
        return weak_opponents

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_actions = history.opponent_actions[-1]
        self.avg_contributions = np.mean(prev_round_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if sum(prev_round_actions) == 1:
            return Action.C
        cooperative_opponents = self.detect_cooperative_opponents(history.opponent_actions)
        weak_opponents = self.identify_weak_opponents(history.opponent_payoffs)
        if len(cooperative_opponents) > 0:
            return Action.D
        elif len(weak_opponents) > 0:
            return Action.D
        self.threshold = self.calculate_threshold(self.avg_contributions)
        if self.avg_contributions >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in a repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_last_action = None
        self.last_round_number = 0
        self.cooperate_count = 0
        self.defect_count = 0
        self.payoff_difference = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_last_action = history.opponent_actions[-1]
        current_round_number = state.round_number - 1
        if current_round_number < 5:
            cooperate_majority = np.mean(opponent_last_action) > 0.5
            return Action.C if cooperate_majority else Action.D
        self.cooperate_count += history.my_actions[-1]
        self.defect_count += not history.my_actions[-1]
        payoff_diff = np.mean(history.my_payoffs[self.last_round_number:]) - np.mean(history.my_payoffs[:self.last_round_number])
        self.payoff_difference = payoff_diff
        if (opponent_last_action == 0).sum() >= 2 or np.mean(opponent_last_action) < 0.2:
            return Action.D
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.opponent_last_action is not None and np.mean(self.opponent_last_action) < 0.2:
            return Action.C
        if self.payoff_difference > 0.2:
            return Action.C
        else:
            return Action.D
        self.opponent_last_action = opponent_last_action
        self.last_round_number = current_round_number

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize payoffs by exploiting the cooperative tendencies of opponents while adapting to their behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.theta = 0.4
        self.opponent_contribution_rates = np.zeros(self.n_players)
        self.contribute_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_actions_last_round = history.opponent_actions[-1]
        total_contributions_prev = np.sum(opponent_actions_last_round)
        if self.game_description.n_players == 2:
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, opponent_action in enumerate(opponent_actions_last_round):
            if opponent_action:
                self.opponent_contribution_rates[i] += 0.1
                if self.opponent_contribution_rates[i] > 1:
                    self.opponent_contribution_rates[i] = 1
        avg_opponent_contrib_rate = np.mean(self.opponent_contribution_rates[-3:])
        if total_contributions_prev >= (self.n_players - 1) / 2:
            self.contribute_prob = max(0.7, self.contribute_prob + 0.1)
        else:
            self.contribute_prob = min(0.8, self.contribute_prob - 0.3)
        if avg_opponent_contrib_rate > self.theta:
            self.contribute_prob -= 0.2
            if self.contribute_prob < 0:
                self.contribute_prob = 0
        return Action.C if random.random() < self.contribute_prob else Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    Exploitative strategy that aims to maximize payoffs by mirroring opponents' behaviors, 
    while maintaining adaptability and robustness through a punishment mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_threshold = 0.5
        self.opponent_contribution_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_contribution_rates is None:
            self.opponent_contribution_rates = [0.0] * self.game_description.n_players
        for i, action in enumerate(history.opponent_actions[-1, :]):
            self.opponent_contribution_rates[i] += action / state.round_number
        if any((rate < self.punishment_threshold for rate in self.opponent_contribution_rates)):
            return Action.D
        if history.my_actions[-1]:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    A strategic player that adapts to opponents' behavior, balancing cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_payoff = history.my_payoffs[-1]
        if state.round_number < 5:
            avg_contribution = np.mean(history.opponent_actions)
            return Action.C if avg_contribution > 0.5 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            recent_opponent_contributions = history.opponent_actions[-3:, :]
            avg_recent_contribution = np.mean(recent_opponent_contributions)
            if avg_recent_contribution > 0.4:
                return Action.C
            elif my_payoff < np.mean(history.my_payoffs):
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            recent_my_payoffs = history.my_payoffs[-5:]
            avg_recent_my_payoff = np.mean(recent_my_payoffs)
            if avg_recent_my_payoff > np.mean(history.opponent_payoffs[-5:, :]):
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    Exploits cooperation from opponents in the Public Goods Game by initially cooperating,
    then adapting to their behavior, defecting when profitable and adjusting for past outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.permanent_defection = False
        self.permanent_contribution = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[:-1]
        if np.all(opponent_actions == 0):
            self.permanent_defection = True
        elif np.all(opponent_actions == 1):
            self.permanent_contribution = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_opponent_cooperation = np.mean(history.opponent_actions[-1, :])
        my_previous_payoff = history.my_payoffs[-1]
        group_avg_payoff = np.mean(history.opponent_payoffs[-1, :] + [my_previous_payoff])
        if self.permanent_defection:
            return Action.D
        elif self.permanent_contribution:
            return Action.C
        elif avg_opponent_cooperation > 0.5:
            return Action.D
        else:
            return Action.C if my_previous_payoff < group_avg_payoff else Action.D

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    A strategy that adapts to the opponents' behavior and exploits defectors while encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1] * self.game_description.n_players
        avg_payoff = np.mean(history.my_payoffs)
        if state.round_number > 1:
            prev_avg_payoff = np.mean(history.my_payoffs[:-1])
            if avg_payoff > prev_avg_payoff:
                self.alpha -= 0.05
                self.alpha = max(self.alpha, 0)
            elif avg_payoff < prev_avg_payoff:
                self.alpha += 0.05
                self.alpha = min(self.alpha, 1)
        if total_contributions / self.game_description.n_players >= self.alpha:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    A strategy that adapts to the opponents' behavior and exploits their cooperation.
    It initially explores by cooperating in the first round, then adjusts its actions based on the average contribution of all players.
    In the final round, it defects to maximize personal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = 0.5
        self.endgame_manipulation_threshold = self.game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            avg_contribution = np.mean(np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :])))
        else:
            avg_contribution = 0.5
        if avg_contribution >= self.exploitation_threshold:
            return Action.C
        if history is not None and len(history.opponent_actions) > 1:
            consistent_defectors = np.all(~history.opponent_actions[1:, :], axis=0)
            low_contributors = np.sum(history.opponent_actions[1:, :], axis=0) < self.endgame_manipulation_threshold
            if np.any(consistent_defectors | low_contributors):
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    Exploits opponents' cooperative tendencies in the Public Goods Game while adapting to changing conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.k_threshold = game_description.k / game_description.n_players
        self.p_punish = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        opponent_coop_avg = np.mean(history.opponent_actions, axis=0)
        last_round_opponents = history.opponent_actions[-1]
        if np.any(opponent_coop_avg > self.k_threshold):
            return Action.D
        last_round_contributions = sum(last_round_opponents) + history.my_actions[-1]
        if last_round_contributions > self.game_description.k / 2:
            return Action.C
        opponents_defected_last_round = np.any(np.logical_not(last_round_opponents))
        if opponents_defected_last_round and random.random() < self.p_punish:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif state.round_number == 1 and sum(history.opponent_actions[0]) + history.my_actions[0] < self.game_description.k / 4:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    Exploitative strategy using adaptive tit-for-tat with gradual escalation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mirroring_defection_rounds = {}
        self.exploiting_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_actions = history.opponent_actions[-1]
        average_contribution = np.mean(opponents_actions)
        for opponent, action in enumerate(opponents_actions):
            if not action:
                self.mirroring_defection_rounds[opponent] = self.mirroring_defection_rounds.get(opponent, 0) + 1
            else:
                self.mirroring_defection_rounds[opponent] = 0
        if average_contribution >= 0.5 or (average_contribution == 0.5 and random.random() < 0.5):
            return Action.C
        for opponent, rounds in self.mirroring_defection_rounds.items():
            if rounds >= 2:
                return Action.D
        for opponent, action in enumerate(opponents_actions):
            if action and all((history.opponent_actions[i, opponent] for i in range(state.round_number))):
                self.exploiting_opponents.add(opponent)
        if any(self.exploiting_opponents):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive K-threshold"
    
    This strategy aims to balance exploitation with adaptability, taking into account 
    the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_for_average_contributions = math.ceil(game_description.n_rounds / 2)
        self.k_threshold = game_description.k / game_description.n_players * (1 + (game_description.n_players - 1) / game_description.n_rounds)
        self.early_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.early_defection_mode and state.round_number < min(self.game_description.n_players // 2, self.rounds_for_average_contributions):
            opponent_defections = np.sum(~history.opponent_actions[state.round_number - 1])
            if opponent_defections > self.game_description.n_players // 2:
                self.early_defection_mode = True
        if self.early_defection_mode:
            return Action.D
        if history.my_actions.shape[0] < self.rounds_for_average_contributions:
            avg_contributions = np.mean(history.my_actions)
        else:
            start_index = max(0, state.round_number - self.rounds_for_average_contributions)
            avg_contributions = np.mean(np.concatenate((history.my_actions[start_index:], history.opponent_actions[start_index:, :].flatten())))
        if state.round_number == self.game_description.n_rounds - 1:
            if np.all(history.opponent_actions[-1, :] == True):
                return Action.C
            else:
                return Action.D
        if avg_contributions >= self.k_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = math.ceil(game_description.k / game_description.n_players)
        self.last_round_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            self.last_round_defection = True
            return Action.D
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        total_cooperators = opponent_cooperators + 1 if history.my_actions[-1] else opponent_cooperators
        if total_cooperators >= self.contribution_threshold:
            return Action.C
        if total_cooperators < self.contribution_threshold or (history.my_payoffs[-1] > 0 and (not history.my_actions[-1])):
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C if history.opponent_actions[-1, 0] else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    Adaptive and exploitative strategy for the repeated Public Goods Game.
    
    Decision rules:
    - Cooperate in the first round to gather information about opponents' behavior.
    - In subsequent rounds, use a simple decision rule based on the average contribution of all players in the previous round.
    - Intentionally introduce random noise to your behavior every 5th round by randomly choosing between Cooperate (C) and Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.noise_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution = np.mean(np.append(history.my_actions[-1], history.opponent_actions[-1, :]))
        if avg_contribution == 0.5:
            return random.choice([Action.C, Action.D])
        if avg_contribution > 0.5:
            action = Action.D
        elif avg_contribution < 0.5:
            action = Action.C
        else:
            action = random.choice([Action.C, Action.D])
        self.noise_counter += 1
        if self.noise_counter % 5 == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize individual payoff 
    by taking advantage of cooperative opponents while minimizing losses when facing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_threshold = game_description.k / (game_description.n_players * 1.5)
        self.last_round_contributions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions = sum(history.opponent_actions[-1, :] != False) + (history.my_actions[-1] == True)
        avg_contribution = total_contributions / self.game_description.n_players
        self.last_round_contributions = total_contributions
        if avg_contribution > self.defection_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors in the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0
        self.cooperation_threshold = 0.5 * game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_number = state.round_number
        avg_contribution_rate = np.mean(np.sum(history.opponent_actions[-self.game_description.n_rounds // 2:, :], axis=1) / (self.game_description.n_players - 1))
        if self.round_number % (self.game_description.n_rounds // 4) == 0:
            if avg_contribution_rate > np.mean(np.sum(history.opponent_actions[-2 * self.game_description.n_rounds // 2:-self.game_description.n_rounds // 2, :], axis=1) / (self.game_description.n_players - 1)):
                self.cooperation_threshold *= 0.9
            else:
                self.cooperation_threshold *= 1.1
        opponent_avg_contribution_rates = np.mean(history.opponent_actions[-self.game_description.n_rounds // 2:, :], axis=0)
        for i, rate in enumerate(opponent_avg_contribution_rates):
            if rate > 0.5:
                self.cooperation_threshold *= 0.8
            elif rate <= 0.5:
                self.cooperation_threshold *= 1.2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.round_number < self.game_description.n_rounds // 4 and np.sum(history.opponent_actions[-1, :]) < self.game_description.n_players / 4:
            return Action.C
        if avg_contribution_rate > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts by tracking individual player actions,
    adjusting its behavior based on cooperation rates, and responding to changes in the game environment.
    This approach allows it to effectively exploit opponents' strategies while minimizing losses from defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.opponent_cooperations = None
        self.uncooperative_players = set()
        self.last_round_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_cooperations is None:
            self.opponent_cooperations = np.zeros(self.game_description.n_players - 1)
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            self.opponent_cooperations[i] += opponent_action
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                self.uncooperative_players.add(i)
        reciprocal_cooperation = np.mean(self.opponent_cooperations) > self.cooperation_threshold
        punish_defectors = len(self.uncooperative_players) > 0
        round_robin_exploitation = np.sum(history.opponent_actions[-1, :]) >= self.game_description.n_players // 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if reciprocal_cooperation and (not punish_defectors) and (not round_robin_exploitation):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    The Adaptive Exploiter is a strategy designed for the Public Goods Game.
    It cooperates initially to gather information and potentially encourage cooperation,
    then adapts its actions based on opponents' past behaviors, and defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = [0] * game_description.n_players
        self.frequency_tracker = [1 / game_description.n_players] * game_description.n_players
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action:
                self.frequency_tracker[i] += 0.05
            else:
                self.frequency_tracker[i] -= 0.05
        if avg_contribution > 0.5:
            self.threshold += 0.05
        else:
            self.threshold -= 0.05
        opponent_actions = history.opponent_actions[-1, :]
        if np.any(opponent_actions < avg_contribution):
            return Action.D
        elif avg_contribution > self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    The Adaptive Exploiter strategy capitalizes on the cooperation of others while minimizing personal contributions.
    It adapts to opponents' behaviors and its own payoffs, always seeking to maximize individual gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < 5:
            opponent_cooperation_rate = np.mean(history.opponent_actions[:-1, :])
            if opponent_cooperation_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number < self.game_description.n_rounds - 2:
            my_average_payoff = np.mean(history.my_payoffs[:-1])
            group_average_payoff = np.mean(np.mean(history.opponent_payoffs[:-1, :], axis=0))
            if my_average_payoff > group_average_payoff:
                return Action.D
            else:
                last_round_opponents_cooperated = sum(history.opponent_actions[-2, :])
                if last_round_opponents_cooperated >= self.game_description.n_players // 2:
                    return Action.C
                else:
                    return Action.D
        elif state.round_number >= self.game_description.n_rounds - 2:
            my_total_payoff = np.sum(history.my_payoffs[:-1])
            group_average_total_payoff = np.mean(np.sum(history.opponent_payoffs[:-1, :], axis=0))
            if my_total_payoff > group_average_total_payoff * self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        elif history.opponent_actions.shape[1] == 1:
            if state.round_number == 0:
                return Action.C
            else:
                last_opponent_action = history.opponent_actions[-2, 0]
                return Action(last_opponent_action + 1)
        elif np.all(history.opponent_actions[:-1, :] == history.opponent_actions[:-1, 0:1]):
            last_round_opponents_cooperated = sum(history.opponent_actions[-2, :])
            if last_round_opponents_cooperated >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    
    Initially cooperates to gather information about opponents' behavior, 
    then adapts by exploiting non-cooperative behavior while protecting against being taken advantage of.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_round_cooperators == 0 and history.my_actions[-1]:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_round_cooperators <= self.game_description.n_players // 2:
            return Action.D
        elif previous_round_cooperators < 3 * self.game_description.n_players // 4:
            return Action.C if sum(history.opponent_actions[-1, :]) > 0 else Action.D
        else:
            my_previous_payoff = history.my_payoffs[-1]
            average_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_previous_payoff < average_opponent_payoff:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    Conditional Cooperator with Adaptive Threshold strategy.
    
    This strategy starts by cooperating in the first round, then adapts its behavior based on the average contribution of all players.
    It adjusts a threshold value to determine whether to cooperate or defect in subsequent rounds.
    The goal is to balance cooperation with self-interest and adapt to the dynamics of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.avg_contribution = None
        self.defected_early = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.defected_early and state.round_number <= 1 and (history.my_actions[-1] == Action.D):
            self.defected_early = True
            return Action.D
        if self.avg_contribution is None or state.round_number > 0:
            total_cooperators = np.sum(history.opponent_actions[:-1, :] == Action.C)
            self.avg_contribution = total_cooperators / (state.round_number * self.game_description.n_players)
        if self.avg_contribution > self.game_description.k / self.game_description.n_players:
            self.threshold -= 1 / (self.game_description.n_players * self.game_description.n_rounds)
        elif self.avg_contribution < self.game_description.k / self.game_description.n_players:
            self.threshold += 1 / (self.game_description.n_players * self.game_description.n_rounds)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number <= self.game_description.n_players // 2 and self.avg_contribution > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    
    This strategy starts by cooperating in the first round to gather information about opponents' behaviors. 
    In subsequent rounds, it defects if the average contribution of all players in the previous round is below a certain threshold (T), 
    otherwise it cooperates. The last round always results in defection. 
    It also adapts its strategy based on the game's history and punishes or rewards opponents accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5 * game_description.k / game_description.n_players
        self.update_round_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.update_round_counter += 1
        if self.update_round_counter >= 5 and history is not None:
            average_contribution = np.mean(history.opponent_actions[:, :].sum(axis=1) / self.game_description.n_players)
            if average_contribution > self.threshold:
                self.threshold += 0.1
            else:
                self.threshold -= 0.1
            self.update_round_counter = 0
        if history is not None:
            previous_opponent_actions = history.opponent_actions[-1, :]
            average_contribution = np.mean(previous_opponent_actions)
            my_previous_action = history.my_actions[-1]
            if all((not action for action in previous_opponent_actions)):
                return Action.D
            if average_contribution < self.threshold:
                return Action.D
            elif my_previous_action and average_contribution > self.threshold:
                return Action.C
            elif not my_previous_action and average_contribution <= self.threshold:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        average_contribution = np.mean(history.opponent_actions[-1, :])
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        average_opponent_payoff = np.mean(opponent_payoffs)
        if average_contribution > 0.5:
            return Action.D
        elif my_payoff < average_opponent_payoff:
            return Action.C
        else:
            for i in range(self.game_description.n_players - 1):
                if not history.opponent_actions[-1, i] and history.my_actions[-1]:
                    self.punishment_rounds = 2
                    break
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to capitalize on cooperative behavior while avoiding exploitation by others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_opponents_cooperated = None
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.update_opponent_cooperation_rates(history)
        if self.last_round_opponents_cooperated is not None and (not any(self.last_round_opponents_cooperated)):
            return Action.C
        for opponent_index in range(self.game_description.n_players - 1):
            if self.opponent_cooperation_rates[opponent_index] < 0.5 and self.last_round_opponents_cooperated is not None:
                return Action.D
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            return Action.C
        total_cooperators = sum(self.last_round_opponents_cooperated) + 1
        if total_cooperators >= self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

    def update_opponent_cooperation_rates(self, history):
        if self.opponent_cooperation_rates is None and history.my_actions.size > 0:
            self.opponent_cooperation_rates = np.zeros(self.game_description.n_players - 1)
        if history.opponent_actions.size > 0:
            self.last_round_opponents_cooperated = history.opponent_actions[-1, :]
            self.opponent_cooperation_rates += self.last_round_opponents_cooperated * np.ones(self.game_description.n_players - 1) / (history.my_actions.size + 1)

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    An adaptive and exploitative strategy for the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[:state.round_number]
        if len(opponent_actions) > 10:
            opponent_actions = opponent_actions[-10:]
        avg_contribution_ratio = np.mean(np.sum(opponent_actions, axis=0) / len(opponent_actions))
        expected_payoff_cooperate = self.game_description.k / self.game_description.n_players * avg_contribution_ratio + (1 - 1 / self.game_description.n_players)
        expected_payoff_defect = self.game_description.k / self.game_description.n_players * avg_contribution_ratio
        if state.round_number > 1:
            last_round_expected_cooperate = self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-2, :]) + (1 - 1 / self.game_description.n_players)
            last_round_expected_defect = self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-2, :])
            if last_round_expected_defect > last_round_expected_cooperate:
                self.cooperation_threshold += 0.05
            elif last_round_expected_cooperate >= last_round_expected_defect and avg_contribution_ratio > self.cooperation_threshold:
                self.cooperation_threshold -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.all(~history.opponent_actions, axis=0).any():
            return Action.D
        if avg_contribution_ratio > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    Adaptive and exploitative strategy for the Public Goods Game.
    
    The strategy aims to exploit opponents who cooperate by defecting and taking advantage of the shared benefits without contributing. 
    It encourages cooperation while minimizing the risk of being exploited through a punishment mechanism and learning from opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contribution_avg = None
        self.punishment_status = False
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        total_contributions = np.sum(history.opponent_actions)
        if self.opponent_contribution_avg is None:
            self.opponent_contribution_avg = total_contributions / (state.round_number * self.game_description.n_players - 1)
        else:
            self.opponent_contribution_avg = (self.opponent_contribution_avg * (state.round_number - 1) + total_contributions) / state.round_number
        if self.punishment_status:
            self.punishment_status = False if random.random() < 0.5 else True
            return Action.D
        if history.opponent_actions[-1, :].any():
            self.punishment_status = True
            return Action.D
        if self.opponent_contribution_avg >= 0.5:
            return Action.D
        return Action.C if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.mean(history.opponent_actions) == 0.5:
            return Action.C if random.random() < 0.5 else Action.D
        if history.opponent_payoffs[-1, :] > history.opponent_payoffs[-2, :]:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
        else:
            self.cooperation_probability = max(0, self.cooperation_probability - 0.1)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    A strategy that adapts to opponents' behavior, exploiting defectors and cooperating with cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = None
        self.opponent_labels = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_contributions is None:
            self.opponent_contributions = np.zeros(self.game_description.n_players - 1)
            self.opponent_labels = np.empty(self.game_description.n_players - 1, dtype=object)
        for i in range(self.game_description.n_players - 1):
            self.opponent_contributions[i] += history.opponent_actions[-1, i]
            avg_contribution = self.opponent_contributions[i] / state.round_number
            if avg_contribution > 0.5:
                self.opponent_labels[i] = 'Cooperative'
            else:
                self.opponent_labels[i] = 'Defector'
        action = Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i]:
                if self.opponent_labels[i] == 'Cooperative':
                    action = Action.C
                else:
                    action = Action.D
            elif history.opponent_actions[-1, i] is False:
                if self.opponent_labels[i] == 'Cooperative':
                    action = Action.C
                else:
                    action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in the Public Goods Game.
    It initially cooperates, then assesses and exploits others' generosity while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_payoffs_avg = None
        self.opponent_contributions_avg = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number < 4 and self.game_description.n_players > 1):
            return Action.C
        if state.round_number >= 4 and state.round_number <= 6:
            opponent_contributions = np.sum(history.opponent_actions, axis=1)
            opponent_contributions_avg = np.mean(opponent_contributions / (self.game_description.n_players - 1))
            self.opponent_contributions_avg = opponent_contributions_avg
            if opponent_contributions_avg > 0.5 or (opponent_contributions_avg == 0.5 and random.random() < 0.5):
                return Action.D
        my_payoffs = np.array(history.my_payoffs)
        my_last_payoff = my_payoffs[-1]
        if self.my_payoffs_avg is None:
            self.my_payoffs_avg = my_payoffs.mean()
        else:
            self.my_payoffs_avg = (self.my_payoffs_avg * (state.round_number - 2) + my_last_payoff) / (state.round_number - 1)
        opponent_payoffs_avg = np.mean(history.opponent_payoffs, axis=1)
        if my_last_payoff < opponent_payoffs_avg[-1]:
            return Action.C
        elif state.round_number >= self.game_description.n_rounds - 2 or self.game_description.n_players == 1:
            return Action.D
        if history.my_actions[-1] and my_last_payoff > self.my_payoffs_avg:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.persistent_defectors = set()
        self.cooperators_encountered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        for i, action in enumerate(opponent_actions):
            if not action and i not in self.persistent_defectors:
                self.persistent_defectors.add(i)
                if len(self.persistent_defectors) >= 3:
                    return Action.D
        if np.all(history.opponent_actions[-1, :]):
            self.cooperators_encountered = True
        total_cooperation = np.sum(history.opponent_actions[-1, :])
        average_contribution = total_cooperation / self.game_description.n_players
        highest_contributor_index = np.argmax(history.opponent_payoffs[-1, :])
        if average_contribution < 0.5:
            return Action.D
        elif history.opponent_actions[-1, highest_contributor_index]:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    An exploitative strategy in the Public Goods Game that prioritizes exploiting opponents' greed and punishing their defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        all_cooperated = np.all(opponent_actions_last_round)
        any_defected = np.any(~opponent_actions_last_round)
        if self.opponent_defection_count >= 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if all_cooperated:
            return Action.C
        if any_defected:
            self.opponent_defection_count += 1
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in a public goods game.
    It balances exploration (cooperating initially and adjusting the threshold) with exploitation (defecting when opponents are generous).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.average_contribution_threshold = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.threshold_adjustment_rounds = 0
        self.defector_labels = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if np.all(~history.opponent_actions[-1, :]):
            return Action.C
        for opponent_index, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and all(~history.opponent_actions[max(0, state.round_number - 5):state.round_number, opponent_index]):
                self.defector_labels.add(opponent_index)
            elif opponent_index in self.defector_labels and opponent_action:
                self.defector_labels.remove(opponent_index)
        total_contributions = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number % 5 == 0 and state.round_number > 0:
            recent_payoffs = history.my_payoffs[max(0, state.round_number - 4):state.round_number + 1]
            if np.mean(recent_payoffs) > self.average_contribution_threshold:
                self.average_contribution_threshold *= 1.1
            else:
                self.average_contribution_threshold *= 0.9
        if total_contributions == self.average_contribution_threshold * self.game_description.n_players:
            return Action.D
        if any((opponent_index in self.defector_labels for opponent_index, _ in enumerate(history.opponent_actions[-1, :]))):
            return Action.D
        if total_contributions > self.average_contribution_threshold * self.game_description.n_players:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in the Public Goods Game.
    Initially cooperates, then defects against high-contribution opponents and mirrors moderate-contribution ones.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_opponents = self.game_description.n_players - 1
        contribution_ratio = np.sum(opponent_actions) / n_opponents
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif contribution_ratio > 0.7:
            return Action.D
        elif contribution_ratio < 0.3:
            return Action.C
        elif np.random.rand() < contribution_ratio:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    An adaptive strategy for the repeated Public Goods Game.
    It starts by cooperating unconditionally in the initial rounds,
    then monitors opponents' behavior and adapts its actions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_streaks = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_rounds // 4:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution < 0.5:
            return Action.D
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i]:
                self.opponent_defection_streaks[i] += 1
            else:
                self.opponent_defection_streaks[i] = 0
        for i in range(self.game_description.n_players - 1):
            if self.opponent_defection_streaks[i] > 2 or (not history.opponent_actions[-1, i] and avg_contribution > 0.5):
                return Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions.shape[0] >= 2 and (not history.opponent_actions[-2, i]) and history.opponent_actions[-1, i] and (avg_contribution > 0.5):
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    Exploitative and adaptive strategy for the Public Goods Game.
    
    Contributes in situations where cooperation is uncertain or low to create an environment 
    that encourages others to contribute more often, while exploiting opponents when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if opponent_cooperations >= self.n_players // 2:
            return Action.D
        elif opponent_cooperations > self.n_players // 4 and opponent_cooperations < self.n_players // 2:
            return Action.C
        elif history.opponent_actions[-1, :].sum() == 0:
            return Action.C
        elif opponent_cooperations == 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_streaks = [0] * (game_description.n_players - 1)
        self.punishing_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_cooperation = np.mean(history.opponent_actions[-1, :])
        if state.round_number < 5:
            if avg_cooperation >= 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 10:
            my_payoff = history.my_payoffs[-1]
            opponent_payoffs = history.opponent_payoffs[-1, :]
            avg_opponent_payoff = np.mean(opponent_payoffs)
            payoff_diff = (avg_opponent_payoff - my_payoff) / avg_opponent_payoff
            if avg_cooperation >= 0.5 and payoff_diff < 0.2:
                return Action.C
            else:
                return Action.D
        elif state.round_number > 10:
            if np.all(history.opponent_actions[-1, :]):
                if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if not action and self.opponent_defection_streaks[i] < 3:
                self.opponent_defection_streaks[i] += 1
                if self.opponent_defection_streaks[i] == 3:
                    self.punishing_opponents.add(i)
            elif action:
                self.opponent_defection_streaks[i] = 0
        if state.round_number < self.game_description.n_rounds - 1 and any((history.opponent_actions[-i, j] == Action.D for i in range(1, 4) for j in self.punishing_opponents)):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    An exploitative and adaptive strategy for the Public Goods Game.
    
    Initially cooperates with a probability of 0.7 to encourage initial contributions, 
    then adapts based on opponents' behavior in subsequent rounds, exploiting cooperation 
    or defection patterns while maximizing individual payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the strategy with the game description.

        Args:
        game_description (PublicGoodsDescription): The game parameters.
        """
        self.game_description = game_description
        self.initial_cooperation_prob = 0.7
        self.middle_game_defection_prob = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the next action based on the current state and history.

        Args:
        state (GameState): The current game state.
        history (None | PlayerHistory): The player's history or None for the first round.

        Returns:
        Action: The chosen action, either Cooperate (C) or Defect (D).
        """
        if state.round_number < 5:
            return Action.C if random.random() < self.initial_cooperation_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None:
            prev_opponent_actions = history.opponent_actions[-1, :]
            my_prev_payoff = history.my_payoffs[-1]
            avg_contribution = np.mean(history.my_payoffs)
            prev_cooperators = sum(prev_opponent_actions)
            if prev_cooperators > self.game_description.n_players // 2 and my_prev_payoff >= avg_contribution * self.game_description.k / self.game_description.n_players:
                return Action.C
            else:
                return Action.D if random.random() < self.middle_game_defection_prob else Action.C
        return Action.C

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    Gradual Reciprocity with Punishment (GRP) strategy.
    
    This strategy aims to balance cooperation and punishment, adapting to the opponent's behavior while remaining exploitative in nature.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.6 * game_description.k / game_description.n_players
        self.phi = 0.3 * (game_description.k / game_description.n_players) * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_contributions = np.sum(history.opponent_actions[-1, :])
        avg_contrib = opponent_contributions / self.game_description.n_players
        delta = np.mean(history.my_payoffs[-3:]) - np.mean(np.mean(history.opponent_payoffs[-3:, :], axis=0))
        if delta > self.phi:
            p = 0.7 + delta / self.game_description.n_players * 0.3
            return Action.D if random.random() < p else Action.C
        elif avg_contrib >= self.theta:
            return Action.C
        else:
            q = 0.4 + avg_contrib / self.game_description.k * 0.6
            return Action.D if random.random() < q else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    Adaptive Exploiter Strategy: Cooperate initially and reciprocally, 
    but defect when others fail to cooperate. Punish opponents who defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.move_average_window_size = min(game_description.n_rounds // 2, 10)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        last_round_opponent_contributions = np.mean(history.opponent_actions[-1, :])
        if self.move_average_window_size > 0 and len(history.my_payoffs) >= self.move_average_window_size:
            recent_opponent_contributions = history.opponent_actions[-self.move_average_window_size:, :]
            last_round_opponent_contributions = np.mean(recent_opponent_contributions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent_idx, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                return Action.D
        if last_round_opponent_contributions < self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation in the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.thresholds = []
        for i in range(game_description.n_rounds):
            self.thresholds.append(0.5 + game_description.k / game_description.n_players * ((i + 1) / game_description.n_rounds))
        self.cooperative_opponents = set()
        self.exploit_target = None
        self.last_round_cooperator_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.last_round_cooperator_count = sum(history.opponent_actions[-1, :])
        if self.last_round_cooperator_count == 0:
            return Action.C if random.random() < 0.2 else Action.D
        elif self.last_round_cooperator_count == 1:
            return Action.C if random.random() < 0.5 else Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action and i not in self.cooperative_opponents:
                self.cooperative_opponents.add(i)
        if len(self.cooperative_opponents) > 0:
            self.exploit_target = random.choice(list(self.cooperative_opponents))
        if state.round_number < self.game_description.n_rounds - 1:
            threshold = self.thresholds[state.round_number]
            if np.mean(history.my_payoffs) > 1 + self.game_description.k / self.game_description.n_players:
                return Action.C
            elif np.mean(history.my_payoffs) < 0.5:
                return Action.D
            elif self.exploit_target is not None and history.opponent_actions[-1, self.exploit_target]:
                return Action.D
            elif sum(history.opponent_actions[-1, :]) / self.game_description.n_players > threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    An exploitative strategy for the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        threshold = self.game_description.k / self.game_description.n_players * 0.5
        if total_contributions < threshold:
            return Action.D
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = np.mean(history.opponent_payoffs[-1, :])
        if my_payoff < opponent_payoffs and self.game_description.n_players - total_contributions <= self.game_description.n_players / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if all(history.my_actions):
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if total_contributions > threshold and my_payoff < opponent_payoffs:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    Exploit opponents' cooperation in a public goods game by adapting to their behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_consistency = np.full(game_description.n_players - 1, None)
        self.monitoring_phase_actions = [Action.D]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number <= 5:
            action = self.monitoring_phase_actions[state.round_number - 1]
            if len(self.monitoring_phase_actions) < 6:
                next_action = Action.C if action == Action.D else Action.D
                self.monitoring_phase_actions.append(next_action)
        else:
            opponent_cooperations = np.mean(history.opponent_actions[-5:, :], axis=0)
            for i, consistency in enumerate(self.opponent_consistency):
                if consistency is not None and history.opponent_actions[-1, i] != consistency:
                    self.opponent_consistency[i] = None
            action = Action.D
            for i, cooperation_rate in enumerate(opponent_cooperations):
                if self.opponent_consistency[i] is None:
                    if cooperation_rate > 0.5:
                        self.opponent_consistency[i] = True
                    elif cooperation_rate < 0.5 and state.round_number == history.my_actions.shape[0]:
                        action = Action.C if i % 2 == 1 else Action.D
            payoff_monitoring = np.mean(history.my_payoffs[-5:]) < np.mean(history.opponent_payoffs[-5:])
            if payoff_monitoring:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    An exploitative strategy that adapts to the group's behavior and exploits cooperative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_threshold = math.ceil(game_description.n_players / 2)
        self.threshold_adjustment_factor = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_cooperations = np.sum(history.opponent_actions, axis=0)
        average_opponent_payoffs = np.mean(history.opponent_payoffs, axis=1)
        cooperators_in_last_round = sum(history.opponent_actions[-1, :])
        my_payoff_in_last_round = history.my_payoffs[-1]
        if state.round_number > 1:
            average_total_payoffs = np.mean(np.sum(history.opponent_payoffs, axis=1) + history.my_payoffs)
            self.round_threshold += math.floor(average_total_payoffs * self.threshold_adjustment_factor)
        if cooperators_in_last_round <= self.game_description.n_players // 2:
            return Action.D
        average_payoff_per_player = np.mean(history.opponent_payoffs[-1, :]) + my_payoff_in_last_round / (self.game_description.n_players - 1)
        if average_payoff_per_player > 1.5:
            opponent_cooperation_frequencies = opponent_cooperations / state.round_number
            cooperation_threshold = np.mean(opponent_cooperation_frequencies) * 0.8
            return Action.C if any((opponent_cooperation_frequency > cooperation_threshold for opponent_cooperation_frequency in opponent_cooperation_frequencies)) else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contribution_rates = [0.5] * (game_description.n_players - 1)
        self.total_contributions = []
        self.my_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        for i in range(self.game_description.n_players - 1):
            self.opponent_contribution_rates[i] = (self.opponent_contribution_rates[i] * (state.round_number - 1) + history.opponent_actions[-1, i]) / state.round_number
        self.total_contributions.append(sum(history.opponent_actions[-1, :]))
        if all((rate < np.mean(self.opponent_contribution_rates) for rate in self.opponent_contribution_rates)):
            return Action.D
        elif any((rate > np.mean(self.opponent_contribution_rates) for rate in self.opponent_contribution_rates)):
            return Action.C
        if len(self.total_contributions) >= 3:
            trend = self.total_contributions[-1] - self.total_contributions[-2] + (self.total_contributions[-2] - self.total_contributions[-3])
            if trend > 0:
                return Action.C
            elif trend < 0:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    Exploitative strategy that maximizes its own payoff by exploiting the contributions of others while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[:, :])
        cooperation_rate = opponent_cooperations / (state.round_number * self.game_description.n_players - 1)
        self.cooperation_rate_history.append(cooperation_rate)
        if len(self.cooperation_rate_history) >= self.game_description.n_players // 2:
            average_cooperation_rate = np.mean(self.cooperation_rate_history[-self.game_description.n_players // 2:])
            if average_cooperation_rate < 0.5:
                if state.round_number % 2 == 1:
                    return Action.C
                else:
                    self.cooperation_rate_history = []
                    return Action.D
        if state.round_number >= self.game_description.n_rounds // 2 or (history is not None and np.all(history.opponent_actions[-1, :] == False)):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors 
    in the repeated Public Goods Game. It adapts to the game dynamics, identifying opportunities 
    to defect and punish opponents who do not contribute fairly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C if random.random() < self.cooperate_probability else Action.D
        elif state.round_number == 2:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            self.cooperate_probability = opponent_cooperators / (self.game_description.n_players - 1)
            return Action.C if random.random() < self.cooperate_probability else Action.D
        elif state.round_number > 2:
            previous_round_opponents_avg_contribution = np.mean(history.opponent_actions[-1, :])
            if previous_round_opponents_avg_contribution >= 0.5 and history.my_payoffs[-1] >= 1:
                return Action.C
            opponent_defected_last_round = any((not history.opponent_actions[-1, i] for i in range(self.game_description.n_players - 1)))
            if opponent_defected_last_round and any((history.opponent_payoffs[-1, i] > history.my_payoffs[-1] for i in range(self.game_description.n_players - 1))):
                return Action.D
            if state.round_number > 3:
                opponent_played_d_after_my_c = any((not history.opponent_actions[-2, i] and history.my_actions[-2] for i in range(self.game_description.n_players - 1)))
                if opponent_played_d_after_my_c:
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    Adaptive Exploiter strategy for Public Goods Game.
    
    This strategy initially tests cooperation and adapts based on observed behavior,
    willing to defect when beneficial or "punish" by cooperating under certain conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_contributors = 0
        self.previous_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        current_contributors = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        avg_contrib = current_contributors / self.game_description.n_players
        solo_contribution_payoff = self.game_description.k / self.game_description.n_players
        if avg_contrib > 0.5:
            return Action.C
        elif avg_contrib <= 0.5 and self.previous_payoff > solo_contribution_payoff:
            return Action.D
        elif (state.round_number - 1) % 2 == 0:
            return Action.C
        else:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_actions = {}
        self.opponent_payoffs = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        opponent_actions = history.opponent_actions[-1, :]
        if state.round_number < n // 2:
            average_contribution = np.mean(opponent_actions)
            return Action.C if average_contribution > 0.5 else Action.D
        elif n // 2 <= state.round_number < 3 * n // 4:
            opponent_cooperators = np.sum(opponent_actions)
            return Action.C if opponent_cooperators > self.game_description.n_players // 2 else Action.D
        else:
            total_contributions = np.sum(np.mean(history.opponent_actions, axis=1))
            return Action.C if total_contributions >= k / n * self.game_description.n_players else Action.D
        for i in range(self.game_description.n_players):
            if history.opponent_actions[:, i].all() == False:
                return Action.D
        if state.round_number == r - 1:
            return Action.D
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action not in self.opponent_actions:
                self.opponent_actions[opponent_action] = []
                self.opponent_payoffs[opponent_action] = []
            self.opponent_actions[opponent_action].append(state.round_number)
            self.opponent_payoffs[opponent_action].append(history.opponent_payoffs[-1, i])
        for actions in self.opponent_actions.values():
            if len(actions) > 1 and np.allclose(np.diff(actions), 1):
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    Exploitative strategy that adapts to the dynamics of the Public Goods Game.
    Initially gathers information, then identifies and exploits opponents with low cooperation rates.
    Adjusts its approach in late rounds to maximize personal payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
            if self.opponent_cooperation_rates is None:
                self.opponent_cooperation_rates = opponent_cooperation_rates
            else:
                self.opponent_cooperation_rates = (self.opponent_cooperation_rates * state.round_number + opponent_cooperation_rates) / (state.round_number + 1)
        if state.round_number < 5:
            for i, rate in enumerate(self.opponent_cooperation_rates):
                if rate < 0.3:
                    return Action.D
        if self.game_description.n_rounds // 2 <= state.round_number < self.game_description.n_rounds - 2:
            total_contributions = np.sum(history.opponent_actions[-1, :])
            if total_contributions > np.mean(np.sum(history.opponent_actions[:-1, :], axis=0)):
                for i, rate in enumerate(self.opponent_cooperation_rates):
                    if rate < 0.7:
                        return Action.D
            elif state.round_number % 2 == 0:
                return Action.D
        if self.game_description.n_rounds - 2 <= state.round_number < self.game_description.n_rounds:
            for i, rate in enumerate(self.opponent_cooperation_rates):
                if rate > 0.7:
                    return Action.C
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    This strategy initially cooperates and then adapts to opponents' behavior.
    It punishes free-riding while maintaining a cooperative stance when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        avg_contribution = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        punish_opponent = history.my_actions[-1] == True and np.any(history.opponent_actions[-1, :] == False)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if punish_opponent or avg_contribution < 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    An adaptive strategy for the Public Goods Game that balances cooperation and defection to maximize payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.exploitation_intensity = 1.0
        self.noise_tolerance = 2
        self.opponent_classifications = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution > self.threshold:
            self.threshold -= 0.05
        else:
            self.threshold += 0.05
        if self.opponent_classifications is None:
            opponent_cooperations = np.mean(history.opponent_actions, axis=0)
            self.opponent_classifications = np.where(opponent_cooperations > 0.7, 'Cooperators', np.where(opponent_cooperations < 0.3, 'Defectors', 'Unpredictables'))
        cooperators = sum((1 for classification in self.opponent_classifications if classification == 'Cooperators'))
        defectors = sum((1 for classification in self.opponent_classifications if classification == 'Defectors'))
        if cooperators > defectors:
            self.exploitation_intensity -= 0.05
        else:
            self.exploitation_intensity += 0.05
        if avg_contribution >= self.threshold and random.random() < self.exploitation_intensity:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    Exploit opponents' behaviors while adapting to the environment.
    Cooperate initially and then respond to the average contribution level.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []
        self.opponent_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(self.cooperation_history) < state.round_number:
            self.cooperation_history.append(history.my_actions[-1])
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            self.opponent_cooperators.append(opponent_cooperators)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_contribution = np.mean(self.cooperation_history)
        opponent_avg_contribution = np.mean(self.opponent_cooperators)
        if state.round_number > 0 and all((not x for x in history.opponent_actions[-1])):
            return Action.C
        if avg_contribution == 1:
            return Action.D
        if opponent_avg_contribution > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.opponent_cooperation_rates is None:
            self.opponent_cooperation_rates = np.zeros(self.game_description.n_players - 1)
        last_round_opponent_actions = history.opponent_actions[-1, :]
        for i in range(len(last_round_opponent_actions)):
            if last_round_opponent_actions[i]:
                self.opponent_cooperation_rates[i] += 1 / (state.round_number + 1)
        if state.round_number < 5:
            if sum(history.opponent_actions[-1, :]) >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        avg_contribution_rate = np.mean(self.opponent_cooperation_rates)
        consistently_cooperative_opponents = np.sum(self.opponent_cooperation_rates > 0.5)
        if avg_contribution_rate > 0.5 and consistently_cooperative_opponents >= 1:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        for i in range(len(self.opponent_cooperation_rates)):
            if self.opponent_cooperation_rates[i] < 0.1:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors.
    Cooperates in the first round, then defects if most players cooperated previously,
    unless an opponent has defected every time so far.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_always_defects = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_always_defects is None:
            self.opponent_always_defects = np.all(~history.opponent_actions, axis=0)
        else:
            self.opponent_always_defects &= ~history.opponent_actions[-1, :]
        if state.round_number < self.game_description.n_rounds - 1 and np.any(self.opponent_always_defects):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        total_contributions_prev = np.sum(history.my_actions[:-1]) + np.sum(history.opponent_actions[:-1, :])
        threshold = (self.game_description.n_players - 1) / self.game_description.k
        if total_contributions_prev >= threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_history = None
        self.exploitable_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_history is None:
            self.opponent_history = [[0, 0] for _ in range(self.game_description.n_players)]
            self.exploitable_opponents = set()
        for player, actions in enumerate(history.opponent_actions[-1, :]):
            self.opponent_history[player][Action.C.value if actions else Action.D.value] += 1
        tau = 0.5
        for player, (cooperate_count, defect_count) in enumerate(self.opponent_history):
            ratio = cooperate_count / (cooperate_count + defect_count) if cooperate_count + defect_count > 0 else 1.0
            if ratio < tau and player not in self.exploitable_opponents:
                self.exploitable_opponents.add(player)
        theta = (self.game_description.n_players + self.game_description.k - 1) / (2 * self.game_description.n_players)
        total_contributions = sum(history.my_actions[-1:]) + np.sum(history.opponent_actions[-1, :])
        if total_contributions > theta:
            return Action.D
        for player in self.exploitable_opponents:
            if not history.opponent_actions[-1, player]:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    An exploitative strategy for the repeated Public Goods Game.
    
    In the initial rounds, it contributes to gather information about opponents' behavior.
    Then, it monitors and adapts to opponents' actions by mirroring their average contributions.
    Finally, it exploits cooperative opponents while punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.monitoring_phase = True
        self.exploitation_phase_opponents = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.monitoring_phase:
            avg_contribution = np.mean(history.opponent_actions[-1, :])
            if avg_contribution > 0.5:
                return Action.C
            else:
                return Action.D
        if state.round_number == 5:
            self.exploitation_phase_opponents = {}
            for i in range(self.game_description.n_players):
                opponent_contributions = history.opponent_actions[:, i]
                avg_contribution = np.mean(opponent_contributions)
                self.exploitation_phase_opponents[i] = (avg_contribution, np.sum(opponent_contributions > 0.5))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i in range(self.game_description.n_players):
            opponent_contributions, num_cooperations = self.exploitation_phase_opponents[i]
            if num_cooperations > (state.round_number + 1) // 2:
                if random.random() < opponent_contributions:
                    return Action.C
        for i in range(self.game_description.n_players):
            previous_action = history.opponent_actions[-2, i]
            current_action = history.opponent_actions[-1, i]
            if previous_action and (not current_action):
                self.exploitation_phase_opponents[i] = (0.0, -10)
        return Action.D

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    Adaptive Exploitation (AE) strategy for Public Goods Game.
    Balances individual incentives with collective welfare, aiming to be exploitative yet robust.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_avg_contributions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if self.opponent_avg_contributions is None:
            opponent_actions = history.opponent_actions[:3, :]
            avg_contributions = np.mean(opponent_actions, axis=0)
            self.opponent_avg_contributions = avg_contributions
        if np.any(self.opponent_avg_contributions < 0.5):
            non_cooperative_game = True
        else:
            non_cooperative_game = False
        if not non_cooperative_game:
            most_cooperative_opponent_actions = history.opponent_actions[-1, np.argmax(self.opponent_avg_contributions)]
            return Action.C if most_cooperative_opponent_actions else Action.D
        else:
            total_contribution_last_round = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
            if total_contribution_last_round < self.game_description.k * self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game (PGG) that adapts to group behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_contributions = sum(history.opponent_actions[-1, :])
        my_contribution = history.my_actions[-1]
        my_payoff = history.my_payoffs[-1]
        average_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
        threshold = self.game_description.k / self.game_description.n_players * 0.5
        if opponent_contributions == 0:
            return Action.C
        elif opponent_contributions == 1 and (not my_contribution):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if opponent_contributions / self.game_description.n_players < threshold:
            return Action.D
        elif my_payoff > average_opponent_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    Exploitative strategy "Adaptive Mirror with Decay" that balances cooperation and defection,
    taking into account the game parameters and previous actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.decay = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        non_cooperative_rounds = sum((1 for i in range(len(history.my_actions)) if np.mean(history.opponent_actions[i, :]) < 0.5))
        self.decay = min(0.5, non_cooperative_rounds / self.game_description.n_players)
        cooperation_threshold = max(0.0, 0.5 - self.decay)
        opponent_contributions = np.mean(history.opponent_actions[-2:, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif opponent_contributions >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    A strategy that balances exploitation with adaptability in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        threshold = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_contributions >= threshold:
            return Action.D
        else:
            acr = total_contributions / self.game_description.n_players
            self.p = acr * (self.game_description.k / self.game_description.n_players)
            if self.p > 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    Tit-for-Tat with Conditional Cooperation (TFT-CC) strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        total_previous_round_contributions = previous_round_cooperators + (history.my_actions[-1] if history else 0)
        threshold = self.game_description.k / self.game_description.n_players * 0.5
        average_contribution = total_previous_round_contributions / self.game_description.n_players
        if previous_round_cooperators < self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if average_contribution >= threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting 
    the contributions of others while minimizing its own. It adapts to the game's 
    history, using a combination of cooperation and defection to achieve this goal.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.contribution_threshold = 0.6
        self.total_contributions = [0] * self.n_players
        self.round_contributions = [0] * self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        for i in range(self.n_players - 1):
            self.total_contributions[i] += history.opponent_actions[-1, i]
            self.round_contributions[i] = int(history.opponent_actions[-1, i])
        opponents_are_cooperators = [self.round_contributions[i] >= self.contribution_threshold * state.round_number for i in range(self.n_players - 1)]
        if all(opponents_are_cooperators):
            return Action.D
        p_cooperation = np.mean([self.k / self.n_players * (1 - self.total_contributions[i] / state.round_number) for i in range(self.n_players - 1)])
        if history.my_actions[-1] == Action.D:
            recent_opponent_defection = [history.opponent_actions[-1, i] == False for i in range(self.n_players - 1)]
            if any(recent_opponent_defection):
                return random.choice([Action.C, Action.D])
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return Action.C if random.random() < p_cooperation else Action.D

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    An exploitative Public Goods Game (PGG) strategy that balances cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        window_size = min(5, len(history.my_payoffs))
        recent_contributions = history.opponent_actions[-window_size:, :]
        average_contribution = np.mean(np.sum(recent_contributions, axis=0))
        if self.opponent_contributions is None:
            self.opponent_contributions = np.zeros((self.game_description.n_players - 1,))
        self.opponent_contributions += history.opponent_actions[-1, :]
        opponent_averages = self.opponent_contributions / len(history.my_payoffs)
        soft_opponents = np.where(opponent_averages > average_contribution)[0]
        recent_cooperators = history.opponent_actions[-1, :] == 1
        punish_opponents = np.where(recent_cooperators & (history.my_payoffs[-1] < history.opponent_payoffs[-1, :]))[0]
        self_payoff_average = np.mean(history.my_payoffs)
        if self_payoff_average < np.mean(np.min(history.opponent_payoffs, axis=0)):
            return Action.D
        if len(soft_opponents) > 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(punish_opponents) > 0:
            return Action.D
        if average_contribution >= self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    An exploitative strategy designed to capitalize on a wide range of opponent behaviors in the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = history.opponent_actions[-1, :].sum()
        total_players = self.game_description.n_players
        if opponent_cooperations > total_players * 0.5:
            return Action.D
        elif opponent_cooperations < total_players * 0.3:
            return Action.C
        elif history.my_actions[-1]:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    Exploitative strategy aiming to capitalize on opponents' cooperation while minimizing its own cooperative efforts.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.3
        self.punishment_factor = 0.1
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_cooperators = np.sum(opponent_cooperations[-1])
        avg_opponent_payoffs = np.mean(history.opponent_payoffs[-1])
        if state.round_number > 3:
            opponent_defections = self.game_description.n_players - 1 - total_cooperators
            if opponent_defections == 0:
                self.cooperation_probability = max(0, self.cooperation_probability - self.punishment_factor)
            elif avg_opponent_payoffs > history.my_payoffs[-1]:
                self.cooperation_probability = min(1, self.cooperation_probability + self.contribution_threshold)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators > self.game_description.n_players * self.contribution_threshold:
            self.cooperation_probability = min(1, self.cooperation_probability + self.punishment_factor)
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0
        self.cooperation_rate = 0.5
        self.opponent_labels = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_rate else Action.D
        self._update_opponent_labels(history)
        self.avg_payoff = np.mean(history.my_payoffs)
        self._adjust_cooperation_rate()
        total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if total_contributions < self.game_description.n_players / 2:
            return Action.D
        elif total_contributions > 3 * self.game_description.n_players / 4:
            return Action.D
        else:
            cooperation_prob = self.avg_payoff / (self.game_description.k + 1)
            return Action.C if random.random() < min(1, max(0, cooperation_prob)) else Action.D

    def _update_opponent_labels(self, history: PlayerHistory):
        if self.opponent_labels is None:
            self.opponent_labels = np.full((self.game_description.n_players - 1,), False)
        for i in range(self.game_description.n_players - 1):
            opponent_cooperation_rate = np.mean(history.opponent_actions[:-1, i])
            self.opponent_labels[i] = opponent_cooperation_rate > 2 / 3

    def _adjust_cooperation_rate(self):
        if self.avg_payoff < self.game_description.n_players / 4:
            self.cooperation_rate += 0.1
        elif self.avg_payoff > 3 * self.game_description.n_players / 4:
            self.cooperation_rate -= 0.1
        self.cooperation_rate = min(1, max(0, self.cooperation_rate))

    def _exploitation_mode(self):
        cooperators = sum(self.opponent_labels)
        if cooperators >= len(self.opponent_labels) // 2:
            return Action.C
        return Action.D

    def _last_round_logic(self):
        return Action.D

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    Adaptive and exploitative strategy for the Public Goods Game.
    
    This strategy initially cooperates, then punishes opponents who defect while others cooperate,
    exploits overly cooperative opponents, and adapts to changing opponent behaviors over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.opponent_ewma_contributions: NDArray[np.float64] | None = None
        self.punish_opponents: list[int] = []
        self.punishment_rounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_ewma_contributions is None:
            self.opponent_ewma_contributions = np.mean(history.opponent_actions[-1, :], axis=0)
        else:
            decay_factor = 0.2
            current_round_contributions = history.opponent_actions[-1, :]
            new_ewma_contributions = decay_factor * current_round_contributions + (1 - decay_factor) * self.opponent_ewma_contributions
            self.opponent_ewma_contributions = new_ewma_contributions
        if len(self.punish_opponents) > 0:
            return Action.D
        opponent_avg_contribution = np.mean(self.opponent_ewma_contributions)
        threshold = 0.5
        if opponent_avg_contribution < threshold:
            self.punish_opponents = [i for i, x in enumerate(history.opponent_actions[-1, :]) if not x]
            return Action.D
        recent_cooperators = np.sum(history.opponent_actions[-1, :], axis=0) > self.n_players * 0.75
        if recent_cooperators:
            return Action.C
        if state.round_number == self.n_rounds - 1 or opponent_avg_contribution == threshold:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    A strategic approach for the Public Goods Game that adapts to various opponent behaviors,
    focusing on exploitation while being mindful of robustness across different game scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_with_majority_cooperation = 0
        self.punished_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self._identify_punished_opponents(history)
        acr = self._calculate_average_contribution_rate(history, state.round_number)
        if state.round_number >= self.game_description.n_rounds - 3:
            return self._adjust_strategy_for_final_rounds(acr, history)
        if acr >= 0.5:
            return Action.C
        if state.round_number <= 3:
            return self._mirror_majority_action(history)
        return Action.D

    def _calculate_average_contribution_rate(self, history: PlayerHistory, current_round: int) -> float:
        """Calculate the average contribution rate (ACR) of all players over the last 3 rounds."""
        total_cooperators = np.sum(history.opponent_actions[max(0, current_round - 3):current_round, :])
        return total_cooperators / (self.game_description.n_players * min(3, current_round))

    def _identify_punished_opponents(self, history: PlayerHistory):
        """Identify consistently defecting opponents and add them to the set of punished opponents."""
        recent_actions = history.opponent_actions[-5:, :]
        for opponent in range(self.game_description.n_players - 1):
            defector_rate = np.sum(recent_actions[:, opponent]) / len(recent_actions)
            if defector_rate > 0.7 and opponent not in self.punished_opponents:
                self.punished_opponents.add(opponent)

    def _mirror_majority_action(self, history: PlayerHistory) -> Action:
        """Mirror the majority action of the group from previous round."""
        previous_round_actions = history.opponent_actions[-1, :]
        cooperators = np.sum(previous_round_actions)
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D

    def _adjust_strategy_for_final_rounds(self, acr: float, history: PlayerHistory) -> Action:
        """Adjust strategy for final rounds based on group's average payoff."""
        if np.mean(history.my_payoffs[-3:]) < self.game_description.k / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    
    This strategy adapts to opponents' cooperative tendencies while minimizing 
    the impact of potential retaliatory actions. It cooperates when average 
    contributions are high and defects otherwise, with a punishment mechanism 
    to discourage repeated defection by opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players / 2
        self.contribution_thresholds = []
        self.defector_count = {}
        for i in range(game_description.n_players - 1):
            self.defector_count[i] = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(self.contribution_thresholds) >= 5:
            self.threshold = np.mean(self.contribution_thresholds[-5:])
        opponent_contributions = sum(history.opponent_actions[-1, :])
        total_contributions = history.my_actions[-1] + opponent_contributions
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i]:
                self.defector_count[i] += 1
                if self.defector_count[i] >= 2 and total_contributions / self.game_description.n_players > self.threshold:
                    return Action.D
        self.contribution_thresholds.append(total_contributions)
        if opponent_contributions >= self.game_description.k / self.game_description.n_players * 2:
            probability = opponent_contributions / (self.game_description.k / self.game_description.n_players)
            return Action.C if random.random() < probability else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    The Adaptive Exploiter strategy prioritizes self-interest by 
    initially mirroring opponents, then conditionally cooperating,
    and finally exploiting in late rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.consecutive_cooperators is None and history.my_actions.size > 1:
            n_players = self.game_description.n_players - 1
            self.consecutive_cooperators = np.full(n_players, True)
        opponents_last_round = history.opponent_actions[-1, :]
        if state.round_number >= 2 and self.consecutive_cooperators is not None:
            cooperators_last_round = np.where(opponents_last_round == 1)
            defectors_last_round = np.where(opponents_last_round == 0)
            self.consecutive_cooperators[defectors_last_round] = False
        if state.round_number < 5:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            avg_payoff = np.mean(history.my_payoffs[-1])
            expected_coop_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
            if avg_payoff >= expected_coop_payoff:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize payoffs by exploiting opponents' behaviors in the repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.most_cooperative_opponent = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            average_contribution = np.mean(history.opponent_actions[-1, :])
            if average_contribution > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            opponent_contributions = np.mean(history.opponent_actions[:-1, :], axis=0)
            if self.most_cooperative_opponent is None or opponent_contributions[self.most_cooperative_opponent] < np.max(opponent_contributions):
                self.most_cooperative_opponent = np.argmax(opponent_contributions)
            return Action.C if history.opponent_actions[-1, self.most_cooperative_opponent] else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            total_contributions = np.sum(history.my_actions) + np.sum(np.sum(history.opponent_actions, axis=1))
            if state.round_number > 5 and total_contributions >= np.sum(history.my_payoffs[:-1]) + np.sum(np.sum(history.opponent_payoffs[:-1, :], axis=0)):
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    Adaptive Exploiter strategy for the Public Goods Game.
    
    Initially cooperate to test opponents' willingness to contribute. Then, 
    adapt based on the total contributions in previous rounds and monitor 
    opponents' average contribution rates. Trigger exploitation against 
    significantly under-contributing opponents while maintaining cooperation 
    with others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = game_description.n_players
        self.opponent_contribution_rates = np.zeros(game_description.n_players - 1)
        self.exploitation_targets = set()
        self.last_round_defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for i in range(self.game_description.n_players - 1):
            recent_actions = history.opponent_actions[-3:, i]
            self.opponent_contribution_rates[i] = np.mean(recent_actions)
        average_rate = np.mean(self.opponent_contribution_rates)
        self.exploitation_targets = set(np.where(self.opponent_contribution_rates < 0.5 * average_rate)[0])
        if state.round_number % (self.game_description.n_players // 2) == 0:
            recent_contributions = np.sum(history.opponent_actions[-self.game_description.n_players:, :], axis=1)
            average_contribution = np.mean(recent_contributions)
            self.contribution_threshold += math.copysign(0.2 * self.game_description.n_players, average_contribution - self.contribution_threshold)
        recent_defectors = set(np.where(~history.opponent_actions[-1, :])[0])
        if len(recent_defectors) == self.game_description.n_players - 1:
            return Action.D
        recent_cooperators = np.sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        if recent_cooperators >= self.contribution_threshold and state.round_number < self.game_description.n_rounds - 1:
            action = Action.C
            for i in range(self.game_description.n_players - 1):
                if i in self.exploitation_targets:
                    action = Action.D
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    An exploitative strategy that balances cooperation and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_factor = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators > self.game_description.n_players // 2:
            action = Action.C
        else:
            action = Action.D
        average_payoff = np.mean(history.my_payoffs[-1:])
        expected_cooperative_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
        if average_payoff > expected_cooperative_payoff:
            self.exploitation_factor += 1
        elif average_payoff < expected_cooperative_payoff:
            self.exploitation_factor -= 1
            self.exploitation_factor = max(self.exploitation_factor, 0)
        if self.exploitation_factor >= 3 and state.round_number > 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players in a Public Goods Game.
    Cooperates if opponents' average contribution rate is above a certain threshold, otherwise defects.
    Adjusts the threshold value based on changes in opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * 0.6
        self.last_three_rounds_cooperated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < 5:
            opponent_contributions = sum(history.opponent_actions[-1, :])
            average_contribution_rate = opponent_contributions / self.game_description.n_players
            if average_contribution_rate > self.threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number < 15:
            opponent_contributions = sum(history.opponent_actions[-1, :])
            average_contribution_rate = opponent_contributions / self.game_description.n_players
            if average_contribution_rate > self.threshold:
                self.threshold *= 1.05
                return Action.C
            else:
                self.threshold *= 0.95
                return Action.D
        elif state.round_number >= 15:
            if history.opponent_actions[-3:, :].sum() > 0:
                self.last_three_rounds_cooperated = True
                return Action.C
            else:
                self.last_three_rounds_cooperated = False
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.sum() == 0:
            return Action.D
        if state.round_number % 5 == 0:
            opponent_contributions = sum(history.opponent_actions[-5:, :])
            average_contribution_rate = opponent_contributions / (self.game_description.n_players * 5)
            if average_contribution_rate > self.threshold + 0.1:
                self.threshold *= 1.05
            elif average_contribution_rate < self.threshold - 0.1:
                self.threshold *= 0.95
        return Action.D

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game. It starts by cooperating,
    monitors opponents' behavior, and adapts to maximize personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.monitoring_phase = True
        self.exploitation_phase = False
        self.adaptive_adjustment = 0.5
        self.defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_cooperated = np.sum(history.opponent_actions[-1, :])
        average_contribution = opponents_cooperated / self.game_description.n_players
        if self.monitoring_phase and state.round_number <= 5:
            if average_contribution > 0.5:
                self.exploitation_phase = True
                self.monitoring_phase = False
            return Action.C
        if self.exploitation_phase:
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if not opponent_action and i not in self.defectors:
                    self.defectors.add(i)
            if history.my_actions.size % 5 == 0:
                self.adaptive_adjustment = average_contribution
            if average_contribution > 0.7:
                return Action.C
            elif average_contribution < 0.3:
                return Action.D
            else:
                for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                    if i not in self.defectors:
                        return Action.C
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and len(self.defectors) > 0:
            return Action.D
        if self.game_description.n_players == 2:
            opponent_action = history.opponent_actions[-1, 0]
            if opponent_action:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_payoff_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        if history.opponent_payoffs.shape[0] < 2:
            previous_round_opponent_payoffs = np.zeros(n_players - 1)
        else:
            previous_round_opponent_payoffs = history.opponent_payoffs[-2, :]
        max_payoff = k
        if max_payoff > 0:
            normalized_opponent_payoffs = previous_round_opponent_payoffs / max_payoff
        else:
            return Action.D
        opponent_cooperations_last_round = history.opponent_actions[-1, :]
        average_opponent_contribution = np.mean(opponent_cooperations_last_round)
        if average_opponent_contribution > self.opponent_payoff_threshold:
            cooperate_reciprocally = True
        else:
            cooperate_reciprocally = False
        if history.opponent_actions.shape[0] < 2 or not np.any(opponent_cooperations_last_round):
            defect_exploitatively_with_probability = 1.0
        else:
            defect_exploitatively_with_probability = np.mean(1 - normalized_opponent_payoffs)
        if cooperate_reciprocally:
            return Action.C
        random_number = random.random()
        if random_number < 1 - defect_exploitatively_with_probability:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    The Adaptive Exploiter strategy, which exploits opponents' cooperation while 
    minimizing its own contributions. It adapts to opponents' behavior by punishing 
    defectors and rewarding cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperators = []
        self.my_payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        opponent_cooperators = sum(previous_round_actions)
        my_previous_action = history.my_actions[-1]
        my_payoff = history.my_payoffs[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, opponent_action in enumerate(previous_round_actions):
            opponent_payoff = history.opponent_payoffs[-1, i]
            if my_previous_action and (not opponent_action) and (my_payoff > opponent_payoff):
                self.opponent_cooperators[i] = False
            elif not my_previous_action and opponent_action and (my_payoff < opponent_payoff):
                self.opponent_cooperators[i] = True
        if len(self.opponent_cooperators) != self.game_description.n_players - 1:
            self.opponent_cooperators = [True] * (self.game_description.n_players - 1)
        for i, opponent_action in enumerate(previous_round_actions):
            if self.opponent_cooperators[i]:
                return Action.D
        if opponent_cooperators >= self.game_description.k - 1:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C if state.round_number % 2 else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors by conditionally cooperating 
    and defecting. Initially cooperates, then adapts based on opponents' actions in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = np.sum(opponent_actions)
        any_opponents_cooperated = num_cooperators > 0
        sufficient_contributions = num_cooperators > self.game_description.n_players / 2
        was_punished = history.my_actions[-1] and np.any(~opponent_actions)
        if any_opponents_cooperated and sufficient_contributions:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif was_punished:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    An adaptive strategy that balances individual gain with collective welfare,
    while being robust and adaptive to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round = history.my_actions.shape[0] - 1
        my_prev_payoff = history.my_payoffs[prev_round]
        opponent_prev_actions = history.opponent_actions[prev_round, :]
        total_contributions = sum(opponent_prev_actions) + history.my_actions[prev_round]
        if self.cooperation_history is None:
            self.cooperation_history = np.zeros(self.game_description.n_players - 1, dtype=int)
        else:
            for i, opponent_action in enumerate(opponent_prev_actions):
                if opponent_action:
                    self.cooperation_history[i] += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_payoff = np.mean(np.concatenate(([my_prev_payoff], history.opponent_payoffs[prev_round, :])))
        if any(self.cooperation_history >= 3):
            return Action.C
        if my_prev_payoff > avg_payoff:
            return Action.C
        elif total_contributions < self.game_description.k / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    Conditionally Cooperative Exploiter: An adaptive and exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        opponent_actions = history.opponent_actions[-1, :]
        opponent_cooperators = sum(opponent_actions)
        if all(history.my_payoffs[:state.round_number] == np.ones(state.round_number)):
            return Action.C
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action == False:
                if history.my_payoffs[state.round_number - 1] > 0 or len(self.defectors) >= n_players // 2:
                    self.defectors.add(i)
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action == True:
                if history.opponent_payoffs[-1, i] > 0 or (len(self.defectors) < n_players // 2 and history.my_payoffs[state.round_number - 1] > 0):
                    return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    A strategy that initially cooperates to encourage others, then reciprocates cooperation or defection,
    and finally defects in the endgame to maximize personal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defectors = set()
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.game_description.n_players == 1:
            return Action.C
        for opponent_index in range(self.game_description.n_players - 1):
            if np.all(history.opponent_actions[:, opponent_index] == False):
                self.opponent_defectors.add(opponent_index)
        if any((opponent_index in self.opponent_defectors for opponent_index in range(self.game_description.n_players - 1))):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        recent_cooperators = sum(history.opponent_actions[-1, :])
        if np.any(history.opponent_actions[-1, :] == False):
            return Action.D
        if recent_cooperators / self.game_description.n_players > self.cooperation_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contribution = None
        self.defect_threshold = game_description.k / game_description.n_players * 0.5
        self.punishment_pending = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[:-1]
        opponent_cooperations = np.sum(opponent_actions, axis=0)
        avg_contribution = np.mean(np.where(opponent_cooperations > 0, 1, 0), axis=None)
        self.avg_contribution = avg_contribution if self.avg_contribution is None else (self.avg_contribution * state.round_number + avg_contribution) / (state.round_number + 1)
        opponent_defections = np.sum(np.where(opponent_actions == 0, 1, 0), axis=0)
        if np.any(opponent_defections > 0):
            self.punishment_pending = True
        if self.punishment_pending:
            return Action.D
        elif avg_contribution < self.defect_threshold:
            return Action.D
        else:
            return Action.C
        if state.round_number == game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5
        self.exploitation_window_rounds = random.randint(3, 5)
        self.exploitation_window_counter = 0
        self.punishment_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_contributions = np.mean(history.opponent_actions[:, :], axis=1)
        avg_contribution = np.mean(opponent_contributions)
        if avg_contribution < self.contribution_threshold:
            return Action.D
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            for i, opponent in enumerate(history.opponent_actions[-1, :]):
                if opponent and history.opponent_payoffs[-1, i] > avg_contribution:
                    self.punishment_target = i
                    return Action.D
        if self.exploitation_window_counter == 0 or state.round_number % self.exploitation_window_rounds != 0:
            self.exploitation_window_counter += 1
        else:
            self.exploitation_window_counter = 0
            return Action.D
        if random.random() < 0.2:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_mode = set()
        self.defection_threshold = game_description.k / game_description.n_players * 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_contributions = sum(opponent_actions)
        avg_contribution_per_player = total_contributions / self.game_description.n_players
        if len(self.exploitation_mode) > 0:
            return Action.D
        if avg_contribution_per_player < self.defection_threshold:
            return Action.D
        for opponent_idx, action in enumerate(opponent_actions):
            if not action and history.my_actions[-1]:
                self.exploitation_mode.add(opponent_idx)
        if state.round_number < 3:
            if total_contributions >= self.game_description.n_players // 2:
                return Action.C
        for opponent_idx, action in enumerate(opponent_actions):
            if not action and history.my_actions[-1] and (opponent_idx not in self.exploitation_mode):
                self.exploitation_mode.add(opponent_idx)
        if state.round_number == self.game_description.n_rounds - 1:
            if total_contributions > self.defection_threshold * self.game_description.n_players:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    Exploitative strategy using conditional cooperation with punishment in the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing_opponent = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        if history is not None:
            observation_phase_cooperation_rate = np.mean(history.opponent_actions[:state.round_number // 2, :].sum(axis=1) / self.game_description.n_players)
            if observation_phase_cooperation_rate > 0.5:
                return Action.C
        if history is not None and state.round_number >= 2:
            previous_round_opponent_actions = history.opponent_actions[-1, :]
            cooperators_in_previous_round = sum(previous_round_opponent_actions)
            if cooperators_in_previous_round < self.game_description.n_players / 2:
                self.punishing_opponent = True
        if history is not None and state.round_number >= self.game_description.n_rounds // 3:
            recent_cooperation_rate = np.mean(history.opponent_actions[-state.round_number // 3:, :].sum(axis=1) / self.game_description.n_players)
            if recent_cooperation_rate > 0.5 and self.punishing_opponent:
                self.punishing_opponent = False
        return Action.D if self.punishing_opponent or (history is not None and np.all(history.opponent_actions == 0)) else Action.C

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    Adaptive and exploitative strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_contribution_history = []
        self.opponent_type_counts = [0] * (game_description.n_players - 1)
        self.exploit_cooperator_id = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        my_contribution = history.my_actions[-1]
        self.my_contribution_history.append(my_contribution)
        recent_cooperators = sum(history.opponent_actions[-1, :])
        for i, opponent_actions in enumerate(history.opponent_actions.T):
            if len(opponent_actions) > 0:
                cooperation_rate = np.mean(opponent_actions)
                self.opponent_type_counts[i] += 1 if cooperation_rate >= 0.5 else -1
        punish_freeloaders = False
        for i, opponent_type_count in enumerate(self.opponent_type_counts):
            if opponent_type_count < -len(history.my_actions) * 0.75:
                punish_freeloaders = True
                break
        exploit_cooperator_id = np.argmax(self.opponent_type_counts)
        self.exploit_cooperator_id = exploit_cooperator_id if self.opponent_type_counts[exploit_cooperator_id] > 0 else None
        if recent_cooperators >= self.game_description.n_players // 2:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif punish_freeloaders and random.random() < 0.25:
            return Action.C
        elif recent_cooperators == self.game_description.n_players // 2:
            return Action.D
        elif self.exploit_cooperator_id is not None:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    A strategic approach for the Public Goods Game that adapts to the overall level of cooperation 
    and exploits known defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploited_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number >= 2:
            for player, action in enumerate(history.opponent_actions[-2, :]):
                if not action:
                    self.exploited_players.add(player)
        if cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif any(self.exploited_players):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if cooperators >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    Exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitability_scores = None
        self.last_round_cooperate = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.exploitability_scores is None:
            n_players = self.game_description.n_players - 1
            self.exploitability_scores = [0.5] * n_players
        opponent_actions = history.opponent_actions[-1, :]
        my_action = history.my_actions[-1]
        average_contribution = np.mean(opponent_actions)
        for i, (opponent_cooperated, opponent_exploitability_score) in enumerate(zip(opponent_actions, self.exploitability_scores)):
            if my_action and (not opponent_cooperated):
                self.exploitability_scores[i] = min(opponent_exploitability_score + 0.1, 1)
            elif not my_action and opponent_cooperated:
                self.exploitability_scores[i] = max(opponent_exploitability_score - 0.1, 0)
        if average_contribution >= 0.5:
            return Action.C
        opponent_to_defect = np.argmax(self.exploitability_scores)
        if self.exploitability_scores[opponent_to_defect] > 0.7:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            my_payoff = history.my_payoffs[-1]
            opponent_payoffs = history.opponent_payoffs[-1, :]
            average_opponent_payoff = np.mean(opponent_payoffs)
            self.last_round_cooperate = my_payoff >= average_opponent_payoff
        return Action.D if not self.last_round_cooperate else Action.C

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_c = np.mean(history.opponent_actions[-1, :])
        if self.punishment_target is not None and history.my_payoffs[-1] < 1:
            return Action.D
        elif history.opponent_actions[-1, 0]:
            self.punishment_target = history.opponent_actions[-1, 0]
        else:
            self.punishment_target = None
        if avg_c >= 0.5 and self.punishment_target is not None:
            self.punishment_target = None
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif len(history.opponent_actions) > 2 and avg_c < 0.4:
            return Action.D if random.random() < state.round_number / self.game_description.n_rounds else Action.C
        elif avg_c >= 0.5:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    Exploitative strategy that initially cooperates and adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_contributions = np.zeros(game_description.n_players)
        self.total_contributions_history = []
        self.adaptation_flags = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            return Action.D if state.round_number % 2 == 1 else Action.C
        opponent_contributions = history.opponent_actions[-1, :]
        avg_opponent_contribution = np.mean(opponent_contributions)
        self.total_contributions_history.append(np.sum(opponent_contributions))
        for i, (contribution, flag) in enumerate(zip(opponent_contributions, self.adaptation_flags)):
            if contribution == 0:
                self.adaptation_flags[i] = True
        if avg_opponent_contribution > 0.5 or not any(self.total_contributions_history):
            return Action.D
        elif np.mean(self.total_contributions_history) < self.total_contributions_history[-1]:
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    An adaptive exploiter strategy for the Public Goods Game.
    
    The strategy starts with cooperation to gather information about opponents' behavior,
    then mirrors the majority's action while planning to exploit in subsequent rounds.
    It punishes defectors using a grim trigger approach and exploits cooperative tendencies late in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_number += 1
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number <= 5:
            if prev_cooperators >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number > 5 and state.round_number < self.game_description.n_rounds - 2:
            if history.my_actions[-1] == False and prev_cooperators >= self.game_description.n_players // 2:
                return Action.D
            elif history.opponent_payoffs[-1, :].mean() > self.game_description.k / self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds - 2:
            if prev_cooperators >= self.game_description.n_players // 2:
                return Action.D
            else:
                return Action.C
        elif history.opponent_payoffs[-1, :].mean() > self.game_description.k / self.game_description.n_players:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    The Adaptive Predator strategy aims to exploit opponents by initially cooperating,
    then mirroring and escalating based on the majority action, and finally defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if np.sum(history.opponent_actions[-1]) == 0 and random.random() < 0.5:
            return Action.C
        elif np.sum(history.opponent_actions[-1]) == 1:
            majority_action = history.opponent_actions[-1][np.where(history.opponent_actions[-1])[0]]
            return Action.C if majority_action else Action.D
        total_contributions = np.sum(history.opponent_actions[-1])
        if total_contributions >= self.game_description.n_players // 2:
            majority_action = history.opponent_actions[-1][np.where(np.bincount(history.opponent_actions[-1]) == max(np.bincount(history.opponent_actions[-1])))[0]]
            return Action.C if majority_action else Action.D
        elif random.random() < self.game_description.k / self.game_description.n_players:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game.
    
    This strategy aims to maximize its own payoff by identifying opportunities 
    to free-ride on others' contributions while maintaining a cooperative stance 
    when necessary. By monitoring opponents' behaviors and adjusting the ACT 
    threshold, this strategy can effectively exploit both cooperative and defective 
    behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = set()
        self.free_riders = set()
        self.opponent_history = []
        self.act_threshold = game_description.k / game_description.n_players * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        self.opponent_history.append(history.opponent_actions[-1])
        for i, action in enumerate(history.opponent_actions[-1]):
            if action:
                self.cooperators.add(i)
            else:
                self.free_riders.add(i)
        if len(self.cooperators) > self.game_description.n_players // 2:
            self.act_threshold += 0.1
        elif len(self.free_riders) > self.game_description.n_players // 2:
            self.act_threshold -= 0.1
        total_contributions = sum(history.opponent_actions[-1]) + history.my_actions[-1]
        if total_contributions < self.act_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Defection with Conditional Cooperation.
    
    This strategy adapts to the average contribution rate and responds to individual opponents' behavior,
    aiming to maximize payoffs by encouraging cooperation when possible, punishing non-reciprocation and frequent defection,
    and minimizing losses in cases where cooperation is unlikely to be reciprocated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5
        self.exploration_phase_rounds = 2
        self.exploration_interval = 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents = self.game_description.n_players - 1
        opponent_actions = history.opponent_actions[-1, :]
        my_previous_action = history.my_actions[-1]
        total_contributions = np.sum(opponent_actions) + int(my_previous_action)
        average_contribution_rate = total_contributions / self.game_description.n_players
        exploration_phase = state.round_number % self.exploration_interval == 0 and state.round_number > 0
        if exploration_phase:
            return Action(random.choice([True, False]))
        elif average_contribution_rate >= self.contribution_threshold:
            return Action.C
        else:
            opponent_defection_rates = np.sum(history.opponent_actions, axis=0) / state.round_number
            frequent_defectors = np.where(opponent_defection_rates > 0.5)[0]
            if len(frequent_defectors) > 0:
                return Action.D
            elif opponents == sum(opponent_actions):
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to exploit the willingness of other players 
    to cooperate by initially cooperating, matching contributions, punishing defectors, 
    and defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(previous_opponent_actions)
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        if average_contribution < 0.5 or (previous_opponent_actions == False).any():
            self.punishment_rounds = 2
            return Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        if history.my_payoffs.size > 0 and np.all(history.my_actions):
            return Action.C
        return Action.C if average_contribution >= 0.5 else Action.D

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    An adaptive strategy for the Public Goods Game that exploits various types of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = np.zeros(game_description.n_players)
        self.contribution_threshold = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        opponents_cooperated_last_round = np.sum(history.opponent_actions[-1, :])
        total_contributions_last_round = np.sum(np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :])))
        if opponents_cooperated_last_round < n_players // 2:
            return Action.D
        for i, opponent_cooperation_rate in enumerate(self.cooperation_rates):
            if opponent_cooperation_rate < 0.5 and history.opponent_actions[-1, i]:
                return Action.D
        for i, opponent_cooperation_rate in enumerate(self.cooperation_rates):
            if opponent_cooperation_rate > 0.75:
                self.cooperation_rates[i] = min(1, self.cooperation_rates[i] + (opponent_cooperation_rate - self.cooperation_rates[i]) / 10)
        for i, opponent_payoff_history in enumerate(history.opponent_payoffs):
            if np.all(opponent_payoff_history < k * n_players // 2):
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_contributions_last_round < k * n_players // 2 and (not self.contribution_threshold):
            self.contribution_threshold = True
            self.cooperation_rates *= 0.8
        recent_opponent_actions = history.opponent_actions[max(-10, -state.round_number):, :]
        if np.mean(recent_opponent_actions) < 0.5:
            self.cooperation_rates *= 0.9
        return Action.C if random.random() < max(self.cooperation_rates) else Action.D

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history and parameters.
    Aims to maximize its own payoff by exploiting contributions of others while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_contributions = 0
        self.opponents_total_payoffs = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_contributions = sum(history.opponent_actions[-1, :]) + self.my_contributions
        opponents_total_cooperations = np.sum(np.any(history.opponent_actions, axis=0), axis=None)
        for i in range(self.game_description.n_players - 1):
            opponent_defections = np.all(~history.opponent_actions[:, i])
            opponent_contributions_exceed_half = history.opponent_payoffs[:state.round_number, i].sum() > self.game_description.n_rounds / (2 * self.game_description.k)
            if opponent_defections:
                return Action.C
            elif opponent_contributions_exceed_half:
                return Action.D
        if total_contributions <= self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy.
    
    Encourages cooperation while adapting to group behavior and punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_players = set()
        self.consecutive_defections = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(previous_round_actions)
        for i, action in enumerate(previous_round_actions):
            if not action:
                self.consecutive_defections[i] = self.consecutive_defections.get(i, 0) + 1
                if self.consecutive_defections[i] >= 2 or sum(history.opponent_actions[:, i]) / state.round_number < 1 / self.game_description.n_players:
                    self.punished_players.add(i)
            else:
                self.consecutive_defections[i] = 0
        if average_contribution > 0.5:
            return Action.C
        if random.random() < 0.5 - average_contribution:
            if len(self.punished_players) > 0 and random.random() < 0.7:
                return Action.D
        if sum(history.opponent_actions[-1, :]) >= 0.8 * self.game_description.n_players:
            if random.random() < 0.9:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective welfare,
    adapting to the behavior of opponents while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_baseline = True
        self.reciprocity_focus = 0.5
        self.adaptive_flexibility = 0.1
        self.exploitation_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > self.game_description.n_players / 2 else Action.D
        opponents = self.game_description.n_players - 1
        recent_cooperations = np.mean(history.opponent_actions[-3:, :].astype(np.float64))
        recent_contributions = np.sum(history.opponent_actions[-1, :], dtype=int)
        if recent_contributions >= self.game_description.n_players / 2:
            probability = recent_cooperations * (self.reciprocity_focus + random.random() * self.adaptive_flexibility)
            return Action.C if random.random() < probability else Action.D
        exploitation_rate = np.sum(history.opponent_actions[-3:, :], axis=0, dtype=int) / 3
        exploitation_opponents = np.where(exploitation_rate > self.exploitation_threshold)[0]
        if len(exploitation_opponents):
            return Action.D if random.random() < 0.5 else Action.C
        contributions_trend = np.diff(np.sum(history.opponent_actions, axis=1, dtype=int))
        if state.round_number > 2 and np.all(contributions_trend[-3:] < 0):
            self.adaptive_flexibility += 0.1
        last_round = state.round_number == self.game_description.n_rounds - 1
        if last_round:
            return Action.C if recent_contributions > opponents / 2 else Action.D
        early_defections = np.sum(history.opponent_actions[:2, :], axis=0, dtype=int) >= 1
        if np.any(early_defections):
            self.reciprocity_focus -= 0.2
        return Action.C

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Collective Strategy: Adaptive Reciprocity with Gradual Cooperation.
    
    This strategy promotes cooperation while allowing for flexibility and adaptation to a wide range of opponent behaviors,
    aiming to achieve the best collective outcome.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 1.0
        self.opponent_cooperation_rates = [1.0] * game_description.n_players
        self.noise_tolerance = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(opponent_actions)
        if history.my_actions[-1]:
            for i, action in enumerate(opponent_actions):
                if not action:
                    self.opponent_cooperation_rates[i] *= 0.9
        if average_contribution > 0.5:
            return Action.C
        else:
            defect_probability = (0.5 - average_contribution) / 0.5
            if random.random() < (1 - self.noise_tolerance * random.random()) * defect_probability:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_contributions = sum(history.my_actions) + np.sum(opponent_actions, axis=0)
            if total_contributions >= self.game_description.n_players / 2:
                return Action.C
        if state.round_number > 1 and average_contribution < self.game_description.k / 4:
            return Action.D
        if average_contribution > 0.5:
            for i, rate in enumerate(self.opponent_cooperation_rates):
                self.opponent_cooperation_rates[i] = min(1.0, rate * 1.01)
        return Action.D

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    A collective strategy that promotes cooperation by reciprocating others' contributions 
    while gradually adapting to changes in the group's behavior. It forgives occasional defections, 
    encourages players to cooperate and rewards consistent contributors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.contribution_expectations = [1] * game_description.n_players
        self.moving_average_contributions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players <= 1:
            return Action.C
        elif self.game_description.n_players == 2 and state.round_number != self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_contributions = opponent_cooperators + (history.my_actions[-1] == Action.C)
        self.moving_average_contributions.append(total_contributions / self.game_description.n_players)
        if len(self.moving_average_contributions) > 3:
            self.moving_average_contributions.pop(0)
        moving_average = np.mean(self.moving_average_contributions)
        if moving_average > self.cooperation_threshold + 0.05:
            self.cooperation_threshold += 0.1
        elif moving_average < self.cooperation_threshold - 0.05:
            self.cooperation_threshold -= 0.1
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if opponent_action == Action.D and history.my_payoffs[-1] > history.opponent_payoffs[-1, i]:
                self.contribution_expectations[i] -= 0.2
            elif opponent_action == Action.C and history.my_payoffs[-1] < history.opponent_payoffs[-1, i]:
                self.contribution_expectations[i] = max(1, self.contribution_expectations[i])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if total_contributions / self.game_description.n_players > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    An adaptive exploitative strategy for the Public Goods Game.
    
    Cooperates initially to gather information, detects opponents' cooperation patterns,
    and exploits them while minimizing own contributions. Retaliates against defectors
    and maintains group cooperation when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_threshold = 0.8
        self.retaliation_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions)
        defectors = self.game_description.n_players - 1 - cooperators
        if state.round_number <= 3:
            return Action.C
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        altruists = np.where(opponent_cooperation_rates >= self.opponent_cooperation_threshold)[0]
        if len(altruists) > 0:
            return Action.C
        if cooperators > defectors:
            return Action.D
        if self.retaliation_target is not None:
            opponent_to_retaliate = self.retaliation_target
            self.retaliation_target = None
            if history.opponent_actions[-1, opponent_to_retaliate] == 0:
                return Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i] == 0 and history.opponent_actions[-2, i] == 1:
                self.retaliation_target = i
        return Action.C

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_target = None
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            previous_cooperators = sum(history.opponent_actions[-1, :])
            if self.punishment_target is not None and self.punishment_rounds > 0:
                self.punishment_rounds -= 1
                return Action.D
            if previous_cooperators >= self.game_description.n_players // 2:
                for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                    if not opponent_action:
                        self.punishment_target = i
                        self.punishment_rounds = 2
                        break
        if history is not None and state.round_number < self.game_description.n_rounds - 1:
            return Action.C if previous_cooperators >= self.game_description.n_players // 2 else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        average_contribution = np.mean(history.my_actions)
        if average_contribution >= self.contribution_threshold:
            return Action.C
        opponent_contributions = np.sum(history.opponent_actions, axis=0)
        min_contribution_player = np.argmin(opponent_contributions)
        if random.random() < 0.5:
            return history.opponent_actions[-1, min_contribution_player]
        else:
            return not history.opponent_actions[-1, min_contribution_player]
        for i in range(self.game_description.n_players):
            if np.all(~history.opponent_actions[-2:, i]):
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    Exploitative Strategy: "Kite" for the Public Goods Game.

    This strategy adapts to opponents' behaviors while maintaining a balance between cooperation and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_history = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        my_last_action = history.my_actions[-1]
        total_contribution = np.sum(opponent_actions) + int(my_last_action)
        if total_contribution > self.game_description.n_players // 2:
            return Action.C
        for opponent_index, opponent_action in enumerate(opponent_actions):
            if opponent_action == False and (opponent_index not in self.opponent_defection_history or self.opponent_defection_history[opponent_index] >= 2):
                self.opponent_defection_history[opponent_index] = self.opponent_defection_history.get(opponent_index, 0) + 1
                return Action.D
        opponent_cooperators = np.sum(opponent_actions)
        exploitation_payoff = self.game_description.k / self.game_description.n_players * opponent_cooperators + 1
        cooperation_payoff = self.game_description.k / self.game_description.n_players * (opponent_cooperators + 1)
        if exploitation_payoff > cooperation_payoff:
            return Action.D
        for opponent_index, opponent_action in enumerate(opponent_actions):
            if opponent_action == True and (opponent_index not in self.opponent_defection_history or self.opponent_defection_history[opponent_index] >= 2):
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(opponent_actions) == 1 and np.all(~opponent_actions):
            return Action.D
        if np.all(~history.opponent_actions[-2:, :]):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    Adaptive Exploiter strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0
        self.opponent_last_round_cooperation = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.round_number > 0):
            self.round_number += 1
            return Action.C
        self.round_number = state.round_number
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        avg_contribution_prev_round = np.mean(prev_round_opponent_actions)
        if self.game_description.n_players == 2:
            return Action.C if state.round_number % 2 != 0 else Action.D
        if self.opponent_last_round_cooperation is None or avg_contribution_prev_round >= 0.5:
            return Action.C
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            self.opponent_last_round_cooperation = False
            return Action.D
        else:
            self.opponent_last_round_cooperation = True
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    The Adaptive Exploiter strategy prioritizes identifying and capitalizing on free-riding behavior 
    while cautiously cooperating with more cooperative opponents to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_classification = [None] * game_description.n_players

    def classify_opponents(self, history: PlayerHistory) -> None:
        for i in range(self.game_description.n_players - 1):
            cooperation_rate = np.mean(history.opponent_actions[:, i])
            if cooperation_rate > 0.75:
                self.opponent_classification[i] = 'COOP'
            elif cooperation_rate < 0.25:
                self.opponent_classification[i] = 'FR'
            else:
                self.opponent_classification[i] = 'MIX'

    def calculate_average_contribution(self, history: PlayerHistory) -> float:
        recent_rounds = max(1, min(3, len(history.my_actions)))
        average_contribution = np.mean(np.sum(history.opponent_actions[-recent_rounds:, :], axis=1))
        return average_contribution

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.opponent_classification = [None] * self.game_description.n_players
            return Action.C
        self.classify_opponents(history)
        average_contribution = self.calculate_average_contribution(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponents_actions_last_round = history.opponent_actions[-1, :]
        last_opponents_action_rates = np.mean(opponents_actions_last_round, axis=0)
        for opponent_classification in self.opponent_classification:
            if opponent_classification == 'FR':
                return Action.D
            elif opponent_classification == 'COOP' and average_contribution > 0.5:
                return Action.C
            elif opponent_classification == 'MIX':
                last_opponent_action = history.opponent_actions[-1, :]
                if np.any(last_opponent_action):
                    return Action.C if random.choice(np.where(last_opponent_action)[0]) % 2 == 0 else Action.D
                else:
                    return Action.C
        return Action.C

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    A collective strategy for the Public Goods Game, prioritizing collective welfare by encouraging contributions when others cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing_opponent = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.punishing_opponent is not None and history.opponent_actions[-1, self.punishing_opponent]:
            self.punishing_opponent = None
            return Action.D
        average_contribution = np.mean(history.opponent_actions[-1, :] + (history.my_actions[-1] if history.my_actions.size else 0))
        if average_contribution > 0.5:
            return Action.C
        opponents_who_defected = np.where(history.opponent_actions[-1, :] == False)[0]
        if len(opponents_who_defected) == 1 and average_contribution > 0.5:
            self.punishing_opponent = opponents_who_defected[0]
            return Action.D
        else:
            return Action.D













class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.turn_count = 0
        self.last_three_turns_contributions = []
        self.consecutive_defections = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.turn_count = 0
            self.last_three_turns_contributions = []
            self.consecutive_defections = [0] * self.game_description.n_players
            return Action.C
        self.turn_count += 1
        total_contributions = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        if total_contributions > self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        self.last_three_turns_contributions.append(total_contributions)
        if len(self.last_three_turns_contributions) > 3:
            self.last_three_turns_contributions.pop(0)
        if len(self.last_three_turns_contributions) >= 2 and self.last_three_turns_contributions[-1] < self.last_three_turns_contributions[-2]:
            action = Action.D
        if history.my_actions[-1]:
            self.consecutive_defections = [defections + 1 if not action else 0 for defections, action in zip(self.consecutive_defections, history.opponent_actions[-1, :])]
        else:
            self.consecutive_defections = [defections + 1 if action else 0 for defections, action in zip(self.consecutive_defections, history.opponent_actions[-1, :])]
        if any((defection >= 2 for defection in self.consecutive_defections)):
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            action = Action.C
        return action

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for repeated Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.first_player = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and state.round_number > 1:
            all_defected = np.all(history.opponent_actions[:-1] == False)
            if all_defected:
                return Action.D
        if history is not None:
            total_contributions = np.sum(history.my_actions[-1:]) + np.sum(history.opponent_actions[-1, :])
        else:
            total_contributions = 0
        if state.round_number == self.game_description.n_rounds - 1:
            if total_contributions >= self.game_description.n_players // 4:
                return Action.C
        if total_contributions >= self.game_description.n_players // 2:
            return Action.C
        elif total_contributions >= self.game_description.n_players // 4:
            cooperation_level = total_contributions / self.game_description.n_players
            probability = (1 - cooperation_level) * 2
            return Action.C if random.random() > probability else Action.D
        else:
            return Action.D



class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Gradual Cooperation with Punishment strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperations = history.opponent_actions[-1, :]
        cooperation_rate = np.mean(opponent_cooperations)
        if cooperation_rate > self.cooperation_threshold:
            return Action.C
        total_contributions = sum(history.my_actions[-1:]) + np.sum(opponent_cooperations)
        if total_contributions <= self.game_description.n_players - 1 - np.sum(opponent_cooperations):
            return Action.D
        if np.sum(opponent_cooperations) > (self.game_description.n_players - 1) / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    A strategy that initiates cooperation and adapts to the overall group behavior.
    It balances between cooperation and defection to maximize both individual and collective payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[:-1] if len(history.opponent_actions) > 1 else history.opponent_actions
        total_cooperations = np.sum(opponent_actions)
        avg_cooperation_rate = total_cooperations / (state.round_number * self.game_description.n_players)
        if avg_cooperation_rate > 0.5:
            chosen_action = Action.C
        elif avg_cooperation_rate == 0.5:
            chosen_action = random.choice([Action.C, Action.D])
        else:
            chosen_action = Action.D
        my_payoffs = history.my_payoffs[:-1] if len(history.my_payoffs) > 1 else history.my_payoffs
        avg_my_payoff = np.mean(my_payoffs)
        opponent_avg_payoffs = np.mean(history.opponent_payoffs, axis=1)
        avg_opponent_payoff = np.mean(opponent_avg_payoffs)
        if avg_my_payoff < avg_opponent_payoff:
            opponent_actions_last_round = history.opponent_actions[-1]
            most_frequent_action = Action.C if sum(opponent_actions_last_round) >= len(opponent_actions_last_round) // 2 else Action.D
            chosen_action = most_frequent_action
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.all(history.opponent_actions[-1] == False):
            return Action.D
        elif len(my_payoffs) > 1 and avg_my_payoff < np.mean(opponent_avg_payoffs):
            opponent_cooperations = sum(np.sum(opponent_actions, axis=0))
            if opponent_cooperations > self.game_description.n_players // 2:
                return Action.D
        self.prev_cooperation_rate = avg_cooperation_rate
        return chosen_action

class Strategy_COLLECTIVE_129(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperate-Punish (ACP)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions_history = []
        self.last_round_total_contributions = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        total_contributions = np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] or history.my_payoffs.size > 0))
        self.total_contributions_history.append(total_contributions)
        average_endowment = self.game_description.n_players / 2
        average_contribution = np.mean(self.total_contributions_history)
        if total_contributions >= average_endowment:
            return Action.C
        if history.my_payoffs.size > 0 and history.my_payoffs[-1] >= np.mean(history.opponent_payoffs[:, -1]):
            return Action.C
        if len(self.total_contributions_history) > 1:
            contribution_diff = self.total_contributions_history[-1] - self.last_round_total_contributions
            if contribution_diff >= self.game_description.k / self.game_description.n_players:
                return Action.C
        self.last_round_total_contributions = total_contributions
        return Action.D

class Strategy_COLLECTIVE_130(BaseStrategy):
    """
    Adaptive Collective Optimism strategy.
    Prioritizes cooperation while being responsive to opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution > 0.5:
            return Action.C if random.random() < 0.9 else Action.D
        elif avg_contribution < 0.5 and state.round_number != self.game_description.n_rounds - 1:
            return Action.D if random.random() < 0.6 else Action.C
        elif sum(history.my_actions) == 0:
            return Action.D if random.random() < 0.8 else Action.C
        total_contributions = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        prev_total_contributions = np.sum(history.opponent_actions[-2, :]) + history.my_actions[-2] if len(history.my_actions) > 1 else 0
        if total_contributions > prev_total_contributions:
            return Action.C if random.random() < 0.9 else Action.D
        if random.random() < 0.1:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_131(BaseStrategy):
    """
    A collective strategy that prioritizes group well-being by adapting to the opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_calculations = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if 3 <= state.round_number < 10:
            opponent_actions = history.opponent_actions
            total_contributions = np.sum(opponent_actions, axis=1)
            acr = np.mean(total_contributions) / self.game_description.n_players
            self.acr_calculations.append(acr)
            if acr >= 0.5:
                return Action.C
            else:
                return Action.D
        if state.round_number >= 10:
            opponent_actions = history.opponent_actions
            last_opponent_action = opponent_actions[-1, 0]
            if last_opponent_action or (not self.acr_calculations[-1] >= 0.5 and (not last_opponent_action)):
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_actions = history.opponent_actions
        all_cooperate = np.all(opponent_actions == True, axis=0)
        all_defect = np.all(opponent_actions == False, axis=0)
        if any(all_cooperate):
            return Action.C
        elif any(all_defect):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_132(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation and reciprocity while adapting to the group's behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        average_contribution = np.mean(opponent_actions)
        if average_contribution > 0.5:
            return Action.C
        for i, action in enumerate(opponent_actions):
            if not action and np.mean(history.opponent_payoffs[-1, :]) < average_contribution:
                return Action.D
        for i, action in enumerate(opponent_actions):
            if action and history.opponent_payoffs[-1, i] > average_contribution * self.game_description.k:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_133(BaseStrategy):
    """
    A collective strategy for the Public Goods Game (PGG) that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.collective_payoff_trend = 0
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        my_action_last_round = history.my_actions[-1]
        avg_contribution = np.mean(np.concatenate((opponent_actions_last_round, [my_action_last_round])))
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C if avg_contribution > 0.5 else Action.D
        if self.punishment_phase:
            self.punishment_phase = False
            return Action.D
        if state.round_number >= 2 and np.all(~(history.my_actions[-3:] | np.any(history.opponent_actions[-3:], axis=1))):
            self.punishment_phase = True
            return Action.D
        my_payoffs_last_rounds = history.my_payoffs[-3:]
        if len(my_payoffs_last_rounds) == 3:
            self.collective_payoff_trend = np.mean(my_payoffs_last_rounds[1:]) - my_payoffs_last_rounds[0]
        if avg_contribution > 0.5 or (avg_contribution >= 0.5 and self.collective_payoff_trend > 0):
            return Action.C
        if np.sum(opponent_actions_last_round) == 0:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_134(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that balances individual self-interest with collective benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperated_recently = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        opponent_cooperators = sum(opponent_actions)
        if not np.all(opponent_actions):
            return Action.D
        if state.round_number > 1:
            opponent_actions_prev_round = history.opponent_actions[-2, :]
            if np.all(opponent_actions) and np.all(opponent_actions_prev_round):
                self.cooperated_recently = True
        avg_contribution = (opponent_cooperators + 1) / self.game_description.n_players
        if avg_contribution > 0.5 or self.cooperated_recently:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_135(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and opponent behaviors.
    It balances individual incentives with collective welfare by cooperating initially,
    punishing free-riders, and encouraging others to contribute through noisy cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) == 0):
            return Action.C
        previous_round_contributions = sum(np.array(history.opponent_actions)[-1, :] + [history.my_actions[-1]])
        n_cooperators = np.sum(np.array(history.opponent_actions)[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_round_contributions >= self.k / 2 * self.n_players:
            return Action.C
        if n_cooperators > self.n_players // 2 and previous_round_contributions < self.k / 2 * self.n_players:
            return Action.D
        p = self.k / self.n_players * (n_cooperators / self.n_players)
        return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_136(BaseStrategy):
    """
    A collective strategy for the Public Goods Game that balances individual incentives with collective welfare.
    It adapts to the game's history and parameters, promoting cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishing and state.round_number < self.game_description.n_rounds - 1:
            self.punishing = False
            return Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history else 0)
        avg_contribution /= self.game_description.n_players
        if not self.punishing and state.round_number > 1:
            total_contributions = np.sum(history.opponent_actions[-2, :]) + (history.my_actions[-2] if history else 0)
            group_payoff = self.game_description.k / self.game_description.n_players * total_contributions
            opponent_defected = not all(history.opponent_actions[-2, :])
            if opponent_defected and group_payoff < self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                self.punishing = True
        if avg_contribution > self.game_description.k / self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_137(BaseStrategy):
    """
    This strategy plays cooperate (C) in the first round, 
    then uses tit-for-tat for the majority of rounds and switches to Pavlov near the end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.endgame_adjustment_enabled = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        rounds_remaining = self.game_description.n_rounds - state.round_number
        if not self.endgame_adjustment_enabled and rounds_remaining < 5:
            self.endgame_adjustment_enabled = True
        if self.endgame_adjustment_enabled:
            my_payoff_last_round = history.my_payoffs[-1]
            opponent_average_payoff_last_round = np.mean(history.opponent_payoffs[-1, :])
            if my_payoff_last_round > opponent_average_payoff_last_round:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif cooperators >= self.game_description.n_players // 2 or (cooperators == self.game_description.n_players // 2 and random.random() < 0.5):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_138(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contribution_threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            avg_cooperation_rate = sum(history.my_actions) / len(history.my_actions)
            if avg_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_contributions >= self.avg_contribution_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_139(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Escalation.
    
    This strategy prioritizes cooperation while adapting to the group's behavior, 
    promoting a culture of mutual support and escalating cooperation over time. 
    By starting with cooperation and gradually increasing expectations for collective action, 
    this strategy encourages others to follow suit, ultimately maximizing collective payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 5:
            cooperators = sum(history.opponent_actions[-1, :])
            avg_contribution = cooperators / self.game_description.n_players
            if avg_contribution > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number <= self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            avg_contribution = cooperators / self.game_description.n_players
            if avg_contribution >= 0.75 or all(history.opponent_actions[-1, :]):
                return Action.C
            else:
                previous_round_cooperators = sum(history.opponent_actions[-2, :])
                previous_avg_contribution = previous_round_cooperators / self.game_description.n_players
                if previous_avg_contribution < 0.25 and all(history.opponent_actions[-1, :]):
                    return Action.C
                else:
                    return Action.D
        elif state.round_number <= self.game_description.n_rounds - 1:
            cooperators = sum(history.opponent_actions[-1, :])
            previous_cooperators = sum(history.opponent_actions[-2, :])
            avg_contribution = (cooperators + previous_cooperators) / (self.game_description.n_players * 2)
            if avg_contribution >= 0.5 or all([all(history.opponent_actions[-i, :]) for i in range(1, 3)]):
                return Action.C
            else:
                return Action.D
        elif self.game_description.n_players == 1:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_140(BaseStrategy):
    """
    A collective adaptive strategy that encourages cooperation in public goods games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5
        self.punish_opponent = None
        self.forgiven_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        previous_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_contribution_rate = previous_contributions / self.game_description.n_players
        if self.punish_opponent is not None:
            return Action.D
        elif any(history.opponent_actions[-1, :] == False) and avg_contribution_rate > 0.5:
            self.punish_opponent = history.opponent_actions[-1, :].argmin()
            return Action.D
        if any(history.opponent_actions[-1, :] == True) and avg_contribution_rate > 0.5:
            self.forgiven_players.add(self.punish_opponent)
            self.punish_opponent = None
        return Action.C if avg_contribution_rate >= self.contribution_threshold else Action.D

class Strategy_EXPLOITATIVE_129(BaseStrategy):
    """
    Exploitative strategy "Adaptive Exploiter" for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < 6:
            opponent_defected = np.any(~history.opponent_actions[-1, :])
            return Action.D if opponent_defected else Action.C
        avg_payoff_opponents = (np.mean(history.opponent_payoffs[-2:, :]) + np.mean(self.prev_round_payoffs)) / 2 if self.prev_round_payoffs is not None else np.mean(history.opponent_payoffs[-1, :])
        threshold = self.game_description.k / self.game_description.n_players
        self.prev_round_payoffs = history.opponent_payoffs[-1, :]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D if avg_payoff_opponents > threshold else Action.C

class Strategy_EXPLOITATIVE_130(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors and payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.exploitation_targets = []
        self.my_payoff_avg = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperation_rate = np.mean(history.my_actions)
        opponent_cooperations = history.opponent_actions.sum(axis=0)
        most_cooperative_players = np.argsort(opponent_cooperations)[-1:]
        self.exploitation_targets = most_cooperative_players
        if self.my_payoff_avg is None:
            self.my_payoff_avg = history.my_payoffs.mean()
        else:
            self.my_payoff_avg = 0.9 * self.my_payoff_avg + 0.1 * history.my_payoffs[-1]
        opponent_payoff_avg = np.mean(history.opponent_payoffs, axis=1).mean()
        if self.my_payoff_avg < opponent_payoff_avg:
            return Action.D
        else:
            return Action.C
        for target in self.exploitation_targets:
            if history.opponent_actions[-1, target]:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions.shape[1] == 1 and history.opponent_actions[-2, 0]:
            return Action.C
        if avg_cooperation_rate >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_131(BaseStrategy):
    """
    An exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.first_round_done = False
        self.majority_cooperated_last_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.first_round_done:
            avg_opponent_contribution = np.mean(history.opponent_actions[0])
            if avg_opponent_contribution < 0.5:
                self.majority_cooperated_last_round = False
            else:
                self.majority_cooperated_last_round = True
            self.first_round_done = True
        if state.round_number < 6 and (not self.first_round_done):
            avg_opponent_contribution = np.mean(history.opponent_actions[-1])
            return Action.C if avg_opponent_contribution >= 0.5 else Action.D
        elif state.round_number <= 15:
            opponent_cooperators_last_round = sum(history.opponent_actions[-1, :])
            majority_cooperated = opponent_cooperators_last_round > self.game_description.n_players // 2
            if majority_cooperated and (self.majority_cooperated_last_round is None or self.majority_cooperated_last_round):
                return Action.C
            else:
                return Action.D
        elif state.round_number <= self.game_description.n_rounds - 1:
            total_contributions = sum(history.opponent_actions[-1, :])
            if total_contributions < self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_132(BaseStrategy):
    """
    Exploitative strategy that mimics opponents' cooperation level while punishing repeated defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.repeat_defectors = set()
        self.last_round_avg_contribution = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (len(history.my_actions) == 1 and len(history.opponent_actions) == 1):
            return Action.C
        total_contributions = sum((1 for action in history.opponent_actions[-1] if action))
        avg_contribution = total_contributions / self.game_description.n_players
        self.last_round_avg_contribution = avg_contribution
        if len(history.my_payoffs) >= 2:
            previous_payoff, before_previous_payoff = (history.my_payoffs[-1], history.my_payoffs[-2])
            repeated_defector = any((opponent_actions[-1] == False and opponent_actions[-2] == False for opponent_actions in history.opponent_actions.T))
            if repeated_defector:
                self.repeat_defectors.add(True)
            elif not repeated_defector or previous_payoff > before_previous_payoff:
                self.repeat_defectors.discard(True)
        if state.round_number + 1 >= self.game_description.n_rounds:
            return Action.D
        if len(self.repeat_defectors) > 0:
            return Action.D
        elif avg_contribution >= 0.5 or (avg_contribution == 0.5 and self.last_round_avg_contribution is not None and (self.last_round_avg_contribution >= 0.5)):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_133(BaseStrategy):
    """
    Exploit the contributions of others while minimizing our own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.first_round_played = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if not self.first_round_played:
            self.first_round_played = True
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            return Action.C if history.my_actions[-1] else Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution >= self.threshold:
            contribute = False
        else:
            contribute = True
        my_payoff = history.my_payoffs[-1]
        opponent_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if my_payoff > opponent_avg_payoff:
            contribute = False
        elif my_payoff < opponent_avg_payoff:
            contribute = not contribute
        return Action.C if contribute else Action.D

class Strategy_EXPLOITATIVE_134(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game, 
    which punishes non-cooperative behavior while allowing for forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == 1 and (not history.my_actions[0]):
            self.punishment_rounds = float('inf')
        total_contributions = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_contributions < self.game_description.n_players * 0.5 and history.my_actions[-1]:
            self.punishment_rounds += 2
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        elif total_contributions >= self.game_description.n_players * 0.5:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_135(BaseStrategy):
    """
    An adaptive and exploitative strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c_threshold = 0.5
        self.gradual_escalation_slope = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_c = np.mean(history.opponent_actions[-1, :])
        pg_payoff = self.game_description.k / self.game_description.n_players * sum(history.my_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif avg_c > self.avg_c_threshold:
            return Action.C
        elif 0.25 < avg_c <= self.avg_c_threshold:
            cooperation_prob = (avg_c - 0.25) * self.gradual_escalation_slope
            return Action.C if random.random() < cooperation_prob else Action.D
        elif avg_c <= 0.25 or pg_payoff < self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_136(BaseStrategy):
    """
    Exploitative strategy that adapts to changing circumstances and punishes free riders.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * 0.5
        self.punishment_targets = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if len(history.my_actions) > self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-self.game_description.n_rounds // 2:, :])
            if cooperators.sum() < (self.game_description.n_players - 1) * self.game_description.n_rounds // 4:
                self.threshold *= 0.9
            elif cooperators.sum() > (self.game_description.n_players - 1) * self.game_description.n_rounds // 2:
                self.threshold *= 1.1
        if len(history.opponent_actions) > self.game_description.n_rounds // 2:
            opponent_defections = sum(~history.opponent_actions[-self.game_description.n_rounds // 2:, :])
            for i in range(self.game_description.n_players - 1):
                if opponent_defections[i] > self.game_description.n_rounds // 4 and history.opponent_payoffs[-1, i] > avg_payoff:
                    self.punishment_targets.add(i)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_payoff < self.threshold:
            return Action.D
        for target in self.punishment_targets:
            if history.opponent_actions[-1, target]:
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_137(BaseStrategy):
    """
    An adaptive exploiter strategy for the N-Player Public Goods Game.
    This strategy cooperates initially, then defects when opponents cooperate,
    and punishes opponents who defect after it cooperated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing_opponent = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishing_opponent:
            self.punishing_opponent = False
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        if history.my_actions[-1] and (not np.all(prev_round_opponent_actions)):
            self.punishing_opponent = True
            return Action.D
        avg_contribution = np.mean(history.opponent_actions[-1, :])
        if avg_contribution > 0.5:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_138(BaseStrategy):
    """
    Exploitative strategy that adapts to various opponent behaviors by balancing cooperation and defection.
    Employs a dynamic approach with mirror, punishment, and forgiveness components.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mirror_threshold = 0.5
        self.punishment_rounds = 1
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_contributions = np.mean(history.opponent_actions[-1, :])
        if opponent_contributions >= self.mirror_threshold:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs.size > 0 and history.my_payoffs[-1] == 0:
            self.consecutive_defections += 1
            action = Action.D
        elif history.my_actions[-1] == False and opponent_contributions >= self.mirror_threshold:
            action = Action.C
        if self.consecutive_defections > 0 and opponent_contributions >= self.mirror_threshold:
            action = Action.C
            self.consecutive_defections = 0
        if state.round_number % 5 == 0 and state.round_number != 0:
            self.mirror_threshold = np.mean(history.opponent_actions[:, :].sum(axis=1))
        return action

class Strategy_EXPLOITATIVE_139(BaseStrategy):
    """
    A strategy that uses adaptive reciprocity with optimistic restart.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defected_every_time = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history.opponent_actions.shape[0] > 1 and np.all(history.opponent_actions[:, 0] == False):
            self.opponent_defected_every_time = True
        if history.opponent_actions.shape[0] >= 2 and np.all(np.sum(history.opponent_actions[-2:, :], axis=1) == self.game_description.n_players - 1):
            return Action.C
        avg_contribution = np.mean(history.my_payoffs[-1] / (self.game_description.k / self.game_description.n_players))
        if avg_contribution > 0.5:
            return Action.C
        elif self.opponent_defected_every_time or history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C