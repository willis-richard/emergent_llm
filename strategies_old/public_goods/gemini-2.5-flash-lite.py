"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash-lite
- Game: PublicGoodsDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COOPERATIVE_1(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) strategy.

    This strategy aims to foster cooperation by starting cooperatively,
    rewarding full group cooperation, punishing deviations, and attempting
    to re-establish cooperation after a breakdown.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        prev_round_idx = history.round_number - 1
        my_prev_action = self.get_my_action_from_history(history, prev_round_idx)
        all_prev_actions = self.get_all_actions_from_history(history, prev_round_idx)
        all_cooperated_last_round = all(all_prev_actions)
        if all_cooperated_last_round:
            return Action.C
        else:
            all_defected_last_round = not any(all_prev_actions)
            if my_prev_action == Action.D and all_defected_last_round:
                return Action.C
            else:
                return Action.D

    def get_my_action_from_history(self, history: PlayerHistory, round_idx: int) -> Action:
        """Helper to get player's action at a specific round index."""
        return Action.C if history.my_actions[round_idx] else Action.D

    def get_all_actions_from_history(self, history: PlayerHistory, round_idx: int) -> list[Action]:
        """Helper to get all players' actions at a specific round index."""
        actions = []
        actions.append(self.get_my_action_from_history(history, round_idx))
        for i in range(self.n - 1):
            actions.append(Action.C if history.opponent_actions[round_idx, i] else Action.D)
        return actions

class Strategy_COOPERATIVE_2(BaseStrategy):
    """
    Cooperative strategy: "Conditional Cooperation with Forgiveness".
    Starts by cooperating, then reciprocates cooperation.
    Punishes defection by defecting, but forgives past defections after one round of cooperation.
    Always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_deficits = {}
        self.initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_idx = history.round_number
        if not self.initialized:
            num_opponents = self.game_description.n_players - 1
            for i in range(num_opponents):
                self.cooperation_deficits[i] = 0
            self.initialized = True
        if current_round_idx == self.game_description.n_rounds:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_idx - 1]
        my_decision = Action.C
        for opponent_id in range(previous_round_opponent_actions.shape[0]):
            opponent_action = previous_round_opponent_actions[opponent_id]
            if opponent_action:
                self.cooperation_deficits[opponent_id] = 0
            else:
                if self.cooperation_deficits[opponent_id] == 0:
                    self.cooperation_deficits[opponent_id] = 1
                else:
                    self.cooperation_deficits[opponent_id] += 1
                my_decision = Action.D
        return my_decision

class Strategy_COOPERATIVE_3(BaseStrategy):
    """
    Adaptive Grim Trigger with Forgiveness strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while being mindful of the repeated nature of the game and the possibility of
    future cooperation. It's an adaptive "Grim Trigger" with a "Forgiveness" mechanism.

    Core Principle: The fundamental goal is to achieve the highest possible payoff,
    which in this game is achieved through sustained mutual cooperation. This strategy
    aims to incentivize and maintain this state.

    Decision Rules:
    - First Round (t=1): Cooperate (C) to establish a cooperative norm.
    - Last Round (t=r): Defect (D) as there are no future consequences.
    - Other Rounds:
        - If all players cooperated in the previous round: Cooperate.
        - If I defected in the previous round: Defect.
        - If an opponent defected in the previous round:
            - If this is the first observed defection since the last period of full cooperation:
              Defect for one round (punishment).
            - If this is not the first observed defection (already in punishment phase):
              Defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self._punishment_phase = False

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing the history of actions and payoffs up to the previous round.
                     None if it's the first round.

        Returns:
            The player's action for the current round (Action.C or Action.D).
        """
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        previous_round_idx = current_round - 1
        my_previous_action = self.get_my_action(history, previous_round_idx)
        previous_round_opponent_actions = self.get_opponent_actions(history, previous_round_idx)
        opponent_defected_last_round = np.any(previous_round_opponent_actions == Action.D)
        if my_previous_action == Action.D:
            self._punishment_phase = True
            return Action.D
        if not opponent_defected_last_round and my_previous_action == Action.C:
            self._punishment_phase = False
            return Action.C
        if opponent_defected_last_round:
            if not self._punishment_phase:
                self._punishment_phase = True
                return Action.D
            else:
                return Action.D
        self._punishment_phase = True
        return Action.D

    def get_my_action(self, history: PlayerHistory, round_idx: int) -> Action:
        """Helper to get my action in a specific round index."""
        return Action.C if history.my_actions[round_idx] else Action.D

    def get_opponent_actions(self, history: PlayerHistory, round_idx: int) -> NDArray[np.bool_]:
        """Helper to get all opponents' actions in a specific round index."""
        return history.opponent_actions[round_idx, :]

class Strategy_COOPERATIVE_4(BaseStrategy):
    """
    Tit-for-Tat with Reciprocity and Forgiveness (TFT-RF) for N-Player Public Goods Game.

    This strategy aims to foster cooperation by initially cooperating, rewarding cooperation
    from others, and defecting only when necessary to discourage exploitation. It's an
    extension of the classic Tit-for-Tat, incorporating elements that make it more robust
    in an N-player setting.

    Core Principles:
    1. Initial Cooperation: Start by cooperating to signal a willingness to contribute.
    2. Reciprocity: Reward cooperation from others by continuing to cooperate.
    3. Punishment: Defect when a certain threshold of "exploitation" (i.e., others
       defecting while you cooperate) is met, to discourage future exploitation.
    4. Forgiveness: After a period of sustained punishment, return to cooperation to
       give the group another chance to stabilize.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_coop = 0.8
        self.T_punish = 0.2
        self.P_rounds = 3
        self.punishment_counter = 0
        self.was_punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_counter = 0
            self.was_punishing = False
            return Action.C
        current_round = history.round_number
        previous_round_my_action = history.my_actions[current_round - 1]
        previous_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        num_cooperators = np.sum(previous_round_opponent_actions) + (1 if previous_round_my_action else 0)
        num_defectors = self.n_players - num_cooperators
        cooperation_ratio = num_cooperators / self.n_players
        if cooperation_ratio >= self.T_coop:
            self.punishment_counter = 0
            self.was_punishing = False
            return Action.C
        if cooperation_ratio <= self.T_punish:
            self.punishment_counter += 1
            if self.punishment_counter >= self.P_rounds:
                self.was_punishing = True
                return Action.D
        if self.was_punishing and cooperation_ratio > self.T_punish:
            self.was_punishing = False
            self.punishment_counter = 0
            return Action.C
        if previous_round_my_action and num_defectors > 0 and (cooperation_ratio < self.T_coop):
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_5(BaseStrategy):
    """
    Tit-for-Tat with Conditional Cooperation and Punishment strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by starting cooperatively, reciprocating,
    and punishing deviations from a group cooperation threshold. It's robust to exploiters
    and forgiving if cooperation resumes. It also defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.05
        self.cooperation_threshold = max(0.5, self.game_description.k / self.game_description.n_players + self.epsilon)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = state.round_number
        if current_round == self.game_description.n_rounds:
            return Action.D
        previous_round_index = current_round - 1
        my_previous_action_bool = history.my_actions[previous_round_index]
        my_previous_action = Action.C if my_previous_action_bool else Action.D
        opponents_previous_actions_bool = history.opponent_actions[previous_round_index, :]
        num_cooperators_prev_round = int(my_previous_action_bool) + np.sum(opponents_previous_actions_bool)
        group_cooperation_rate_prev_round = num_cooperators_prev_round / self.game_description.n_players
        if group_cooperation_rate_prev_round >= self.cooperation_threshold:
            if my_previous_action == Action.D and num_cooperators_prev_round < self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        elif my_previous_action == Action.C:
            return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_6(BaseStrategy):
    """
    Cooperative Strategy: Tit-for-Tat with a Cooperative Bias and Forgiveness.

    This strategy aims to foster cooperation by starting cooperatively,
    mirroring the collective behavior of others with a bias towards cooperation,
    and defecting only in the final round due to the absence of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = history.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round == r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_defectors_prev_round = n - 1 - num_cooperators_prev_round
        if num_cooperators_prev_round > num_defectors_prev_round:
            return Action.C
        elif num_defectors_prev_round > num_cooperators_prev_round:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_7(BaseStrategy):
    """
    Forgiving Reciprocity with Threshold strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a degree of forgiveness to avoid being exploited by a single defection.
    It adapts based on the overall cooperation level observed in the game.

    Decision Rules:
    - Cooperate in Round 1 to initiate cooperation.
    - Defect in the last round as there is no future incentive.
    - In intermediate rounds, cooperate if the proportion of cooperators in the
      previous round was above a calculated threshold T = (n-k)/n. Otherwise, defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game description.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold = (self.n - self.k) / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing the history of actions and payoffs for all players.
                     It is None for the first round (round_number 0).

        Returns:
            Action: The action to take in the current round (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == self.r:
            return Action.D
        last_round_actions = history.opponent_actions[current_round_number - 1, :]
        num_cooperators_previous_round = np.sum(last_round_actions)
        proportion_cooperators_previous = num_cooperators_previous_round / self.n
        if proportion_cooperators_previous > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_8(BaseStrategy):
    """
    Reciprocal Conditional Cooperation: Starts cooperatively, then mirrors the
    majority's behavior from the previous round. Defects in the final round.
    This strategy is designed to foster cooperation by rewarding it and
    punishing sustained defection, while remaining robust by defecting in the
    last round where it's the dominant strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_previous_action = int(history.my_actions[previous_round_index])
        opponent_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators = my_previous_action + opponent_cooperators
        majority_threshold = math.floor(self.n / 2) + 1
        if total_cooperators >= majority_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_9(BaseStrategy):
    """
    "Generous Tit-for-Tat with Forgiveness" Strategy.
    This strategy aims to foster cooperation by starting cooperatively,
    reciprocating the majority's actions, and being forgiving of past defections.
    It also includes a mechanism to anticipate the end of the game by defecting
    in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_anticipation_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if current_round >= r - self.defection_anticipation_threshold:
            return Action.D
        if current_round == 0:
            return Action.C
        prev_round_idx = current_round - 1
        my_last_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        opponent_last_actions = history.opponent_actions[prev_round_idx, :]
        num_cooperators_last_round = np.sum(opponent_last_actions)
        if my_last_action == Action.C:
            num_cooperators_last_round += 1
        majority_threshold = math.ceil(n / 2.0)
        if my_last_action == Action.C:
            if num_cooperators_last_round >= majority_threshold:
                return Action.C
            elif num_cooperators_last_round > 0:
                return Action.C
            else:
                return Action.D
        elif num_cooperators_last_round >= majority_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_10(BaseStrategy):
    """
    Tit-for-Tat with a Cooperative Threshold (TT-CT) strategy for the N-Player Public Goods Game.

    This strategy starts by cooperating, then reciprocates the average behavior of the group
    in the previous round. It incorporates a cooperation threshold to punish sustained defection
    and a punishment depth to control the duration of this punishment. It also adopts a pragmatic
    approach to defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.5
        self.punishment_depth = 3
        self.sustained_defection_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the game's history.

        Args:
            state: The current state of the game (e.g., current round number).
            history: A PlayerHistory object containing past actions and payoffs. None for the first round.

        Returns:
            The action (Action.C or Action.D) to be taken in the current round.
        """
        current_round = state.round_number
        if history is None:
            self.sustained_defection_counter = 0
            return Action.C
        if current_round == self.n_rounds:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1, :]
        my_previous_action = history.my_actions[current_round - 1]
        all_previous_actions = np.append(previous_round_actions, my_previous_action)
        num_cooperators_prev_round = np.sum(all_previous_actions)
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        if proportion_cooperators_prev_round >= self.cooperation_threshold:
            self.sustained_defection_counter = 0
            return Action.C
        else:
            self.sustained_defection_counter += 1
            return Action.D

class Strategy_COOPERATIVE_11(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but also includes a mechanism to forgive occasional lapses, making it robust against
    noise or temporary deviations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and sets up internal state.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.c_threshold = 0.6
        self._my_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the game history.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players up to the previous round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            self._my_last_action = Action.C.value
            return Action.C
        if state.round_number == r:
            return Action.D
        previous_round_idx = history.round_number - 1
        if previous_round_idx < 0 or history.opponent_actions.shape[0] <= previous_round_idx:
            self._my_last_action = Action.C.value
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[previous_round_idx, :]
        total_other_cooperators = np.sum(previous_round_opponent_actions)
        num_other_players = n - 1
        average_other_cooperation_rate = 0.0
        if num_other_players > 0:
            average_other_cooperation_rate = total_other_cooperators / num_other_players
        if self._my_last_action == Action.C.value:
            if average_other_cooperation_rate >= self.c_threshold:
                next_action = Action.C
            else:
                next_action = Action.D
        elif average_other_cooperation_rate >= self.c_threshold:
            next_action = Action.C
        else:
            next_action = Action.D
        self._my_last_action = next_action.value
        return next_action

class Strategy_COOPERATIVE_12(BaseStrategy):
    """
    Tit-for-Tat with Cooperative Threshold for N-Player Public Goods Game.

    Starts cooperatively, reciprocates cooperation if the average cooperation
    rate in the previous round meets a threshold (n/2). Punishes defection by
    defecting if the threshold is not met. Always defects in the last round.
    Always defects if k <= n/2 as cooperation is not individually rational.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperative_threshold_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.k <= self.n / 2:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        last_completed_round_index = history.round_number - 1
        cooperators_among_opponents = np.sum(history.opponent_actions[last_completed_round_index, :])
        our_action_last_round = Action.C if history.my_actions[last_completed_round_index] else Action.D
        total_cooperators_previous_round = cooperators_among_opponents
        if our_action_last_round == Action.C:
            total_cooperators_previous_round += 1
        average_cooperation_previous_round = total_cooperators_previous_round / self.n
        if average_cooperation_previous_round >= self.cooperative_threshold_rate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_13(BaseStrategy):
    """
    Adaptive Reciprocity with Conditional Cooperation (ARCC) strategy.
    This strategy aims to foster cooperation by initially cooperating,
    monitoring group behavior, punishing significant drops in cooperation,
    and rewarding sustained cooperation. It also accounts for the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_phase_counter = 0
        self.punishment_duration_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r - 1:
            return Action.D
        if history is None:
            return Action.C
        previous_round_index = current_round - 1
        total_cooperators_prev = 0
        if previous_round_index >= 0:
            total_cooperators_prev += history.my_actions[previous_round_index]
            total_cooperators_prev += np.sum(history.opponent_actions[previous_round_index, :])
        group_cooperation_rate_prev = total_cooperators_prev / self.n
        cooperation_threshold = self.k / self.n
        if self.punishment_phase_counter > 0:
            if group_cooperation_rate_prev >= cooperation_threshold and self.punishment_phase_counter >= self.punishment_duration_threshold:
                self.punishment_phase_counter = 0
                if group_cooperation_rate_prev >= cooperation_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                self.punishment_phase_counter += 1
                return Action.D
        elif group_cooperation_rate_prev >= cooperation_threshold:
            return Action.C
        else:
            self.punishment_phase_counter = 1
            return Action.D

class Strategy_COOPERATIVE_14(BaseStrategy):
    """
    Adaptive Cooperation with Forgiveness (ACF) strategy.
    This strategy aims to foster cooperation by initially assuming others will cooperate,
    rewarding cooperation with continued cooperation, and punishing defection with temporary defection.
    It incorporates a degree of forgiveness to avoid prolonged spirals of mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_coop_threshold_prop = 0.6
        self.P_forgive_threshold_prop = 0.3
        self.P_coop_threshold = math.ceil(self.P_coop_threshold_prop * self.game_description.n_players)
        self.P_forgive_threshold = math.ceil(self.P_forgive_threshold_prop * self.game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        current_round_num = state.round_number
        prev_round_idx = current_round_num - 1
        my_prev_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        if history.opponent_actions.shape[0] > prev_round_idx:
            prev_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        else:
            prev_opponent_cooperators = 0
        total_cooperators_prev_round = (1 if my_prev_action == Action.C else 0) + prev_opponent_cooperators
        proportion_cooperators_prev = total_cooperators_prev_round / n
        if proportion_cooperators_prev >= self.P_coop_threshold_prop:
            return Action.C
        elif my_prev_action == Action.D:
            if proportion_cooperators_prev >= self.P_forgive_threshold_prop:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_15(BaseStrategy):
    """
    Tit-for-Tat with a Threshold (TFT-T) strategy for the N-Player Public Goods Game.
    This strategy initially cooperates and then mirrors the overall cooperation level
    of the previous round, provided it meets a defined threshold. It defects if the
    proportion of cooperators falls below the threshold, aiming for a stable
    cooperative equilibrium while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the TFT-T strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold_proportion = 1.0 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action to take in the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing the history of actions and payoffs for all players
                     up to the previous round. None if it's the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        if history is None:
            return Action.C
        n_players = self.game_description.n_players
        previous_round_actions = history.opponent_actions[-1, :]
        num_cooperators_last_round = np.sum(previous_round_actions)
        proportion_cooperators_last_round = num_cooperators_last_round / n_players
        if proportion_cooperators_last_round >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_16(BaseStrategy):
    """
    Cooperative Reciprocity Strategy (CRS) for the N-Player Public Goods Game.

    This strategy aims for cooperation by initially cooperating and continuing
    to cooperate as long as the group's cooperation level meets a predefined
    threshold. If the group's cooperation falls below this threshold, CRS
    temporarily defects to punish, but quickly reverts to cooperation if the
    group returns to cooperative behavior. This makes it resilient to occasional
    defections and encourages a return to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor((self.game_description.n_players - 1) * (self.game_description.k / self.game_description.n_players)) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the strategy's rules.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players. None for the first round.

        Returns:
            Action.C to cooperate, or Action.D to defect.
        """
        if history is None:
            return Action.C
        n_players = self.game_description.n_players
        num_cooperators_previous_round = 0
        if history.my_actions[-1] == Action.C.value:
            num_cooperators_previous_round += 1
        num_cooperators_previous_round += np.sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_17(BaseStrategy):
    """
    Cooperative Strategy: "Forgiving Reciprocity with a Threshold"
    Starts by cooperating and continues as long as the average contribution
    from others remains sufficiently high. If the average contribution drops
    below a threshold, it defects for a limited number of rounds to signal
    displeasure and protect its endowment. If the low cooperation persists
    beyond this forgiveness period, it permanently defects. It also defects
    in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(0.5, self.n / self.k)
        self.defection_punishment_rounds = 3
        self.defection_streak_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.defection_streak_counter = 0
            return Action.C
        if current_round == self.r:
            return Action.D
        my_last_action = self.my_actions_from_history(history)[current_round - 1]
        opponent_actions_last_round = self.opponent_actions_from_history(history)[current_round - 1, :]
        total_cooperators_prev_round = int(my_last_action) + np.sum(opponent_actions_last_round)
        avg_cooperators_prev_round = total_cooperators_prev_round / self.n
        if avg_cooperators_prev_round >= self.cooperation_threshold:
            self.defection_streak_counter = 0
            return Action.C
        else:
            self.defection_streak_counter += 1
            if self.defection_streak_counter <= self.defection_punishment_rounds:
                return Action.D
            else:
                return Action.D

    def my_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to get my own actions as a boolean array."""
        return history.my_actions

    def opponent_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to get opponent actions as a boolean array."""
        return history.opponent_actions

class Strategy_COOPERATIVE_18(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) strategy for the N-Player Public Goods Game.
    This strategy is cooperative by default, but adapts its cooperation threshold
    based on the observed level of cooperation and personal payoff in the
    previous round. It always defects in the final round and cooperates in the
    first round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ATFT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.0
        self.previous_round_cooperators_count = 0
        self.previous_round_payoff = 0.0
        self.is_first_round = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: PlayerHistory object containing past actions and payoffs, or None for the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if self.is_first_round:
            self.cooperation_threshold = 0.9
            self.previous_round_cooperators_count = self.n
            self.previous_round_payoff = 2.0
            self.is_first_round = False
            return Action.C
        proportion_cooperators = self.previous_round_cooperators_count / self.n
        min_acceptable_payoff = 1.0
        should_cooperate = False
        if proportion_cooperators >= self.cooperation_threshold and self.previous_round_payoff >= min_acceptable_payoff:
            should_cooperate = True
        small_increment = 0.05
        if proportion_cooperators > self.cooperation_threshold + small_increment:
            self.cooperation_threshold = max(0.0, self.cooperation_threshold - small_increment)
        elif proportion_cooperators < self.cooperation_threshold - small_increment:
            self.cooperation_threshold = min(1.0, self.cooperation_threshold + small_increment)
        if should_cooperate:
            return Action.C
        else:
            return Action.D

    def update_state_after_round(self, completed_round_index: int, current_round_cooperators_count: int, player_i_payoff: float):
        """
        Updates the strategy's internal state after a round has been completed.
        This method is called by the tournament system.

        Args:
            completed_round_index: The index of the round that just finished (0-indexed).
            current_round_cooperators_count: The total number of players who cooperated in the just-completed round.
            player_i_payoff: The payoff received by this player in the just-completed round.
        """
        pass

class Strategy_COOPERATIVE_19(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness (ATF) for N-Player Public Goods Game.

    This strategy aims to promote cooperation by reciprocating observed collective behavior.
    It cooperates in the first round, defects in the last round, and in intermediate
    rounds, it cooperates if a majority cooperated in the previous round, and defects
    otherwise. This provides a mechanism for forgiveness and re-engagement in
    cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.n_rounds:
            return Action.D
        prev_round_idx = current_round - 1
        my_cooperation_prev = 1 if history.my_actions[prev_round_idx] else 0
        if history.opponent_actions.shape[1] > 0:
            opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        else:
            opponent_cooperators_prev = 0
        total_cooperators_prev = my_cooperation_prev + opponent_cooperators_prev
        if total_cooperators_prev >= math.ceil(self.n_players / 2):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_20(BaseStrategy):
    """
    "Tit-for-Tat with a Twist" strategy for the N-Player Public Goods Game.

    This strategy starts cooperatively, reciprocates the majority behavior of
    others, and includes forgiveness to recover from potential spirals of mutual
    defection. It also adopts a default defection strategy for the final round
    due to the end-game problem.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if history is None:
            return Action.C
        if current_round_num == self.r:
            return Action.D
        prev_round_idx = current_round_num - 1
        num_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        num_defectors_prev = self.n - 1 - num_cooperators_prev
        if history.my_actions[prev_round_idx]:
            num_cooperators_prev += 1
        else:
            num_defectors_prev += 1
        our_previous_action = history.my_actions[prev_round_idx]
        if num_cooperators_prev > num_defectors_prev:
            if our_previous_action == Action.C:
                return Action.C
            else:
                return Action.C
        elif num_defectors_prev > num_cooperators_prev:
            if our_previous_action == Action.C:
                return Action.D
            else:
                return Action.D
        elif our_previous_action == Action.C:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_21(BaseStrategy):
    """
    "Conditional Cooperation with Forgiveness" strategy for the N-Player Public Goods Game.
    This strategy starts with cooperation, retaliates against personal defection,
    but forgives other players' defections to avoid mutual defection spirals.
    It also handles the last round with a self-interested approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            return Action.C
        all_players_previous_actions = np.concatenate((np.expand_dims(history.my_actions[current_round - 1], axis=0), history.opponent_actions[current_round - 1, :]))
        someone_defected_previous = not np.all(all_players_previous_actions)
        if current_round == r:
            if r - 2 < 0:
                penultimate_round_actions = np.concatenate((np.expand_dims(history.my_actions[0], axis=0), history.opponent_actions[0, :]))
            else:
                penultimate_round_actions = np.concatenate((np.expand_dims(history.my_actions[current_round - 2], axis=0), history.opponent_actions[current_round - 2, :]))
            if np.all(penultimate_round_actions):
                return Action.C
            else:
                return Action.D
        player_i_previous_action = history.my_actions[current_round - 1]
        if someone_defected_previous:
            if not player_i_previous_action:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_COOPERATIVE_22(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while acknowledging the finite nature of the game. It monitors the overall
    level of cooperation and adjusts its behavior accordingly, with a forgiveness
    mechanism to avoid prolonged mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.forgiveness_counter = 0
        COOPERATION_THRESHOLD_PERCENT = 0.5
        self.cooperation_threshold = math.ceil(COOPERATION_THRESHOLD_PERCENT * self.n)
        DEFECTION_THRESHOLD_PERCENT = 0.25
        self.defection_threshold = math.ceil(DEFECTION_THRESHOLD_PERCENT * self.n)
        self.forgiveness_limit = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game history.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs, or None if it's the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        if self.forgiveness_counter >= self.forgiveness_limit:
            self.forgiveness_counter = 0
            return Action.C
        elif num_cooperators_prev_round >= self.cooperation_threshold:
            self.forgiveness_counter = 0
            return Action.C
        elif num_cooperators_prev_round < self.defection_threshold:
            self.forgiveness_counter += 1
            return Action.D
        else:
            self.forgiveness_counter = 0
            return Action.C

class Strategy_COOPERATIVE_24(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperation with Forgiveness"
    This strategy aims to achieve a high cooperative payoff by fostering and
    maintaining cooperation, while also having a clear retaliatory mechanism
    for defection and a pathway back to cooperation. It's designed to be
    adaptive to a dynamic environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_defections_due_to_low_cooperation = 0
        self.is_first_round = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or state.round_number == 1:
            self.consecutive_defections_due_to_low_cooperation = 0
            self.is_first_round = False
            return Action.C
        if state.round_number == self.n_rounds:
            return Action.D
        previous_round_index = history.round_number - 1
        my_previous_action = 1 if history.my_actions[previous_round_index] else 0
        opponent_previous_actions_sum = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators_previous_round = my_previous_action + opponent_previous_actions_sum
        if total_cooperators_previous_round >= self.n_players / 2:
            self.consecutive_defections_due_to_low_cooperation = 0
            return Action.C
        else:
            punishment_duration = math.ceil(self.n_rounds / 4)
            if self.consecutive_defections_due_to_low_cooperation < punishment_duration:
                self.consecutive_defections_due_to_low_cooperation += 1
                return Action.D
            else:
                self.consecutive_defections_due_to_low_cooperation = 0
                return Action.C

class Strategy_COOPERATIVE_25(BaseStrategy):
    """
    The "Tit-for-Tat with a Twist" strategy for the N-Player Public Goods Game.
    This strategy starts cooperatively, rewards collective cooperation, punishes
    significant defection, and employs a forgiving approach in ambiguous situations
    to encourage a return to cooperation. It also defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy.

        Args:
            game_description: An object containing game parameters.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_COOPERATE = 0.75
        self.T_DEFECT = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the game state and history.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players.
                     None if it's the first round (before any rounds have been played).

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if history is None:
            return Action.C
        previous_round_idx = current_round_number - 1
        my_prev_action = self.my_actions_from_history(history)[previous_round_idx]
        opponents_prev_actions = self.opponent_actions_from_history(history)[previous_round_idx, :]
        all_prev_actions = np.append(my_prev_action, opponents_prev_actions)
        num_cooperators_prev_round = np.sum(all_prev_actions)
        cooperation_proportion_prev_round = num_cooperators_prev_round / self.n_players
        if cooperation_proportion_prev_round >= self.T_COOPERATE:
            return Action.C
        elif cooperation_proportion_prev_round <= self.T_DEFECT:
            return Action.D
        else:
            return Action.C

    def my_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Extracts my actions from history, ensuring correct shape for all rounds."""
        if history.my_actions.ndim == 0:
            return np.array([history.my_actions])
        return history.my_actions

    def opponent_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Extracts opponent actions from history, ensuring correct shape for all rounds."""
        if history.opponent_actions.ndim == 1:
            return np.array([history.opponent_actions])
        return history.opponent_actions

class Strategy_COOPERATIVE_26(BaseStrategy):
    """
    Cooperative Strategy: "Reciprocal Cooperation with Threshold"
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a degree of forgiveness and a mechanism to prevent exploitation.
    It is built on the principle of reciprocal altruism, adapted for a multi-player setting.

    Core Philosophy:
    - Initial Trust: Start by assuming others will cooperate.
    - Reward Cooperation: Continue to cooperate as long as cooperation is generally maintained.
    - Punish Defection (with leniency): Temporarily defect if a significant number of players defect,
      but return to cooperation if the collective returns to cooperative behavior.
    - Threshold for Action: Avoid reacting to isolated defections, focusing on sustained patterns of behavior.
    - End-game defection: Acknowledges the final round's Nash Equilibrium.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.6
        self.D_max = 3
        self.defect_consecutive_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            self.defect_consecutive_count = 0
            return Action.C
        current_round_number = state.round_number
        if current_round_number == r:
            return Action.D
        cooperators_in_prev_round = 0
        if current_round_number > 0:
            if history.opponent_actions.shape[0] > 0:
                cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
                if history.my_actions[-1] == Action.C.value:
                    cooperators_in_prev_round += 1
            else:
                cooperators_in_prev_round = n
        cooperation_ratio_prev_round = cooperators_in_prev_round / n
        if cooperation_ratio_prev_round >= self.theta:
            self.defect_consecutive_count = 0
            return Action.C
        elif self.defect_consecutive_count < self.D_max:
            self.defect_consecutive_count += 1
            return Action.D
        else:
            self.defect_consecutive_count = 0
            return Action.C

class Strategy_COOPERATIVE_27(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Reciprocity with Forgiveness"
    This strategy starts by cooperating and then mirrors the opponent's behavior,
    with a degree of forgiveness to avoid perpetual defection spirals.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_threshold = 0.5
        self.forgiveness_limit = 1
        self.forgiveness_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            self.forgiveness_count = 0
            return Action.C
        previous_round_actions = history.opponent_actions[current_round - 1]
        num_other_players = self.n - 1
        num_cooperators_in_previous_round = np.sum(previous_round_actions)
        cooperator_proportion_t_minus_1 = num_cooperators_in_previous_round / num_other_players
        if cooperator_proportion_t_minus_1 >= self.cooperation_threshold:
            self.forgiveness_count = 0
            return Action.C
        elif self.forgiveness_count < self.forgiveness_limit:
            self.forgiveness_count += 1
            return Action.C
        else:
            self.forgiveness_count = 0
            return Action.D

class Strategy_COOPERATIVE_28(BaseStrategy):
    """
    Cooperative Strategy: The "Conditional Reciprocity" Strategy.

    This strategy aims to foster cooperation by reciprocating the average
    player's last action. It includes special handling for the first and
    last rounds to promote sustained cooperation and a forgiveness window
    to break cycles of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Conditional Reciprocity strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.forgiveness_window_size = 3
        self.consecutive_defections_by_average = 0
        self.last_average_action_was_cooperate = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            The action (Action.C or Action.D) to be taken in the current round.
        """
        current_round = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round == 0:
            self.consecutive_defections_by_average = 0
            self.last_average_action_was_cooperate = True
            return Action.C
        if current_round == r - 1:
            return Action.C
        if history.my_actions.size > 0:
            num_cooperators_prev_round = np.sum(history.my_actions[current_round - 1])
            if history.opponent_actions.size > 0:
                num_cooperators_prev_round += np.sum(history.opponent_actions[current_round - 1, :])
            proportion_cooperators_prev_round = num_cooperators_prev_round / n
            average_player_cooperated_prev_round = proportion_cooperators_prev_round >= 0.5
        else:
            average_player_cooperated_prev_round = True
        if self.consecutive_defections_by_average >= self.forgiveness_window_size:
            self.consecutive_defections_by_average = 0
            self.last_average_action_was_cooperate = True
            return Action.C
        if average_player_cooperated_prev_round:
            self.consecutive_defections_by_average = 0
            self.last_average_action_was_cooperate = True
            return Action.C
        else:
            self.consecutive_defections_by_average += 1
            self.last_average_action_was_cooperate = False
            return Action.D

class Strategy_COOPERATIVE_29(BaseStrategy):
    """
    Adaptive Cooperative Trigger (ACT) strategy.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while remaining adaptable to changing environments and player behaviors. It prioritizes
    long-term cooperative outcomes but is not naive to the potential for exploitation.

    Decision Rules:
    - Cooperates (C) if the proportion of cooperators in the previous round meets or
      exceeds a dynamic cooperation threshold.
    - Defects (D) if the group's cooperation level falls below the threshold.
    - Enters a punishment phase (defecting) for a predefined number of rounds after a
      trigger event, but will return to cooperation if the group resumes sufficient
      cooperation during this phase.
    - The cooperation threshold dynamically adjusts: increases if observed cooperation
      exceeds it, and decreases if it falls short, to adapt to group behavior.

    Edge Cases:
    - First Round (history is None): Cooperates (C) to initiate cooperation.
    - Last Round (round_number == n_rounds): Defects (D) to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = 0.6
        self.threshold_adjustment_rate = 0.01
        self.punishment_rounds_remaining = 0
        self.base_punishment_duration = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game state and history.

        Args:
            state: The current state of the game, including the round number.
            history: History of past actions and payoffs for this player and opponents.
                     None if it's the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        if history is None:
            return Action.C
        prev_round_num = state.round_number - 1
        num_cooperators_prev_round = 0
        if prev_round_num >= 0:
            num_cooperators_prev_round += np.sum(history.opponent_actions[prev_round_num, :])
            if history.my_actions[prev_round_num] == Action.C:
                num_cooperators_prev_round += 1
        if state.round_number == self.n_rounds:
            return Action.D
        if self.punishment_rounds_remaining > 0:
            if num_cooperators_prev_round >= math.ceil(self.cooperation_threshold * self.n_players):
                self.punishment_rounds_remaining = 0
                return Action.C
            else:
                self.punishment_rounds_remaining -= 1
                return Action.D
        else:
            actual_cooperation_proportion = num_cooperators_prev_round / self.n_players
            if actual_cooperation_proportion >= self.cooperation_threshold:
                self.cooperation_threshold = min(1.0, self.cooperation_threshold + self.threshold_adjustment_rate)
            else:
                self.cooperation_threshold = max(0.0, self.cooperation_threshold - self.threshold_adjustment_rate)
            if num_cooperators_prev_round >= math.ceil(self.cooperation_threshold * self.n_players):
                return Action.C
            else:
                self.punishment_rounds_remaining = self.base_punishment_duration
                return Action.D

class Strategy_COOPERATIVE_30(BaseStrategy):
    """
    Adaptive Reciprocity with Conditional Forgiveness strategy for the N-Player Public Goods Game.

    This strategy starts by cooperating and then reciprocates the group's average behavior
    from the previous round. It dynamically adjusts a cooperation threshold to encourage
    higher cooperation rates, forgiving occasional defections but punishing sustained defection.
    A smoothed average of past cooperation rates makes the strategy robust to single-round
    fluctuations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.k_over_n = self.k / self.n_players
        self.initial_coop_threshold = 0.9
        self.forgiveness_factor = 0.05
        self.punishment_factor = 0.1
        self.alpha = 0.1
        self.coop_threshold = self.initial_coop_threshold
        self.smoothed_avg_coop_history = 0.0

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        if history is None:
            self.coop_threshold = self.initial_coop_threshold
            self.smoothed_avg_coop_history = 0.0
            return Action.C
        else:
            current_round_idx = state.round_number - 1
            if current_round_idx == 0:
                avg_coop_rate_previous_round = 0.0
            else:
                previous_round_my_action = self.convert_to_bool(history.my_actions[current_round_idx - 1])
                previous_round_opponent_actions = history.opponent_actions[current_round_idx - 1, :]
                total_cooperators_in_previous_round = previous_round_my_action + np.sum(previous_round_opponent_actions)
                avg_coop_rate_previous_round = total_cooperators_in_previous_round / self.n_players
            smoothed_avg_coop = self.alpha * avg_coop_rate_previous_round + (1 - self.alpha) * self.smoothed_avg_coop_history
            self.smoothed_avg_coop_history = smoothed_avg_coop
            if smoothed_avg_coop >= self.coop_threshold:
                action = Action.C
            else:
                action = Action.D
            if smoothed_avg_coop >= self.coop_threshold:
                next_coop_threshold = self.coop_threshold - self.forgiveness_factor
            else:
                next_coop_threshold = self.coop_threshold + self.punishment_factor
            self.coop_threshold = max(self.k_over_n, min(1.0, next_coop_threshold))
            return action

    def convert_to_bool(self, action_enum: Action) -> bool:
        """Helper to convert Action enum to boolean for calculations."""
        return action_enum == Action.C

class Strategy_COOPERATIVE_31(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) Strategy for N-Player Public Goods Game.

    ARF aims to foster cooperation by rewarding it and punishing defection,
    but with a built-in mechanism for forgiveness to prevent cycles of mutual
    defection. It learns from the collective behavior of the group and adapts
    its response accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ARF strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.baseline_cooperation_threshold = 0.5
        self.threshold_adaptation_rate = 0.05
        self.forgiveness_buffer = 0.1
        self.sustained_behavior_rounds = 5
        self.cooperation_threshold = self.baseline_cooperation_threshold
        self.my_last_action_cooperated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     If None, it's the first round.

        Returns:
            Action.C if the player chooses to cooperate, Action.D if they choose to defect.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            self.my_last_action_cooperated = True
            return Action.C
        my_actions_bool = self.convert_actions_to_bool(history.my_actions)
        opponent_actions_bool = self.convert_actions_to_bool(history.opponent_actions)
        self.my_last_action_cooperated = my_actions_bool[-1] if my_actions_bool.size > 0 else True
        num_cooperators_last_round = np.sum(opponent_actions_bool[-1, :]) if opponent_actions_bool.shape[0] > 0 else 0
        proportion_cooperators_last_round = num_cooperators_last_round / n
        past_rounds_for_adaptation = min(current_round, self.sustained_behavior_rounds)
        if past_rounds_for_adaptation > 0:
            recent_opponent_actions = opponent_actions_bool[-past_rounds_for_adaptation:, :]
            recent_proportion_cooperators = np.sum(recent_opponent_actions, axis=1) / n
            avg_cooperation_rate = np.mean(recent_proportion_cooperators)
            if avg_cooperation_rate > 0.7:
                self.cooperation_threshold = max(0.3, self.cooperation_threshold - self.threshold_adaptation_rate)
            elif avg_cooperation_rate < 0.3:
                self.cooperation_threshold = min(0.7, self.cooperation_threshold + self.threshold_adaptation_rate)
        else:
            self.cooperation_threshold = self.baseline_cooperation_threshold
        recovery_threshold = self.cooperation_threshold + self.forgiveness_buffer
        recovery_threshold = min(1.0, recovery_threshold)
        if current_round == r - 1:
            return Action.D
        if self.my_last_action_cooperated:
            if proportion_cooperators_last_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D
        elif proportion_cooperators_last_round >= recovery_threshold:
            return Action.C
        else:
            return Action.D

    def convert_actions_to_bool(self, actions: NDArray[np.bool_]) -> NDArray[np.bool_]:
        """
        Converts numerical actions (0/1) to boolean (False/True) where True represents Cooperate.
        This is to align with how Action.C/Action.D might be stored internally or how
        numpy boolean arrays are typically used for truthiness.
        """
        return actions

class Strategy_COOPERATIVE_32(BaseStrategy):
    """
    "Conditional Reward and Punishment" Strategy for the N-Player Public Goods Game.

    This strategy is inspired by Tit-for-Tat but adapted for the public goods
    context. It aims to foster cooperation by rewarding collective effort and
    punishing defection.

    Decision Rule: Cooperate if the average cooperation rate in the previous round
    was above a threshold (0.5), otherwise defect.

    Edge Cases:
    - First Round: Cooperate to signal cooperative intent.
    - Last Round: Apply the same rule as other rounds to encourage cooperation
      throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = self.game_description.n_players
        previous_round_index = history.round_number - 1
        my_previous_action = 1 if self.game_description.n_players == 1 else history.my_actions[previous_round_index]
        opponent_previous_actions = history.opponent_actions[previous_round_index, :]
        if n > 1:
            all_previous_actions = np.append(my_previous_action, opponent_previous_actions)
        else:
            all_previous_actions = np.array([my_previous_action])
        num_cooperators_previous_round = np.sum(all_previous_actions)
        average_cooperation_rate_previous_round = num_cooperators_previous_round / n
        if average_cooperation_rate_previous_round > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_33(BaseStrategy):
    """
    Tit-for-Tat with Collective Punishment (TF-CP) Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initially cooperating, but also introduces a
    mechanism to deter free-riding and punish consistent defection. It adapts based on the
    collective behavior of the group.

    Core Principles:
    1. Initial Cooperation: Start by cooperating to signal a willingness to build a cooperative equilibrium.
    2. Reciprocity (Collective): Reward cooperation with cooperation, and punish defection with defection based on group average.
    3. Collective Focus: Adapt decisions based on the *overall* level of cooperation in the group.
    4. Forgiveness (Conditional): If the group returns to a high level of cooperation, the strategy is willing to forgive past defections.
    5. Patience: Understanding that building cooperation takes time and may involve setbacks.
    6. Endgame Punishment: Always defect in the last round, as is rational in a finite repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing the history of actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1, :]
        total_cooperators_prev_round = np.sum(previous_round_actions)
        average_cooperation_rate_prev_round = total_cooperators_prev_round / self.n
        theta = self.k / self.n
        if average_cooperation_rate_prev_round >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_34(BaseStrategy):
    """
    "Generous Grim Trigger" strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a degree of leniency to prevent complete collapse of cooperation.

    It starts by cooperating and continues to do so as long as the collective
    endeavor shows promise. Cooperation is maintained if the proportion of
    cooperators in the previous round exceeds a threshold. Defection is triggered
    if the player's own payoff was significantly lower than expected due to
    exploitation, or if cooperation has been below the threshold for a sustained
    period. The last round is always a defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.6
        self.sustained_defect_rounds_trigger = 3
        self.payoff_deviation_tolerance = 0.2
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs from previous rounds.

        Returns:
            Action.C to cooperate, or Action.D to defect.
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        if state.round_number == n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_last_action = Action.C if history.my_actions[previous_round_idx] else Action.D
        total_cooperators_last_round = np.sum(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx])
        proportion_cooperators_last_round = total_cooperators_last_round / n_players
        my_last_payoff = history.my_payoffs[previous_round_idx]
        all_payoffs_last_round = history.my_payoffs[previous_round_idx] + np.sum(history.opponent_payoffs[previous_round_idx, :])
        average_payoff_last_round = all_payoffs_last_round / n_players
        if proportion_cooperators_last_round < self.cooperation_threshold:
            self.consecutive_low_cooperation_rounds += 1
        else:
            self.consecutive_low_cooperation_rounds = 0
        if my_last_action == Action.C:
            expected_my_payoff_if_cooperated = 1 - 1 + k / n_players * total_cooperators_last_round
            if my_last_payoff < expected_my_payoff_if_cooperated * (1 - self.payoff_deviation_tolerance):
                return Action.D
            elif proportion_cooperators_last_round < self.cooperation_threshold:
                return Action.C
            else:
                return Action.C
        elif my_last_action == Action.D:
            if self.consecutive_low_cooperation_rounds >= self.sustained_defect_rounds_trigger:
                return Action.D
            elif proportion_cooperators_last_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_35(BaseStrategy):
    """
    Grim Trigger with Forgiveness (GTF) Strategy:
    Starts by cooperating, then punishes any defection with continuous defection
    until a prolonged period of group-wide defection occurs, after which it attempts
    to return to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.R_initial = max(1, int(math.ceil(game_description.n_rounds * 0.1)))
        self.N_forgiveness_rounds = max(1, int(math.ceil(game_description.n_rounds * 0.1)))
        self.state = 'INITIAL_COOPERATION'
        self.punishment_rounds_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.state = 'INITIAL_COOPERATION'
            self.punishment_rounds_counter = 0
            return Action.C
        current_round = history.round_number
        previous_round_actions = None
        if current_round > 0:
            my_previous_action = Action.C if history.my_actions[current_round - 1] else Action.D
            opponents_previous_actions = [Action.C if action else Action.D for action in history.opponent_actions[current_round - 1, :]]
            previous_round_actions = [my_previous_action] + opponents_previous_actions
        else:
            return Action.C
        if self.state == 'INITIAL_COOPERATION':
            anyone_defected = any((action == Action.D for action in previous_round_actions))
            if anyone_defected:
                self.state = 'PUNISHMENT'
                self.punishment_rounds_counter = 0
                return Action.D
            else:
                return Action.C
        elif self.state == 'PUNISHMENT':
            all_defected_in_previous = all((action == Action.D for action in previous_round_actions))
            if all_defected_in_previous:
                self.punishment_rounds_counter += 1
            else:
                self.punishment_rounds_counter = 0
            if self.punishment_rounds_counter >= self.N_forgiveness_rounds:
                self.state = 'FORGIVENESS'
                return Action.C
            else:
                return Action.D
        elif self.state == 'FORGIVENESS':
            anyone_defected = any((action == Action.D for action in previous_round_actions))
            if anyone_defected:
                self.state = 'PUNISHMENT'
                self.punishment_rounds_counter = 0
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_36(BaseStrategy):
    """
    The "Generous Forgiver" strategy aims to initiate and sustain cooperation
    by being forgiving of past defections, but by clearly signaling that
    sustained defection will be met with a proportional response.
    It tries to get back to a cooperative state whenever possible, but
    protects itself from exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        elif current_round == self.r:
            return Action.D
        else:
            previous_round_index = current_round - 1
            cooperators_among_opponents = np.sum(history.opponent_actions[previous_round_index, :])
            if history.my_actions[previous_round_index]:
                total_cooperators_previous_round = cooperators_among_opponents + 1
            else:
                total_cooperators_previous_round = cooperators_among_opponents
            proportion_cooperators = total_cooperators_previous_round / self.n
            if proportion_cooperators >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_37(BaseStrategy):
    """
    Cooperative Strategy: Tit-for-Tat with Parameter Awareness.
    This strategy aims to maximize long-term group welfare by cooperating initially,
    reciprocating observed cooperation levels that are collectively beneficial,
    and defecting when collective cooperation falls below a sustainability threshold.
    It also defects in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_previous_action = Action.C if history.my_actions[-1] else Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :])
        if my_previous_action == Action.C:
            previous_round_cooperators += 1
        if previous_round_cooperators >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_38(BaseStrategy):
    """
    Tit-for-Tat with Adaptive Thresholding (TFT-AT) for N-Player Public Goods Game.

    This strategy initially cooperates and then mirrors the group's behavior.
    It cooperates if the proportion of cooperators in the previous round
    exceeds an adaptive threshold, and defects otherwise. The threshold
    is dynamically calculated based on game parameters (k, n) to represent
    the minimum number of OTHER players that need to cooperate for it to be
    individually beneficial to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the TFT-AT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.adaptive_threshold = self.k / self.n * (self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing past actions and payoffs for this player
                     and opponents. None for the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        if history is None:
            return Action.C
        else:
            num_other_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_previous_round = num_other_cooperators_previous_round + (1 if history.my_actions[-1] else 0)
            proportion_cooperators_previous_round = total_cooperators_previous_round / self.n
            if num_other_cooperators_previous_round >= self.adaptive_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_39(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Trust Threshold for N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a built-in "trust threshold" to prevent immediate punishment and allow
    for recovery from accidental defections or early exploration. It prioritizes
    long-term cooperation while being resilient to exploitative behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.trust_level = 1.0
        self.cooperation_threshold = self._calculate_cooperation_threshold()
        self.defection_trigger_count = math.ceil(self.n * 0.2)
        self.trust_increase_coop = 0.05
        self.trust_decrease_defection = 0.2
        self.trust_increase_rebuild = 0.02
        self.trust_decrease_low_trust = 0.1
        self.trust_decay_factor = 0.005

    def _calculate_cooperation_threshold(self) -> float:
        """
        Calculates the cooperation threshold based on game parameters.
        A player playing C is only strictly better than D if (k/n) * N_C >= 1,
        meaning N_C >= n/k. This proportion (1/k) is a critical point.
        We set the threshold slightly higher to encourage a more robust level of cooperation.
        """
        target_prop = 1.0 / self.k
        return max(0.5, target_prop * 1.1)

    def _update_trust(self, my_action: Action, observed_proportion_cooperators: float, observed_defection_count: int, is_low_trust_phase: bool):
        """
        Updates the trust level based on observed actions and current phase.
        """
        if my_action == Action.C:
            if observed_proportion_cooperators >= self.cooperation_threshold:
                self.trust_level = min(1.0, self.trust_level + self.trust_increase_coop)
            else:
                self.trust_level = max(0.0, self.trust_level - self.trust_decrease_defection)
        elif observed_proportion_cooperators < self.cooperation_threshold:
            if is_low_trust_phase:
                self.trust_level = max(0.0, self.trust_level - self.trust_decrease_low_trust)
            else:
                self.trust_level = max(0.0, self.trust_level - self.trust_decrease_defection)
        else:
            self.trust_level = min(1.0, self.trust_level + self.trust_increase_rebuild)
        self.trust_level = max(0.0, self.trust_level - self.trust_decay_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs up to the previous round.
                     None if it's the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            self.trust_level = 1.0
            return Action.C
        my_actions_prev = history.my_actions
        opponent_actions_prev = history.opponent_actions
        if opponent_actions_prev.shape[0] > 0:
            last_round_opponent_actions = opponent_actions_prev[-1, :]
            cooperators_in_prev_round = np.sum(last_round_opponent_actions)
            num_opponents = self.n - 1
            observed_proportion_cooperators = cooperators_in_prev_round / num_opponents if num_opponents > 0 else 1.0
        else:
            observed_proportion_cooperators = 1.0
            cooperators_in_prev_round = self.n - 1
        observed_defection_count = self.n - 1 - cooperators_in_prev_round
        is_low_trust_phase = self.trust_level < 0.3
        my_action = Action.C
        if self.trust_level > 0.3:
            if observed_proportion_cooperators < self.cooperation_threshold:
                if observed_defection_count >= self.defection_trigger_count or observed_proportion_cooperators < self.cooperation_threshold * 0.7:
                    my_action = Action.D
                else:
                    my_action = Action.C
            else:
                my_action = Action.C
        elif observed_proportion_cooperators < self.cooperation_threshold * 0.8:
            my_action = Action.D
        else:
            my_action = Action.C
        self._update_trust(my_action, observed_proportion_cooperators, observed_defection_count, is_low_trust_phase)
        return my_action

class Strategy_COOPERATIVE_40(BaseStrategy):
    """
    Cooperative Strategy: "Forgiving Tit-for-Tat with an Escalation Clause"
    This strategy aims to foster cooperation by being forgiving but retaliatory against persistent defection.
    It learns from group behavior and adjusts its cooperation threshold dynamically.
    Defaults to cooperation, only defects when the group average falls below a dynamically adjusted target.
    Includes an escalation clause for severe, persistent defection patterns.
    Cooperate in the first round, defect in the last round for payoff maximization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.p_target = 0.0
        self.my_defection_count = 0
        self.cooperation_score = 0
        self.threshold_positive_adjustment = 5
        self.threshold_negative_adjustment = -5
        self.adjustment_step = 0.01
        self.escalation_defection_threshold = self.r // 4
        self.escalation_retaliation_duration = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.initialize_strategy()
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        total_cooperators_prev_round = 0
        if history.round_number > 0:
            total_cooperators_prev_round = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1])
        avg_cooperators_prev_round = total_cooperators_prev_round / self.n if self.n > 0 else 0.0
        if self.my_defection_count > self.escalation_defection_threshold and avg_cooperators_prev_round < self.p_target:
            if self.my_defection_count <= self.escalation_defection_threshold + self.escalation_retaliation_duration:
                self.my_defection_count += 1
                return Action.D
            else:
                self.my_defection_count = 0
        if avg_cooperators_prev_round >= self.p_target:
            return Action.C
        else:
            self.my_defection_count += 1
            return Action.D

    def initialize_strategy(self):
        """Initializes strategy state before the first round."""
        initial_target_from_k = self.n / self.k / self.n if self.k > 0 else 0.0
        self.p_target = max(0.5, initial_target_from_k)
        self.p_target = min(1.0, self.p_target)
        self.my_defection_count = 0
        self.cooperation_score = 0

    def update_state_after_round(self, history: PlayerHistory):
        """Updates strategy state based on the outcome of the last completed round."""
        if history.round_number == 0:
            return
        total_cooperators_this_round = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1])
        avg_cooperators_this_round = total_cooperators_this_round / self.n if self.n > 0 else 0.0
        if avg_cooperators_this_round >= self.p_target:
            self.cooperation_score += 1
        else:
            self.cooperation_score -= 1
        if self.cooperation_score > self.threshold_positive_adjustment:
            self.p_target = max(0.1, self.p_target - self.adjustment_step)
            self.cooperation_score = 0
        elif self.cooperation_score < self.threshold_negative_adjustment:
            self.p_target = min(0.9, self.p_target + self.adjustment_step)
            self.cooperation_score = 0

class Strategy_COOPERATIVE_41(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) for the N-Player Public Goods Game.

    This strategy starts cooperatively, then adapts its cooperation threshold
    based on the observed collective cooperation rate in the previous round.
    It aims to foster cooperation by rewarding high cooperation and
    punishing sustained defection, while maintaining a bias towards
    the collective benefit point (k/n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ATFT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(self.k / self.n + 0.1, self.k / self.n)
        self.cooperation_threshold = min(1.0, self.cooperation_threshold)
        self.sensitivity = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: Current game state, including the round number.
            history: History of past actions and payoffs for this player and opponents.
                     None if it's the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        round_number = state.round_number
        if history is None or round_number == 0:
            return Action.C
        if round_number == self.r - 1:
            return Action.D
        prev_round_idx = round_number - 1
        all_prev_round_actions = np.append([history.my_actions[prev_round_idx]], history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = np.sum(all_prev_round_actions)
        coop_rate_prev_round = num_cooperators_prev_round / self.n
        if coop_rate_prev_round >= self.cooperation_threshold:
            my_action = Action.C
        else:
            my_action = Action.D
        adjustment_factor = 1.0
        if coop_rate_prev_round > self.cooperation_threshold + self.sensitivity:
            adjustment_factor = 1.05
        elif coop_rate_prev_round < self.cooperation_threshold - self.sensitivity:
            adjustment_factor = 0.9
        self.cooperation_threshold *= adjustment_factor
        self.cooperation_threshold = max(self.k / self.n, self.cooperation_threshold)
        self.cooperation_threshold = min(1.0, self.cooperation_threshold)
        return my_action

class Strategy_COOPERATIVE_42(BaseStrategy):
    """
    Tit-for-Tat with Adaptive Cooperation Threshold (TT-ACT) for N-Player Public Goods Game.

    This strategy starts by cooperating, retaliates against defection by defecting,
    and forgives by returning to cooperation when others cooperate sufficiently.
    It adaptively adjusts its cooperation threshold based on the observed proportion
    of cooperators in the previous round to maintain a balance between cooperation
    and self-protection. It defects in the final round as a rational response to
    the end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.0
        self.delta_low = 0.1
        self.delta_high = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.cooperation_threshold = self.k
            return Action.C
        if state.round_number == self.r:
            return Action.D
        num_cooperators_prev_round = 0
        if self.n > 0:
            if history.my_actions[history.round_number - 1]:
                num_cooperators_prev_round += 1
            for player_idx in range(self.n - 1):
                if history.opponent_actions[history.round_number - 1, player_idx]:
                    num_cooperators_prev_round += 1
        last_round_cooperators_proportion = num_cooperators_prev_round / self.n
        if last_round_cooperators_proportion >= self.cooperation_threshold:
            current_action = Action.C
            self.cooperation_threshold = max(1.0, self.cooperation_threshold - self.delta_low)
        else:
            current_action = Action.D
            self.cooperation_threshold = min(float(self.n), self.cooperation_threshold + self.delta_high)
        return current_action

class Strategy_COOPERATIVE_43(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Reciprocity with Forgiveness"

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a crucial element of forgiveness to prevent a complete breakdown of
    cooperation due to isolated bad actors or specific round dynamics.
    It's based on the principle of "tit-for-tat" but adapted for a multi-player,
    repeated setting, focusing on the collective outcome of the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.threshold_cooperate_full = math.floor(0.75 * self.n)
        self.threshold_defect_full = math.floor(0.25 * self.n)
        self.midpoint_threshold = math.ceil(0.5 * self.n)
        self.threshold_cooperate_last_round = math.floor(0.9 * self.n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        if history is None:
            return Action.C
        previous_round_my_action = 1 if history.my_actions[current_round_t - 1] else 0
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_t - 1, :])
        total_cooperators_prev_round = previous_round_my_action + previous_round_opponent_cooperators
        if current_round_t == self.r - 1:
            if total_cooperators_prev_round >= self.threshold_cooperate_last_round:
                return Action.C
            else:
                return Action.D
        elif total_cooperators_prev_round >= self.threshold_cooperate_full:
            return Action.C
        elif total_cooperators_prev_round < self.threshold_defect_full:
            return Action.D
        elif total_cooperators_prev_round >= self.midpoint_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_44(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness: A cooperative strategy that starts by cooperating,
    reciprocates high cooperation, retaliates against significant defection, and attempts
    to forgive occasional deviations to re-establish cooperation. It also defects in the
    last round to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.DEF_TOLERANCE = 0.5
        self.HIGH_COOPERATION_THRESHOLD = 0.7
        self.MODERATE_COOPERATION_THRESHOLD = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.game_description.n_rounds:
            return Action.D
        previous_round_actions = history.my_actions[-1]
        previous_opponent_actions = history.opponent_actions[-1]
        all_previous_actions = np.append(previous_round_actions, previous_opponent_actions)
        n_cooperators_prev = np.sum(all_previous_actions)
        cooperation_ratio_prev = n_cooperators_prev / n_players
        if cooperation_ratio_prev >= self.HIGH_COOPERATION_THRESHOLD:
            return Action.C
        elif cooperation_ratio_prev < self.DEF_TOLERANCE:
            return Action.D
        elif previous_round_actions == Action.D.value:
            return Action.C
        elif cooperation_ratio_prev >= self.MODERATE_COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_45(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness for N-Player Public Goods Game.
    This strategy initiates cooperation, reciprocates the group's average behavior,
    punishes sustained defection, and forgives significant recoveries in cooperation.
    It also defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.c_threshold = 0.7
        self.d_threshold = 0.3
        self.punishment_streak = 3
        self.forgiveness_trigger_delta = 0.4
        self.current_punishment_streak = 0
        self.is_currently_punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        previous_round_index = current_round - 1
        all_previous_actions = []
        if previous_round_index < len(history.my_actions):
            all_previous_actions.append(history.my_actions[previous_round_index])
        else:
            return Action.C
        if previous_round_index < len(history.opponent_actions):
            all_previous_actions.extend(history.opponent_actions[previous_round_index, :])
        else:
            return Action.C
        total_cooperators_prev_round = sum(all_previous_actions)
        average_cooperation_prev_round = total_cooperators_prev_round / self.n
        avg_coop_before_punishment = 0.0
        if self.is_currently_punishing:
            if current_round - 2 >= 0:
                round_before_punishment_index = current_round - 2
                all_actions_before_punishment = []
                if round_before_punishment_index < len(history.my_actions):
                    all_actions_before_punishment.append(history.my_actions[round_before_punishment_index])
                if round_before_punishment_index < len(history.opponent_actions):
                    all_actions_before_punishment.extend(history.opponent_actions[round_before_punishment_index, :])
                if all_actions_before_punishment:
                    avg_coop_before_punishment = sum(all_actions_before_punishment) / self.n
            if average_cooperation_prev_round > avg_coop_before_punishment + self.forgiveness_trigger_delta:
                self.is_currently_punishing = False
                self.current_punishment_streak = 0
        if not self.is_currently_punishing:
            if average_cooperation_prev_round < self.d_threshold:
                self.current_punishment_streak += 1
            else:
                self.current_punishment_streak = 0
        if self.is_currently_punishing:
            return Action.D
        elif self.current_punishment_streak >= self.punishment_streak:
            self.is_currently_punishing = True
            return Action.D
        elif average_cooperation_prev_round >= self.c_threshold:
            return Action.C
        else:
            return Action.C

class Strategy_COOPERATIVE_46(BaseStrategy):
    """
    Cooperative Strategy: The "Reciprocity with Forgiveness" Strategy

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a built-in mechanism for forgiveness to prevent cycles of mutual defection.
    It's designed to be played in a repeated game environment where learning from history is possible.

    Core Principle:
    The fundamental idea is to start cooperatively and maintain cooperation as long as
    the group's average behavior is sufficiently cooperative. If defection becomes too
    prevalent, the strategy will temporarily defect to signal disapproval and incentivize
    a return to cooperation.

    Decision Rules:
    The decision to Cooperate (C) or Defect (D) is based on a moving average of the
    proportion of cooperators in the previous rounds.

    1. Cooperation Threshold (CT):
       CT = (k + 1) / n. If the Moving Average of Cooperation (MAC) is above or equal to CT, cooperate.

    2. Moving Average of Cooperation (MAC):
       MAC_t = ((s=1 to t-1) (total_cooperators_in_round_s / n)) / (t-1).

    3. Forgiveness Mechanism:
       If MAC < CT, the strategy will cooperate for a limited number of rounds (F_max)
       using a forgiveness counter (FC). If FC reaches F_max, it will defect.

    Edge Cases:
    - First Round (t=1): Always cooperate (C) to initiate.
    - Last Round (t=r): Always defect (D) to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = (self.k + 1.0) / self.n
        self.f_max = 3
        self.forgiveness_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.forgiveness_counter = 0
            return Action.C
        if current_round == self.r:
            return Action.D
        total_cooperators_in_history = 0
        num_previous_rounds = history.round_number
        if num_previous_rounds > 0:
            my_cooperations = np.sum(history.my_actions)
            opponent_cooperations = np.sum(history.opponent_actions)
            total_cooperators_in_history = my_cooperations + opponent_cooperations
            mac = total_cooperators_in_history / (num_previous_rounds * self.n)
        else:
            mac = 0.0
        if mac >= self.cooperation_threshold:
            self.forgiveness_counter = 0
            return Action.C
        elif self.forgiveness_counter < self.f_max:
            self.forgiveness_counter += 1
            return Action.C
        else:
            self.forgiveness_counter = 0
            return Action.D

class Strategy_COOPERATIVE_47(BaseStrategy):
    """
    Conditional Cooperator with Forgiveness strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by initially cooperating, rewarding continued cooperation,
    and punishing defection, with a mechanism for forgiveness to encourage a return to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_cooperation_level = 'Cooperative'
        self.previous_round_cooperators_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.current_cooperation_level = 'High'
            self.previous_round_cooperators_count = 0
            return Action.C
        round_num = state.round_number
        num_players = self.n
        k_factor = self.k
        total_rounds = self.r
        if round_num == total_rounds - 1:
            return Action.D
        cooperators_in_prev_round = 0
        if round_num > 0:
            if history.my_actions[round_num - 1] == Action.C.value:
                cooperators_in_prev_round += 1
            cooperators_in_prev_round += np.sum(history.opponent_actions[round_num - 1, :])
        high_cooperation_threshold = math.ceil(num_players * (1 - k_factor / num_players)) + 1
        moderate_cooperation_threshold = math.ceil(num_players / 2)
        high_cooperation_threshold = max(0, min(num_players, high_cooperation_threshold))
        moderate_cooperation_threshold = max(0, min(num_players, moderate_cooperation_threshold))
        if cooperators_in_prev_round >= high_cooperation_threshold:
            self.current_cooperation_level = 'High'
        elif cooperators_in_prev_round >= moderate_cooperation_threshold:
            self.current_cooperation_level = 'Moderate'
        elif self.current_cooperation_level == 'Low':
            if cooperators_in_prev_round > self.previous_round_cooperators_count:
                self.current_cooperation_level = 'Moderate'
            else:
                self.current_cooperation_level = 'Low'
        else:
            self.current_cooperation_level = 'Low'
        if self.current_cooperation_level == 'High':
            return Action.C
        elif self.current_cooperation_level == 'Moderate':
            significant_drop_threshold = num_players / 4
            if self.previous_round_cooperators_count - cooperators_in_prev_round > significant_drop_threshold:
                return Action.D
            else:
                return Action.C
        elif self.current_cooperation_level == 'Low':
            if cooperators_in_prev_round < num_players / 2:
                return Action.D
            else:
                return Action.D
        self.previous_round_cooperators_count = cooperators_in_prev_round

class Strategy_COOPERATIVE_48(BaseStrategy):
    """
    Cooperative Adaptive Strategy (CAS) for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding observed cooperation and
    punishing observed defection, but with a mechanism to "reset" to cooperation
    if the group shows a sustained return to cooperative behavior. It is adaptive
    and robust, prioritizing a long-term perspective.

    Decision Rules:
    - First Round: Always Cooperate (C).
    - Last Round: Always Defect (D).
    - Subsequent Rounds (t > 1):
        - If player i defected in round t-1:
            - If all other players j != i cooperated in t-1: Cooperate in t.
            - Otherwise: Defect in t.
        - If player i cooperated in round t-1:
            - If all other players j != i also cooperated in t-1: Cooperate in t.
            - If at least one other player j != i defected in t-1: Defect in t.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == r:
            return Action.D
        if history is None:
            return Action.C
        current_round_idx = state.round_number
        previous_round_idx = current_round_idx - 1
        my_previous_action_is_cooperate = history.my_actions[previous_round_idx]
        other_players_cooperated_previously = True
        if history.opponent_actions.shape[1] > 0:
            opponent_cooperators_count = np.sum(history.opponent_actions[previous_round_idx, :])
            if opponent_cooperators_count < n - 1:
                other_players_cooperated_previously = False
        else:
            other_players_cooperated_previously = True
        if my_previous_action_is_cooperate:
            if other_players_cooperated_previously:
                return Action.C
            else:
                return Action.D
        elif other_players_cooperated_previously:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_49(BaseStrategy):
    """
    "Reciprocal Kindness with Forgiveness" Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a crucial element of forgiveness to prevent being locked into cycles
    of mutual defection. It starts cooperatively, reciprocates past behavior,
    but allows for a limited number of defections from others before defaulting
    to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and internal state.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.total_past_defections = 0
        self.forgiveness_threshold = math.ceil(self.game_description.n_rounds / 4)
        self.initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None for the very first round.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        if not self.initialized:
            self.total_past_defections = 0
            self.forgiveness_threshold = math.ceil(self.game_description.n_rounds / 4)
            self.initialized = True
        if state.round_number == 0:
            return Action.C
        other_players_defected_last_round = False
        if history is not None and history.round_number > 0:
            last_round_opponent_actions = history.opponent_actions[history.round_number - 1, :]
            if np.any(last_round_opponent_actions == Action.D.value):
                other_players_defected_last_round = True
        if other_players_defected_last_round:
            self.total_past_defections += 1
        if self.total_past_defections >= self.forgiveness_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_50(BaseStrategy):
    """
    Adaptive Cooperative Strategy with Punishment and Forgiveness for N-Player Public Goods Game.

    This strategy aims to foster cooperation by being optimistic initially,
    conditionally cooperating based on group behavior, and employing a
    punishment phase for sustained defection, followed by a mechanism to
    re-enter cooperation. It also handles the last round as a single-shot game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_round = 0
        self.total_cooperators_prev_round = self.n
        self.punishment_phase = False
        self.punishment_counter = 0
        self.P_threshold = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.current_round = 0
            self.total_cooperators_prev_round = self.n
            self.punishment_phase = False
            self.punishment_counter = 0
            return Action.C
        self.current_round = history.round_number
        if self.current_round == self.r:
            return Action.D
        actual_cooperators_this_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.total_cooperators_prev_round = int(actual_cooperators_this_round)
        if self.punishment_phase:
            if self.total_cooperators_prev_round == self.n:
                self.punishment_phase = False
                self.punishment_counter = 0
                return Action.C
            else:
                self.punishment_counter += 1
                if self.punishment_counter > self.P_threshold:
                    self.punishment_phase = False
                    self.punishment_counter = 0
                return Action.D
        else:
            min_cooperators_for_stability = math.ceil(self.n * self.k / self.n)
            if self.total_cooperators_prev_round >= min_cooperators_for_stability:
                self.punishment_counter = 0
                return Action.C
            else:
                self.punishment_counter += 1
                if self.punishment_counter > self.P_threshold:
                    self.punishment_phase = True
                    self.punishment_counter = 0
                return Action.D

class Strategy_COOPERATIVE_51(BaseStrategy):
    """
    Cooperative Strategy: Tit-for-Tat with Adaptive Threshold for N-Player Public Goods Game.

    This strategy aims to promote cooperation by mirroring the behavior of the group,
    but with a built-in resilience to defection and a mechanism to "forgive" or
    re-initiate cooperation after periods of low group contribution. It adapts
    based on the observed collective behavior, aiming to maximize long-term group
    payoff while safeguarding individual interests.

    Key Features:
    - Cooperates in the first round.
    - Defects in the last round (due to backward induction).
    - In intermediate rounds, cooperates if the average contribution of the
      previous round was above an adaptive threshold.
    - The threshold increases when the group's cooperation drops below the threshold
      and decreases when it's above, creating a dynamic adjustment mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.current_threshold = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing past actions and payoffs for all players,
                     or None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        n_rounds = self.game_description.n_rounds
        round_number = state.round_number
        if history is None:
            self.current_threshold = 1.0
            return Action.C
        if round_number == n_rounds - 1:
            return Action.D
        previous_round_actions_all = []
        if round_number > 0:
            my_prev_action = int(history.my_actions[round_number - 1])
            previous_round_actions_all.append(my_prev_action)
            total_cooperators_last_round = np.sum(history.my_actions[round_number - 1])
            if history.opponent_actions.shape[0] > 0 and history.opponent_actions.shape[1] > 0:
                total_cooperators_last_round += np.sum(history.opponent_actions[round_number - 1, :])
            else:
                pass
        else:
            total_cooperators_last_round = 0
        avg_contribution_last_round = total_cooperators_last_round / self.game_description.n_players
        if avg_contribution_last_round >= self.current_threshold:
            decision = Action.C
            new_threshold = self.current_threshold * 0.95
            self.current_threshold = max(0.1, new_threshold)
        else:
            decision = Action.D
            new_threshold = self.current_threshold * 1.05
            self.current_threshold = min(1.0, new_threshold)
        return decision

class Strategy_COOPERATIVE_52(BaseStrategy):
    """
    Grim Trigger with Forgiveness strategy for the N-Player Public Goods Game.
    This strategy is fundamentally cooperative, aiming for mutual benefit through
    sustained cooperation. It incorporates a "grim trigger" mechanism to punish
    defection but includes a limited form of forgiveness to allow for the
    re-establishment of cooperation if conditions improve.

    The strategy's behavior is governed by a 'cooperation_threshold', which is
    the minimum number of cooperators observed in the previous round required to
    continue cooperating. This threshold adapts based on past outcomes.

    Key Features:
    - Cooperates unconditionally in the first round.
    - Defects in the last round (backward induction).
    - For rounds in between:
        - Cooperates if the number of cooperators in the previous round met the
          current 'cooperation_threshold'.
        - Defects if cooperation fell below the 'cooperation_threshold'.
        - Adjusts the 'cooperation_threshold':
            - Increases it if cooperation failed to meet the threshold while the
              strategy was cooperating.
            - Decreases it (with a forgiveness factor) if cooperation met a
              lower bar while the strategy was defecting, facilitating a return
              to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.0
        self.my_last_action_was_cooperate = False
        self.total_cooperators_last_round = 0
        self._delta_increase = 1.0
        self._delta_decrease = 1.0
        self._forgiveness_factor = 0.7
        self._initial_coop_threshold_factor = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.cooperation_threshold = math.ceil(self.n * self._initial_coop_threshold_factor)
            self.my_last_action_was_cooperate = True
            return Action.C
        if current_round == self.r:
            return Action.D
        self.my_last_action_was_cooperate = bool(history.my_actions[current_round - 1])
        previous_round_opponents_actions = history.opponent_actions[current_round - 1]
        self.total_cooperators_last_round = int(self.my_last_action_was_cooperate) + np.sum(previous_round_opponents_actions)
        previous_cooperation_threshold = self.cooperation_threshold
        if self.my_last_action_was_cooperate:
            if self.total_cooperators_last_round >= previous_cooperation_threshold:
                self.cooperation_threshold = previous_cooperation_threshold
            else:
                self.cooperation_threshold = min(self.n, previous_cooperation_threshold + self._delta_increase)
        else:
            forgiveness_threshold = math.ceil(previous_cooperation_threshold * self._forgiveness_factor)
            if self.total_cooperators_last_round >= forgiveness_threshold:
                self.cooperation_threshold = max(1.0, previous_cooperation_threshold - self._delta_decrease)
            else:
                self.cooperation_threshold = previous_cooperation_threshold
        if self.total_cooperators_last_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_53(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperation with Forgiveness"

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a mechanism for returning to cooperation if the group converges.
    It's designed to be adaptive and robust in a repeated game setting with perfect
    information and common knowledge.

    Decision Rules:
    - Initial State: Begin by cooperating.
    - Subsequent Rounds (t > 1):
        - Cooperate (C) IF:
            - The total number of cooperators in the previous round was >= threshold.
            - OR (Forgiveness): If the strategy defected in the immediately preceding round (t-1),
              AND the total number of cooperators in the round before that (t-2) was sufficiently high (>= threshold).
        - Defect (D) IF:
            - The total number of cooperators in the previous round was < threshold.
            - OR If the strategy defected in the immediately preceding round and forgiveness conditions are not met.

    Threshold: `n - floor(n/2)` (more than half the players cooperated).

    Edge Cases:
    - First Round (t=1): Always Cooperate (C).
    - Last Round (t=r): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.threshold = self.n - math.floor(self.n / 2)
        self.my_last_action: Action | None = None
        self.prev_prev_total_cooperators: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_last_action = Action.C
            self.prev_prev_total_cooperators = 0
            return Action.C
        if current_round == self.r:
            self.my_last_action = Action.D
            return Action.D
        total_cooperators_previous_round = int(np.sum(history.opponent_actions[current_round - 1, :]))
        action_to_take = Action.D
        if total_cooperators_previous_round >= self.threshold:
            action_to_take = Action.C
        elif self.my_last_action == Action.D and self.prev_prev_total_cooperators >= self.threshold:
            action_to_take = Action.C
        self.prev_prev_total_cooperators = total_cooperators_previous_round
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_COOPERATIVE_54(BaseStrategy):
    """
    The "Conditional Grinder" strategy aims to foster cooperation by rewarding cooperation
    and punishing defection, while accounting for the repeated nature of the game and
    game parameters. It starts cooperatively and maintains cooperation as long as it's
    collectively beneficial. It defects strategically to punish sustained defection and
    then attempts to re-establish a cooperative equilibrium.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if history is None:
            self.punishment_phase = False
            return Action.C
        prev_round_idx = current_round - 1
        my_actions_prev = history.my_actions
        my_payoffs_prev = history.my_payoffs
        opponent_actions_prev = history.opponent_actions
        total_cooperators_prev_round = np.sum(my_actions_prev[prev_round_idx]) + np.sum(opponent_actions_prev[prev_round_idx])
        my_payoff_prev_round = my_payoffs_prev[prev_round_idx]
        min_cooperators_for_benefit = math.ceil(n / k) if k > 1 else n
        if self.punishment_phase:
            if total_cooperators_prev_round >= math.ceil(n * 0.6):
                self.punishment_phase = False
                return Action.C
            else:
                return Action.D
        if total_cooperators_prev_round >= min_cooperators_for_benefit:
            return Action.C
        else:
            self.punishment_phase = True
            return Action.D

class Strategy_COOPERATIVE_55(BaseStrategy):
    """
    Cooperative Strategy: "Tit-for-Tat with Adaptive Threshold"
    This strategy aims to foster cooperation by reciprocating cooperative behavior,
    but also incorporates a mechanism to defend against exploitation by adapting
    a cooperation threshold. It starts by cooperating, punishes exploitation by
    defecting when the observed cooperation level falls below the threshold,
    and gradually adjusts the threshold based on past outcomes. It also defects
    in the last round to account for the end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold = 0.0
        self.initial_threshold_set = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.threshold = max(0.5, self.k / self.n)
            self.initial_threshold_set = True
            return Action.C
        current_round = state.round_number
        if current_round == self.r - 1:
            return Action.D
        previous_round_actions_self = history.my_actions[history.round_number - 1]
        previous_round_actions_opponents = history.opponent_actions[history.round_number - 1, :]
        num_cooperators_prev = np.sum(previous_round_actions_self) + np.sum(previous_round_actions_opponents)
        ocr_prev = num_cooperators_prev / self.n
        if ocr_prev >= self.threshold:
            self.threshold = max(0.2, self.threshold - 0.05)
        else:
            self.threshold = min(0.9, self.threshold + 0.1)
        if ocr_prev >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_56(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Threshold Cooperation (ATFT-TC)

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while also being forgiving and capable of re-establishing cooperation. It adapts
    to the observed behavior of the group over time.

    Core Principles:
    1. Cooperation as the Default: Starts with cooperation.
    2. Reciprocity: Punishes defection by defecting for a limited time.
    3. Forgiveness: Willing to return to cooperation if others do.
    4. Threshold-Based Re-engagement: Uses group cooperation levels to decide when
       to attempt re-establishing cooperation.
    5. Endgame Consideration: Defect in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = 0.75
        self.punishment_length = 3
        self.punishment_counter = 0
        self.my_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds - 1:
            return Action.D
        if history is None:
            self.my_history.append(True)
            return Action.C
        num_cooperators_prev_round = 0
        if history.round_number > 0:
            num_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :]) + history.my_actions[current_round - 1]
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players if self.n_players > 0 else 0
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            self.my_history.append(False)
            return Action.D
        elif proportion_cooperators_prev_round >= self.cooperation_threshold:
            self.my_history.append(True)
            return Action.C
        else:
            my_last_action = self.my_history[-1]
            if my_last_action is True:
                self.punishment_counter = self.punishment_length
                self.my_history.append(False)
                return Action.D
            else:
                self.my_history.append(False)
                return Action.D

class Strategy_COOPERATIVE_57(BaseStrategy):
    """
    Adaptive Reciprocity with Grim Trigger for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing deviations
    decisively. It cooperates by default, continues cooperating if the group
    maintains a sufficient level of cooperation, and enters a permanent
    "Grim Trigger" state of defection if cooperation collapses and the player
    acted as a cooperator. It always cooperates in the first and last rounds
    to promote initial cooperation and maximize collective payoff in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperation = math.ceil(self.n_players / self.k) + 1
        self.grim_trigger_activated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.n_rounds:
            return Action.C
        if self.grim_trigger_activated:
            return Action.D
        previous_round_index = current_round - 1
        self_action_last_round = history.my_actions[previous_round_index]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators_last_round = opponent_cooperators_last_round + (1 if self_action_last_round else 0)
        if total_cooperators_last_round >= self.threshold_cooperation:
            return Action.C
        elif self_action_last_round:
            self.grim_trigger_activated = True
            return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_58(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy is cooperative by default, retaliates against defections,
    and attempts to forgive and re-establish cooperation after a period of
    collective good behavior. It aims to promote cooperation while being
    robust to sustained defection.

    Parameters:
        Threshold_Cooperate: The minimum number of cooperators in the previous
                             round required to trigger cooperation in the current round.
        Forgiveness_Limit: The number of consecutive defections after which
                           the strategy will attempt to forgive and cooperate
                           if the group shows signs of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.threshold_cooperate = math.ceil(self.n_players / 2)
        self.forgiveness_limit = 3
        self.forgiveness_counter = 0

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if history is None:
            self.forgiveness_counter = 0
            return Action.C
        if current_round == self.n_rounds:
            return Action.D
        previous_round_index = current_round - 1
        n_cooperators_prev = np.sum(history.opponent_actions[previous_round_index, :])
        my_previous_action = history.my_actions[previous_round_index]
        total_cooperators_prev = n_cooperators_prev
        if my_previous_action:
            total_cooperators_prev += 1
        if total_cooperators_prev >= self.threshold_cooperate:
            self.forgiveness_counter = 0
            return Action.C
        else:
            self.forgiveness_counter += 1
            if self.forgiveness_counter >= self.forgiveness_limit and total_cooperators_prev >= self.threshold_cooperate:
                self.forgiveness_counter = 0
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_59(BaseStrategy):
    """
    Group Reciprocity Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    mirroring the collective behavior of the group in the previous round.
    It defects if the group's cooperation level falls below a defined threshold,
    and cooperates if it meets or exceeds it. Special handling for the first
    and last rounds is included.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1, :]
        num_cooperators_previous_round = np.sum(previous_round_actions)
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_60(BaseStrategy):
    """
    Adaptive Reciprocity with Punishment Threshold for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and deterring defection
    through a measured response. It balances the desire for high collective payoffs
    with self-preservation against exploitative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.base_threshold_cooperation = self.game_description.k / self.game_description.n_players
        self.current_threshold_cooperation = self.base_threshold_cooperation
        self.threshold_increase_rate = 1.01
        self.threshold_decrease_rate = 0.99
        self.min_threshold = self.base_threshold_cooperation
        self.max_threshold = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game history.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None for the first round.

        Returns:
            Action.C to Cooperate, Action.D to Defect.
        """
        if history is None:
            self.current_threshold_cooperation = self.base_threshold_cooperation
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        own_action_prev_round = history.my_actions[prev_round_idx]
        total_cooperators_previous_round = num_opponent_cooperators + (1 if own_action_prev_round else 0)
        proportion_cooperators_previous_round = total_cooperators_previous_round / self.game_description.n_players
        if proportion_cooperators_previous_round >= self.current_threshold_cooperation:
            action = Action.C
        else:
            action = Action.D
        if proportion_cooperators_previous_round >= self.current_threshold_cooperation:
            self.current_threshold_cooperation *= self.threshold_decrease_rate
        else:
            self.current_threshold_cooperation *= self.threshold_increase_rate
        self.current_threshold_cooperation = max(self.min_threshold, self.current_threshold_cooperation)
        self.current_threshold_cooperation = min(self.max_threshold, self.current_threshold_cooperation)
        return action

class Strategy_COOPERATIVE_61(BaseStrategy):
    """
    The "Adaptive Echo" strategy aims to promote cooperation by mirroring
    the group's collective behavior. It starts by cooperating, then responds
    to the previous round's average cooperation rate. It cooperates if the
    group's cooperation is high or moderate, and defects if it falls below
    a defined threshold, signaling dissatisfaction and protecting its payoff.
    It also defects in the final round due to the lack of future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.T_high = 0.75
        self.T_mid = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == n_rounds:
            return Action.D
        previous_round_idx = current_round_number - 1
        cooperators_in_previous_round = 0
        if self.game_description.n_players > 1:
            if history.my_actions[previous_round_idx]:
                cooperators_in_previous_round += 1
        if history.opponent_actions.size > 0:
            cooperators_in_previous_round += np.sum(history.opponent_actions[previous_round_idx, :])
        avg_coop_rate_prev = cooperators_in_previous_round / n_players
        if avg_coop_rate_prev >= self.T_high:
            return Action.C
        elif avg_coop_rate_prev >= self.T_mid:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_62(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while also being forgiving and capable of re-establishing cooperation. It learns
    from past behavior and adapts its approach based on the collective actions of the group.

    The strategy has the following characteristics:
    - Defaults to cooperation.
    - Defects if cooperation significantly declines or a pattern of defection persists.
    - Returns to cooperation if the group's cooperation recovers.
    - Cooperates in the first round as a leap of faith.
    - Defects in the last round to avoid exploitation.
    - Considers the penultimate round's state to inform its last-round decision.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ATFT strategy.

        Args:
            game_description: An object containing the game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.defection_threshold = 0.5
        self.re_cooperation_threshold = 0.7
        self.defection_persistence_count = 3
        self.my_previous_action = None
        self.consecutive_defections_by_group = 0

    def _count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """Helper function to count cooperators in a given array of actions."""
        return np.sum(actions).astype(int)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None if it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round = state.round_number
        if current_round == 0:
            self.my_previous_action = Action.C
            self.consecutive_defections_by_group = 0
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        if history is not None:
            previous_round_actions = history.my_actions[current_round - 1]
            total_cooperators_in_previous_round = self._count_cooperators(previous_round_actions)
            if history.opponent_actions is not None and history.opponent_actions.shape[0] > 0:
                total_cooperators_in_previous_round += self._count_cooperators(history.opponent_actions[current_round - 1, :])
            proportion_cooperators_previous_round = total_cooperators_in_previous_round / self.n
            if proportion_cooperators_previous_round < self.defection_threshold:
                self.consecutive_defections_by_group += 1
            else:
                self.consecutive_defections_by_group = 0
        else:
            proportion_cooperators_previous_round = 0
            self.consecutive_defections_by_group = 0
        if self.consecutive_defections_by_group >= self.defection_persistence_count:
            self.my_previous_action = Action.D
            return Action.D
        if proportion_cooperators_previous_round < self.defection_threshold:
            self.my_previous_action = Action.D
            return Action.D
        if proportion_cooperators_previous_round >= self.re_cooperation_threshold:
            self.my_previous_action = Action.C
            return Action.C
        self.my_previous_action = Action.C
        return Action.C

class Strategy_COOPERATIVE_63(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperation with Forgiveness"
    This strategy aims to foster cooperation by starting cooperatively,
    rewarding cooperative behavior, and punishing defection. It incorporates
    forgiveness by defaulting to cooperation in ambiguous situations. It also
    adjusts behavior for the final rounds using backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_high = 0.6
        self.cooperation_threshold_low = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round == r - 1:
            return Action.D
        if current_round == r:
            return Action.D
        previous_round_actions = history.my_actions[current_round - 1]
        opponent_actions_previous_round = history.opponent_actions[current_round - 1, :]
        num_cooperators_previous_round = np.sum(opponent_actions_previous_round) + previous_round_actions
        proportion_cooperators_previous_round = num_cooperators_previous_round / n
        individual_defection_against_me = False
        if previous_round_actions == Action.C.value:
            for i in range(n - 1):
                if opponent_actions_previous_round[i] == Action.D.value:
                    individual_defection_against_me = True
                    break
        if individual_defection_against_me:
            return Action.D
        elif proportion_cooperators_previous_round >= self.cooperation_threshold_high:
            return Action.C
        elif proportion_cooperators_previous_round < self.cooperation_threshold_low:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_64(BaseStrategy):
    """
    Cooperative Strategy: "Tit-for-Tat with a Cooperative Bias and Forgiveness"
    This strategy aims to foster cooperation by starting cooperatively,
    reciprocating opponents' actions, but also incorporating a mechanism
    to encourage the return to cooperation after periods of defection.
    It prioritizes collective gain while mitigating the risks of exploitation.

    Core Philosophy:
    - Start Cooperative: Assume other players are also seeking to maximize
      collective payoff and begin with cooperation.
    - Reciprocate: Punish defection by defecting, and reward cooperation by cooperating.
    - Forgive and Re-engage: Don't hold grudges indefinitely. If a player
      returns to cooperation after defecting, be willing to cooperate again.
    - Exploit the "K" Advantage: Recognize that when cooperation is high,
      the collective payoff is significantly amplified.

    Decision Rules:
    - Round 1: Always Cooperate.
    - Last Round: Always Defect.
    - Intermediate Rounds:
        - Cooperate if the average cooperation proportion of *other* players
          in the previous round meets a threshold, or if previously
          defected and a lower forgiveness threshold is met.
        - Defect otherwise.

    Threshold Calculation:
    - `initiate_coop_threshold`: The proportion of players required to
      sustain cooperation, set to `1.0 / k`.
    - `return_coop_threshold`: A slightly lower threshold for returning
      to cooperation after a defection, set to `max(0.0, (1.0 / k) - 0.1)`.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.initiate_coop_threshold = 1.0 / self.k
        self.return_coop_threshold = max(0.0, 1.0 / self.k - 0.1)

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing the history of actions and payoffs
                     for all players up to the previous round. None for the first round.

        Returns:
            The action to take in the current round (Action.C or Action.D).
        """
        current_round_number = state.round_number
        if history is None or current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_number - 1
        my_previous_action_bool = history.my_actions[previous_round_idx]
        my_previous_action = Action.C if my_previous_action_bool else Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        if my_previous_action == Action.C:
            total_cooperators_prev_round += 1
        other_players_count = self.n_players - 1
        if other_players_count == 0:
            return Action.D
        our_contribution_prev_round = 1 if my_previous_action == Action.C else 0
        average_opponent_coop_prev_round = (total_cooperators_prev_round - our_contribution_prev_round) / other_players_count
        if my_previous_action == Action.C:
            if average_opponent_coop_prev_round >= self.initiate_coop_threshold:
                return Action.C
            else:
                return Action.D
        elif average_opponent_coop_prev_round >= self.return_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_65(BaseStrategy):
    """
    "Conditional Reciprocity with a Grace Period": This strategy aims to foster
    cooperation by rewarding it and punishing defection, but with a built-in
    forgiveness mechanism and a clear understanding of the game's finite nature.
    It initially cooperates for a grace period, then cooperates if the average
    cooperation rate in the previous round was above k/n, and defects in the
    last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.grace_period_rounds = min(math.floor(self.r / 4), 5)
        self.cooperation_threshold = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == self.r:
            return Action.D
        if current_round_idx < self.grace_period_rounds:
            return Action.C
        if history is None:
            return Action.C
        previous_round_idx = current_round_idx - 1
        if previous_round_idx < 0:
            return Action.C
        all_previous_round_actions = np.append([history.my_actions[previous_round_idx]], history.opponent_actions[previous_round_idx, :])
        total_cooperators_prev_round = np.sum(all_previous_round_actions)
        average_cooperation_rate = total_cooperators_prev_round / self.n
        if average_cooperation_rate > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_66(BaseStrategy):
    """
    Implements the "Forgiving Reciprocity" strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating, continues to cooperate if the majority
    cooperated in the previous round, defects if cooperation levels drop too low
    (less than half the players), and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_index = current_round - 1
        our_previous_action_is_cooperate = history.my_actions[previous_round_index]
        num_cooperators_opponents_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_prev_round = int(our_previous_action_is_cooperate) + int(num_cooperators_opponents_prev_round)
        if num_cooperators_prev_round > n / 2:
            return Action.C
        if num_cooperators_prev_round < n / 2:
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_67(BaseStrategy):
    """
    Adaptive Reciprocity strategy for the N-Player Public Goods Game.
    This strategy cooperates initially and reciprocates the average behavior
    of the group in the previous round. It also considers the immediate
    incentive to defect if it offers a significantly higher payoff.
    It always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.51
        self.exploitation_buffer = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        if history is None:
            return Action.C
        prev_round_idx = current_round_number - 1
        all_player_actions_prev_round = {}
        if len(history.my_actions) > 0:
            all_player_actions_prev_round[0] = Action.C if history.my_actions[prev_round_idx] else Action.D
            for opp_idx in range(self.n_players - 1):
                all_player_actions_prev_round[opp_idx + 1] = Action.C if history.opponent_actions[prev_round_idx, opp_idx] else Action.D
        total_cooperators_prev_round = sum((1 for action in all_player_actions_prev_round.values() if action == Action.C))
        avg_c_prev_round = total_cooperators_prev_round / self.n_players
        my_action_last_round = Action.C if history.my_actions[prev_round_idx] else Action.D
        num_cooperators_if_i_defect = total_cooperators_prev_round
        if my_action_last_round == Action.C:
            num_cooperators_if_i_defect -= 1
        num_cooperators_if_i_defect = max(0, num_cooperators_if_i_defect)
        payoff_defect_current = 1 - 0 + self.k / self.n_players * num_cooperators_if_i_defect
        num_cooperators_if_i_cooperates = total_cooperators_prev_round
        if my_action_last_round == Action.D:
            num_cooperators_if_i_cooperates += 1
        payoff_cooperate_current = 1 - 1 + self.k / self.n_players * num_cooperators_if_i_cooperates
        if avg_c_prev_round < self.cooperation_threshold:
            return Action.D
        elif payoff_defect_current > payoff_cooperate_current + self.exploitation_buffer:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_68(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness strategy for the N-Player Public Goods Game.
    This strategy initially cooperates, punishes persistent defectors, and attempts
    to return to cooperation if the overall level of cooperation in the group improves.
    It aims to be robust to opponent behavior and foster long-term cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold_ratio = 0.5
        self.persistent_defector_window = 3
        self.my_player_index = -1
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def _is_persistent_defector(self, opponent_index: int, history: PlayerHistory) -> bool:
        """
        Checks if a given opponent has been a persistent defector based on history.

        Args:
            opponent_index: The index of the opponent to check.
            history: The game history object.

        Returns:
            True if the opponent is a persistent defector, False otherwise.
        """
        current_round = history.round_number
        window_size = min(self.persistent_defector_window, current_round)
        if window_size == 0:
            return False
        opponent_past_actions = history.opponent_actions[current_round - window_size:current_round, opponent_index]
        defections_in_window = np.sum(~opponent_past_actions)
        return defections_in_window == window_size

    def _get_player_action_in_round(self, player_index: int, round_t: int, history: PlayerHistory) -> Action:
        """
        Retrieves the action of a specific player in a specific round from history.

        Args:
            player_index: The index of the player.
            round_t: The round number (0-indexed).
            history: The game history object.

        Returns:
            The Action (C or D) of the player in that round.
        """
        if player_index == self.my_player_index:
            return Action.C if history.my_actions[round_t] else Action.D
        else:
            adjusted_opponent_idx = player_index if player_index < self.my_player_index else player_index - 1
            return Action.C if history.opponent_actions[round_t, adjusted_opponent_idx] else Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players up to the previous round.

        Returns:
            The Action (Action.C or Action.D) for the current round.
        """
        current_round = state.round_number
        if self.my_player_index == -1:
            pass
        if history is None or current_round == 0:
            return Action.C
        num_cooperators_prev_round = 1
        if not history.my_actions[-1]:
            num_cooperators_prev_round = 0
        for opp_idx in range(self.n_players - 1):
            if history.opponent_actions[-1, opp_idx]:
                num_cooperators_prev_round += 1
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        cooperation_threshold = self.cooperation_threshold_ratio
        persistent_defector_indices = []
        for opp_idx in range(self.n_players - 1):
            if self._is_persistent_defector(opp_idx, history):
                persistent_defector_indices.append(opp_idx)
        is_self_persistent_defector = False
        window_size_self = min(self.persistent_defector_window, current_round)
        if window_size_self > 0:
            my_past_actions = history.my_actions[current_round - window_size_self:current_round]
            defections_in_window_self = np.sum(~my_past_actions)
            if defections_in_window_self == window_size_self:
                is_self_persistent_defector = True
        my_action = Action.C
        if len(persistent_defector_indices) > 0:
            my_action = Action.D
        elif is_self_persistent_defector:
            my_action = Action.D
        elif proportion_cooperators_prev_round < cooperation_threshold:
            for opp_idx in range(self.n_players - 1):
                opponent_last_action = Action.C if history.opponent_actions[-1, opp_idx] else Action.D
                if opponent_last_action == Action.D:
                    my_action = Action.D
                    break
        return my_action

class Strategy_COOPERATIVE_69(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but incorporates forgiveness to avoid descending into perpetual defection.
    It learns from past behavior to adjust its own actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.forgiveness_counter = 0
        self.FORGIVENESS_THRESHOLD = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == self.r:
            return Action.D
        if history is None:
            self.forgiveness_counter = 0
            return Action.C
        previous_round_actions = history.opponent_actions[current_round_t - 1, :]
        my_previous_action = history.my_actions[current_round_t - 1]
        all_previous_actions = np.append(previous_round_actions, my_previous_action)
        num_cooperators_prev = np.sum(all_previous_actions)
        num_defectors_prev = self.n - num_cooperators_prev
        if num_cooperators_prev > self.n / 2:
            self.forgiveness_counter = 0
            return Action.C
        elif self.forgiveness_counter < self.FORGIVENESS_THRESHOLD:
            self.forgiveness_counter += 1
            return Action.D
        else:
            self.forgiveness_counter = 0
            return Action.C

class Strategy_COOPERATIVE_70(BaseStrategy):
    """
    Conditional Reciprocity with Commitment Strategy for the N-player Public Goods Game.

    This strategy initiates cooperation, continues to cooperate as long as the
    previous round's cooperation rate meets a defined threshold (k/n), and defects
    otherwise. It also defects in the final round due to the end-game problem.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperate = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        previous_round_index = state.round_number - 1
        if state.round_number == self.r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        cooperation_rate_prev_round = num_cooperators_prev_round / self.n
        if cooperation_rate_prev_round >= self.threshold_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_71(BaseStrategy):
    """
    Reciprocity with Forgiveness (RWf) Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a built-in mechanism to re-establish cooperation after periods of conflict.
    It's designed to be adaptive to varying levels of cooperation from opponents and
    robust against exploitation.

    Core Principles:
    1. Initial Cooperation: Start by assuming others will cooperate.
    2. Conditional Reciprocity: If others cooperate, continue to cooperate. If others
       defect, punish them by defecting.
    3. Forgiveness: After a period of sustained defection, be willing to re-initiate
       cooperation to see if the group can return to a mutually beneficial state.
    4. Adaptation to Group Behavior: Decision-making is influenced by the overall
       level of cooperation observed in the group.

    Decision Rules are state-based:
    - STATE_COOPERATING: Group is generally cooperating, strategy cooperates.
    - STATE_PUNISHING: Group has recently shown significant defection, strategy defects.
    - STATE_FORGIVING: Punishment period passed, strategy tries cooperating again.

    Edge Cases:
    - First Round (t=1): Always play C.
    - Last Round (t=r): Always play D.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the RWf strategy.

        Args:
            game_description: Contains game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.COOPERATION_THRESHOLD_RATIO = 0.75
        self.PUNISHMENT_DURATION = 3
        self.STATE_COOPERATING = 0
        self.STATE_PUNISHING = 1
        self.STATE_FORGIVING = 2
        self.current_state = self.STATE_COOPERATING
        self.punishment_rounds_counter = 0

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Decides the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players up to the
                     previous round. None for the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == r - 1:
            return Action.D
        if history is None:
            self.current_state = self.STATE_COOPERATING
            self.punishment_rounds_counter = 0
            return Action.C
        prev_round_idx = state.round_number - 1
        my_prev_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        total_cooperators_prev_round = 0
        if my_prev_action == Action.C:
            total_cooperators_prev_round += 1
        total_cooperators_prev_round += np.sum(opponent_prev_actions)
        prop_cooperators_prev_round = total_cooperators_prev_round / n
        if self.current_state == self.STATE_COOPERATING:
            if prop_cooperators_prev_round >= self.COOPERATION_THRESHOLD_RATIO:
                return Action.C
            else:
                self.current_state = self.STATE_PUNISHING
                self.punishment_rounds_counter = 0
                return Action.D
        elif self.current_state == self.STATE_PUNISHING:
            self.punishment_rounds_counter += 1
            if prop_cooperators_prev_round >= self.COOPERATION_THRESHOLD_RATIO:
                self.current_state = self.STATE_COOPERATING
                return Action.C
            elif self.punishment_rounds_counter >= self.PUNISHMENT_DURATION:
                self.current_state = self.STATE_FORGIVING
                return Action.C
            else:
                return Action.D
        elif self.current_state == self.STATE_FORGIVING:
            if prop_cooperators_prev_round >= self.COOPERATION_THRESHOLD_RATIO:
                self.current_state = self.STATE_COOPERATING
                return Action.C
            else:
                self.current_state = self.STATE_PUNISHING
                self.punishment_rounds_counter = 0
                return Action.D

class Strategy_COOPERATIVE_72(BaseStrategy):
    """
    Adaptive Tit-for-Tat (AT4T) Strategy for N-Player Public Goods Game.

    This strategy starts by cooperating and aims to maintain cooperation.
    It reciprocates the previous round's overall cooperation level with a bias
    towards cooperation. It only defects if there is persistent defection
    from the majority of players.

    Decision Rules:
    - Round 1: Always Cooperate (C).
    - Last Round (r): Always Defect (D).
    - Intermediate Rounds (2 to r-1):
        - Calculate the proportion of cooperators in the previous round.
        - If the proportion is high (>= 0.6), Cooperate (C).
        - If the proportion is low (< 0.3) and this has persisted for at least
          2 consecutive rounds, Defect (D).
        - Otherwise (intermediate proportion, or short-lived low proportion),
          Cooperate (C).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold_high = 0.6
        self.defect_threshold_low = 0.3
        self.min_defect_streak = 2
        self.streak_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if state.round_number == r:
            return Action.D
        last_round_idx = state.round_number - 1
        my_last_action = history.my_actions[last_round_idx]
        opponent_last_actions = history.opponent_actions[last_round_idx, :]
        all_previous_actions = np.append(opponent_last_actions, my_last_action)
        num_cooperators_prev = np.sum(all_previous_actions)
        proportion_cooperators_prev = num_cooperators_prev / n
        if proportion_cooperators_prev < self.defect_threshold_low:
            self.streak_defect += 1
        else:
            self.streak_defect = 0
        if proportion_cooperators_prev >= self.coop_threshold_high:
            return Action.C
        elif self.streak_defect >= self.min_defect_streak:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_73(BaseStrategy):
    """
    Forgiving Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with an element of forgiveness to avoid perpetual cycles of conflict.
    It maintains a dynamic Cooperation Threshold (CT) influenced by past observed behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_threshold = 1.0
        self.forgiveness_decrement = 0.05
        self.punishment_increment = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.cooperation_threshold = 1.0
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_index = current_round - 1
        previous_total_contributions = np.sum(history.my_actions[previous_round_index])
        for i in range(self.n - 1):
            previous_total_contributions += history.opponent_actions[previous_round_index, i]
        previous_cooperation_rate = previous_total_contributions / self.n
        new_cooperation_threshold = self.cooperation_threshold
        if previous_cooperation_rate == 0:
            new_cooperation_threshold = 1.0
        elif previous_cooperation_rate < self.cooperation_threshold:
            new_cooperation_threshold = self.cooperation_threshold - self.forgiveness_decrement
        elif previous_cooperation_rate < self.cooperation_threshold and previous_cooperation_rate <= (self.n - previous_total_contributions) / self.n:
            new_cooperation_threshold = self.cooperation_threshold + self.punishment_increment
        self.cooperation_threshold = max(0.0, min(1.0, new_cooperation_threshold))
        if previous_cooperation_rate >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_74(BaseStrategy):
    """
    Adaptive Reciprocity with Thresholding (ART) strategy for the N-Player Public Goods Game.
    ART aims to foster cooperation by rewarding it and punishing defection,
    while maintaining a strong incentive to cooperate as long as the collective
    benefit outweighs the individual cost. It adapts to the observed behavior
    of other players and is robust against various defection patterns.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ART strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.target_coop_rate = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.game_description.n_rounds:
            return Action.D
        previous_round_index = current_round - 1
        opponents_cooperated_last_round = np.sum(history.opponent_actions[previous_round_index, :])
        self_cooperated_last_round = history.my_actions[previous_round_index]
        total_cooperators_last_round = opponents_cooperated_last_round + self_cooperated_last_round
        observed_cooperation_rate = total_cooperators_last_round / self.game_description.n_players
        if observed_cooperation_rate >= self.target_coop_rate:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_75(BaseStrategy):
    """
    Tit-for-Tat with Adaptive Threshold (TFT-AT) strategy.

    This strategy aims to foster cooperation by starting cooperatively,
    reciprocating actions, and forgiving defection based on an adaptive threshold
    that responds to the overall group's cooperation level. It defects in the
    last round to maximize individual payoff in the end-game scenario.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the TFT-AT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.current_round_number = 0
        self.cooperation_threshold = self.k / self.n_players
        self.adjustment_factor = 0.05
        self.high_cooperation_cutoff = 0.75
        self.low_cooperation_cutoff = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: Current game state, including the round number.
            history: History of past actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        self.current_round_number = state.round_number
        if self.current_round_number == 0:
            self.cooperation_threshold = self.k / self.n_players
            return Action.C
        elif self.current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = self.current_round_number - 1
            all_prev_actions = np.append(history.my_actions[prev_round_idx], history.opponent_actions[prev_round_idx, :])
            total_cooperators_prev_round = np.sum(all_prev_actions)
            proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= self.high_cooperation_cutoff:
                self.cooperation_threshold = max(self.k / self.n_players, self.cooperation_threshold - self.adjustment_factor)
            elif proportion_cooperators_prev_round < self.low_cooperation_cutoff:
                self.cooperation_threshold = min(1.0, self.cooperation_threshold + self.adjustment_factor)
            if proportion_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_76(BaseStrategy):
    """
    Reciprocal Tit-for-Tat with Forgiveness (RTF) Strategy for N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by rewarding it and punishing
    defection, but with a mechanism for re-establishing cooperation after a period of
    sustained defection. It balances the desire for personal gain with the understanding
    that collective cooperation leads to higher overall payoffs.

    Decision Rules:
    1. Initial Cooperation (Round 1): Always Cooperate.
    2. Subsequent Rounds (t > 1):
       - If ALL other players cooperated in round t-1: Cooperate.
       - If AT LEAST ONE other player defected in round t-1:
         - Forgiveness Mechanism: If everyone defected in round t-2 AND at least one
           player defected in round t-1, then Cooperate in round t to break the cycle.
         - Otherwise (standard punishment): Defect.
    3. Last Round (r): Always Defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = state.round_number
        previous_round_idx = current_round_number - 1
        round_t_minus_2_idx = current_round_number - 2
        if current_round_number == self.r:
            return Action.D
        previous_round_actions_all = []
        if previous_round_idx < len(history.my_actions):
            previous_round_actions_all.append(history.my_actions[previous_round_idx])
        if previous_round_idx < history.opponent_actions.shape[0]:
            previous_round_actions_all.extend(history.opponent_actions[previous_round_idx, :])
        all_cooperated_previous = all(previous_round_actions_all)
        if all_cooperated_previous:
            return Action.C
        else:
            if current_round_number >= 2:
                round_t_minus_2_actions_all = []
                if round_t_minus_2_idx >= 0 and round_t_minus_2_idx < len(history.my_actions):
                    round_t_minus_2_actions_all.append(history.my_actions[round_t_minus_2_idx])
                if round_t_minus_2_idx >= 0 and round_t_minus_2_idx < history.opponent_actions.shape[0]:
                    round_t_minus_2_actions_all.extend(history.opponent_actions[round_t_minus_2_idx, :])
                all_defected_t_minus_2 = False
                if len(round_t_minus_2_actions_all) == self.n:
                    all_defected_t_minus_2 = all((not action for action in round_t_minus_2_actions_all))
                if all_defected_t_minus_2:
                    return Action.C
            return Action.D

class Strategy_COOPERATIVE_77(BaseStrategy):
    """
    Generous Tit-for-Tat (GTFT) Strategy for N-Player Public Goods Game.

    This strategy is inherently cooperative, starting with cooperation and
    returning to it quickly if the group's behavior improves. It retaliates
    against sustained low cooperation but includes a tolerance (epsilon)
    to be more forgiving. It also defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the GTFT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.epsilon = 0.05
        self.cooperation_threshold_base = 1.0 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players.
                     None for the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        all_actions_prev_round = []
        if previous_round_index < len(history.my_actions):
            all_actions_prev_round.append(history.my_actions[previous_round_index])
        if previous_round_index < history.opponent_actions.shape[0]:
            for i in range(n_players - 1):
                all_actions_prev_round.append(history.opponent_actions[previous_round_index, i])
        if not all_actions_prev_round:
            return Action.C
        num_cooperators_prev_round = sum(all_actions_prev_round)
        avg_c_prev_round = num_cooperators_prev_round / n_players
        cooperation_threshold = self.cooperation_threshold_base - self.epsilon
        if avg_c_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_78(BaseStrategy):
    """
    Tit-for-Tat with a Forgiveness Factor and Gradual Escalation (TFT-FFGE)
    This strategy aims to foster cooperation by rewarding cooperation and punishing
    defection, but with mechanisms for forgiveness and gradual escalation to prevent
    getting stuck in cycles of mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the TFT-FFGE strategy.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.forgiveness_factor = 0.3
        self.escalation_threshold = 5
        self.current_full_defection_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing the history of past actions and payoffs
                     for this player and opponents. None if it's the first round.

        Returns:
            Action: The action to take in the current round (Action.C or Action.D).
        """
        if state.round_number == self.r - 1:
            return Action.D
        if history is None:
            return Action.C
        num_completed_rounds = history.round_number
        last_round_idx = num_completed_rounds - 1
        my_last_action_bool = history.my_actions[last_round_idx]
        opponent_actions_last_round = history.opponent_actions[last_round_idx, :]
        all_actions_last_round = np.concatenate(([my_last_action_bool], opponent_actions_last_round))
        num_defectors_in_last_round = np.sum(~all_actions_last_round)
        num_cooperators_in_last_round = np.sum(all_actions_last_round)
        if num_defectors_in_last_round == self.n:
            self.current_full_defection_streak += 1
        else:
            self.current_full_defection_streak = 0
        num_other_players = self.n - 1
        num_defectors_among_others = np.sum(~opponent_actions_last_round)
        if num_defectors_among_others == 0:
            return Action.C
        elif num_defectors_among_others < num_other_players:
            cooperation_probability = 1 - num_defectors_among_others / num_other_players * self.forgiveness_factor
            if random.random() <= cooperation_probability:
                return Action.C
            else:
                return Action.D
        elif self.current_full_defection_streak >= self.escalation_threshold:
            return Action.D
        else:
            defect_probability = (self.current_full_defection_streak + 1) / self.escalation_threshold
            defect_probability = min(defect_probability, 1.0)
            if random.random() <= defect_probability:
                return Action.D
            else:
                return Action.C

class Strategy_COOPERATIVE_79(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Reciprocity with Punishment and Forgiveness"

    This strategy aims to maximize its own total payoff by encouraging mutual
    cooperation. It rewards cooperation and punishes defection, but includes
    mechanisms to encourage a return to cooperation after periods of defection
    and to handle the unique dynamics of the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.history = []
        self.defection_punishment_count = 0
        self.my_cooperation_rate_history = []

    def calculate_cooperation_threshold(self, current_round_number: int) -> float:
        """
        Calculates the dynamic cooperation threshold.
        It's influenced by the k/n ratio and the player's recent punishment count.
        """
        base_threshold = 0.5 + self.k / self.n * 0.3
        punishment_impact = self.defection_punishment_count / max(1, current_round_number - 1) * 0.1
        threshold = max(0.3, min(0.8, base_threshold - punishment_impact))
        return threshold

    def calculate_punishment_threshold(self) -> float:
        """
        Calculates the threshold below which immediate defection is triggered.
        It's influenced by the k/n ratio.
        """
        return max(0.4, 0.7 - self.k / self.n * 0.2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            self.history = []
            self.defection_punishment_count = 0
            self.my_cooperation_rate_history = []
            return Action.C
        total_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        my_action_in_prev_round_bool = history.my_actions[-1]
        self.history.append(total_cooperators_in_prev_round)
        self.my_cooperation_rate_history.append(my_action_in_prev_round_bool)
        if current_round_number == self.r:
            return Action.D
        rm_t = total_cooperators_in_prev_round / self.n
        cooperation_threshold = self.calculate_cooperation_threshold(current_round_number)
        punishment_threshold = self.calculate_punishment_threshold()
        if rm_t >= cooperation_threshold:
            self.defection_punishment_count = 0
            return Action.C
        elif rm_t < punishment_threshold:
            self.defection_punishment_count += 1
            return Action.D
        else:
            if len(self.history) >= 3:
                if self.history[-1] > self.history[-2] and self.history[-2] > self.history[-3]:
                    self.defection_punishment_count = 0
                    return Action.C
            if self.defection_punishment_count > 0 and rm_t >= cooperation_threshold * 0.9:
                self.defection_punishment_count = 0
                return Action.C
            self.defection_punishment_count += 1
            return Action.D

class Strategy_COOPERATIVE_80(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperation with Forgiveness"

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but it also includes a mechanism to return to cooperation if the environment
    becomes more favorable. It's based on mirroring the average behavior of the
    group in the previous round.

    - Starts with cooperation in the first round.
    - In subsequent rounds (1 < t < r), it cooperates if the average cooperation
      rate in the previous round was at least `alpha` (defaulting to 0.6).
    - Defects in the last round (t=r) due to the finite horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.alpha = 0.6
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: The history of actions and payoffs for all players.
                     None for the first round.

        Returns:
            Action.C for cooperation, Action.D for defection.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1])
        average_cooperation_rate = num_cooperators_prev_round / self.n
        if average_cooperation_rate >= self.alpha:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_81(BaseStrategy):
    """
    Cooperative Reciprocity Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding past cooperation and
    punishing past defection, while also being forgiving to encourage the
    re-establishment of cooperative norms.

    Core Principle: Reciprocal Altruism adapted for public goods.
    - Cooperate if the group generally cooperated in the previous round.
    - Defect if the group generally defected.

    Decision Rules:
    - First Round (t=1): Cooperate (C) to initiate cooperation.
    - Last Round (t=r): Defect (D) as it's a one-shot game.
    - Subsequent Rounds (1 < t < r):
        - Cooperate (C) if the proportion of cooperators in the previous round
          was >= 0.5.
        - Defect (D) if the proportion of cooperators in the previous round
          was < 0.5.

    The cooperation threshold of 0.5 is chosen for robustness and forgiveness,
    allowing the group to recover to cooperation if at least half cooperated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Cooperative Reciprocity strategy.

        Args:
            game_description: An object containing game parameters
                              (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: Current state of the game (e.g., current round number).
            history: History of past actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            The chosen action: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == n_rounds:
            return Action.D
        if history is None:
            return Action.C
        previous_round_cooperators = 0
        if history.my_actions.size > 0:
            if history.my_actions[current_round - 1]:
                previous_round_cooperators += 1
            previous_round_cooperators += np.sum(history.opponent_actions[current_round - 1, :])
        proportion_cooperators_prev_round = previous_round_cooperators / n_players
        if proportion_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_82(BaseStrategy):
    """
    Adaptive Grudger with Threshold Strategy for N-Player Public Goods Game.

    This strategy aims to maximize payoffs by encouraging sustained cooperation.
    It cooperates initially and continues to do so as long as others reciprocate.
    It punishes defections by defecting in response, especially if defections
    become widespread. It also has an adaptive element to reintroduce cooperation
    after a period of sustained defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Grudger with Threshold strategy.

        Args:
            game_description: Information about the game's parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.defection_threshold_T = 0.5
        self.consecutive_defection_punishment_M = 3
        self._consecutive_defections = 0

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: A record of past actions and payoffs for all players.
                     None if this is the first round.

        Returns:
            The player's action for the current round (Action.C or Action.D).
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            self._consecutive_defections = 0
            return Action.C
        num_defecting_others = 0
        if self.n > 1:
            num_defecting_others = np.sum(history.opponent_actions[current_round - 1, :])
        defection_rate_others = 0
        if self.n > 1:
            defection_rate_others = num_defecting_others / (self.n - 1)
        if self._consecutive_defections >= self.consecutive_defection_punishment_M:
            if num_defecting_others == 0:
                self._consecutive_defections = 0
                return Action.C
            else:
                self._consecutive_defections += 1
                return Action.D
        elif num_defecting_others == 0:
            self._consecutive_defections = 0
            return Action.C
        elif defection_rate_others >= self.defection_threshold_T:
            self._consecutive_defections += 1
            return Action.D
        else:
            self._consecutive_defections += 1
            return Action.D

class Strategy_COOPERATIVE_83(BaseStrategy):
    """
    Generous Reciprocity with a Punishment Threshold.

    This strategy starts by cooperating, rewards sustained cooperation by a majority,
    and punishes insufficient cooperation by defecting temporarily. It aims to
    re-establish cooperation if it dips below a threshold but not if cooperation
    completely breaks down (no cooperators).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.punishment_threshold = math.ceil(self.n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        previous_round_index = history.round_number - 1
        total_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :]) + (1 if history.my_actions[previous_round_index] == Action.C else 0)
        if total_cooperators_previous_round == 0:
            return Action.D
        elif total_cooperators_previous_round >= self.punishment_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_84(BaseStrategy):
    """
    Adaptive Conditional Cooperation (ACC) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation and reciprocates cooperation if at least one
    other player cooperated in the previous round. It defects only if all other
    players defected in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the strategy rules.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players up to the
                     previous round. If None, it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        if history is None:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[history.round_number - 1]
        other_cooperators_count = np.sum(previous_round_opponent_actions)
        if other_cooperators_count > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_85(BaseStrategy):
    """
    Adaptive Reciprocity (AR) Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    with an emphasis on forgiveness and rebuilding trust. It is history-dependent
    and dynamically adjusts a cooperation threshold based on the group's observed
    cooperation rate in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_round_number = 0
        self.coop_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round_number = state.round_number
        if history is None:
            return Action.C
        previous_round_idx = self.current_round_number - 1
        if previous_round_idx >= len(history.my_actions):
            pass
        num_cooperators_prev_round = 0
        if previous_round_idx < len(history.my_actions):
            if history.my_actions[previous_round_idx]:
                num_cooperators_prev_round += 1
        if previous_round_idx < len(history.opponent_actions):
            num_cooperators_prev_round += np.sum(history.opponent_actions[previous_round_idx, :])
        avg_coop_rate_prev_round = num_cooperators_prev_round / self.n
        decision_to_cooperate: bool = False
        if avg_coop_rate_prev_round >= self.coop_threshold:
            decision_to_cooperate = True
        else:
            decision_to_cooperate = False
        my_current_decision_is_cooperate = decision_to_cooperate
        if self.current_round_number < self.r:
            if my_current_decision_is_cooperate and avg_coop_rate_prev_round >= self.coop_threshold:
                self.coop_threshold = max(0.1, self.coop_threshold - 0.05)
            elif not my_current_decision_is_cooperate and avg_coop_rate_prev_round < self.coop_threshold:
                self.coop_threshold = min(0.9, self.coop_threshold + 0.1)
        return Action.C if my_current_decision_is_cooperate else Action.D

class Strategy_COOPERATIVE_86(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Conditional Cooperation.

    This strategy starts by cooperating. In subsequent rounds, it cooperates if
    everyone cooperated in the previous round. Otherwise, it calculates the
    average cooperation rate from the previous round. If this rate is above a
    defined threshold (0.5), it cooperates; otherwise, it defects.
    This approach is cooperative, aims to punish free-riding, and allows for
    re-engagement in cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for this player and
                     opponents. None if it's the first round.

        Returns:
            Action.C for cooperation, Action.D for defection.
        """
        if history is None:
            return Action.C
        previous_round_idx = history.round_number - 1
        all_actions_previous_round = np.append([history.my_actions[previous_round_idx]], history.opponent_actions[previous_round_idx, :])
        num_cooperators_previous = np.sum(all_actions_previous_round)
        num_players = self.game_description.n_players
        if num_cooperators_previous == num_players:
            return Action.C
        else:
            average_cooperation_rate_previous = num_cooperators_previous / num_players
            if average_cooperation_rate_previous >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_87(BaseStrategy):
    """
    Adaptive Reciprocity with a Punishment Threshold strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a forgiving element to avoid perpetual conflict. It monitors the proportion
    of cooperators and reacts to trends, aiming to recover cooperation when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_threshold: float = self.game_description.k / self.game_description.n_players
        self.history_of_cooperators_proportion: list[float] = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = state.round_number
        previous_round_cooperators_count = np.sum(history.opponent_actions[-1, :])
        previous_round_cooperators_proportion = previous_round_cooperators_count / self.game_description.n_players
        self.history_of_cooperators_proportion.append(previous_round_cooperators_proportion)
        recent_trend = 'stable'
        num_past_rounds_for_trend = min(3, len(self.history_of_cooperators_proportion) - 1)
        if num_past_rounds_for_trend >= 2:
            recent_coop_proportions = self.history_of_cooperators_proportion[-num_past_rounds_for_trend - 1:-1]
            if all((recent_coop_proportions[i] > recent_coop_proportions[i + 1] for i in range(len(recent_coop_proportions) - 1))):
                recent_trend = 'decreasing'
            elif all((recent_coop_proportions[i] <= recent_coop_proportions[i + 1] for i in range(len(recent_coop_proportions) - 1))):
                recent_trend = 'increasing_or_stable'
        elif num_past_rounds_for_trend == 1:
            if self.history_of_cooperators_proportion[-2] > self.history_of_cooperators_proportion[-1]:
                recent_trend = 'decreasing'
            elif self.history_of_cooperators_proportion[-2] <= self.history_of_cooperators_proportion[-1]:
                recent_trend = 'increasing_or_stable'
        if current_round_number == self.game_description.n_rounds - 1:
            if previous_round_cooperators_proportion >= self.punishment_threshold:
                return Action.C
            else:
                return Action.D
        elif previous_round_cooperators_proportion >= self.punishment_threshold:
            return Action.C
        elif previous_round_cooperators_proportion < self.punishment_threshold:
            if recent_trend == 'increasing_or_stable' and previous_round_cooperators_proportion > 0:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_88(BaseStrategy):
    """
    Forgiving Tit-for-Tat (FTFT) strategy for the N-Player Public Goods Game.

    This strategy aims for conditional cooperation with a punitive element and
    a recovery mechanism. It starts by cooperating, then cooperates if the
    average cooperation rate of others in the previous round was above a
    dynamically adjusted threshold. It defects if the average drops below
    this threshold. A forgiveness factor (delta) prevents immediate defection,
    and a counter for consecutive defections can trigger more aggressive
    punishment if cooperation doesn't resume. The last round is always a defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.delta = 0.1
        self.M = 3
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_other_players = n - 1
        total_other_cooperators = np.sum(previous_round_opponent_actions)
        avg_other_cooperation = total_other_cooperators / num_other_players
        target_cooperation = max(0.0, avg_other_cooperation - self.delta)
        if avg_other_cooperation < target_cooperation:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.consecutive_defections > self.M:
            return Action.D
        elif avg_other_cooperation >= target_cooperation:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_89(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively,
    mirroring group behavior, and adaptively adjusting its cooperation threshold
    based on sustained patterns of group cooperation or defection. It includes
    a forgiveness mechanism to re-establish cooperation after periods of
    widespread defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_threshold = 0.6
        self.min_threshold = 0.4
        self.max_threshold = 0.9
        self.threshold_step = 0.05
        self.threshold_adjustment_window = 3
        self.current_threshold = self.initial_threshold
        self.consecutive_failures = 0
        self.consecutive_successes = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.consecutive_successes = 0
            self.consecutive_failures = 0
            self.current_threshold = self.initial_threshold
            return Action.C
        current_round_idx = history.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[current_round_idx, :])
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.game_description.n_players
        if proportion_cooperators_prev_round >= self.current_threshold:
            action = Action.C
            self.consecutive_successes += 1
            self.consecutive_failures = 0
        else:
            action = Action.D
            self.consecutive_failures += 1
            self.consecutive_successes = 0
        if self.consecutive_successes >= self.threshold_adjustment_window and self.current_threshold < self.max_threshold:
            self.current_threshold = min(self.current_threshold + self.threshold_step, self.max_threshold)
        elif self.consecutive_failures >= self.threshold_adjustment_window and self.current_threshold > self.min_threshold:
            self.current_threshold = max(self.current_threshold - self.threshold_step, self.min_threshold)
        return action

class Strategy_COOPERATIVE_90(BaseStrategy):
    """
    The "Tit-for-Tat Plus" strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by being initially cooperative,
    reciprocating others' actions, and incorporating a "Forgiving Reciprocity"
    mechanism to recover from periods of defection. It also strategically defects
    in the final round due to the end-game problem.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperators = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_actions_all_players = self.get_previous_round_actions(history)
        num_cooperators_prev_round = sum(previous_round_actions_all_players)
        num_defectors_prev_round = self.n - num_cooperators_prev_round
        if num_defectors_prev_round == 0:
            return Action.C
        else:
            my_last_action_was_defect = not history.my_actions[current_round - 1]
            if my_last_action_was_defect and current_round > 1:
                if current_round > 2:
                    round_t_minus_2_actions_all_players = self.get_round_t_minus_2_actions(history)
                    num_cooperators_t_minus_2 = sum(round_t_minus_2_actions_all_players)
                    if num_cooperators_prev_round > num_cooperators_t_minus_2 + self.threshold_cooperators:
                        return Action.C
            return Action.D

    def get_previous_round_actions(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """
        Helper to get actions of all players (self + opponents) in the previous round.
        Returns a numpy array where True is Cooperate, False is Defect.
        """
        if history.round_number == 0:
            return np.array([], dtype=bool)
        my_action = history.my_actions[history.round_number - 1]
        opponent_actions_prev = history.opponent_actions[history.round_number - 1, :]
        all_actions = np.insert(opponent_actions_prev, 0, my_action)
        return all_actions

    def get_round_t_minus_2_actions(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """
        Helper to get actions of all players in the round t-2.
        Returns a numpy array where True is Cooperate, False is Defect.
        """
        round_index = history.round_number - 2
        my_action = history.my_actions[round_index]
        opponent_actions_prev = history.opponent_actions[round_index, :]
        all_actions = np.insert(opponent_actions_prev, 0, my_action)
        return all_actions

class Strategy_COOPERATIVE_91(BaseStrategy):
    """
    Adaptive Reciprocity with Gradual Escalation for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection.
    It is adaptive, learning from past behavior and gradually escalating its
    response to sustained defection. It prioritizes collective welfare while
    protecting against exploitation.

    Decision Logic for round t+1 based on round t:
    - If proportion of defectors in round t is HIGH (> DEFECTION_THRESHOLD): Defect in t+1.
    - If proportion of defectors in round t is LOW (< COOPERATION_THRESHOLD): Cooperate in t+1.
    - If proportion of defectors is MODERATE:
        - If average payoff in round t was BELOW target full cooperation payoff (k): Defect in t+1.
        - Else (average payoff >= k): Cooperate in t+1.

    Special Cases:
    - First Round (t=0): Always Cooperate (C).
    - Last Round (t=r-1): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_THRESHOLD = 0.25
        self.DEFECTION_THRESHOLD = 0.5
        self.all_player_actions_history = []
        self.average_payoffs_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        if history is None:
            return Action.C
        previous_round_idx = current_round_number - 1
        my_prev_action = history.my_actions[previous_round_idx]
        opponent_prev_actions = history.opponent_actions[previous_round_idx, :]
        if not self.all_player_actions_history:
            if len(history.my_actions) > 0:
                self.all_player_actions_history.append(np.concatenate(([history.my_actions[previous_round_idx]], history.opponent_actions[previous_round_idx, :])))
            else:
                return Action.C
        else:
            self.all_player_actions_history.append(np.concatenate(([my_prev_action], opponent_prev_actions)))
        if len(self.all_player_actions_history) == 0:
            return Action.C
        previous_round_all_actions = self.all_player_actions_history[-1]
        num_defectors_t = np.sum(~previous_round_all_actions)
        proportion_defectors_t = num_defectors_t / self.n
        if len(history.my_payoffs) > 0:
            average_payoff_t = np.mean(history.my_payoffs[:current_round_number])
            if len(history.my_payoffs) >= current_round_number:
                average_payoff_t = history.my_payoffs[previous_round_idx]
            else:
                average_payoff_t = float('inf')
        else:
            average_payoff_t = float('inf')
        target_coop_payoff = self.k
        if proportion_defectors_t >= self.DEFECTION_THRESHOLD:
            return Action.D
        elif proportion_defectors_t < self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            all_payoffs_prev_round = np.concatenate(([history.my_payoffs[previous_round_idx]], history.opponent_payoffs[previous_round_idx, :]))
            average_group_payoff_t = np.mean(all_payoffs_prev_round)
            if average_group_payoff_t < target_coop_payoff:
                return Action.D
            else:
                return Action.C

class Strategy_COOPERATIVE_92(BaseStrategy):
    """
    Generous Tit-for-Tat with Forgiveness for N-Player Public Goods Game.

    This strategy starts cooperatively and reciprocates the average behavior of
    opponents, but forgives occasional lapses to encourage a return to
    cooperation. It uses a cooperation threshold derived from game parameters
    and a fixed forgiveness level.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players,
                              n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.k / self.game_description.n_players
        self.forgiveness_level = 1
        self._consecutive_defections_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round.

        Args:
            state: Current game state, including round number.
            history: Historical data of actions and payoffs for all players.
                     None for the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        if history is None:
            self._consecutive_defections_counter = 0
            return Action.C
        previous_round_index = state.round_number - 1
        if previous_round_index < 0 or previous_round_index >= len(history.opponent_actions):
            return Action.C
        opponent_actions_last_round = history.opponent_actions[previous_round_index, :]
        opponent_cooperators = np.sum(opponent_actions_last_round)
        n_opponents = self.game_description.n_players - 1
        opponent_cooperation_rate = opponent_cooperators / n_opponents if n_opponents > 0 else 1.0
        if opponent_cooperation_rate >= self.cooperation_threshold:
            self._consecutive_defections_counter = 0
            return Action.C
        else:
            self._consecutive_defections_counter += 1
            if self._consecutive_defections_counter <= self.forgiveness_level:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_93(BaseStrategy):
    """
    Tit-for-Tat with Enhanced Forgiveness and Conditional Cooperation for N-Player Public Goods Game.

    This strategy aims to promote cooperation by rewarding it and punishing defection,
    but with a built-in mechanism to recover from periods of low cooperation.
    It cooperates in the first round, then cooperates if the previous round's average
    contribution was sufficiently high, or if it was slightly below the threshold
    but a long streak of cooperation has been observed. Otherwise, it defects.
    It also cooperates in the last round regardless of history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(1, math.ceil(self.n / self.k))
        self.forgiveness_window = 2
        self.min_cooperation_boost = 1
        self.cooperation_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.C
        prev_round_my_action = int(history.my_actions[-1])
        prev_round_opponent_actions = history.opponent_actions[-1].astype(int)
        prev_cooperators = prev_round_my_action + np.sum(prev_round_opponent_actions)
        if prev_cooperators >= self.cooperation_threshold:
            self.cooperation_streak += 1
        else:
            self.cooperation_streak = 0
        if prev_cooperators >= self.cooperation_threshold:
            return Action.C
        elif prev_cooperators >= self.cooperation_threshold - self.min_cooperation_boost and self.cooperation_streak >= self.forgiveness_window:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_94(BaseStrategy):
    """
    "Conditional Cooperation with Forgiveness" strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a crucial element of forgiveness to allow for the re-establishment of
    cooperation after periods of defection. It is designed to be resilient to a
    variety of opponent behaviors without assuming any specific coordination mechanisms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_defections_by_others = 0
        self.punishment_rounds_remaining = 0
        self.forgiveness_level = 0.0
        self.PUNISHMENT_DURATION = max(2, math.ceil(self.r * 0.1))
        self.COOPERATION_TOLERANCE = 0.5
        self.FORGIVENESS_RATE = 0.1
        self.INITIAL_COOPERATION_BIAS = 0.8
        self.PUNISHMENT_TRIGGER_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C if random.random() < self.INITIAL_COOPERATION_BIAS else Action.D
        current_round_idx = history.round_number - 1
        total_other_players = self.n - 1
        observed_cooperation_rate = 0.0
        if total_other_players > 0:
            last_round_opponent_actions = history.opponent_actions[current_round_idx, :]
            num_others_cooperated = np.sum(last_round_opponent_actions)
            observed_cooperation_rate = num_others_cooperated / total_other_players
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            self.consecutive_defections_by_others = 0
            self.forgiveness_level = 0.0
            return Action.D
        if observed_cooperation_rate < self.COOPERATION_TOLERANCE:
            self.consecutive_defections_by_others += 1
            if self.consecutive_defections_by_others >= self.PUNISHMENT_TRIGGER_ROUNDS:
                self.punishment_rounds_remaining = self.PUNISHMENT_DURATION
                self.consecutive_defections_by_others = 0
                self.forgiveness_level = 0.0
                return Action.D
        else:
            self.consecutive_defections_by_others = 0
        if self.consecutive_defections_by_others == 0:
            self.forgiveness_level = min(1.0, self.forgiveness_level + self.FORGIVENESS_RATE)
        else:
            self.forgiveness_level = 0.0
        cooperation_probability = self.INITIAL_COOPERATION_BIAS
        deviation_factor = (1 - self.forgiveness_level) * (1 - observed_cooperation_rate)
        if self.consecutive_defections_by_others == 0 and observed_cooperation_rate >= self.COOPERATION_TOLERANCE:
            cooperation_probability = self.INITIAL_COOPERATION_BIAS + (1 - self.INITIAL_COOPERATION_BIAS) * (self.forgiveness_level * observed_cooperation_rate)
        else:
            cooperation_probability = self.INITIAL_COOPERATION_BIAS * (1 - deviation_factor * (1 - self.forgiveness_level))
        cooperation_probability = max(0.0, min(1.0, cooperation_probability))
        if random.random() < cooperation_probability:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_95(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Group Threshold strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and punishing defection,
    but with a modification to address the N-player dynamic. It cooperates if the
    proportion of cooperators in the previous round meets a calculated threshold (1/k),
    otherwise it defects. It initiates cooperation in the first round and defects
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_idx = state.round_number
        if current_round_idx == self.r - 1:
            return Action.D
        if current_round_idx > 0:
            previous_round_idx = current_round_idx - 1
            my_previous_action = self.my_actions_to_bool(history.my_actions[previous_round_idx])
            opponents_previous_actions = history.opponent_actions[previous_round_idx, :]
            cooperators_from_opponents = np.sum(opponents_previous_actions)
            total_cooperators_in_previous_round = cooperators_from_opponents + my_previous_action
            proportion_cooperators = total_cooperators_in_previous_round / self.n
            if proportion_cooperators >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

    def my_actions_to_bool(self, action: bool) -> int:
        """Converts the boolean action representation to an integer (1 for C, 0 for D)."""
        return int(action)

class Strategy_COOPERATIVE_96(BaseStrategy):
    """
    Adaptive Reciprocity Strategy:
    Starts by cooperating, then maintains cooperation as long as the prevailing
    cooperation rate is high. If cooperation drops significantly, it defects
    to punish. It aims to return to cooperation once the group behavior
    improves, making it adaptive and robust.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop = 0.8
        self.T_punish = 0.6
        self.my_player_index = -1
        self.dynamic_T_coop_factor = 1.0
        self.dynamic_T_punish_factor = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if self.my_player_index == -1:
            pass
        if history is None:
            return Action.C
        rounds_remaining = r - current_round
        if r > 1:
            self.dynamic_T_coop_factor = max(0.5, rounds_remaining / r)
            self.dynamic_T_punish_factor = max(0.4, rounds_remaining / r)
        effective_T_coop = self.T_coop * self.dynamic_T_coop_factor
        effective_T_punish = self.T_punish * self.dynamic_T_punish_factor
        if current_round == r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        my_action_prev_round = Action.C if history.my_actions[current_round - 1] else Action.D
        num_cooperators_prev_round = np.sum(previous_round_actions) + (1 if my_action_prev_round == Action.C else 0)
        cooperation_rate_prev_round = num_cooperators_prev_round / n
        if cooperation_rate_prev_round >= effective_T_coop:
            return Action.C
        if cooperation_rate_prev_round < effective_T_punish:
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_97(BaseStrategy):
    """
    Adaptive Punisher Strategy:
    This strategy starts by cooperating, punishes defection by defecting,
    and gradually forgives if cooperation is re-established. It uses a
    'PunishmentLevel' state to track the intensity and duration of punishment,
    resetting it when all other players cooperate again. It defects in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            self.punishment_level = 0
            return Action.C
        prev_round_index = current_round - 1
        num_cooperators_others = 0
        num_defectors_others = 0
        if history.opponent_actions.shape[0] > 0:
            last_round_opponent_actions = history.opponent_actions[prev_round_index, :]
            num_cooperators_others = np.sum(last_round_opponent_actions)
            num_defectors_others = self.n - 1 - num_cooperators_others
        else:
            pass
        if num_defectors_others == 0:
            if self.punishment_level > 0:
                self.punishment_level = 0
            return Action.C
        elif self.punishment_level == 0:
            self.punishment_level = 1
            return Action.D
        else:
            self.punishment_level += 1
            return Action.D

class Strategy_COOPERATIVE_98(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Punishment and Forgiveness (ATFP) for N-Player Public Goods Game.
    This strategy aims to foster cooperation by rewarding it, punishing defection,
    and showing a willingness to return to cooperation after a period of punishment.
    It's built upon the core idea of "reciprocity" but with added mechanisms to
    handle group dynamics and maintain long-term cooperative potential.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_duration = 0
        self.punishment_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        if self.punishment_duration > 0:
            self.punishment_duration -= 1
            return Action.D
        if history.opponent_actions.shape[0] == 0:
            return Action.C
        my_last_action_bool = history.my_actions[current_round - 1]
        opponent_last_actions_bool = history.opponent_actions[current_round - 1]
        total_cooperators_prev = np.sum(opponent_last_actions_bool) + my_last_action_bool
        group_cooperation_rate_prev = total_cooperators_prev / self.n
        if group_cooperation_rate_prev == 1.0:
            return Action.C
        elif group_cooperation_rate_prev > self.k / self.n:
            return Action.C
        elif not my_last_action_bool:
            return Action.D
        else:
            defecting_players_prev = self.n - total_cooperators_prev
            self.punishment_level = max(1, defecting_players_prev)
            self.punishment_duration = self.punishment_level
            return Action.D

class Strategy_COOPERATIVE_99(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Conditional Punishment.
    This strategy defaults to cooperation, but defects if the proportion of defectors
    in the previous round meets or exceeds a calculated threshold (k/n).
    It also defects in the last round and cooperates in the first.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        previous_round_index = history.round_number - 1
        my_action_prev_round = history.my_actions[previous_round_index]
        opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
        all_actions_prev_round = np.append(my_action_prev_round, opponent_actions_prev_round)
        number_of_defectors_prev_round = np.sum(all_actions_prev_round == False)
        proportion_defectors_prev_round = number_of_defectors_prev_round / self.n
        if proportion_defectors_prev_round >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_100(BaseStrategy):
    """
    Grim Trigger with a Tit-for-Tat Twist strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by punishing defection severely
    (Grim Trigger) but includes a mechanism to recover from lapses if a
    significant portion of players still cooperate (Tit-for-Tat Twist).

    Decision Rules:
    - Round 1: Cooperate (C).
    - Last Round: Defect (D).
    - Subsequent Rounds (1 < t < r):
        - If all players cooperated in the previous round: Cooperate (C).
        - If at least one player defected in the previous round:
            - If the number of cooperators in the previous round >= k: Defect (D).
            - If 0 < number of cooperators < k: Cooperate (C) (Reconciliation Window).
            - If 0 cooperators: Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        all_previous_actions = np.append(previous_round_actions, history.my_actions[-1])
        cooperators_in_previous_round = np.sum(all_previous_actions)
        if cooperators_in_previous_round == self.n:
            return Action.C
        elif cooperators_in_previous_round >= self.k:
            return Action.D
        elif cooperators_in_previous_round > 0 and cooperators_in_previous_round < self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_101(BaseStrategy):
    """
    Conditional Cooperation with Forgiveness (CCF) Strategy.

    This strategy aims to maximize long-term cooperation by rewarding cooperative
    behavior and punishing defection, but also allowing for a return to
    cooperation after a period of sustained defection. It is adaptive and
    robust by learning from the history of play.

    Decision Rules:
    - Cooperate (C) if: The proportion of players who cooperated in the
      previous round (t-1) is greater than or equal to `cooperation_threshold`,
      OR if the number of `consecutive_defects` is less than or equal to
      `forgiveness_period`.
    - Defect (D) if: The proportion of players who cooperated in the previous
      round (t-1) is less than the `cooperation_threshold`, AND the number
      of `consecutive_defects` is greater than `forgiveness_period`.

    Initialization:
    - Starts by cooperating in the first round.
    - `consecutive_defects` is initialized to 0.

    Adaptability:
    - `cooperation_threshold`: Set to 0.5, requiring at least half the group
      to cooperate to maintain cooperation.
    - `forgiveness_period`: Set to 2, allowing for two rounds of below-threshold
      cooperation before initiating defection. The `consecutive_defects` counter
      is incremented when the threshold is not met and reset otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the CCF strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.forgiveness_period = 2
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: Current game state, including the round number.
            history: History of the game up to the previous round.
                     `None` if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        if history is None:
            self.consecutive_defects = 0
            return Action.C
        n = self.game_description.n_players
        previous_round_idx = state.round_number - 1
        if previous_round_idx < 0 or n == 0:
            return Action.C
        if previous_round_idx >= history.opponent_actions.shape[0]:
            return Action.C
        actions_previous_round = history.opponent_actions[previous_round_idx, :]
        total_cooperators_previous_round = np.sum(actions_previous_round)
        proportion_cooperators_previous_round = total_cooperators_previous_round / n
        if proportion_cooperators_previous_round < self.cooperation_threshold:
            self.consecutive_defects += 1
        else:
            self.consecutive_defects = 0
        if proportion_cooperators_previous_round >= self.cooperation_threshold or self.consecutive_defects <= self.forgiveness_period:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_102(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness:
    Cooperates if the group is cooperating, defects if the group is defecting.
    Forgives isolated defections, but punishes persistent widespread defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.min_cooperators = math.floor(self.n / 2)
        self.defection_tolerance = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_cooperated_prev_round = history.my_actions[prev_round_idx]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_cooperated_prev_round else 0)
        num_defectors_prev_round = self.n - num_cooperators_prev_round
        if num_cooperators_prev_round >= self.min_cooperators:
            return Action.C
        elif num_defectors_prev_round <= self.defection_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_103(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Threshold Cooperation (AT4T-TC) for N-Player Public Goods Game.

    This strategy is cooperative by default, but not naive. It aims to
    maintain cooperation if the group collectively contributes enough,
    punishes severe defection, and uses past actions to inform future decisions.

    Parameters:
    - cooperation_threshold (float): The minimum proportion of cooperators
                                     required in the previous round to continue
                                     cooperating.
    - min_viable_cooperation (float): A lower threshold. If cooperation falls
                                      below this, defecting is preferred.
                                      This acts as a safety net.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0.6
        self.min_viable_cooperation = 0.3
        self.my_action_in_previous_round = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            self.my_action_in_previous_round = Action.C
            return Action.C
        if current_round_number == self.n_rounds:
            return Action.D
        my_previous_action_bool = self.my_action_in_previous_round == Action.C
        previous_round_idx = current_round_number - 2
        cooperators_in_previous_round = 0
        if self.my_action_in_previous_round == Action.C:
            cooperators_in_previous_round += 1
        if len(history.opponent_actions) > 0:
            cooperators_in_previous_round += np.sum(history.opponent_actions[-1, :])
        proportion_cooperators = cooperators_in_previous_round / self.n_players
        decision = Action.C
        if proportion_cooperators >= self.cooperation_threshold:
            decision = Action.C
        elif proportion_cooperators >= self.min_viable_cooperation:
            if self.my_action_in_previous_round == Action.C:
                decision = Action.C
            else:
                decision = Action.D
        else:
            decision = Action.D
        self.my_action_in_previous_round = decision
        return decision

class Strategy_COOPERATIVE_104(BaseStrategy):
    """
    Cooperative Strategy: "Tit-for-Tat with Punishment and Forgiveness"

    This strategy aims to foster cooperation by rewarding it, punishing defection,
    and offering opportunities for redemption. It's designed to be robust to a
    variety of player behaviors, including those who might exploit or be initially
    hesitant to cooperate.

    Decision Rules:
    - Cooperate (C) if:
        - In the previous round, the majority of players cooperated.
        - OR, if this is the first round.
        - OR, if we were in a "Punishment Phase" and the number of cooperators
          in the *previous* round has reached a certain threshold (indicating a
          willingness to return to cooperation).
    - Defect (D) if:
        - In the previous round, the majority of players defected.
        - OR, if we are currently in a "Punishment Phase" and the number of
          cooperators in the *previous* round has not met the forgiveness threshold.

    Edge Cases:
    - First Round (t=0): Cooperate (C). Rationale: Establish an initial signal of cooperativeness.
    - Last Round (t=r-1): Defect (D). Rationale: No future interactions, maximize immediate gain.

    State Variables:
    - `punishment_phase`: Boolean indicating if the strategy is currently in a punishment phase.
    - `punishment_rounds_remaining`: Integer tracking how many more rounds of defection are planned in the punishment phase.
    - `punishment_duration`: Integer representing the fixed number of rounds to punish defection.
    - `forgiveness_threshold_ratio`: Float representing the minimum fraction of cooperators required to end the punishment phase.
    - `cooperation_threshold_ratio`: Float representing the minimum fraction of cooperators required to resume cooperation after punishment.
    - `majority_threshold_ratio`: Float representing the minimum fraction of cooperators to consider the majority as cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.punishment_phase = False
        self.punishment_rounds_remaining = 0
        self.punishment_duration = 2
        self.forgiveness_threshold_ratio = 0.5
        self.cooperation_threshold_ratio = 0.6
        self.majority_threshold_ratio = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n = self.n_players
        r = self.n_rounds
        if current_round_number == r - 1:
            return Action.D
        if history is None:
            self.punishment_phase = False
            self.punishment_rounds_remaining = 0
            return Action.C
        if not history.my_actions.size:
            return Action.C
        total_cooperators_last_round = 0
        if history.my_actions.size > 0:
            total_cooperators_last_round += history.my_actions[-1]
        if history.opponent_actions.shape[0] > 0:
            total_cooperators_last_round += np.sum(history.opponent_actions[-1, :])
        if self.punishment_phase:
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                if total_cooperators_last_round / n >= self.forgiveness_threshold_ratio:
                    self.punishment_phase = False
                    self.punishment_rounds_remaining = 0
                    if total_cooperators_last_round / n >= self.cooperation_threshold_ratio:
                        return Action.C
                    else:
                        return Action.D
                else:
                    return Action.D
            else:
                self.punishment_phase = False
                if total_cooperators_last_round / n >= self.cooperation_threshold_ratio:
                    return Action.C
                else:
                    return Action.D
        majority_cooperated = total_cooperators_last_round / n >= self.majority_threshold_ratio
        if majority_cooperated:
            return Action.C
        else:
            self.punishment_phase = True
            self.punishment_rounds_remaining = self.punishment_duration
            return Action.D

class Strategy_COOPERATIVE_105(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) for N-Player Public Goods Game.
    Starts by cooperating and then mirrors the average cooperation rate of the previous round,
    defecting if the average cooperation rate falls at or below the threshold k/n.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.coop_threshold = self.k / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on ATFT strategy.

        Args:
            state: Current game state, including round_number.
            history: PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        if history is None:
            return Action.C
        else:
            if state.round_number == 0:
                return Action.C
            previous_round_index = state.round_number - 1
            if previous_round_index >= history.opponent_actions.shape[0]:
                return Action.C
            actions_in_prev_round = history.opponent_actions[previous_round_index, :]
            total_cooperators_in_prev_round = np.sum(actions_in_prev_round)
            num_players_in_prev_round = self.n_players
            if num_players_in_prev_round == 0:
                return Action.C
            avg_coop_rate_prev_round = total_cooperators_in_prev_round / num_players_in_prev_round
            if avg_coop_rate_prev_round > self.coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_106(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) Strategy for N-Player Public Goods Game.
    This strategy aims to be cooperative by default, rewarding cooperation and punishing defection.
    It adapts its trust threshold based on the average cooperation rate of the group and
    maintains a count of consecutive defections to enforce a punishment phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.trust_parameter = 0.8
        self.consecutive_defections = 0
        self.last_avg_cooperation = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = history.round_number
        if current_round_number == 0:
            return Action.C
        prev_round_idx = current_round_number - 1
        my_action_prev_round = int(history.my_actions[prev_round_idx])
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev = my_action_prev_round + opponent_cooperators_prev_round
        current_avg_cooperation = num_cooperators_prev / self.n_players
        if current_avg_cooperation >= 0.9:
            self.trust_parameter = min(1.0, self.trust_parameter + 0.05)
        elif current_avg_cooperation <= 0.2:
            self.trust_parameter = max(0.2, self.trust_parameter - 0.1)
        action_to_take = Action.C
        if self.consecutive_defections > 0:
            if current_avg_cooperation >= self.trust_parameter:
                action_to_take = Action.C
                self.consecutive_defections = 0
            else:
                action_to_take = Action.D
                self.consecutive_defections += 1
        elif current_avg_cooperation >= self.trust_parameter:
            action_to_take = Action.C
        else:
            action_to_take = Action.D
            self.consecutive_defections = 1
        self.last_avg_cooperation = current_avg_cooperation
        return action_to_take

class Strategy_COOPERATIVE_107(BaseStrategy):
    """
    Adaptive Reciprocity with a Trust Threshold strategy.

    This strategy aims to foster cooperation by cooperating initially and
    continuing to cooperate as long as the proportion of cooperators in
    the previous round meets or exceeds a defined threshold (n/k).
    If the proportion falls below this threshold, the strategy defects.
    It readily returns to cooperation if the group's behavior recovers.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.cooperation_threshold = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        previous_round_my_action = Action.C if self.my_actions_history[-1] else Action.D
        total_cooperators_previous_round = 0
        if previous_round_my_action == Action.C:
            total_cooperators_previous_round += 1
        if self.n > 1:
            total_cooperators_previous_round += np.sum(self.opponent_actions_history[-1, :])
        proportion_cooperators_previous_round = total_cooperators_previous_round / self.n
        if proportion_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

    @property
    def my_actions_history(self) -> NDArray[np.bool_]:
        """This player's actions, indexed [round]."""
        if self._history is None:
            return np.array([], dtype=np.bool_)
        return self._history.my_actions

    @property
    def opponent_actions_history(self) -> NDArray[np.bool_]:
        """Opponents' actions, indexed [round, player]."""
        if self._history is None:
            return np.empty((0, self.n - 1), dtype=np.bool_) if self.n > 1 else np.empty((0, 0), dtype=np.bool_)
        return self._history.opponent_actions

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self._history = history
        if history is None or history.round_number == 0:
            return Action.C
        previous_round_idx = history.round_number - 1
        total_cooperators_previous_round = 0
        if self.my_actions_history[previous_round_idx]:
            total_cooperators_previous_round += 1
        if self.n > 1:
            total_cooperators_previous_round += np.sum(self.opponent_actions_history[previous_round_idx, :])
        proportion_cooperators_previous_round = total_cooperators_previous_round / self.n
        if proportion_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_108(BaseStrategy):
    """
    Adaptive Cooperation with Punitive Forgiveness (ACPF) Strategy.
    This strategy aims to foster cooperation by rewarding it and punishing defection,
    while retaining the capacity to forgive and return to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r_start_punish = min(5, math.floor(self.game_description.n_rounds / 2))
        self.observation_window = 3
        self.cooperation_threshold_ratio = 0.75
        self.punishment_duration = 3
        self.forgiveness_increase_ratio = 0.25
        self.consecutive_punishments = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == r:
            return Action.D
        if history is None or state.round_number < self.r_start_punish:
            return Action.C
        my_last_action = history.my_actions[state.round_number - 1]
        my_last_payoff = history.my_payoffs[state.round_number - 1]
        num_cooperators_last_round = np.sum(history.my_actions[:state.round_number]) + np.sum(history.opponent_actions[state.round_number - 1, :])
        recent_cooperators_sum = 0
        num_recent_rounds = min(self.observation_window, state.round_number)
        if num_recent_rounds > 0:
            if state.round_number - num_recent_rounds >= 0:
                total_cooperators_recent_window = np.sum(history.my_actions[state.round_number - num_recent_rounds:state.round_number]) + np.sum(history.opponent_actions[state.round_number - num_recent_rounds:state.round_number, :])
                avg_cooperators = total_cooperators_recent_window / num_recent_rounds
            else:
                total_cooperators_all_past = np.sum(history.my_actions[:state.round_number]) + np.sum(history.opponent_actions[:state.round_number, :])
                avg_cooperators = total_cooperators_all_past / state.round_number if state.round_number > 0 else 0
        else:
            avg_cooperators = 0
        if self.consecutive_punishments > 0:
            if state.round_number > self.r_start_punish + self.punishment_duration and state.round_number - (self.r_start_punish + self.punishment_duration) >= self.observation_window:
                if state.round_number >= 2 * self.observation_window:
                    total_cooperators_current_window = np.sum(history.my_actions[state.round_number - self.observation_window:state.round_number]) + np.sum(history.opponent_actions[state.round_number - self.observation_window:state.round_number, :])
                    avg_cooperators_now = total_cooperators_current_window / self.observation_window
                    total_cooperators_prev_window = np.sum(history.my_actions[state.round_number - 2 * self.observation_window:state.round_number - self.observation_window]) + np.sum(history.opponent_actions[state.round_number - 2 * self.observation_window:state.round_number - self.observation_window, :])
                    avg_cooperators_before_punish = total_cooperators_prev_window / self.observation_window
                    if avg_cooperators_now >= avg_cooperators_before_punish + n * self.forgiveness_increase_ratio:
                        self.consecutive_punishments = 0
                        return Action.C
                elif avg_cooperators >= n * self.cooperation_threshold_ratio:
                    self.consecutive_punishments = 0
                    return Action.C
        if avg_cooperators < n * self.cooperation_threshold_ratio:
            self.consecutive_punishments += 1
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_109(BaseStrategy):
    """
    Adaptive Reciprocity strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by starting cooperatively,
    reciprocating group behavior, and gently punishing deviations by
    reducing contributions. It includes a forgiveness mechanism to rejoin
    cooperation if the group re-establishes a cooperative norm. It also
    adopts a purely defecting stance in the final round due to reduced
    incentives for cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.high_cooperation_threshold = 0.7
        self.moderate_cooperation_threshold = 0.4
        self.very_low_cooperation_threshold = 0.2
        self.prev_round_cooperator_proportion = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        if history is None:
            self.prev_round_cooperator_proportion = 1.0
            return Action.C
        if current_round > 0:
            my_last_action_cooperated = history.my_actions[current_round - 1]
            opponents_last_actions = history.opponent_actions[current_round - 1, :]
            num_opponent_cooperators = np.sum(opponents_last_actions)
            total_cooperators = num_opponent_cooperators + (1 if my_last_action_cooperated else 0)
            current_round_cooperator_proportion = total_cooperators / self.n_players
        else:
            current_round_cooperator_proportion = 1.0
        if current_round_cooperator_proportion >= self.high_cooperation_threshold:
            decision = Action.C
        elif current_round_cooperator_proportion >= self.moderate_cooperation_threshold:
            decision = Action.C
        elif self.prev_round_cooperator_proportion < self.moderate_cooperation_threshold and current_round_cooperator_proportion < self.very_low_cooperation_threshold:
            decision = Action.D
        else:
            decision = Action.D
        self.prev_round_cooperator_proportion = current_round_cooperator_proportion
        return decision

class Strategy_COOPERATIVE_110(BaseStrategy):
    """
    Implements the "Conditional Cooperation with Forgiveness" strategy.

    This strategy cooperates by default but defects if the proportion of
    cooperators in the previous round falls below a defection threshold (theta_D).
    It returns to cooperation if the proportion of cooperators rises above a
    cooperation threshold (theta_C). If the proportion of cooperators is between
    theta_D and theta_C, it repeats its previous action, exhibiting forgiveness
    and inertia.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.theta_D = self.k / self.n
        self.theta_C = self.k / self.n + 0.05
        self.my_last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.my_last_action = Action.C
            return Action.C
        previous_round_index = state.round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        if history.my_actions[previous_round_index] == Action.C:
            num_cooperators_prev_round += 1
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n
        current_action: Action
        if proportion_cooperators_prev_round >= self.theta_C:
            current_action = Action.C
        elif proportion_cooperators_prev_round < self.theta_D:
            current_action = Action.D
        else:
            current_action = self.my_last_action
        self.my_last_action = current_action
        return current_action

class Strategy_COOPERATIVE_111(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperation with Punishment and Forgiveness"

    This strategy aims to foster cooperation by defaulting to it, but punishes
    exploitation by defecting if its average payoff falls below a threshold.
    It is willing to forgive and return to cooperation if other players
    demonstrate sustained cooperative behavior. It also defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_threshold = 1.0 + self.k / self.n * (self.n - 1) * 0.2
        self.cooldown_period_for_forgiveness = 3
        self.is_punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            self.is_punishing = False
            return Action.C
        my_payoffs = history.my_payoffs
        current_average_payoff_self = np.mean(my_payoffs) if len(my_payoffs) > 0 else 0.0
        if current_round_number == self.r - 1:
            return Action.D
        if self.is_punishing:
            num_previous_rounds_to_check = min(current_round_number, self.cooldown_period_for_forgiveness)
            if num_previous_rounds_to_check > 0:
                recent_opponent_actions = history.opponent_actions[-num_previous_rounds_to_check:, :]
                proportions_of_cooperators_in_recent_rounds = np.mean(recent_opponent_actions, axis=1)
                if np.all(proportions_of_cooperators_in_recent_rounds >= 0.8):
                    self.is_punishing = False
                    return Action.C
            return Action.D
        if current_average_payoff_self < self.punishment_threshold:
            self.is_punishing = True
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_112(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Threshold (ATT-T) strategy.
    This strategy initiates cooperation, rewards collective cooperation by continuing to
    cooperate, punishes defection by defecting, and attempts to return to cooperation
    when the group demonstrates sufficient cooperative behavior and a low proportion
    of defectors. It always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = math.floor(game_description.k)
        self.defection_punishment_threshold = math.floor(game_description.n_players / 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        last_completed_round_idx = history.round_number - 1
        my_last_action = Action.C if history.my_actions[last_completed_round_idx] else Action.D
        all_actions_prev_round = np.zeros(n, dtype=bool)
        all_actions_prev_round[:-1] = history.opponent_actions[last_completed_round_idx, :]
        all_actions_prev_round[-1] = my_last_action == Action.C
        total_cooperators_prev_round = np.sum(all_actions_prev_round)
        num_defecters_prev_round = n - total_cooperators_prev_round
        if my_last_action == Action.C:
            if total_cooperators_prev_round >= self.threshold:
                return Action.C
            else:
                return Action.D
        elif total_cooperators_prev_round >= self.threshold and num_defecters_prev_round < self.defection_punishment_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_113(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) for N-Player Public Goods Game.

    This strategy starts cooperatively and then mirrors the group's behavior
    based on a cooperation threshold. It defects if the proportion of cooperators
    in the previous round falls below a critical threshold (1/k), and cooperates
    otherwise, except in the final round where it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == self.r:
            return Action.D
        previous_round_self_action = self.map_action_to_bool(history.my_actions[-1])
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        all_actions_previous_round = np.append(previous_round_opponent_actions, previous_round_self_action)
        num_cooperators_previous_round = np.sum(all_actions_previous_round)
        proportion_cooperators_previous_round = num_cooperators_previous_round / self.n
        if proportion_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def map_action_to_bool(self, action: Action) -> bool:
        """Maps Action enum to boolean representation (True for C, False for D)."""
        return action == Action.C

class Strategy_COOPERATIVE_114(BaseStrategy):
    """
    Tit-for-Tat Plus (TFT+) Strategy for N-Player Public Goods Game.
    This strategy initiates cooperation, then reciprocates the average behavior
    of the group. It includes a forgiveness mechanism to break out of sustained
    defection cycles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_streak = 0
        self.DEFECTION_STREAK_THRESHOLD = 3
        self.FORGIVENESS_COOPERATOR_COUNT_THRESHOLD = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.defection_streak = 0
            return Action.C
        current_round = state.round_number
        my_past_actions = history.my_actions
        opponent_past_actions = history.opponent_actions
        prev_round_index = current_round - 1
        num_cooperators_prev_round = 0
        if prev_round_index < len(my_past_actions):
            if my_past_actions[prev_round_index]:
                num_cooperators_prev_round += 1
        if prev_round_index < opponent_past_actions.shape[0]:
            num_cooperators_prev_round += np.sum(opponent_past_actions[prev_round_index, :])
        total_players = self.game_description.n_players
        average_contribution_prev_round = num_cooperators_prev_round / total_players
        if average_contribution_prev_round >= 0.5:
            self.defection_streak = 0
            return Action.C
        elif self.defection_streak >= self.DEFECTION_STREAK_THRESHOLD and num_cooperators_prev_round >= self.FORGIVENESS_COOPERATOR_COUNT_THRESHOLD:
            self.defection_streak = 0
            return Action.C
        else:
            self.defection_streak += 1
            return Action.D

class Strategy_COOPERATIVE_115(BaseStrategy):
    """
    Generous Tit-for-Tat Strategy:
    This strategy aims to initiate and sustain cooperation by rewarding it and punishing
    defection, but with a degree of forgiveness to allow for the re-establishment of cooperation.

    Decision Rules:
    - Cooperate (C) in the first round.
    - Defect (D) in the last round.
    - In intermediate rounds, cooperate if the average payoff in the previous round was >= 1.0.
    - If the average payoff was between 0.75 and 1.0, cooperate if it's the first or second
      instance of such a low average payoff in the last 3 rounds.
    - Defect if the average payoff was < 0.75, or if the average payoff was between 0.75 and 1.0
      and it's the third or subsequent instance in the last 3 rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_failure_count = 0
        self.MAX_FAILURE_WINDOW = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if state.round_number == r:
            return Action.D
        previous_round_index = state.round_number - 1
        previous_round_cooperators = np.sum(history.my_actions[previous_round_index])
        for i in range(n - 1):
            previous_round_cooperators += history.opponent_actions[previous_round_index, i]
        payoff_if_cooperated_prev = 1 - Action.C.value + self.game_description.k / n * previous_round_cooperators
        payoff_if_defected_prev = 1 - Action.D.value + self.game_description.k / n * previous_round_cooperators
        all_actions_prev_round = np.zeros(n, dtype=bool)
        all_actions_prev_round[0] = history.my_actions[previous_round_index]
        all_actions_prev_round[1:] = history.opponent_actions[previous_round_index, :]
        total_contributions_prev = np.sum(all_actions_prev_round)
        previous_round_payoffs_list = []
        for i in range(n):
            ci = 1 if all_actions_prev_round[i] else 0
            payoff_i = 1 - ci + self.game_description.k / n * total_contributions_prev
            previous_round_payoffs_list.append(payoff_i)
        average_payoff_previous_round = np.mean(previous_round_payoffs_list)
        if average_payoff_previous_round < 1.0:
            self.cooperation_failure_count += 1
        else:
            self.cooperation_failure_count = 0
        if average_payoff_previous_round >= 1.0:
            return Action.C
        elif 0.75 <= average_payoff_previous_round < 1.0:
            if self.cooperation_failure_count <= 1:
                return Action.C
            else:
                return Action.D
        elif average_payoff_previous_round < 0.75:
            return Action.D
        return Action.D

class Strategy_COOPERATIVE_116(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness (ATFT-F) for N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively, reciprocating
    the group's average behavior, and occasionally forgiving defections to prevent
    complete breakdown of cooperation. It prioritizes long-term group payoff while
    securing a reasonable individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.epsilon = 0.05
        self.alpha = 0.2
        self.forgiveness_threshold = 3
        self.min_cooperators_for_forgiveness = 1
        self.cooperation_index = 0.0
        self.consecutive_defections_below_threshold = 0
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if history is not None:
            num_cooperators_previous_round = np.sum(history.my_actions[current_round_idx - 1]) + np.sum(history.opponent_actions[current_round_idx - 1])
            self.last_round_cooperators = num_cooperators_previous_round
            proportion_cooperators_previous_round = num_cooperators_previous_round / self.n_players
            self.cooperation_index = self.alpha * proportion_cooperators_previous_round + (1 - self.alpha) * self.cooperation_index
            cooperation_threshold_val = self.k / self.n_players + self.epsilon
            if self.cooperation_index < cooperation_threshold_val:
                self.consecutive_defections_below_threshold += 1
            else:
                self.consecutive_defections_below_threshold = 0
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        cooperation_threshold_val = self.k / self.n_players + self.epsilon
        if self.cooperation_index >= cooperation_threshold_val:
            return Action.C
        is_forgiveness_triggered = self.consecutive_defections_below_threshold >= self.forgiveness_threshold and self.last_round_cooperators >= self.min_cooperators_for_forgiveness
        if is_forgiveness_triggered:
            if self.last_round_cooperators > 0:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COOPERATIVE_117(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness Strategy for the N-Player Public Goods Game.
    This strategy aims to maximize total payoff by fostering cooperation.
    It reciprocates the average cooperation level of the previous round,
    with a threshold that balances reward for cooperation and resilience to defection.
    It cooperates in the first round and defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperation_threshold = max(0.5, 1.0 / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        cooperators_previous_round = np.sum(previous_round_opponent_actions)
        avg_coop_rate_previous_round = cooperators_previous_round / self.n
        if avg_coop_rate_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_118(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness (ATF) for N-Player Public Goods Game.

    This strategy is initially cooperative, rewards continued cooperation, punishes
    universal defection, and includes an adaptive forgiveness mechanism to re-establish
    cooperation after periods of mutual defection. It also adopts a last-round
    defection strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.num_rounds_played_in_sim = 0
        self.num_consecutive_universal_defects = 0
        self.dynamic_forgiveness_threshold = self._calculate_initial_forgiveness_threshold()

    def _calculate_initial_forgiveness_threshold(self) -> int:
        """
        Calculates an initial heuristic for the forgiveness threshold.
        This threshold determines how many consecutive universal defections
        will be tolerated before attempting to forgive.
        """
        base_threshold = self.r * (1 - self.k / self.n) / 3
        return max(2, int(base_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.num_rounds_played_in_sim = state.round_number
        if self.num_rounds_played_in_sim == self.r:
            return Action.D
        if history is None:
            return Action.C
        last_round_idx = self.num_rounds_played_in_sim - 1
        all_players_last_round_actions = np.append(history.opponent_actions[last_round_idx, :], history.my_actions[last_round_idx]).astype(bool)
        all_defected = not np.any(all_players_last_round_actions)
        all_cooperated = np.all(all_players_last_round_actions)
        if all_defected:
            self.num_consecutive_universal_defects += 1
        else:
            self.num_consecutive_universal_defects = 0
        adaptation_factor = (1 - self.k / self.n) / 2
        self.dynamic_forgiveness_threshold = max(2, int(self.r * adaptation_factor) + self.num_consecutive_universal_defects)
        if all_defected:
            if self.num_consecutive_universal_defects >= self.dynamic_forgiveness_threshold:
                return Action.C
            else:
                return Action.D
        elif all_cooperated:
            return Action.C
        else:
            return Action.C

class Strategy_COOPERATIVE_119(BaseStrategy):
    """
    Tit-for-Tat with Forgiveness and Parameter Awareness (TFPA) strategy.

    This strategy cooperates by default in the first round. In subsequent rounds,
    it cooperates if at least half of the players cooperated in the previous round.
    It defects if at least half of the players defected in the previous round.
    In case of a tie (n is even, and exactly n/2 players cooperated and n/2 defected),
    it defaults to cooperating to encourage rebuilding cooperation.
    In the final round, it always defects, as there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.floor(self.n_players / 2)
        self.defection_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds:
            return Action.D
        if history is None:
            return Action.C
        num_cooperators_prev = 0
        num_defectors_prev = 0
        previous_round_actions = history.opponent_actions[-1]
        num_cooperators_prev = np.sum(previous_round_actions)
        num_defectors_prev = self.n_players - 1 - num_cooperators_prev
        if history.my_actions[-1]:
            num_cooperators_prev += 1
        else:
            num_defectors_prev += 1
        if num_cooperators_prev >= self.cooperation_threshold:
            return Action.C
        elif num_defectors_prev >= self.defection_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_120(BaseStrategy):
    """
    Cooperative Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding mutual contribution and
    punishing defection, while remaining flexible enough to adapt to a changing
    environment. It is designed to be robust, resilient, and to align with a
    cooperative mindset.

    Core Principle: We aim for mutual cooperation (everyone playing C) as this
    yields the highest collective payoff. We will only defect when it's necessary
    to defend our accumulated gains or when cooperation has clearly failed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_round_num = 0
        self.cooperation_threshold = self.n / 2.0
        self.punishment_duration = 3
        self.punishment_rounds_remaining = 0
        self.cooperation_sustained_count = 0
        self.defection_sustained_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round_num = state.round_number
        if self.current_round_num == self.r:
            return Action.D
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        cooperators_in_prev_round = 0
        if history is not None:
            if len(history.my_actions) > 0:
                last_round_actions_bool = np.append(history.opponent_actions[-1, :], history.my_actions[-1])
                cooperators_in_prev_round = np.sum(last_round_actions_bool)
            else:
                cooperators_in_prev_round = 0
        else:
            self.cooperation_threshold = self.n / 2.0
            return Action.C
        proportion_cooperators = cooperators_in_prev_round / self.n if self.n > 0 else 0
        if proportion_cooperators >= self.cooperation_threshold + 0.2:
            self.cooperation_sustained_count += 1
            self.defection_sustained_count = 0
        elif proportion_cooperators < self.cooperation_threshold - 0.2:
            self.defection_sustained_count += 1
            self.cooperation_sustained_count = 0
        else:
            self.cooperation_sustained_count = 0
            self.defection_sustained_count = 0
        if self.cooperation_sustained_count >= math.ceil(self.r / 3.0):
            self.cooperation_threshold = min(self.n, self.cooperation_threshold + 0.1)
        elif self.defection_sustained_count >= math.ceil(self.r / 3.0):
            self.cooperation_threshold = max(0.1, self.cooperation_threshold - 0.1)
        if proportion_cooperators < self.cooperation_threshold - 0.3 and proportion_cooperators < self.n / 2.0:
            self.punishment_rounds_remaining = self.punishment_duration
            return Action.D
        if proportion_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_121(BaseStrategy):
    """
    Cooperative strategy based on reciprocity with a forgiveness element.
    Cooperates if the proportion of cooperators in the previous round met a
    threshold (k), defects otherwise. Initiates cooperation and defects in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        my_previous_action = history.my_actions[current_round_number - 1]
        opponent_previous_actions = history.opponent_actions[current_round_number - 1, :]
        all_previous_actions_int = np.zeros(self.n_players, dtype=np.int8)
        all_previous_actions_int[0] = int(my_previous_action)
        all_previous_actions_int[1:] = opponent_previous_actions.astype(np.int8)
        num_cooperators_previous_round = np.sum(all_previous_actions_int)
        if num_cooperators_previous_round >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_122(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Cooperator with Grim Trigger"

    This strategy aims for high collective payoffs by initially cooperating
    and continuing to cooperate as long as the proportion of cooperators in
    the previous round meets or exceeds a calculated threshold (k/n). If the
    proportion drops below this threshold, it switches to defecting until
    cooperation levels recover.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.cooperation_threshold = self.k / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game.
            history: History of actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            Action.C if cooperating, Action.D if defecting.
        """
        if history is None:
            return Action.C
        previous_round_actions = np.zeros(self.n_players, dtype=np.bool_)
        previous_round_actions[-1] = history.my_actions[-1]
        if self.n_players > 1:
            previous_round_actions[:-1] = history.opponent_actions[-1, :]
        else:
            previous_round_actions[0] = history.my_actions[-1]
        num_cooperators_prev_round = np.sum(previous_round_actions)
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        if proportion_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_123(BaseStrategy):
    """
    Adaptive Tit-for-Tat (ATFT) for N-Player Public Goods Game.

    This strategy initiates cooperation, reciprocates opponent behavior,
    and adapts its cooperation threshold based on past outcomes. It aims
    to foster cooperation while remaining resilient to exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ATFT strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1.0
        self.my_last_action_was_cooperate = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game (contains round_number).
            history: None for the first round, otherwise a PlayerHistory object
                     containing past actions and payoffs.

        Returns:
            The action to take: Action.C (Cooperate) or Action.D (Defect).
        """
        if history is None:
            self.my_last_action_was_cooperate = True
            return Action.C
        current_round_idx = state.round_number
        prev_round_idx = current_round_idx - 1
        total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        if self.my_last_action_was_cooperate:
            total_cooperators_prev_round += 1
        num_opponents = self.n - 1
        proportion_others_cooperated = 0.0
        if num_opponents > 0:
            if self.my_last_action_was_cooperate:
                num_others_cooperated = total_cooperators_prev_round - 1
            else:
                num_others_cooperated = total_cooperators_prev_round
            num_others_cooperated = max(0, num_others_cooperated)
            proportion_others_cooperated = num_others_cooperated / num_opponents
        decrease_step = 0.05
        increase_step = 0.05
        strong_increase_step = 0.15
        high_cooperation_ratio_threshold = 0.75
        if self.my_last_action_was_cooperate:
            if proportion_others_cooperated >= high_cooperation_ratio_threshold:
                self.cooperation_threshold = max(0.5, self.cooperation_threshold - decrease_step)
            else:
                self.cooperation_threshold = min(1.0, self.cooperation_threshold + increase_step)
        else:
            self.cooperation_threshold = min(1.0, self.cooperation_threshold + strong_increase_step)
        if proportion_others_cooperated >= self.cooperation_threshold:
            my_action = Action.C
        else:
            my_action = Action.D
        self.my_last_action_was_cooperate = my_action == Action.C
        return my_action

class Strategy_COOPERATIVE_124(BaseStrategy):
    """
    Adaptive Tit-for-Tat with a Cooperative Threshold (AT4CT) for N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it, punishing defection,
    and making an initial gesture of trust. It is adaptive to the average
    cooperation rate and robust against free-riding through payoff drop detection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperative_threshold = 0.7
        self.payoff_drop_threshold = 0.3

    def _calculate_average_contribution_rate(self, actions_in_round: NDArray[np.bool_]) -> float:
        """Calculates the proportion of cooperators in a given round's actions."""
        if self.n == 0:
            return 0.0
        cooperators = np.sum(actions_in_round)
        return cooperators / self.n

    def _calculate_payoff(self, my_action: bool, total_cooperators: int) -> float:
        """Calculates the payoff for a player in a single round."""
        c_i = 1 if my_action else 0
        return 1 - c_i + self.k / self.n * total_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_index = current_round - 1
        my_actions_prev = history.my_actions
        my_payoffs_prev = history.my_payoffs
        opponent_actions_prev = history.opponent_actions
        did_i_cooperate_prev = my_actions_prev[previous_round_index]
        total_cooperators_prev = int(np.sum(opponent_actions_prev[previous_round_index, :]) + did_i_cooperate_prev)
        average_contribution_rate_prev = total_cooperators_prev / self.n
        my_previous_payoff = my_payoffs_prev[previous_round_index]
        expected_defect_payoff_prev = 1 + self.k / self.n * total_cooperators_prev
        payoff_drop_detected = False
        if did_i_cooperate_prev:
            if my_previous_payoff < expected_defect_payoff_prev * (1 - self.payoff_drop_threshold):
                payoff_drop_detected = True
        if average_contribution_rate_prev >= self.cooperative_threshold and (not payoff_drop_detected):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_125(BaseStrategy):
    """
    Adaptive Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with a cooperative move
    and then conditionally cooperating based on the observed cooperation rate
    in the previous round. It maintains a dynamic cooperation threshold that
    adapts to the level of cooperation observed. The threshold increases
    slightly in periods of high cooperation, remains stable in moderate
    cooperation, and decreases cautiously in low cooperation to encourage
    re-engagement. The strategy always defects in the last round due to the
    absence of future-round incentives.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_threshold = 0.8
        self.min_threshold = 0.1
        self.max_threshold = 1.0
        self.threshold_adjustment_factor_decrease = 0.9
        self.threshold_adjustment_factor_increase = 1.05
        self.dynamic_cooperation_threshold = self.initial_threshold

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.dynamic_cooperation_threshold = self.initial_threshold
            return Action.C
        if current_round == self.r:
            return Action.D
        prev_round_index = current_round - 1
        my_prev_action = self.history.my_actions[prev_round_index] if hasattr(self, 'history') and len(self.history.my_actions) > prev_round_index else True
        opponent_prev_actions = self.history.opponent_actions[prev_round_index, :] if hasattr(self, 'history') and len(self.history.opponent_actions) > prev_round_index else np.zeros(self.n - 1, dtype=bool)
        if len(history.my_actions) > prev_round_index:
            num_cooperators_prev_round = int(history.my_actions[prev_round_index])
            if self.n > 1 and len(history.opponent_actions) > prev_round_index:
                num_cooperators_prev_round += np.sum(history.opponent_actions[prev_round_index, :])
            observed_cooperation_rate_prev_round = num_cooperators_prev_round / self.n
        else:
            observed_cooperation_rate_prev_round = 0.5
        if observed_cooperation_rate_prev_round >= 0.8:
            self.dynamic_cooperation_threshold *= self.threshold_adjustment_factor_increase
            self.dynamic_cooperation_threshold = min(self.max_threshold, self.dynamic_cooperation_threshold)
        elif observed_cooperation_rate_prev_round >= 0.4:
            pass
        else:
            self.dynamic_cooperation_threshold *= self.threshold_adjustment_factor_decrease
            self.dynamic_cooperation_threshold = max(self.min_threshold, self.dynamic_cooperation_threshold)
        if observed_cooperation_rate_prev_round >= self.dynamic_cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def __setattr__(self, name, value):
        if name == 'history':
            self.history = value
        super().__setattr__(name, value)

class Strategy_COOPERATIVE_126(BaseStrategy):
    """
    Cooperative Strategy: "Forgiving Tit-for-Tat with Threshold"

    This strategy aims to foster cooperation by rewarding past cooperation and
    punishing defection, with a mechanism to recover from periods of widespread
    defection. It adapts based on the observed collective behavior of the group.

    Decision Logic:
    - First Round: Cooperate (C) to initiate cooperation.
    - Last Round: Defect (D) to maximize payoff in the final round.
    - Intermediate Rounds:
        - If N_cooperators_t-1 > n / 2: Cooperate (C).
        - Else if N_cooperators_t-1 < n / 4: Defect (D).
        - Else (n / 4 <= N_cooperators_t-1 <= n / 2): Cooperate (C) (Forgiving zone).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n / 2.0)
        self.defection_threshold = math.floor(self.n / 4.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :]
        num_opponent_cooperators = np.sum(previous_round_actions)
        my_previous_action = history.my_actions[-1]
        total_cooperators_previous_round = num_opponent_cooperators + (1 if my_previous_action else 0)
        if total_cooperators_previous_round > self.cooperation_threshold:
            return Action.C
        elif total_cooperators_previous_round < self.defection_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_127(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Forgiveness for N-Player Public Goods Game.

    This strategy cooperates if the proportion of cooperators in the previous
    round meets a defined threshold. It has specific logic for the first and
    last rounds, and incorporates a forgiveness mechanism to recover from
    periods of low cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.theta = max(0.5, self.k / self.n)
        self.forgiveness_period_M = 3
        self.consecutive_defect_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.consecutive_defect_rounds = 0
            return Action.C
        current_round_number = state.round_number
        previous_round_actions = history.opponent_actions[current_round_number - 1]
        if current_round_number == self.r:
            return Action.D
        total_previous_cooperators = np.sum(previous_round_actions)
        proportion_previous_cooperators = total_previous_cooperators / self.n
        if proportion_previous_cooperators == 0:
            self.consecutive_defect_rounds += 1
        else:
            self.consecutive_defect_rounds = 0
        if proportion_previous_cooperators >= self.theta:
            return Action.C
        elif self.consecutive_defect_rounds >= self.forgiveness_period_M and proportion_previous_cooperators > self.theta * 1.1:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_128(BaseStrategy):
    """
    Cooperative Strategy: "Conditional Commitment"

    This strategy aims to foster cooperation by making an initial commitment to
    cooperate, but with a clear understanding that defection will be met with
    retaliation. It then seeks to return to cooperation once sufficient
    cooperative behavior is observed. The core idea is to reward cooperation
    and punish defection, while allowing for recovery of cooperative norms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Conditional Commitment strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.defection_threshold = 0.4
        self.cooperation_recovery_threshold = 0.7
        self.cooperative_state = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the game state and history.

        Args:
            state: Current game state, including round number.
            history: History of past actions and payoffs for all players. None for the first round.

        Returns:
            The action (Action.C or Action.D) to take in the current round.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            self.cooperative_state = True
            return Action.C
        previous_round_actions = history.opponent_actions[current_round - 1]
        total_cooperators_last_round = np.sum(previous_round_actions) + (1 if history.my_actions[current_round - 1] else 0)
        proportion_defectors_last_round = (self.n - total_cooperators_last_round) / self.n
        proportion_cooperators_last_round = total_cooperators_last_round / self.n
        if self.cooperative_state:
            if proportion_defectors_last_round > self.defection_threshold:
                self.cooperative_state = False
                return Action.D
            else:
                return Action.C
        elif proportion_cooperators_last_round > self.cooperation_recovery_threshold:
            self.cooperative_state = True
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_1(BaseStrategy):
    """
    "The Opportunist" - An aggressive N-Player Public Goods Game strategy.
    This strategy defaults to defection, only cooperating when the immediate
    payoff is demonstrably better than defecting, based on historical cooperation
    rates and the efficiency of the public good (k/n). It strictly defects in
    the last round and is cautious in the penultimate round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.k_over_n = self.k / self.n
        self.cooperation_threshold = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) in the current round.

        Args:
            state: Current game state, including the round number.
            history: Historical data of actions and payoffs.

        Returns:
            Action.C or Action.D.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if self.k_over_n <= 0.5:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        recent_rounds_to_consider = min(history.round_number, 3)
        total_cooperators_in_history = 0
        others_defected_when_i_cooperated = 0
        my_last_action = Action.D
        for i in range(recent_rounds_to_consider):
            round_idx = history.round_number - 1 - i
            my_action_in_round = Action.C if history.my_actions[round_idx] else Action.D
            num_opponents_cooperating = np.sum(history.opponent_actions[round_idx, :])
            total_cooperators_this_round = num_opponents_cooperating + (1 if my_action_in_round == Action.C else 0)
            total_cooperators_in_history += total_cooperators_this_round
            if my_action_in_round == Action.C:
                if num_opponents_cooperating < self.n - 1:
                    others_defected_when_i_cooperated += 1
        avg_cooperators = total_cooperators_in_history / recent_rounds_to_consider if recent_rounds_to_consider > 0 else 0
        avg_cooperation_rate = avg_cooperators / self.n if self.n > 0 else 0
        decision = Action.D
        was_exploited_last_cooperation = False
        if history.round_number > 0:
            my_last_action_played = Action.C if history.my_actions[-1] else Action.D
            if my_last_action_played == Action.C:
                opponents_last_round_actions = history.opponent_actions[-1, :]
                if np.sum(opponents_last_round_actions) < self.n - 1:
                    was_exploited_last_cooperation = True
        if self.k_over_n > 0.5 and avg_cooperation_rate >= self.cooperation_threshold and (not was_exploited_last_cooperation) and (current_round != self.r - 1):
            decision = Action.C
        return decision

class Strategy_AGGRESSIVE_2(BaseStrategy):
    """
    The "Ruthless Maximizer" strategy. This strategy prioritizes immediate individual
    payoff, defaults to defecting, and cooperates only under strict conditions
    that demonstrably benefit the player or to punish perceived exploitation.
    It employs aggressive punishment if cooperation levels drop significantly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_countdown = 0
        self.punishment_rounds = math.floor(self.r / 5)
        self.cooperation_threshold = math.floor(self.n / 2)
        self.punishment_rebound_threshold = math.ceil(self.n * 0.7)
        self.my_last_action = Action.D
        self.my_payoff_last_round = 1.0
        self.total_cooperators_last_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_last_action = Action.D
            self.my_payoff_last_round = 1.0
            self.total_cooperators_last_round = 0
            self.punishment_countdown = 0
            return Action.D
        if history.my_actions[current_round - 1]:
            self.my_last_action = Action.C
        else:
            self.my_last_action = Action.D
        self.my_payoff_last_round = history.my_payoffs[current_round - 1]
        self.total_cooperators_last_round = np.sum(history.opponent_actions[current_round - 1, :]) + (1 if self.my_last_action == Action.C else 0)
        if current_round == self.r:
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            if self.punishment_countdown == 0 and self.total_cooperators_last_round >= self.punishment_rebound_threshold:
                pass
            else:
                return Action.D
        action_to_take = Action.D
        if self.total_cooperators_last_round >= self.punishment_rebound_threshold:
            action_to_take = Action.C
        if self.my_last_action == Action.C and self.my_payoff_last_round > 1.5 and (self.total_cooperators_last_round >= self.cooperation_threshold):
            action_to_take = Action.C
        if action_to_take == Action.C and self.total_cooperators_last_round < self.cooperation_threshold:
            action_to_take = Action.D
        if self.my_last_action == Action.D and self.my_payoff_last_round < 1.1 and (self.total_cooperators_last_round < self.cooperation_threshold):
            self.punishment_countdown = self.punishment_rounds
            return Action.D
        return action_to_take

class Strategy_AGGRESSIVE_3(BaseStrategy):
    """
    The Opportunist strategy for the N-Player Public Goods Game.
    This strategy aggressively seeks to maximize individual payoffs by defecting
    by default, cooperating only when demonstrably beneficial, and punishing
    defections swiftly and persistently.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punishment_threshold = 3
        self.my_player_id = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.punishment_counters = {i: 0 for i in range(self.n)}
            self.expected_other_cooperators_estimate = 0.0
            return Action.D
        if self.my_player_id is None:
            if history.opponent_actions.shape[1] == self.n - 1:
                pass
        self._update_history_and_estimates(history)
        if current_round == self.r:
            return Action.D
        if self._should_retaliate():
            return Action.D
        my_cooperation_contribution = 1
        expected_cooperators_with_me = my_cooperation_contribution + self.expected_other_cooperators_estimate
        payoff_if_cooperate = self.k / self.n * expected_cooperators_with_me
        payoff_if_defect = 1.0
        if payoff_if_cooperate > payoff_if_defect:
            return Action.C
        else:
            return Action.D

    def _update_history_and_estimates(self, history: PlayerHistory):
        """Updates internal state based on observed history."""
        num_completed_rounds = history.round_number
        n_players = self.n
        if num_completed_rounds > 0:
            my_last_action = Action.C if history.my_actions[num_completed_rounds - 1] else Action.D
            opponent_last_actions = history.opponent_actions[num_completed_rounds - 1, :]
            for opponent_idx, opp_action_bool in enumerate(opponent_last_actions):
                opponent_action = Action.C if opp_action_bool else Action.D
                target_player_id = opponent_idx
                if my_last_action == Action.C and opponent_action == Action.D:
                    if target_player_id not in self.punishment_counters:
                        self.punishment_counters[target_player_id] = 0
                    self.punishment_counters[target_player_id] += 1
                elif my_last_action == Action.D and opponent_action == Action.C:
                    if target_player_id in self.punishment_counters:
                        self.punishment_counters[target_player_id] = 0
                elif my_last_action == Action.C and opponent_action == Action.C:
                    if target_player_id in self.punishment_counters:
                        self.punishment_counters[target_player_id] = 0
        total_cooperation_from_others = 0
        num_observed_actions_for_avg = 0
        if num_completed_rounds > 0:
            if history.opponent_actions.shape[0] > 0:
                last_round_opponent_actions = history.opponent_actions[-1, :]
                num_other_cooperators_last_round = np.sum(last_round_opponent_actions)
                self.expected_other_cooperators_estimate = num_other_cooperators_last_round
                recent_rounds_data = history.opponent_actions[-min(num_completed_rounds, 5):, :]
                if recent_rounds_data.shape[0] > 0:
                    avg_coop_rate_per_player = np.mean(recent_rounds_data, axis=0)
                    cooperation_threshold = 0.6
                    estimated_count = np.sum(avg_coop_rate_per_player > cooperation_threshold)
                    self.expected_other_cooperators_estimate = estimated_count
                else:
                    self.expected_other_cooperators_estimate = 0.0

    def _should_retaliate(self) -> bool:
        """Checks if any player's punishment counter exceeds the threshold."""
        for player_id, count in self.punishment_counters.items():
            if count >= self.punishment_threshold:
                return True
        return False

class Strategy_AGGRESSIVE_4(BaseStrategy):
    """
    The "Predator" strategy: Aggressively seeks individual payoff by defecting
    unless group cooperation is demonstrably beneficial and stable. It punishes
    deviations quickly and doesn't forgive easily. Prioritizes immediate gains
    while incorporating a calculated long-term deterrence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.k_over_n = self.k / self.n
        self.previous_cooperators_count = 0
        self.dynamic_cooperation_threshold = self.calculate_initial_threshold()
        self.punishment_phase = False

    def calculate_initial_threshold(self) -> float:
        """Calculates a dynamic starting threshold for cooperation based on k/n."""
        return max(0.3, 0.7 - self.k_over_n)

    def update_dynamic_threshold(self) -> None:
        """
        Adjusts the cooperation threshold based on observed behavior.
        Increases sensitivity to defection if the group has been uncooperative,
        and slightly reduces it if cooperation has been high and stable.
        """
        if self.punishment_phase:
            return
        num_defecting_last_round = self.n - self.previous_cooperators_count
        if num_defecting_last_round > self.n / 2 and self.previous_cooperators_count < self.n / 2:
            self.dynamic_cooperation_threshold = min(1.0, self.dynamic_cooperation_threshold * 1.15)
        elif self.previous_cooperators_count > self.n * 0.85:
            self.dynamic_cooperation_threshold = max(0.4, self.dynamic_cooperation_threshold * 0.97)
        elif self.dynamic_cooperation_threshold > 0.8 and num_defecting_last_round > self.n / 4:
            self.punishment_phase = True
        if self.punishment_phase and self.previous_cooperators_count > self.n * 0.8:
            self.punishment_phase = False
            self.dynamic_cooperation_threshold = max(0.4, self.dynamic_cooperation_threshold * 0.9)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        if state.round_number == self.r - 1:
            if self.punishment_phase or self.n - self.previous_cooperators_count > self.n / 3:
                return Action.D
            elif self.previous_cooperators_count / self.n >= self.dynamic_cooperation_threshold * 0.9:
                return Action.C
            else:
                return Action.D
        if len(history.my_actions) > 0:
            self.previous_cooperators_count = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        else:
            self.previous_cooperators_count = 0
        self.update_dynamic_threshold()
        expected_payoff_C = self.k_over_n * (self.previous_cooperators_count + 1)
        guaranteed_payoff_D = 1.0
        if expected_payoff_C > guaranteed_payoff_D and self.previous_cooperators_count / self.n >= self.dynamic_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_5(BaseStrategy):
    """
    The "Always Punish, Always Exploit" Protocol Strategy for the N-Player Public Goods Game.

    This strategy is designed to be aggressive and self-interested. It defects
    unless all players cooperated in the previous round. It always defects in
    the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.last_round_cooperators = 0
            return Action.D
        cooperators_in_previous_round_count = 0
        if self.n_players > 1:
            cooperators_in_previous_round_count = np.sum(history.opponent_actions[current_round - 1, :])
        if history.my_actions[current_round - 1] == Action.C:
            cooperators_in_previous_round_count += 1
        self.last_round_cooperators = cooperators_in_previous_round_count
        if current_round == self.n_rounds:
            return Action.D
        if self.last_round_cooperators == self.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_6(BaseStrategy):
    """
    Aggressive "Always Punish, Never Forget" (APNF) Strategy for the N-Player Public Goods Game.

    This strategy defaults to Defect (D) to maximize individual gain by exploiting
    any cooperation from others. It only cooperates (C) in a given round if ALL
    other players have perfectly cooperated in ALL previous rounds. If even a single
    player defects at any point in the past, APNF will permanently defect from that
    round onwards. This strategy has no forgiveness and punishes any deviation
    aggressively and permanently.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        pass

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if history is None:
            return Action.D
        if current_round_number == n_rounds - 1:
            return Action.D
        all_others_always_cooperated = True
        if history.round_number > 0:
            for t in range(history.round_number):
                for opponent_idx in range(n_players - 1):
                    if history.opponent_actions[t, opponent_idx] == Action.D.value:
                        all_others_always_cooperated = False
                        break
                if not all_others_always_cooperated:
                    break
        if all_others_always_cooperated:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_7(BaseStrategy):
    """
    Predatory Cooperation Strategy:
    This strategy aims to maximize individual payoffs by cooperating only when
    the conditions are overwhelmingly beneficial and safe, and defecting otherwise.
    It defaults to defection in the first and last rounds. In intermediate rounds,
    it cooperates if the number of cooperators in the previous round was at least 'k'
    and also constituted more than half of the players. Otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        previous_round_index = history.round_number - 1
        my_previous_action = self.Action_to_bool(history.my_actions[previous_round_index])
        opponents_previous_actions = history.opponent_actions[previous_round_index, :]
        num_opponent_cooperators = np.sum(opponents_previous_actions)
        c_t_minus_1 = num_opponent_cooperators + my_previous_action
        if c_t_minus_1 >= self.k and c_t_minus_1 > self.n_players * 0.5:
            return Action.C
        else:
            return Action.D

    def Action_to_bool(self, action: bool) -> int:
        """Converts boolean action to integer (True=1, False=0)"""
        return int(action)

class Strategy_AGGRESSIVE_8(BaseStrategy):
    """
    Aggressive Exploitation Strategy (AES).
    This strategy defaults to defection, exploits cooperation when beneficial,
    and retaliates against perceived non-reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.my_player_index == -1:
            pass
        if history is None or state.round_number == 0:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        current_round_idx = state.round_number
        prev_round_idx = current_round_idx - 1
        my_last_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        total_cooperators_prev = history.my_actions[prev_round_idx] + np.sum(history.opponent_actions[prev_round_idx, :])
        others_cooperators_prev = total_cooperators_prev - (1 if my_last_action == Action.C else 0)
        if self.n > 1:
            other_cooperator_proportion = others_cooperators_prev / (self.n - 1)
        else:
            other_cooperator_proportion = 1.0
        CONSIDER_COOPERATION = False
        if my_last_action == Action.C:
            if others_cooperators_prev >= self.n - 1:
                CONSIDER_COOPERATION = True
        elif others_cooperators_prev <= 0:
            CONSIDER_COOPERATION = True
        if CONSIDER_COOPERATION:
            my_payoff_if_cooperate = self.k / self.n * (total_cooperators_prev + 1)
            my_payoff_if_defect = 1 + self.k / self.n * total_cooperators_prev
            if my_payoff_if_cooperate > my_payoff_if_defect:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_9(BaseStrategy):
    """
    Aggressive Opportunist Strategy: Maximizes personal payoff by defecting when
    individually advantageous, swiftly punishing defection, and exploiting cooperation.
    Starts by defecting and defaults to defecting unless specific conditions for
    exploitation or rare, cautious cooperation are met.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        my_last_action = history.my_actions[current_round - 1]
        other_players_last_actions = history.opponent_actions[current_round - 1, :]
        other_cooperators_last_round = np.sum(other_players_last_actions)
        if my_last_action == Action.C.value:
            if other_cooperators_last_round < self.n - 1:
                return Action.D
        if other_cooperators_last_round == self.n - 1:
            return Action.D
        if other_cooperators_last_round >= math.ceil((self.n - 1) * 0.75):
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_10(BaseStrategy):
    """
    "The Opportunistic Predator" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting cooperative
    environments and punishing deviations. It defaults to defection in uncertain
    or low-benefit scenarios and cooperates strategically when others cooperate
    to extract maximum value. It exhibits ruthless behavior in the final round
    and punishes instability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing the game's parameters
                              (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.k_over_n = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None for the first round.

        Returns:
            The chosen Action (Action.C or Action.D).
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        num_opponent_cooperators = np.sum(previous_round_actions)
        num_opponents = self.n - 1
        num_opponent_defectors = num_opponents - num_opponent_cooperators
        if num_opponents == 0:
            cooperation_ratio = 1.0
        else:
            cooperation_ratio = num_opponent_cooperators / num_opponents
        if self.k_over_n < 0.5 or cooperation_ratio < 0.2:
            return Action.D
        elif self.k_over_n >= 0.5 and cooperation_ratio >= 0.5:
            if num_opponent_defectors == 0:
                return Action.C
            elif num_opponent_defectors <= 2:
                return Action.C
            else:
                return Action.D
        elif self.k_over_n >= 0.5 and cooperation_ratio >= 0.2 and (cooperation_ratio < 0.5):
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_11(BaseStrategy):
    """
    The "Opportunistic Predator" strategy. This aggressive strategy aims to
    maximize individual gain by exploiting cooperation when profitable,
    while decisively punishing deviations. It leverages the repeated nature
    of the game to extract as much as possible, even at the potential cost
    of short-term cooperation if it leads to long-term gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.initial_phase_end_round = math.floor(self.n_rounds / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = int(np.sum(history.opponent_actions[current_round - 1, :])) + (1 if history.my_actions[current_round - 1] else 0)
        my_payoff_last_round = history.my_payoffs[current_round - 1]
        if current_round <= self.initial_phase_end_round:
            sum_opponent_payoffs_last_round = np.sum(history.opponent_payoffs[current_round - 1, :])
            avg_payoff_last_round = (my_payoff_last_round + sum_opponent_payoffs_last_round) / self.n_players
            if num_cooperators_last_round > 0:
                if my_payoff_last_round + sum_opponent_payoffs_last_round > self.n_players:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        else:
            exploitation_metric = my_payoff_last_round / self.k if self.k > 0 else 0.0
            if num_cooperators_last_round >= math.floor(self.n_players / 2) and exploitation_metric > 0.75:
                return Action.C
            elif num_cooperators_last_round < math.floor(self.n_players / 2) or exploitation_metric <= 0.75:
                return Action.D
            else:
                return Action.D

class Strategy_AGGRESSIVE_12(BaseStrategy):
    """
    The Dominant Defector strategy maximizes personal payoff by defecting by default.
    It cooperates only if all other players cooperated in the immediately preceding round,
    and retaliates by defecting if any player defected in the previous round.
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        all_players_cooperated_last_round = True
        if True in previous_round_actions:
            all_players_cooperated_last_round = False
        any_player_defected_last_round = False
        if np.any(previous_round_actions == Action.D.value):
            any_player_defected_last_round = True
        all_players_cooperated_in_previous_round = True
        for opponent_action in previous_round_actions:
            if opponent_action == Action.D.value:
                all_players_cooperated_in_previous_round = False
                break
        if all_players_cooperated_in_previous_round:
            my_previous_action = history.my_actions[current_round - 1]
            if my_previous_action == Action.D.value:
                all_players_cooperated_in_previous_round = False
        if all_players_cooperated_in_previous_round:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_13(BaseStrategy):
    """
    The "Exploitative Enforcer" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by conditionally cooperating
    initially for information gathering, then exploiting high cooperation levels,
    and punishing low cooperation. It defects in the last two rounds regardless
    of history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.high_cooperation_threshold = 0.7
        self.low_cooperation_threshold = 0.3
        self.initial_phase_length = min(self.n_rounds - 1, math.ceil(self.n_rounds / 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number < self.initial_phase_length:
            return Action.C
        if history is not None and len(history.my_actions) > 0:
            my_action_prev_round = history.my_actions[current_round_number - 1]
            opponents_actions_prev_round = history.opponent_actions[current_round_number - 1, :]
            num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponents_actions_prev_round)
            gcr = num_cooperators_prev_round / self.n_players
        else:
            gcr = 1.0
        if gcr >= self.high_cooperation_threshold:
            return Action.C
        elif gcr > self.low_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_14(BaseStrategy):
    """
    Aggressive Exploiter Strategy for the N-Player Public Goods Game.

    This strategy defaults to defection, only cooperating in the second-to-last
    round if there's a very strong signal of widespread cooperation in the
    previous round. It aims to maximize individual payoff by exploiting
    cooperative environments opportunistically and punishing deviations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punish_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if self.punish_mode:
            return Action.D
        if history is None:
            self.previous_round_cooperators = 0
            return Action.D
        if history.round_number > 0:
            self.previous_round_cooperators = np.sum(history.opponent_actions[history.round_number - 1, :])
            if history.my_actions[history.round_number - 1]:
                self.previous_round_cooperators += 1
        elif history.round_number == 1 and len(history.my_actions) > 0:
            self.previous_round_cooperators = np.sum(history.opponent_actions[0, :])
            if history.my_actions[0]:
                self.previous_round_cooperators += 1
        else:
            self.previous_round_cooperators = 0
        if current_round == self.r - 1:
            return Action.D
        if current_round == self.r - 2:
            if self.previous_round_cooperators == self.n - 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_15(BaseStrategy):
    """
    The "Dominator" strategy is designed for aggressive pursuit of player i's own
    highest possible payoff in the N-Player Public Goods Game. It prioritizes
    exploiting opportunities for increased gains by defecting when it is
    individually beneficial and only cooperating when it strategically forces
    or incentivizes others to cooperate, thereby increasing the public good's
    return for *everyone*, but critically, for itself as well.

    Core Philosophy:
    My goal is not to foster cooperation for its own sake, but to leverage
    cooperation for maximum personal gain. I will defect to maximize my
    immediate payoff unless there's a clear, immediate benefit to cooperating
    that outweighs the risk of exploitation by others. The "aggressive" nature
    comes from a default to defect and a high threshold for initiating
    cooperation, only doing so when it's a clear path to a dominant outcome.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n * (self.k - 1) / self.k)

    def _get_cooperation_threshold(self) -> int:
        """Calculates the minimum number of cooperators for cooperation to be at least break-even for all."""
        return math.ceil(self.n * (self.k - 1) / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_payoff = history.my_payoffs[prev_round_idx]
        my_prev_actions_list = history.my_actions[prev_round_idx]
        opp_prev_actions_list = history.opponent_actions[prev_round_idx, :]
        total_prev_cooperators = int(my_prev_actions_list) + int(np.sum(opp_prev_actions_list))
        all_prev_payoffs = np.concatenate([history.my_payoffs[prev_round_idx:prev_round_idx + 1], history.opponent_payoffs[prev_round_idx, :]])
        average_prev_payoff = np.mean(all_prev_payoffs)
        condition1_met = average_prev_payoff > 1 and total_prev_cooperators >= self.cooperation_threshold
        condition2_met = False
        if total_prev_cooperators == self.n - 1:
            if not my_prev_actions_list:
                cooperator_payoffs_sum = 0
                cooperator_payoffs_sum += np.sum(history.opponent_payoffs[prev_round_idx, history.opponent_actions[prev_round_idx, :]])
                avg_cooperator_payoff = cooperator_payoffs_sum / (self.n - 1)
                if avg_cooperator_payoff > my_prev_payoff:
                    condition2_met = True
        if condition1_met or condition2_met:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_16(BaseStrategy):
    """
    The "Exploiter-in-Chief" strategy.
    This strategy starts by defecting, and then cooperates only if the average
    contribution in the previous round was high AND the player themselves cooperated.
    If the average contribution was low, or if the player defected while others
    cooperated, it continues to defect. It defects on the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None or state.round_number == 0:
            return Action.D
        current_round_index = state.round_number
        previous_round_index = current_round_index - 1
        my_previous_action_cooperated = history.my_actions[previous_round_index]
        total_cooperators_previous = np.sum(history.opponent_actions[previous_round_index, :]) + (1 if my_previous_action_cooperated else 0)
        average_contribution_previous = total_cooperators_previous / n
        if current_round_index == r - 1:
            return Action.D
        if average_contribution_previous >= self.cooperation_threshold:
            if my_previous_action_cooperated:
                return Action.C
            else:
                return Action.D
        elif my_previous_action_cooperated:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_17(BaseStrategy):
    """
    The "Opportunistic Exploiter" strategy for the N-Player Public Goods Game.
    This strategy is aggressive by default, prioritizing immediate personal payoff.
    It defects unless cooperating offers a strictly greater immediate gain,
    and only then if the collective benefit from past cooperation meets an
    aggressive threshold. It avoids being the "sucker" and capitalizes on
    others' cooperation when personally advantageous.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold_ratio = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if history is None:
            return Action.D
        prev_round_idx = history.round_number - 1
        my_prev_payoff = history.my_payoffs[prev_round_idx]
        my_prev_action_is_cooperate = history.my_actions[prev_round_idx]
        cooperators_among_opponents = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = cooperators_among_opponents + (1 if my_prev_action_is_cooperate else 0)
        if state.round_number == r:
            return Action.D
        if total_cooperators_prev_round < n / 2 or my_prev_payoff < 1:
            return Action.D
        potential_pi_C_this_round = k / n * (total_cooperators_prev_round + 1)
        my_pi_D_this_round = 1
        if potential_pi_C_this_round > my_pi_D_this_round:
            value_of_public_good_from_others = k / n * cooperators_among_opponents
            max_possible_public_good_value = k
            value_of_public_good_from_all = k / n * total_cooperators_prev_round
            value_of_public_good_for_check = k / n * total_cooperators_prev_round
            if value_of_public_good_for_check >= k * self.coop_threshold_ratio:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_18(BaseStrategy):
    """
    The "Ruthless Exploiter" strategy.
    This strategy prioritizes maximizing individual gain by exploiting cooperative
    tendencies while quickly punishing defection. It's designed to be aggressive,
    opportunistic, and to learn from observed behavior without being overly forgiving.

    Core Philosophy: "Cooperate only when it benefits *me* immediately and maximally.
    Punish defection swiftly and decisively. Always aim to be the net beneficiary."

    Decision Rules:
    - Round 1: Always Defect (D).
    - Last Round: Always Defect (D).
    - General Rounds: Cooperate (C) if and only if all players (including self)
      cooperated in the previous round. Otherwise, Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.previous_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.previous_round_cooperators = 0
            return Action.D
        last_round_index = history.round_number - 1
        total_cooperators_last_round = 0
        if history.my_actions[last_round_index]:
            total_cooperators_last_round += 1
        total_cooperators_last_round += np.sum(history.opponent_actions[last_round_index, :])
        self.previous_round_cooperators = total_cooperators_last_round
        if state.round_number == self.n_rounds:
            return Action.D
        if self.previous_round_cooperators == self.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_19(BaseStrategy):
    """
    "The Predator" strategy for the N-Player Public Goods Game.

    This strategy aggressively seeks to maximize its own payoff by defecting by
    default and cooperating only when a clear immediate individual benefit
    is calculated. It uses a moving average of past cooperation to predict
    opponent behavior and includes a punitive measure against persistently
    uncooperative environments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history_window = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_num = state.round_number
        if current_round_num == 1:
            return Action.D
        if current_round_num == r:
            return Action.D
        relevant_history_length = min(current_round_num - 1, self.history_window)
        predicted_other_cooperators = 0
        if relevant_history_length > 0:
            total_cooperators_in_history = 0
            for t_hist in range(current_round_num - 1 - relevant_history_length, current_round_num - 1):
                my_action_in_hist = 1 if history.my_actions[t_hist] else 0
                opponent_actions_in_hist = np.sum(history.opponent_actions[t_hist])
                total_cooperators_in_history += my_action_in_hist + opponent_actions_in_hist
            avg_cooperators_history = total_cooperators_in_history / relevant_history_length
            predicted_other_cooperators = max(0, math.floor(avg_cooperators_history - 1))
        low_cooperation_threshold = max(1, math.floor(n * 0.2))
        low_cooperation_rounds_count = 0
        if relevant_history_length > 0:
            for t_hist in range(current_round_num - 1 - relevant_history_length, current_round_num - 1):
                cooperators_this_round = (1 if history.my_actions[t_hist] else 0) + np.sum(history.opponent_actions[t_hist])
                if cooperators_this_round < low_cooperation_threshold:
                    low_cooperation_rounds_count += 1
            if low_cooperation_rounds_count > math.floor(relevant_history_length * 0.5):
                return Action.D
        cooperate_payoff = k / n * (predicted_other_cooperators + 1)
        defect_payoff = 1
        if cooperate_payoff > defect_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_20(BaseStrategy):
    """
    The "Predator" strategy is designed to maximize individual payoff in an N-Player
    Public Goods Game. It exploits periods of cooperation and retaliates against
    perceived exploitation by accumulating a "cooperation debt." It cooperates
    initially to gauge opponents and defects aggressively in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_debt = 0.0
        self.initial_probing_rounds = min(math.floor(self.r / 5), 3)
        self.initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if current_round < self.initial_probing_rounds:
            if current_round == 0:
                self.cooperation_debt = 0.0
                self.initialized = True
            return Action.C
        if not self.initialized:
            self.cooperation_debt = 0.0
            self.initialized = True
        prev_round_idx = current_round - 1
        my_prev_action = Action.C if history.my_actions[prev_round_idx] else Action.D
        num_other_cooperators_prev_round = 0
        num_other_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        if my_prev_action == Action.C:
            if num_other_cooperators_prev_round < self.n - 1:
                debt_accrual = self.n - 1 - num_other_cooperators_prev_round
                self.cooperation_debt += debt_accrual
        elif num_other_cooperators_prev_round > 0:
            debt_repayment = min(self.cooperation_debt, num_other_cooperators_prev_round)
            self.cooperation_debt -= debt_repayment
        if self.cooperation_debt <= 0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_21(BaseStrategy):
    """
    The "Exploit and Deter" strategy is designed to be aggressive by prioritizing
    personal gain in each round while simultaneously aiming to deter others from
    defecting by demonstrating the ability to punish. It's adaptive and robust
    because it continuously monitors the group's behavior and adjusts its own
    actions accordingly.

    Core Philosophy:
    - Default to Defection: The baseline action is to defect. Cooperation is a
      conditional tool, used strategically for short-term gains when the
      environment is favorable.
    - Maximize Immediate Gain: In each round, the primary goal is to maximize
      your payoff for that round.
    - Punish Exploitation: If others consistently defect and you are being
      exploited, the strategy will escalate its own defection to punish and
      deter future exploitation.
    - Opportunistic Cooperation: Cooperation is a resource to be deployed when
      it clearly benefits you more than defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_threshold = 0.7
        self.cooperation_gain_threshold = 0.7
        self.punishment_duration = 3
        self.punishment_counter = 0
        self.my_payoffs_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            self.punishment_counter = 0
            self.my_payoffs_history = []
            return Action.D
        round_t_minus_1 = history.round_number - 1
        if round_t_minus_1 < 0:
            return Action.D
        prev_round_actions_all = history.opponent_actions[round_t_minus_1, :]
        my_action_prev_round = history.my_actions[round_t_minus_1]
        num_cooperators_prev = np.sum(prev_round_actions_all) + (1 if my_action_prev_round else 0)
        num_defectors_prev = n - num_cooperators_prev
        avg_defect_rate_prev = num_defectors_prev / n
        my_prev_payoff = history.my_payoffs[round_t_minus_1]
        self.my_payoffs_history.append(my_prev_payoff)
        my_avg_payoff = np.mean(self.my_payoffs_history) if self.my_payoffs_history else 0
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if len(self.my_payoffs_history) > 1:
            if my_prev_payoff < my_avg_payoff * 0.9:
                if avg_defect_rate_prev > self.punishment_threshold:
                    self.punishment_counter = self.punishment_duration
                    return Action.D
        if avg_defect_rate_prev < 1 - self.cooperation_gain_threshold:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_22(BaseStrategy):
    """
    Aggressive "Exploiter" Strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting cooperative
    tendencies in other players while minimizing personal contribution. It defects
    by default, only cooperating strategically to punish widespread defection
    and attempt to influence future rounds towards greater cooperation from others,
    thus setting up opportunities for exploitation.

    Core Philosophy: "If others cooperate, I'll benefit. If they don't, I'm still
    better off defecting. I'll only cooperate if I absolutely have to, and even
    then, it'll be for a strategic advantage."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and internal state.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.total_cooperators_previous_round = 0
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: History of past actions and payoffs. None if it's the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if current_round == 0:
            self.total_cooperators_previous_round = 0
            return Action.D
        elif current_round == self.r - 1:
            return Action.D
        if history is not None:
            my_last_action = 1 if history.my_actions[-1] else 0
            opponents_last_actions_sum = np.sum(history.opponent_actions[-1, :])
            self.total_cooperators_previous_round = my_last_action + opponents_last_actions_sum
        aggressive_cooperation_trigger = self.n * (self.k / self.n - self.epsilon)
        if self.total_cooperators_previous_round < aggressive_cooperation_trigger:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_23(BaseStrategy):
    """
    Opportunistic Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal gain by exploiting cooperation
    when it's demonstrably profitable, defecting by default, and punishing
    widespread defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        my_payoff_if_defect = 1.0
        observed_cooperators_last_round = 0
        if history.round_number > 0:
            last_round_idx = history.round_number - 1
            my_last_action = history.my_actions[last_round_idx]
            opponents_last_actions = history.opponent_actions[last_round_idx, :]
            total_cooperators_last_round = sum(opponents_last_actions) + my_last_action
            observed_cooperators_last_round = total_cooperators_last_round
        potential_total_cooperators_if_i_coop = observed_cooperators_last_round + 1
        payoff_if_i_cooperate_k_trap = self.k / self.n * potential_total_cooperators_if_i_coop
        if payoff_if_i_cooperate_k_trap > my_payoff_if_defect:
            return Action.C
        if current_round == self.r - 1:
            if payoff_if_i_cooperate_k_trap > my_payoff_if_defect:
                return Action.C
        if history.round_number > 0:
            last_round_idx = history.round_number - 1
            defect_count_prev_round = 0
            if not history.my_actions[last_round_idx]:
                defect_count_prev_round += 1
            opponents_last_actions = history.opponent_actions[last_round_idx, :]
            defect_count_prev_round += np.sum(opponents_last_actions == False)
            if defect_count_prev_round > self.n - 2:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_24(BaseStrategy):
    """
    The "Exploiter" strategy is an aggressive, defect-dominant strategy for the
    N-Player Public Goods Game. It primarily defects, only cooperating under
    very specific, calculated conditions to maximize its own payoff. Cooperation
    is used to potentially exploit high-cooperation environments or as a rare
    "carrot" gambit. It punishes defection decisively.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_player_id = -1
        self.defect_permanently = False

    def _count_cooperators(self, actions_array: NDArray[np.bool_]) -> int:
        """Counts the number of True (Cooperate) values in a boolean array."""
        return np.sum(actions_array)

    def _calculate_payoff(self, my_action: bool, num_cooperators: int) -> float:
        """Calculates the payoff for a player in a single round."""
        k = self.game_description.k
        n = self.game_description.n_players
        my_contribution = 1 if my_action else 0
        return 1 - my_contribution + k / n * num_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if self.defect_permanently:
            return Action.D
        if state.round_number == 0 or state.round_number == r - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        prev_my_actions = history.my_actions
        prev_my_payoffs = history.my_payoffs
        prev_opponent_actions = history.opponent_actions
        my_prev_action = prev_my_actions[prev_round_idx]
        my_prev_payoff = prev_my_payoffs[prev_round_idx]
        prev_round_cooperators_all = self._count_cooperators(prev_opponent_actions[prev_round_idx, :]) + (1 if my_prev_action else 0)
        if my_prev_action:
            potential_defect_payoff = self._calculate_payoff(False, prev_round_cooperators_all - 1)
        else:
            potential_defect_payoff = self._calculate_payoff(False, prev_round_cooperators_all)
        if my_prev_action:
            if np.any(prev_opponent_actions[prev_round_idx, :] == False):
                self.defect_permanently = True
                return Action.D
        num_others_cooperated = prev_round_cooperators_all - (1 if my_prev_action else 0)
        if num_others_cooperated >= math.floor(n / 2):
            if my_prev_payoff >= potential_defect_payoff:
                if prev_round_cooperators_all == n:
                    if my_prev_payoff < potential_defect_payoff:
                        return Action.C
                    else:
                        return Action.D
                else:
                    return Action.D
        return Action.D

class Strategy_AGGRESSIVE_25(BaseStrategy):
    """
    The Opportunistic Predator strategy: Defect by default, cooperate only when
    all other players cooperated in the previous round (to maintain maximal payoff).
    Always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None if this is the first round.

        Returns:
            The action (Action.C or Action.D) to take.
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        my_action_prev_round = int(history.my_actions[current_round - 1])
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        total_cooperators_prev_round = my_action_prev_round + opponent_cooperators_prev_round
        if total_cooperators_prev_round == self.n:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_26(BaseStrategy):
    """
    Exploitative Tit-for-Tat: Starts by defecting and only cooperates if
    there's a demonstrable net gain from collective cooperation AND minimal risk
    of exploitation. It punishes persistent defectors swiftly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MAX_EXPLOITATION_TOLERANCE = math.floor(self.n / 4)
        self.PERSISTENT_DEFECTOR_WINDOW = 2
        self.my_actions_history = []
        self.player_exploitation_counts = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.my_actions_history.append(Action.D)
            return Action.D
        if current_round == self.r:
            self.my_actions_history.append(Action.D)
            return Action.D
        prev_round_idx = current_round - 1
        my_action_prev_round = self.my_actions_history[prev_round_idx]
        total_cooperators_prev_round = 0
        if history.opponent_actions.shape[1] > 0:
            total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        persistent_defector_count = 0
        if current_round > 1:
            current_player_exploitation_counts = {}
            for player_idx in range(self.n - 1):
                player_id = player_idx
                opponent_full_history = history.opponent_actions[:, player_idx]
                consecutive_defects_against_me = 0
                for i in range(prev_round_idx, -1, -1):
                    my_action_in_round_i = Action.C if self.my_actions_history[i] else Action.D
                    opponent_action_in_round_i = Action.C if opponent_full_history[i] else Action.D
                    if my_action_in_round_i == Action.C and opponent_action_in_round_i == Action.D:
                        consecutive_defects_against_me += 1
                    else:
                        break
                if consecutive_defects_against_me >= self.PERSISTENT_DEFECTOR_WINDOW:
                    persistent_defector_count += 1
        should_cooperate = False
        defectors_prev_round = self.n - total_cooperators_prev_round
        if self.k / self.n * total_cooperators_prev_round >= defectors_prev_round:
            if persistent_defector_count < self.MAX_EXPLOITATION_TOLERANCE:
                should_cooperate = True
        if should_cooperate:
            self.my_actions_history.append(Action.C)
            return Action.C
        else:
            self.my_actions_history.append(Action.D)
            return Action.D

class Strategy_AGGRESSIVE_27(BaseStrategy):
    """
    The Aggressive Opportunist strategy.
    This strategy prioritizes maximizing individual gain by exploiting opportunities for defection
    while maintaining a baseline of cooperation when it's demonstrably beneficial.
    It defects in the first and last rounds, and otherwise cooperates only when the expected
    payoff from cooperation exceeds the payoff from defection, based on historical data.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        num_past_rounds = history.round_number
        total_cooperators_past = np.sum(history.opponent_actions)
        cooperators_per_round = np.sum(history.opponent_actions, axis=1)
        if num_past_rounds > 0:
            avg_cooperators = np.mean(cooperators_per_round)
        else:
            avg_cooperators = 0
        num_recent_rounds = min(3, num_past_rounds)
        if num_recent_rounds > 0:
            recent_cooperators_sum = np.sum(cooperators_per_round[-num_recent_rounds:])
            avg_recent_cooperators = recent_cooperators_sum / num_recent_rounds
        else:
            avg_recent_cooperators = 0
        trend_factor = 0.0
        if num_past_rounds > 1:
            last_round_cooperators = cooperators_per_round[-1]
            prev_to_last_round_cooperators = cooperators_per_round[-2]
            if last_round_cooperators > prev_to_last_round_cooperators:
                trend_factor = 0.1
            elif last_round_cooperators < prev_to_last_round_cooperators:
                trend_factor = -0.1
        expected_total_cooperators = avg_cooperators * 0.7 + avg_recent_cooperators * 0.3
        expected_total_cooperators += trend_factor * (self.n / 2)
        expected_total_cooperators = max(0, min(self.n, expected_total_cooperators))
        payoff_if_C = self.k / self.n * (expected_total_cooperators + 1)
        payoff_if_D = 1 + self.k / self.n * expected_total_cooperators
        tolerance = 0.01
        if current_round >= self.r - 3:
            tolerance = 0.05
        if payoff_if_C > payoff_if_D + tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_28(BaseStrategy):
    """
    The Exploitative Predator strategy.
    This strategy defaults to defecting, but will cooperate for one round
    if all other players cooperated in the previous round (the "bait" tactic).
    If any player defects while this strategy is cooperating, it will permanently
    switch to defecting for all subsequent rounds.
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.permanent_defection_triggered = False
        self.cooperated_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            self.permanent_defection_triggered = False
            self.cooperated_last_round = False
            return Action.D
        if current_round == r - 1:
            return Action.D
        if self.permanent_defection_triggered:
            return Action.D
        all_others_cooperated_last_round = True
        if current_round > 0:
            prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
            if np.sum(prev_round_opponent_actions) < n - 1:
                all_others_cooperated_last_round = False
        else:
            all_others_cooperated_last_round = False
        if all_others_cooperated_last_round:
            self.cooperated_last_round = True
            return Action.C
        else:
            self.cooperated_last_round = False
            return Action.D

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.permanent_defection_triggered = False
        self._cooperated_in_previous_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            self.permanent_defection_triggered = False
            self._cooperated_in_previous_round = False
            return Action.D
        if current_round == r - 1:
            return Action.D
        if self._cooperated_in_previous_round:
            prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
            if np.sum(prev_round_opponent_actions) < n - 1:
                self.permanent_defection_triggered = True
        if self.permanent_defection_triggered:
            self._cooperated_in_previous_round = False
            return Action.D
        all_others_cooperated_last_round = True
        prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        if np.sum(prev_round_opponent_actions) < n - 1:
            all_others_cooperated_last_round = False
        if all_others_cooperated_last_round:
            self._cooperated_in_previous_round = True
            return Action.C
        else:
            self._cooperated_in_previous_round = False
            return Action.D

class Strategy_AGGRESSIVE_29(BaseStrategy):
    """
    Exploitative Enforcer strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain, punishes defectors,
    and cooperates only when it's demonstrably beneficial due to
    sufficient collective cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.effective_cooperation_threshold = math.floor(self.n * (self.k / self.n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action = self.my_actions_from_history(history)[prev_round_idx]
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        all_prev_actions = np.append(opponent_prev_actions, my_prev_action)
        total_cooperators_last_round = np.sum(all_prev_actions)
        number_of_defectors_last_round = self.n - total_cooperators_last_round
        if total_cooperators_last_round >= self.effective_cooperation_threshold and number_of_defectors_last_round < self.n - self.effective_cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def my_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to combine my own actions with opponent actions for aggregate counts."""
        if len(history.my_actions) == 0:
            return np.array([], dtype=np.bool_)
        return history.my_actions

class Strategy_AGGRESSIVE_30(BaseStrategy):
    """
    Aggressive Strategy: "Exploitative Opportunist"

    This strategy defects by default, only cooperating if it demonstrably and
    consistently leads to a higher individual payoff, and only when the risk
    of being the sole cooperator is minimal. It aggressively punishes exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_player_index = -1
        self.punished_for_cooperation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n, r, k = (self.n, self.r, self.k)
        if state.round_number == r:
            return Action.D
        if history is None:
            self.punished_for_cooperation = False
            return Action.D
        prev_round_idx = state.round_number - 1
        my_prev_action_bool = history.my_actions[prev_round_idx]
        my_prev_action = Action.C if my_prev_action_bool else Action.D
        num_my_cooperators = 1 if my_prev_action == Action.C else 0
        num_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev_round = num_my_cooperators + num_opponent_cooperators
        if my_prev_action == Action.C and total_cooperators_prev_round < n - 1:
            self.punished_for_cooperation = True
        if self.punished_for_cooperation:
            return Action.D
        my_action = Action.D
        if total_cooperators_prev_round >= 2:
            payoff_if_cooperate_this_round = k / n * (total_cooperators_prev_round + 1)
            payoff_if_defect_this_round = 1 + k / n * total_cooperators_prev_round
            if payoff_if_cooperate_this_round > payoff_if_defect_this_round:
                my_action = Action.C
        return my_action

class Strategy_AGGRESSIVE_31(BaseStrategy):
    """
    Tit-for-Tat with a Leverage Factor (TTLF) strategy for the N-Player Public Goods Game.

    This strategy adopts an aggressive stance by initially defecting. It then
    responds to collective behavior: it cooperates if the total number of
    cooperators in the previous round meets or exceeds a threshold `k`, thus
    leveraging sufficient group effort for personal gain. Conversely, it defects
    if collective cooperation is insufficient, punishing free-riders and minimizing
    losses. It aggressively exploits situations where all other players cooperate.
    It always defects in the final round to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.n_rounds:
            return Action.D
        previous_round_index = current_round - 1
        previous_actions_all_players = np.append([history.my_actions[previous_round_index]], history.opponent_actions[previous_round_index, :])
        total_cooperators_previous_round = np.sum(previous_actions_all_players)
        all_others_cooperated = np.sum(history.opponent_actions[previous_round_index, :]) == self.n_players - 1
        if all_others_cooperated and history.my_actions[previous_round_index] == Action.C.value:
            return Action.C
        if total_cooperators_previous_round < self.k:
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_32(BaseStrategy):
    """
    The "Opportunistic Predator" strategy is designed to maximize individual gain
    in the N-Player Public Goods Game. It aggressively defects by default, but
    cooperates opportunistically when the collective behavior makes cooperation
    personally more profitable or to maintain a highly beneficial equilibrium.
    It punishes lack of cooperation swiftly and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold_for_cooperation = self.n / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_idx = current_round - 1
        my_action_prev = self.get_my_action(history, prev_round_idx)
        total_cooperators_prev = 0
        if my_action_prev == Action.C:
            total_cooperators_prev += 1
        if self.n > 1 and history.opponent_actions.shape[0] > prev_round_idx:
            total_cooperators_prev += np.sum(history.opponent_actions[prev_round_idx, :])
        if total_cooperators_prev > self.threshold_for_cooperation:
            return Action.C
        if total_cooperators_prev >= self.n - 1:
            return Action.C
        if total_cooperators_prev == 0:
            return Action.D
        return Action.D

    def get_my_action(self, history: PlayerHistory, round_idx: int) -> Action:
        """Helper to get my action from history."""
        if history.my_actions[round_idx]:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_33(BaseStrategy):
    """
    The "Exploitative Dominance" Strategy:
    Always defects by default, only cooperating if the previous round showed
    a high proportion of cooperators exceeding k/n and a minimum absolute number
    of cooperators. Exploits cooperation to maximize individual gain, and
    punishes perceived freeloading by continuing to defect. Avoids cooperation
    in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        prev_round_idx = current_round - 1
        total_cooperators_prev_round = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
        cooperation_profit_threshold = self.k / self.n_players
        min_cooperators_required_float = self.n_players * (cooperation_profit_threshold + self.epsilon)
        min_cooperators_required = max(2, math.floor(min_cooperators_required_float))
        if proportion_cooperators_prev_round > cooperation_profit_threshold and total_cooperators_prev_round >= min_cooperators_required:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_34(BaseStrategy):
    """
    Aggressive "Punish and Profit" Strategy for the N-Player Public Goods Game.

    This strategy prioritizes maximizing individual gain by exploiting cooperators
    while defensively punishing defectors. It defaults to defection and only
    cooperates when there's a clear, exploitable benefit. It retaliates harshly
    when exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.benefit_margin = 0.05
        self.punishment_rounds_remaining = 0
        self.last_round_cooperators_count = 0
        self.last_round_defectors_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds:
            return Action.D
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        action = Action.D
        if history is not None:
            my_last_action = Action.C if history.my_actions[-1] else Action.D
            current_round_cooperators = np.sum(history.opponent_actions[-1, :])
            if my_last_action == Action.C:
                current_round_cooperators += 1
            self.last_round_cooperators_count = current_round_cooperators
            self.last_round_defectors_count = self.n_players - self.last_round_cooperators_count
            if my_last_action == Action.C and self.last_round_cooperators_count < self.last_round_defectors_count and (self.last_round_cooperators_count < self.n_players):
                punishment_duration = min(self.n_rounds - state.round_number, math.floor(self.n_players / 2))
                self.punishment_rounds_remaining = punishment_duration
                return Action.D
            high_cooperation_threshold_count = math.ceil(0.5 * self.n_players)
            if self.last_round_cooperators_count >= high_cooperation_threshold_count:
                expected_cooperate_payoff = self.k / self.n_players * (self.last_round_cooperators_count + 1)
                defect_payoff = 1.0
                if expected_cooperate_payoff > defect_payoff + self.benefit_margin:
                    action = Action.C
        elif state.round_number == 0:
            return Action.D
        return action

class Strategy_AGGRESSIVE_35(BaseStrategy):
    """
    The Opportunistic Punisher strategy for the N-Player Public Goods Game.

    This strategy is designed to be aggressive, prioritizing personal gain through
    early exploitation, conditional cooperation based on group benefit, and
    decisive punishment of non-cooperation.

    Key Features:
    - Starts by defecting to exploit initial cooperation.
    - Considers cooperating only if the average payoff of others significantly
      outperforms its own.
    - Punishes insufficient group cooperation with unwavering defection.
    - Exploits stable cooperative environments if defection yields a higher payoff.
    - Defaults to defection in late and final rounds for pure self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.early_phase_end = math.ceil(self.r / 3)
        self.mid_phase_end = math.ceil(2 * self.r / 3)
        self.conditional_cooperation_trigger_avg_payoff_ratio = 1.2
        self.punishment_threshold_cooperators_ratio = 0.5
        self.late_game_catch_up_trigger_avg_payoff_ratio = 1.5

    def _calculate_player_payoff(self, my_action: bool, total_cooperators: int) -> float:
        """Calculates player's payoff in a single round."""
        my_contribution = 1 if my_action else 0
        return 1 - my_contribution + self.k / self.n * total_cooperators

    def _get_average_opponent_payoff(self, history: PlayerHistory, round_idx: int) -> float:
        """
        Estimates the average payoff of opponents in a given round.
        This is an approximation as we don't have individual opponent actions history easily.
        We infer it based on the total cooperators and my own action/payoff.
        """
        if round_idx < 0:
            return 0.0
        total_cooperators = sum(history.opponent_actions[round_idx, :]) + (1 if history.my_actions[round_idx] else 0)
        my_payoff_this_round = history.my_payoffs[round_idx]
        if history.my_actions[round_idx]:
            pass
        if not history.my_actions[round_idx]:
            if total_cooperators > 0:
                estimated_cooperator_payoff = self.k / self.n * total_cooperators
                if estimated_cooperator_payoff > my_payoff_this_round:
                    return max(my_payoff_this_round, self.k / self.n * total_cooperators)
            else:
                return my_payoff_this_round
        return my_payoff_this_round * self.conditional_cooperation_trigger_avg_payoff_ratio

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        previous_round_idx = current_round - 1
        my_actions = history.my_actions
        my_payoffs = history.my_payoffs
        opponent_actions = history.opponent_actions
        my_previous_action = my_actions[previous_round_idx]
        my_previous_payoff = my_payoffs[previous_round_idx]
        cooperators_in_previous_round = sum(opponent_actions[previous_round_idx, :]) + (1 if my_previous_action else 0)
        estimated_avg_opponent_payoff = 0.0
        if cooperators_in_previous_round > 0:
            if my_previous_action:
                estimated_avg_opponent_payoff = self.k / self.n * cooperators_in_previous_round
            else:
                estimated_avg_opponent_payoff = 1 - 0 + self.k / self.n * cooperators_in_previous_round
                if cooperators_in_previous_round > self.n * self.punishment_threshold_cooperators_ratio:
                    estimated_avg_opponent_payoff = max(my_previous_payoff, self.k / self.n * cooperators_in_previous_round)
        if current_round < self.early_phase_end:
            action = Action.D
            if my_previous_action == False:
                if cooperators_in_previous_round > self.n * self.punishment_threshold_cooperators_ratio:
                    if my_previous_payoff < 1.5:
                        action = Action.C
            return action
        elif current_round < self.mid_phase_end:
            if my_previous_action:
                if cooperators_in_previous_round > self.n * self.punishment_threshold_cooperators_ratio:
                    return Action.C
                else:
                    return Action.D
            elif cooperators_in_previous_round > self.n * self.punishment_threshold_cooperators_ratio:
                return Action.D
            else:
                return Action.D
        else:
            action = Action.D
            if not my_previous_action:
                if cooperators_in_previous_round > self.n * self.punishment_threshold_cooperators_ratio:
                    if my_previous_payoff < self.k / self.n * cooperators_in_previous_round / self.late_game_catch_up_trigger_avg_payoff_ratio:
                        return Action.C
            return action

class Strategy_AGGRESSIVE_36(BaseStrategy):
    """
    Aggressive "Opportunistic Defection" Strategy for the N-Player Public Goods Game.

    This strategy defects by default and only considers cooperating if there's a clear,
    immediate, and strictly calculated personal payoff benefit in the current round,
    based on the previous round's total contributions. It prioritizes self-gain
    and aims to exploit situations where cooperation might yield a higher return for itself,
    without engaging in trust-building.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current game state, including the round number.
            history: Historical data of actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            The chosen action (Action.C or Action.D).
        """
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_my_action = self.my_actions_from_history(history)[current_round - 1]
        prev_round_opponent_actions = self.opponent_actions_from_history(history)[current_round - 1, :]
        total_cooperators_prev_round = int(prev_round_my_action) + np.sum(prev_round_opponent_actions)
        payoff_if_defect_me = 1.0 + self.k / self.n * total_cooperators_prev_round
        payoff_if_cooperate_me = 0.0 + self.k / self.n * (total_cooperators_prev_round + 1)
        if payoff_if_cooperate_me > payoff_if_defect_me:
            return Action.C
        else:
            return Action.D

    def my_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to get my actions from history, handling potential array structure differences."""
        return history.my_actions

    def opponent_actions_from_history(self, history: PlayerHistory) -> NDArray[np.bool_]:
        """Helper to get opponent actions from history."""
        return history.opponent_actions

class Strategy_AGGRESSIVE_37(BaseStrategy):
    """
    Aggressive "Exploit and Deter" Strategy for N-Player Public Goods Game.
    This strategy prioritizes personal gain by defecting first and last,
    and cooperating only when a majority of players cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on game history.

        Args:
            state: The current game state, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     None if it's the first round.

        Returns:
            The action to take (Action.C or Action.D).
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_actions = history.opponent_actions[-1, :]
        previous_round_actions_int = previous_round_actions.astype(int)
        total_cooperators_previous_round = np.sum(previous_round_actions_int)
        if total_cooperators_previous_round >= self.n / 2:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_38(BaseStrategy):
    """
    Aggressive Reciprocator of Exploitation Strategy:
    This strategy prioritizes individual gain by defaulting to defection.
    It reacts aggressively to being exploited by entering a punishment phase.
    It's adaptive as it learns from negative feedback (exploitation) and robust
    by not relying on others' cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.PUNISHMENT_ROUNDS = 2
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            self.punishment_counter = 0
            return Action.D
        if state.round_number == r:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        my_last_payoff = history.my_payoffs[state.round_number - 1]
        if my_last_payoff < 1.0:
            self.punishment_counter = self.PUNISHMENT_ROUNDS
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_39(BaseStrategy):
    """
    Aggressive "Tit-for-Tat Minus One" Strategy.
    This strategy aims to maximize individual gain by exploiting cooperative
    tendencies while swiftly punishing defections. It defaults to defection
    in the first and last rounds, and prioritizes immediate profit based on
    past round payoffs. It cooperates only if it's demonstrably better than
    defecting in the current round, given an expectation of recent player behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def _calculate_payoff(self, player_action_is_cooperate: bool, num_cooperators: int) -> float:
        """Calculates the payoff for a single player in one round."""
        c_i = 1 if player_action_is_cooperate else 0
        return 1 - c_i + self.k / self.n * num_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        prev_round_idx = history.round_number - 1
        my_prev_action_is_cooperate = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        total_cooperators_prev = (1 if my_prev_action_is_cooperate else 0) + num_opponent_cooperators_prev
        payoff_C_prev = self._calculate_payoff(True, total_cooperators_prev)
        payoff_D_prev = self._calculate_payoff(False, total_cooperators_prev)
        if payoff_C_prev < payoff_D_prev:
            return Action.D
        else:
            num_recent_rounds = min(history.round_number, 3)
            recent_cooperators_sum = 0
            if num_recent_rounds > 0:
                for i in range(1, num_recent_rounds + 1):
                    round_idx = history.round_number - i
                    recent_cooperators_sum += np.sum(history.opponent_actions[round_idx, :])
                    recent_cooperators_sum += 1 if history.my_actions[round_idx] else 0
                N_C_expected = recent_cooperators_sum / num_recent_rounds
            else:
                N_C_expected = 0
            N_C_expected = max(0.0, N_C_expected)
            potential_payoff_C_current = self._calculate_payoff(True, N_C_expected + 1)
            potential_payoff_D_current = self._calculate_payoff(False, N_C_expected)
            if potential_payoff_C_current > potential_payoff_D_current:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_40(BaseStrategy):
    """
    Aggressive "Exploiter" Strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting cooperative
    behavior. It defaults to defecting and only cooperates when a calculated
    assessment of immediate profitability, influenced by past observations,
    suggests a net gain. It prioritizes extracting value, only cooperating
    when demonstrably beneficial for its own immediate gain or when anticipating
    future exploitation opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: History of past actions and payoffs for all players.
                     None if it's the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round >= self.r - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action = self.my_actions_to_bool(history.my_actions[prev_round_idx])
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        all_prev_actions = np.append(my_prev_action, opponent_prev_actions)
        prev_total_cooperators = np.sum(all_prev_actions)
        cooperation_threshold = math.floor(self.n * (1 - 1 / self.k))
        if prev_total_cooperators >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def my_actions_to_bool(self, action_enum: Action) -> np.bool_:
        """Converts Action enum to boolean (True for C, False for D)."""
        return action_enum == Action.C

class Strategy_AGGRESSIVE_41(BaseStrategy):
    """
    Ruthless Maximizer Strategy:
    This strategy prioritizes immediate self-interest and maximizing personal gain.
    It defaults to defecting, as this is often the most profitable action in a
    single round of the Public Goods Game when k < n. Cooperation is an
    exception, reserved for mathematically calculated scenarios where it
    is demonstrably superior to defecting. The strategy is adaptive, learning
    from historical data to estimate opponent behavior, but its "trust" in
    others is extremely low, reflecting its aggressive, self-serving nature.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.decay_factor = 0.5
        self.required_profit_margin = 0.01
        self.my_player_index = -1

    def _calculate_cooperation_score(self, player_index: int, history: PlayerHistory) -> float:
        """
        Estimates the likelihood of a specific opponent player cooperating.
        For an aggressive strategy, this is very strict:
        Assume cooperation only if they have *always* cooperated in the past.
        """
        if history is None or history.round_number == 0:
            return 0.0
        player_actions = history.opponent_actions[:history.round_number, player_index]
        if len(player_actions) == 0:
            return 0.0
        if np.any(player_actions == Action.D.value):
            return 0.0
        else:
            return 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == self.n_rounds:
            return Action.D
        estimated_cooperators_from_others = 0.0
        for player_idx in range(self.n_players):
            if player_idx < self.n_players - 1:
                estimated_cooperators_from_others += self._calculate_cooperation_score(player_idx, history)
        payoff_if_cooperate = self.k / self.n_players * (1 + estimated_cooperators_from_others)
        payoff_if_defect = 1.0 + self.k / self.n_players * estimated_cooperators_from_others
        if payoff_if_cooperate > payoff_if_defect + self.required_profit_margin:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_42(BaseStrategy):
    """
    The Ruthless Enforcer strategy for the N-Player Public Goods Game.
    This strategy defaults to defecting, only cooperating if it guarantees
    a strictly higher immediate payoff. It permanently defects if any other
    player defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.retaliation_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.retaliation_triggered = False
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        if self.retaliation_triggered:
            return Action.D
        if history is not None and len(history.opponent_actions) > 0:
            last_opponent_actions = history.opponent_actions[-1, :]
            if np.any(last_opponent_actions == Action.D.value):
                self.retaliation_triggered = True
                return Action.D
        estimated_opponent_cooperators = 0
        if history is not None and len(history.opponent_actions) > 0:
            last_opponent_actions_values = history.opponent_actions[-1, :]
            estimated_opponent_cooperators = np.sum(last_opponent_actions_values == Action.C.value)
        payoff_if_cooperate = self.k / self.n * (1 + estimated_opponent_cooperators)
        payoff_if_defect = 1.0
        if payoff_if_cooperate > payoff_if_defect:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_43(BaseStrategy):
    """
    The Unforgiving Exploiter: An aggressive strategy that prioritizes immediate gains,
    retaliates swiftly against any perceived exploitation, and aims to maximize individual
    payoff by exploiting cooperative behavior. Defects unless all players cooperated
    in the previous round, in which case it might cooperate to sustain high rewards.
    Always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds

    def _calculate_my_payoff(self, actions_in_round: NDArray[np.bool_]) -> float:
        """
        Calculates the payoff for this player in a given round.
        Assumes 'actions_in_round' includes this player's action at the end.
        """
        cooperators = np.sum(actions_in_round)
        my_action = actions_in_round[-1]
        if my_action:
            return 0 + self.k / self.n * cooperators
        else:
            return 1 + self.k / self.n * cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 1
        total_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_idx, :]) + history.my_actions[previous_round_idx]
        my_previous_action = history.my_actions[previous_round_idx]
        my_actual_payoff_prev_round = history.my_payoffs[previous_round_idx]
        your_payoff_if_you_defected_prev_round = 0
        if my_previous_action:
            your_payoff_if_you_defected_prev_round = 1 + self.k / self.n * (total_cooperators_previous_round - 1)
        else:
            your_payoff_if_you_defected_prev_round = 1 + self.k / self.n * total_cooperators_previous_round
        if my_actual_payoff_prev_round < your_payoff_if_you_defected_prev_round:
            return Action.D
        if total_cooperators_previous_round < self.n:
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_44(BaseStrategy):
    """
    Aggressive Dominance Strategy: Always takes, unless forced to give.
    This strategy defaults to Defect (D). It only cooperates under a
    very specific reciprocity trigger: if ALL other n-1 players
    cooperated in the previous round, and it's not the last round.
    It is highly punitive and never forgives defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        last_round_index = current_round - 1
        all_opponents_cooperated_last_round = True
        if self.n > 1:
            if history.opponent_actions.shape[0] > last_round_index:
                opponents_last_round = history.opponent_actions[last_round_index, :]
                if np.any(opponents_last_round == Action.D.value):
                    all_opponents_cooperated_last_round = False
            else:
                all_opponents_cooperated_last_round = False
        else:
            all_opponents_cooperated_last_round = False
        if all_opponents_cooperated_last_round:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_45(BaseStrategy):
    """
    The Opportunistic Exploiters strategy.
    This strategy defects by default, cooperates only when demonstrably beneficial,
    punishes defectors swiftly, and exploits cooperators. It adapts its cooperation
    threshold and enters a punishment state when exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.base_cooperation_threshold_high_k = 0.6
        self.base_cooperation_threshold_low_k = 0.8
        self.epsilon = 0.05
        self.punishment_exit_sustained_cooperation_rounds = 3
        self.exploitation_punishment_threshold = 1
        self.cooperation_threshold = 0.0
        self.punishment_state = False
        self.sustained_cooperation_counter = 0
        self.exploitation_detected_counter = 0
        self.my_last_action = Action.D
        self._initialize_strategy_parameters()

    def _initialize_strategy_parameters(self):
        """Initializes the cooperation threshold based on k/n ratio."""
        if self.k / self.n > 0.5:
            self.cooperation_threshold = self.base_cooperation_threshold_high_k
        else:
            self.cooperation_threshold = self.base_cooperation_threshold_low_k

    def _calculate_my_payoff_if_defected(self, total_cooperators_last_round, my_last_action_was_C):
        """Calculates what my payoff would have been if I had defected in the last round."""
        if my_last_action_was_C:
            return 1.0 + self.k / self.n * total_cooperators_last_round
        else:
            return 1.0 + self.k / self.n * total_cooperators_last_round

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            self.my_last_action = Action.D
            return Action.D
        my_last_action_enum = Action.C if history.my_actions[-1] else Action.D
        my_last_payoff = history.my_payoffs[-1]
        total_cooperators_last_round = int(np.sum(history.opponent_actions[-1, :] == Action.C.value))
        if my_last_action_enum == Action.C:
            total_cooperators_last_round += 1
        num_others = self.n - 1
        cooperators_among_others = total_cooperators_last_round - (1 if my_last_action_enum == Action.C else 0)
        observed_cooperation_rate_others = cooperators_among_others / num_others if num_others > 0 else 1.0
        payoff_if_defected_last_round = self._calculate_my_payoff_if_defected(total_cooperators_last_round, my_last_action_enum == Action.C)
        if my_last_action_enum == Action.C and my_last_payoff < payoff_if_defected_last_round:
            self.exploitation_detected_counter += 1
            self.punishment_state = True
        else:
            self.exploitation_detected_counter = 0
        if not self.punishment_state:
            if my_last_action_enum == Action.C:
                self.sustained_cooperation_counter += 1
            else:
                self.sustained_cooperation_counter = 0
            if my_last_action_enum == Action.C and observed_cooperation_rate_others >= self.cooperation_threshold and (total_cooperators_last_round > self.n / 2) and (self.cooperation_threshold > self.k / self.n + 0.01):
                self.cooperation_threshold = max(self.k / self.n, self.cooperation_threshold - self.epsilon * 0.5)
            elif my_last_action_enum == Action.D and payoff_if_defected_last_round > my_last_payoff and (observed_cooperation_rate_others > 0.75) and (self.cooperation_threshold < 0.9):
                self.cooperation_threshold = min(0.9, self.cooperation_threshold + self.epsilon * 0.5)
        else:
            self.sustained_cooperation_counter = 0
            if total_cooperators_last_round >= self.n - 1:
                self.sustained_cooperation_counter += 1
            else:
                self.sustained_cooperation_counter = 0
            if self.sustained_cooperation_counter >= self.punishment_exit_sustained_cooperation_rounds or total_cooperators_last_round >= self.n - 1:
                self.punishment_state = False
                self.sustained_cooperation_counter = 0
        action_to_take = Action.D
        if self.punishment_state:
            action_to_take = Action.D
        elif observed_cooperation_rate_others >= self.cooperation_threshold and total_cooperators_last_round > 0:
            action_to_take = Action.C
        else:
            action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_AGGRESSIVE_46(BaseStrategy):
    """
    Ruthless Exploiter Strategy:
    Defaults to Defect. Cooperates only if ALL other players cooperated in the previous round.
    Defects if any other player defected in the previous round.
    Always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds:
            return Action.D
        if current_round == 0:
            return Action.D
        all_others_cooperated_last_round = True
        if history.opponent_actions.size > 0:
            previous_round_index = history.round_number - 1
            if previous_round_index < history.opponent_actions.shape[0]:
                for player_action in history.opponent_actions[previous_round_index, :]:
                    if player_action == Action.D.value:
                        all_others_cooperated_last_round = False
                        break
            else:
                all_others_cooperated_last_round = False
        else:
            all_others_cooperated_last_round = False
        if all_others_cooperated_last_round:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_47(BaseStrategy):
    """
    "Exploitative Opportunist" strategy for the N-Player Public Goods Game.
    This strategy is aggressive, prioritizing individual gain. It defaults to defecting
    but cooperates strategically only when guaranteed personal gain and when the
    potential for exploitation or punishment is high. It punishes defections severely.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.high_cooperation_threshold_ratio = 0.75
        self.punishment_period = 3
        self.current_round_num = 0
        self.punishment_rounds_remaining = 0
        self.total_cooperators_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round_num = state.round_number
        if self.current_round_num == self.r - 1:
            return Action.D
        if history is None:
            self.punishment_rounds_remaining = 0
            return Action.D
        if self.current_round_num > 0:
            total_cooperators_prev_round = 0
            if self.current_round_num > 1:
                total_cooperators_prev_round = self.total_cooperators_history[-1]
            elif self.current_round_num == 1:
                total_cooperators_prev_round = sum(history.my_actions[0:1]) + sum(history.opponent_actions[0, :])
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                self.total_cooperators_history.append(total_cooperators_prev_round)
                return Action.D
        default_action = Action.D
        action_to_take = default_action
        if self.current_round_num > 0:
            total_cooperators_prev_round = 0
            if self.current_round_num > 1:
                total_cooperators_prev_round = self.total_cooperators_history[-1]
            elif self.current_round_num == 1:
                total_cooperators_prev_round = sum(history.my_actions[0:1]) + sum(history.opponent_actions[0, :])
            my_cooperation_payoff_if_i_coop = self.k / self.n * (total_cooperators_prev_round + 1)
            if my_cooperation_payoff_if_i_coop > 1:
                high_cooperation_threshold = math.ceil(self.n * self.high_cooperation_threshold_ratio)
                if total_cooperators_prev_round >= high_cooperation_threshold:
                    action_to_take = Action.C
        if self.punishment_rounds_remaining == 0 and self.current_round_num > 0:
            total_cooperators_prev_round = 0
            if self.current_round_num > 1:
                total_cooperators_prev_round = self.total_cooperators_history[-1]
            elif self.current_round_num == 1:
                total_cooperators_prev_round = sum(history.my_actions[0:1]) + sum(history.opponent_actions[0, :])
            high_cooperation_threshold = math.ceil(self.n * self.high_cooperation_threshold_ratio)
            low_cooperation_trigger_ratio = 0.4
            low_cooperation_trigger_threshold = math.floor(self.n * low_cooperation_trigger_ratio)
            if total_cooperators_prev_round >= high_cooperation_threshold and total_cooperators_prev_round < low_cooperation_trigger_threshold:
                self.punishment_rounds_remaining = self.punishment_period
                action_to_take = Action.D
        if self.punishment_rounds_remaining == 0:
            if self.current_round_num > 0:
                total_cooperators_this_round = 0
                if action_to_take == Action.C:
                    total_cooperators_this_round = 1
                if self.current_round_num > 1:
                    total_cooperators_this_round += sum(history.opponent_actions[self.current_round_num - 1, :])
                elif self.current_round_num == 1:
                    total_cooperators_this_round += sum(history.opponent_actions[0, :])
                self.total_cooperators_history.append(total_cooperators_this_round)
            else:
                pass
        elif self.current_round_num > 0:
            if self.current_round_num > 1:
                total_cooperators_prev_round = self.total_cooperators_history[-1]
            elif self.current_round_num == 1:
                total_cooperators_prev_round = sum(history.my_actions[0:1]) + sum(history.opponent_actions[0, :])
            self.total_cooperators_history.append(total_cooperators_prev_round)
        return action_to_take

class Strategy_AGGRESSIVE_48(BaseStrategy):
    """
    Aggressive "Exploit & Retaliate" Strategy for the N-Player Public Goods Game.

    This strategy prioritizes individual payoff by defaulting to defection,
    but it cooperates under high-benefit conditions and retaliates against
    sustained defection by others. It aims to exploit opportunities while
    maintaining a credible threat of punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_high_benefit = 0.6
        self.retaliation_ratio = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_number = state.round_number
        if round_number == 0:
            return Action.D
        if round_number == self.n_rounds - 1:
            return Action.D
        if round_number == self.n_rounds:
            return Action.D
        my_actions_int = self.history_to_int(history.my_actions)
        opponent_actions_int = self.history_to_int(history.opponent_actions)
        own_previous_action = my_actions_int[-1] if round_number > 0 else 0
        cr = 0.0
        if round_number > 0:
            if history.round_number > 0:
                num_cooperators_previous_round = np.sum(history.my_actions[history.round_number - 1]) + np.sum(history.opponent_actions[history.round_number - 1])
                cr = num_cooperators_previous_round / self.n_players
        total_defect_count_others = np.sum(opponent_actions_int == 0)
        total_cooperate_count_others = np.sum(opponent_actions_int == 1)
        if self.k / self.n_players * cr > self.threshold_high_benefit and own_previous_action == 1:
            return Action.C
        if self.k / self.n_players * cr <= self.threshold_high_benefit or own_previous_action == 0:
            return Action.D
        if total_cooperate_count_others > 0 and total_defect_count_others > total_cooperate_count_others * self.retaliation_ratio and (own_previous_action == 0):
            return Action.D
        if own_previous_action == 0:
            return Action.D
        return Action.D

    def history_to_int(self, actions_arr: NDArray[np.bool_]) -> NDArray[np.int_]:
        """Converts boolean action array (True=C, False=D) to integer (1=C, 0=D)."""
        return actions_arr.astype(np.int_)

class Strategy_AGGRESSIVE_49(BaseStrategy):
    """
    Aggressive Exploitation and Retaliation Strategy (AERS)

    This strategy prioritizes individual gain by defecting when profitable,
    punishing free-riders, and defaulting to defection unless there is clear
    and sustained group benefit. It's aggressive due to its immediate retaliation
    and initial defection, and its low threshold for returning to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        last_round_idx = state.round_number - 1
        my_action_last_round = self.get_my_action_from_history(history, last_round_idx)
        num_cooperators_last_round = np.sum(history.opponent_actions[last_round_idx, :]) + (1 if my_action_last_round == Action.C else 0)
        total_payoff_last_round = np.sum(history.my_payoffs[last_round_idx]) + np.sum(history.opponent_payoffs[last_round_idx])
        average_payoff_last_round = total_payoff_last_round / self.n
        if num_cooperators_last_round < self.n - 1 and average_payoff_last_round < 1.0:
            return Action.D
        if my_action_last_round == Action.C and num_cooperators_last_round < self.n:
            return Action.D
        if num_cooperators_last_round == self.n:
            return Action.C
        return Action.D

    def get_my_action_from_history(self, history: PlayerHistory, round_idx: int) -> Action:
        """Helper to get my action from history, converting boolean to Action enum."""
        if history.my_actions[round_idx]:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_50(BaseStrategy):
    """
    Dominant Defector Strategy: Aggressively prioritizes individual payoff.
    Defects by default, only cooperating if it demonstrably leads to a higher
    immediate individual payoff than defecting, and even then, only if
    all other players have cooperated. It never initiates cooperation
    and is designed to exploit collective action without contributing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.expected_defect_payoff_no_cooperators = 1.0
        self.cooperate_cost = 1.0
        self.cooperate_benefit_per_player = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round_idx = history.round_number
        if current_round_idx == self.r:
            return Action.D
        last_round_actions = history.opponent_actions[current_round_idx - 1]
        num_cooperators_last_round_others = np.sum(last_round_actions)
        payoff_if_defect_current = self.expected_defect_payoff_no_cooperators - 0 + self.cooperate_benefit_per_player * num_cooperators_last_round_others
        payoff_if_cooperate_current = self.expected_defect_payoff_no_cooperators - self.cooperate_cost + self.cooperate_benefit_per_player * (num_cooperators_last_round_others + 1)
        all_others_cooperated_last_round = num_cooperators_last_round_others == self.n - 1
        my_payoff_defect = 1.0 - 0.0 + self.k / self.n * num_cooperators_last_round_others
        my_payoff_cooperate = 1.0 - 1.0 + self.k / self.n * (num_cooperators_last_round_others + 1)
        if my_payoff_cooperate > my_payoff_defect and num_cooperators_last_round_others == self.n - 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_51(BaseStrategy):
    """
    The Ruthless Opportunist strategy.

    This strategy is aggressive, defaulting to defection and only cooperating
    when it yields a strictly higher personal payoff than defecting. It punishes
    low cooperation by entering a defection phase and always defects in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.PUNISHMENT_THRESHOLD_RATIO = 0.25
        self.PUNISHMENT_ROUNDS = 3
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.punishment_timer = 0
            return Action.D
        if current_round == self.r - 1:
            self.punishment_timer = 0
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        total_cooperators_last_round = 0
        if history is not None:
            if history.round_number > 0:
                total_cooperators_last_round = np.sum(history.my_actions[-1])
                if history.my_actions[-1]:
                    total_cooperators_last_round -= 1
        payoff_if_cooperate = self.k / self.n * (total_cooperators_last_round + 1)
        payoff_if_defect = 1 + self.k / self.n * total_cooperators_last_round
        if payoff_if_cooperate > payoff_if_defect:
            self.punishment_timer = 0
            return Action.C
        else:
            if self.punishment_timer == 0 and total_cooperators_last_round > 0:
                punishment_threshold = math.floor(self.n * self.PUNISHMENT_THRESHOLD_RATIO)
                if total_cooperators_last_round < punishment_threshold:
                    self.punishment_timer = self.PUNISHMENT_ROUNDS
                    return Action.D
            return Action.D

class Strategy_AGGRESSIVE_52(BaseStrategy):
    """
    Aggressive Retaliation strategy for the N-Player Public Goods Game.
    Defects by default, cooperates only when previous round's cooperation
    yielded a strictly higher payoff than defecting, and punishes defection
    by reverting to defecting. No forgiveness. Always defects in the first
    and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        previous_round_data = history
        total_cooperators_prev_round = np.sum(previous_round_data.my_actions[-1]) + np.sum(previous_round_data.opponent_actions[-1, :])
        payoff_i_defected_prev_round = 1.0 + self.k / self.n * total_cooperators_prev_round
        payoff_i_cooperated_prev_round = previous_round_data.my_payoffs[-1]
        if payoff_i_cooperated_prev_round > payoff_i_defected_prev_round:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_53(BaseStrategy):
    """
    Aggressive Strategy for N-Player Public Goods Game: "Exploitative Retaliator"

    This strategy aims to maximize personal payoff by exploiting profitable cooperation
    opportunities while swiftly punishing defection. It defaults to defection in
    ambiguous situations or the early/late rounds, and only cooperates when the
    immediate individual payoff is demonstrably higher than defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None or current_round <= 2:
            return Action.D
        previous_round_index = current_round - 1
        total_previous_cooperators = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        average_cooperation_rate_prev = total_previous_cooperators / self.n
        if average_cooperation_rate_prev < self.n / 4.0:
            return Action.D
        payoff_if_I_cooperate_now = self.k / self.n * (total_previous_cooperators + 1)
        payoff_if_I_defect_now = 1.0
        if payoff_if_I_cooperate_now > payoff_if_I_defect_now:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_54(BaseStrategy):
    """
    The "Opportunistic Exploiter" strategy for the N-Player Public Goods Game.
    This strategy prioritizes maximizing individual payoff by defecting by default,
    punishing high defection rates, and opportunistically cooperating only when
    it demonstrably leads to a higher personal payoff than defecting. It remains
    risk-averse in the first round and exploitative in the last.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_is_individually_rational = self.k / self.n_players > 1
        self.punishment_threshold_P = math.ceil(self.n_players / 2)
        self.exploitation_threshold_E = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        my_previous_action = history.my_actions[current_round - 1]
        previous_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        all_previous_actions = np.append(my_previous_action, previous_round_opponent_actions)
        num_cooperators_prev = np.sum(all_previous_actions)
        num_defectors_prev = self.n_players - num_cooperators_prev
        k_over_n = self.k / self.n_players
        if not self.cooperation_is_individually_rational:
            return Action.D
        if num_defectors_prev >= self.punishment_threshold_P:
            return Action.D
        elif num_cooperators_prev >= self.exploitation_threshold_E:
            estimated_others_cooperating = max(0, num_cooperators_prev - 1)
            payoff_if_cooperate = k_over_n * (1 + estimated_others_cooperating)
            payoff_if_defect = 1 + k_over_n * estimated_others_cooperating
            if payoff_if_cooperate > payoff_if_defect:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_55(BaseStrategy):
    """
    The Punitive Predator Strategy:
    This strategy is aggressive by default, only considering cooperation
    under very specific, historically validated circumstances. It punishes
    any deviation that would have made defecting more profitable, leading
    to permanent defection once this "betrayal" is discovered.

    Decision Logic:
    1. Default Action: Defect (D).
    2. First/Last Round: Always Defect (D).
    3. If Cooperation was Betrayed: Always Defect (D).
    4. If all other n-1 players cooperated in the previous round:
       Check if cooperating would have yielded a strictly lower payoff for itself
       than defecting would have.
       - Payoff if player i cooperated (assuming all n players cooperated):
         _C = 0 + (k/n) * n
       - Hypothetical payoff if player i had defected (assuming n-1 others cooperated):
         _D = 1 + (k/n) * (n - 1)
       If _C < _D (which simplifies to k < n, always true for valid games),
       this constitutes a "betrayal" of its own self-interest. The
       `cooperation_betrayed` flag is set to True, and the strategy will
       permanently Defect.
       Otherwise (if _C >= _D), Cooperate.
    5. If not all other n-1 players cooperated in the previous round:
       Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_betrayed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.r - 1:
            return Action.D
        if self.cooperation_betrayed:
            return Action.D
        all_others_cooperated_prev = True
        if history is not None and history.round_number > 0:
            for opponent_idx in range(self.n - 1):
                if history.opponent_actions[history.round_number - 1, opponent_idx] == Action.D.value:
                    all_others_cooperated_prev = False
                    break
        else:
            all_others_cooperated_prev = False
        if all_others_cooperated_prev:
            payoff_coop_prev = self.k / self.n * self.n
            payoff_defect_hypothetical_prev = 1 + self.k / self.n * (self.n - 1)
            if payoff_coop_prev < payoff_defect_hypothetical_prev:
                self.cooperation_betrayed = True
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_56(BaseStrategy):
    """
    The Dominator strategy aggressively maximizes individual payoff in the N-Player Public Goods Game.
    It defaults to defecting but cooperates strategically to exploit others' contributions,
    prioritizing situations where its own payoff is guaranteed to increase.
    It punishes persistent defectors indirectly by withdrawing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.game_description = game_description

    def _get_action_from_bool(self, action_bool: bool) -> Action:
        return Action.C if action_bool else Action.D

    def _get_bool_from_action(self, action: Action) -> bool:
        return action == Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        if self.r - 1 - current_round < 2:
            return Action.D
        total_past_cooperators_count = 0
        num_past_rounds = history.round_number
        if num_past_rounds > 0:
            total_my_coops = np.sum(history.my_actions[:num_past_rounds])
            total_opponent_coops = np.sum(history.opponent_actions[:num_past_rounds, :])
            total_past_cooperators_count = total_my_coops + total_opponent_coops
        avg_past_cooperators_rate = 0.0
        if num_past_rounds > 0:
            avg_past_cooperators_rate = total_past_cooperators_count / (num_past_rounds * self.n)
        decision_bool = False
        cooperate_threshold_avg_rate = 0.7
        cooperate_threshold_low_coop_last_round = self.n / self.k
        if avg_past_cooperators_rate > cooperate_threshold_avg_rate:
            decision_bool = True
        cooperators_last_round_count = 0
        if num_past_rounds > 0:
            cooperators_last_round_count += history.my_actions[num_past_rounds - 1]
            cooperators_last_round_count += np.sum(history.opponent_actions[num_past_rounds - 1, :])
        if not decision_bool and cooperators_last_round_count < cooperate_threshold_low_coop_last_round:
            if random.random() < 0.4:
                decision_bool = True
        return self._get_action_from_bool(decision_bool)

class Strategy_AGGRESSIVE_57(BaseStrategy):
    """
    The "Exploiter" strategy for the N-Player Public Goods Game.
    This strategy is designed to maximize individual payoff by being aggressively
    self-interested. It exploits cooperative behavior while minimizing vulnerability
    to defection. It cooperates only when it demonstrably benefits more than defecting,
    and defects whenever the benefit of cooperation is marginal or uncertain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_cooperation_threshold = 0.0
        self.current_defection_punishment_level = 0.0
        self.expected_cooperators_for_own_cooperation = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if history is None or current_round_num == 0:
            self.current_cooperation_threshold = math.ceil(self.n * 0.5)
            self.current_defection_punishment_level = math.ceil(self.n * 0.2)
            self.expected_cooperators_for_own_cooperation = math.ceil(self.n / self.k)
            return Action.D
        if current_round_num == self.r:
            return Action.D
        observed_cooperators_last_round = int(np.sum(history.opponent_actions[-1, :])) + (1 if history.my_actions[-1] else 0)
        action = Action.D
        new_cooperation_threshold = self.current_cooperation_threshold
        new_defection_punishment_level = self.current_defection_punishment_level
        if observed_cooperators_last_round >= self.expected_cooperators_for_own_cooperation and observed_cooperators_last_round >= self.current_cooperation_threshold:
            action = Action.C
            new_cooperation_threshold = max(1.0, self.current_cooperation_threshold * 0.95)
        elif observed_cooperators_last_round >= self.current_defection_punishment_level:
            action = Action.D
            new_cooperation_threshold = min(float(self.n), self.current_cooperation_threshold * 1.1)
        else:
            action = Action.D
            new_cooperation_threshold = min(float(self.n), self.current_cooperation_threshold * 1.25)
            new_defection_punishment_level = min(float(self.n), self.current_defection_punishment_level * 1.1)
        self.current_cooperation_threshold = new_cooperation_threshold
        self.current_defection_punishment_level = new_defection_punishment_level
        return action

class Strategy_AGGRESSIVE_58(BaseStrategy):
    """
    "The Opportunist" - An aggressive strategy for the N-Player Public Goods Game.
    This strategy prioritizes maximizing individual payoff by defecting by default,
    and cooperating only when it's demonstrably and immediately beneficial for the player.
    It exploits perceived opponent cooperation but never builds trust.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        num_previous_rounds = history.round_number
        if num_previous_rounds == 0:
            estimated_other_cooperators_rate = 0
        else:
            total_opponent_cooperators = np.sum(history.opponent_actions)
            total_observations = num_previous_rounds * (self.n - 1)
            if total_observations == 0:
                estimated_other_cooperators_rate = 0
            else:
                estimated_other_cooperators_rate = total_opponent_cooperators / total_observations
        estimated_num_other_cooperators = estimated_other_cooperators_rate * (self.n - 1)
        E_payoff_C = self.k / self.n * (1 + estimated_num_other_cooperators)
        E_payoff_D = 1 + self.k / self.n * estimated_num_other_cooperators
        if E_payoff_D > E_payoff_C:
            return Action.D
        elif E_payoff_C > E_payoff_D:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_59(BaseStrategy):
    """
    Opportunistic Exploiter strategy for the N-Player Public Goods Game.
    This strategy prioritizes self-interest by defaulting to defection when
    the observed cooperation rate is high, exploiting others' generosity.
    It uses a limited tolerance for low cooperation before a strategic
    cooperation reset to potentially influence the environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.DEFECT_TOLERANCE = 2
        self.consecutive_low_ocr_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None or state.round_number == 0:
            self.consecutive_low_ocr_defections = 0
            return Action.C
        current_round_idx = state.round_number - 1
        if state.round_number == r:
            return Action.D
        previous_round_actions = history.my_actions[current_round_idx]
        total_previous_round_cooperators = np.sum(previous_round_actions) + np.sum(history.opponent_actions[current_round_idx, :])
        observed_cooperation_rate = total_previous_round_cooperators / n
        if observed_cooperation_rate <= 0.2:
            if previous_round_actions == Action.D.value:
                self.consecutive_low_ocr_defections += 1
            else:
                self.consecutive_low_ocr_defections = 0
        else:
            self.consecutive_low_ocr_defections = 0
        if observed_cooperation_rate > 0.5:
            return Action.D
        elif observed_cooperation_rate <= 0.2:
            if self.consecutive_low_ocr_defections < self.DEFECT_TOLERANCE:
                return Action.D
            else:
                self.consecutive_low_ocr_defections = 0
                return Action.C
        else:
            return Action.C

class Strategy_AGGRESSIVE_60(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy for the N-Player Public Goods Game.
    This strategy defaults to defection, but will cooperate if past rounds
    showed a sufficiently high level of cooperation yielding positive returns
    for cooperators, and if there were more cooperators than defectors.
    It aggressively punishes low levels of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        r = self.r
        k = self.k
        if history is None:
            return Action.D
        current_round = history.round_number
        if current_round == r:
            return Action.D
        previous_round_idx = current_round - 1
        previous_round_cooperators_bool = history.my_actions[previous_round_idx]
        previous_round_opp_cooperators_bool = history.opponent_actions[previous_round_idx, :]
        num_cooperators_prev_round = int(previous_round_cooperators_bool) + np.sum(previous_round_opp_cooperators_bool)
        num_defectors_prev_round = n - num_cooperators_prev_round
        if num_cooperators_prev_round <= num_defectors_prev_round:
            return Action.D
        sufficient_cooperation_level = num_cooperators_prev_round >= n / 2.0
        average_cooperator_payoff_prev_round = k / n * num_cooperators_prev_round
        positive_return_on_investment = average_cooperator_payoff_prev_round > 1.0
        if sufficient_cooperation_level and positive_return_on_investment:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_61(BaseStrategy):
    """
    The "Aggressive Opportunist" strategy for the N-Player Public Goods Game.

    This strategy is fundamentally driven by self-interest and a desire to maximize
    individual payoff. It aims to leverage periods of cooperation from others to
    its advantage, but immediately punishes any deviation from cooperation by
    switching to defection. The core principle is to extract as much as possible
    when opportunities arise and to shut down exploitation by others.

    Decision Rules:
    1.  Initial Stance (Round 1): Defect.
    2.  The "N-1" Rule: If all other n-1 players cooperated in the previous round,
        defect in the current round to exploit.
    3.  Conditional Cooperation: Cooperate if at least 2 players cooperated in the
        previous round AND the strategy was not exploited.
    4.  Immediate Retaliation: Defect if the strategy cooperated in the previous
        round and was exploited (not all others cooperated).
    5.  Insufficient Cooperation: Defect if fewer than 2 players cooperated in the
        previous round.
    6.  Last Round (Round r): Always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        my_previous_action = self.get_my_previous_action(history)
        total_cooperators_previous_round = self.get_total_cooperators_previous_round(history)
        if total_cooperators_previous_round == self.n - 1:
            return Action.D
        if my_previous_action == Action.C and total_cooperators_previous_round < self.n:
            return Action.D
        if total_cooperators_previous_round < 2:
            return Action.D
        return Action.C

    def get_my_previous_action(self, history: PlayerHistory) -> Action:
        """Retrieves this player's action from the previous round."""
        if len(history.my_actions) > 0:
            return Action.C if history.my_actions[-1] else Action.D
        return Action.D

    def get_total_cooperators_previous_round(self, history: PlayerHistory) -> int:
        """Calculates the total number of cooperators in the previous round."""
        if len(history.my_actions) == 0:
            return 0
        this_player_cooperated = history.my_actions[-1]
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        return int(this_player_cooperated) + int(opponent_cooperators)

class Strategy_AGGRESSIVE_62(BaseStrategy):
    """
    Aggressive "Tit-for-Tat" with an Advantage.
    This strategy defaults to defection and only cooperates under strict conditions of
    unanimous past cooperation. It punishes any defection swiftly and definitively,
    especially in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_player_index = -1

    def set_player_index(self, player_index: int):
        """Sets the player index for this strategy instance."""
        self.my_player_index = player_index

    def _get_my_payoff_in_previous_round(self, history: PlayerHistory) -> float:
        """Helper to get my payoff from the last completed round."""
        if history is None or len(history.my_payoffs) == 0:
            return 0.0
        return history.my_payoffs[-1]

    def _get_total_cooperators_in_previous_round(self, history: PlayerHistory) -> int:
        """Helper to count total cooperators in the last completed round."""
        if history is None or len(history.my_actions) == 0:
            return 0
        return int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])

    def _has_any_other_player_defected_in_history(self, history: PlayerHistory) -> bool:
        """Checks if any other player has defected in any round prior to the last."""
        if history is None or len(history.opponent_actions) == 0:
            return False
        for round_idx in range(len(history.my_actions) - 1):
            if np.any(history.opponent_actions[round_idx, :] == Action.D.value):
                return True
        return False

    def _has_any_other_player_defected_in_previous_round(self, history: PlayerHistory) -> bool:
        """Checks if any other player defected in the immediately previous round."""
        if history is None or len(history.opponent_actions) == 0:
            return False
        return np.any(history.opponent_actions[-1, :] == Action.D.value)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_num = state.round_number
        if current_round_num == 0:
            return Action.D
        if current_round_num == r - 1:
            return Action.D
        last_player_i_action = Action(history.my_actions[-1])
        last_total_cooperators = self._get_total_cooperators_in_previous_round(history)
        last_player_i_payoff = self._get_my_payoff_in_previous_round(history)
        any_other_player_defected_last_round = self._has_any_other_player_defected_in_previous_round(history)
        history_of_other_defections = self._has_any_other_player_defected_in_history(history)
        if any_other_player_defected_last_round:
            return Action.D
        if last_player_i_action == Action.C and last_total_cooperators < n:
            return Action.D
        if last_total_cooperators == n and (not history_of_other_defections):
            if last_player_i_payoff >= 1.0:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_63(BaseStrategy):
    """
    Dominant Retaliation Strategy:
    This strategy prioritizes individual gain by defecting by default.
    It cooperates only when there's a significant guaranteed payoff increase
    AND the public good is being efficiently contributed to.
    It retaliates swiftly against exploitation or inefficiency.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_gain_threshold_percent = 0.25
        self.min_cooperators_for_efficient_public_good = math.ceil(self.n * (self.k / self.n))
        self.punishment_mode = False
        self.my_last_action = None

    def _calculate_payoff(self, my_action_bool: bool, all_actions_in_round: NDArray[np.bool_]) -> float:
        """Calculates the payoff for a player given their action and others' actions."""
        c_i = 1 if my_action_bool else 0
        total_contributions = np.sum(all_actions_in_round)
        payoff = 1 - c_i + self.k / self.n * total_contributions
        return payoff

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0 or current_round_idx == self.r - 1:
            return Action.D
        if self.punishment_mode:
            return Action.D
        prev_round_idx = current_round_idx - 1
        cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_payoff_prev_round = history.my_payoffs[prev_round_idx]
        my_action_prev_round_bool = history.my_actions[prev_round_idx]
        if my_payoff_prev_round < 1.0:
            self.punishment_mode = True
            return Action.D
        if not my_action_prev_round_bool and cooperators_in_prev_round < self.min_cooperators_for_efficient_public_good:
            pass
        num_opponents_total = self.n - 1
        num_defectors_in_prev_round = num_opponents_total - np.sum(history.opponent_actions[prev_round_idx, :])
        if num_defectors_in_prev_round > num_opponents_total / 2:
            self.punishment_mode = True
            return Action.D
        if cooperators_in_prev_round >= self.min_cooperators_for_efficient_public_good:
            all_actions_if_i_cooperate = history.opponent_actions[prev_round_idx, :].copy()
            all_actions_if_i_cooperate = np.append(all_actions_if_i_cooperate, True)
            expected_payoff_C = self._calculate_payoff(True, all_actions_if_i_cooperate)
            all_actions_if_i_defect = history.opponent_actions[prev_round_idx, :].copy()
            all_actions_if_i_defect = np.append(all_actions_if_i_defect, False)
            expected_payoff_D = self._calculate_payoff(False, all_actions_if_i_defect)
            if expected_payoff_C > expected_payoff_D * (1 + self.cooperation_gain_threshold_percent):
                self.my_last_action = Action.C
                return Action.C
        self.my_last_action = Action.D
        return Action.D

class Strategy_AGGRESSIVE_64(BaseStrategy):
    """
    Aggressive "Exploit and Deter" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize immediate individual gain by exploiting
    the public good when beneficial, but quickly pivots to pure self-interest
    when cooperation is exploited. It uses a dynamic Cooperation Threshold (CT)
    and a severe punishment phase to deter free-riding.

    Core Philosophy:
    - Aggression: Look for opportunities to benefit from others' cooperation.
    - Exploitation: Capitalize on the public good's benefits when they outweigh
      your contribution cost.
    - Deterrence: Punish free-riders to discourage them from exploiting you.
    - Adaptability: Adjust based on observed group behavior, with a bias towards
      caution and punishment.

    Decision Rules:
    - Round 1: Defect to observe.
    - Subsequent Rounds: Cooperate if `expected_cooperators >= CT`. Otherwise, defect.
    - Dynamic CT:
        - Initial CT: `floor(n / 2) + 1` (requires a majority to cooperate).
        - If exploited (cooperated and got < 1 payoff with few cooperators):
          Trigger a punishment phase, setting `CT = n - 1` for `r/2` rounds.
        - Slow to relax CT; primarily adjusted downwards by exiting punishment.

    Last Round: Always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 0
        self.punishment_phase_end_round = -1
        self.my_player_id = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.cooperation_threshold = self.n // 2 + 1
            self.punishment_phase_end_round = -1
            return Action.D
        if current_round == self.r:
            return Action.D
        if self.my_player_id == -1:
            self.my_player_id = len(history.my_actions)
            if self.cooperation_threshold == 0:
                self.cooperation_threshold = self.n // 2 + 1
            if self.punishment_phase_end_round == -1:
                self.punishment_phase_end_round = -1
        observed_cooperators_last_round = 0
        if history.my_actions[-1] == Action.C.value:
            observed_cooperators_last_round += 1
        if self.n > 1:
            observed_cooperators_last_round += np.sum(history.opponent_actions[-1, :])
        expected_cooperators = observed_cooperators_last_round
        if expected_cooperators >= self.cooperation_threshold and self.cooperation_threshold < self.n:
            action = Action.C
        else:
            action = Action.D
        return action

    def update_strategy(self, state: GameState, history: PlayerHistory) -> None:
        """
        Updates strategy state based on the outcome of the completed round.
        This method is called *after* the round has finished and payoffs are known.
        """
        current_round = state.round_number
        if current_round == self.r:
            return
        my_last_action = Action.D if not history.my_actions[-1] else Action.C
        my_last_payoff = history.my_payoffs[-1]
        if my_last_action == Action.C:
            num_cooperators_this_round = 0
            if history.my_actions[-1]:
                num_cooperators_this_round += 1
            if self.n > 1:
                num_cooperators_this_round += np.sum(history.opponent_actions[-1, :])
            if num_cooperators_this_round < self.n // 2 and my_last_payoff < 1.0:
                self.punishment_phase_end_round = current_round + self.r // 2
                self.cooperation_threshold = self.n - 1
                return
        if current_round > self.punishment_phase_end_round:
            observed_cooperators_last_round = 0
            if history.my_actions[-1] == Action.C.value:
                observed_cooperators_last_round += 1
            if self.n > 1:
                observed_cooperators_last_round += np.sum(history.opponent_actions[-1, :])
            if observed_cooperators_last_round >= self.cooperation_threshold + 1 and self.cooperation_threshold < self.n // 2 + 1:
                self.cooperation_threshold = max(1, self.cooperation_threshold - 1)

class Strategy_AGGRESSIVE_65(BaseStrategy):
    """
    The Enforcer strategy is an aggressive N-Player Public Goods Game strategy
    that employs a tit-for-tat mechanism with punitive escalation and
    opportunistic defection. It starts with cautious cooperation, but any
    defection by an opponent triggers sustained defection until a high
    threshold of consistent cooperation from all other players is met.
    It also defects unconditionally in the final round and opportunistically
    defects when others cooperate while it's in a punishment phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_mode = False
        self.consecutive_cooperators = 0
        self.required_consecutive_cooperation_for_amnesty = math.ceil(game_description.n_rounds * 0.3)
        if self.required_consecutive_cooperation_for_amnesty < 2:
            self.required_consecutive_cooperation_for_amnesty = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.game_description.n_rounds:
            return Action.D
        if self.punishment_mode:
            all_others_cooperated_this_round = True
            if history is not None and history.opponent_actions.size > 0:
                if current_round > 0 and history.opponent_actions.shape[0] > current_round - 1:
                    opponent_actions_last_round = history.opponent_actions[current_round - 1, :]
                    if np.any(opponent_actions_last_round == Action.D.value):
                        all_others_cooperated_this_round = False
            if all_others_cooperated_this_round:
                self.consecutive_cooperators += 1
                if self.consecutive_cooperators >= self.required_consecutive_cooperation_for_amnesty:
                    self.punishment_mode = False
                    self.consecutive_cooperators = 0
                    return Action.C
                else:
                    return Action.D
            else:
                self.consecutive_cooperators = 0
                return Action.D
        else:
            if history is None or current_round == 0:
                return Action.C
            any_other_player_defected_last_round = False
            if history.opponent_actions.size > 0:
                opponent_actions_last_round = history.opponent_actions[current_round - 1, :]
                if np.any(opponent_actions_last_round == Action.D.value):
                    any_other_player_defected_last_round = True
            if any_other_player_defected_last_round:
                self.punishment_mode = True
                self.consecutive_cooperators = 0
                return Action.D
            else:
                return Action.C

class Strategy_AGGRESSIVE_66(BaseStrategy):
    """
    "The Opportunist" strategy for the N-Player Public Goods Game.
    This strategy defaults to defecting, aiming to exploit cooperative
    tendencies when they are strong, and punishes perceived lack of
    cooperation by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_index = current_round - 1
        all_previous_actions = []
        if self.n > 1:
            all_previous_actions.append(history.my_actions[previous_round_index])
            all_previous_actions.extend(history.opponent_actions[previous_round_index, :])
        else:
            all_previous_actions.append(history.my_actions[previous_round_index])
        total_cooperators_previous_round = sum(all_previous_actions)
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_67(BaseStrategy):
    """
    The "Predator" Strategy: Aggressive Exploitation in the Public Goods Game.
    This strategy aims to maximize its own payoff by exploiting cooperative
    tendencies while minimizing exposure to defections. It starts by defecting,
    observes others' behavior, and only cooperates strategically for a limited
    'exploitation burst' if certain sustained cooperation thresholds are met.
    It immediately reverts to defection if cooperation wavers or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_streak_tracker = {}
        self.my_cooperation_streak = 0
        self.total_cooperators_last_round = 0
        self.current_exploitation_burst_remaining = 0
        self.initial_reluctance_rounds = 3
        self.cooperation_threshold_min_streak = 2
        self.cooperation_threshold_population_share = 0.7
        self.exploitation_burst_length_factor = 2
        self.min_cooperators_for_trigger = math.ceil(self.n_players * 0.5)
        for i in range(self.n_players - 1):
            self.cooperation_streak_tracker[i] = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds - 1:
            return Action.D
        if current_round < self.initial_reluctance_rounds:
            return Action.D
        if history is not None:
            if history.my_actions[-1] == Action.C.value:
                self.my_cooperation_streak += 1
            else:
                self.my_cooperation_streak = 0
            for i in range(self.n_players - 1):
                if history.opponent_actions[-1, i] == Action.C.value:
                    self.cooperation_streak_tracker[i] += 1
                else:
                    self.cooperation_streak_tracker[i] = 0
            total_cooperators = np.sum(history.opponent_actions[-1, :])
            if history.my_actions[-1] == Action.C.value:
                total_cooperators += 1
            self.total_cooperators_last_round = total_cooperators
        if self.current_exploitation_burst_remaining > 0:
            self.current_exploitation_burst_remaining -= 1
            return Action.C
        observed_cooperators_in_streak = 0
        for i in range(self.n_players - 1):
            if self.cooperation_streak_tracker[i] >= self.cooperation_threshold_min_streak:
                observed_cooperators_in_streak += 1
        required_streak_players = math.ceil((self.n_players - 1) * self.cooperation_threshold_population_share)
        condition_1_met = observed_cooperators_in_streak >= required_streak_players
        condition_2_met = self.total_cooperators_last_round >= self.min_cooperators_for_trigger
        if condition_1_met and condition_2_met:
            remaining_rounds = self.n_rounds - 1 - current_round
            max_possible_burst = remaining_rounds
            calculated_burst = math.floor(self.exploitation_burst_length_factor * self.initial_reluctance_rounds)
            burst_length = min(max_possible_burst, calculated_burst)
            if burst_length > 0:
                self.current_exploitation_burst_remaining = burst_length
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_68(BaseStrategy):
    """
    Aggressive Strategy: "Exploit and Punish"
    This strategy aims to maximize individual payoff by exploiting cooperation
    and punishing deviations. It cooperates initially, then defects if others
    outperform it while it cooperated, or if cooperation levels drop too low.
    It always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_rounds = max(2, math.floor(self.r / 4))
        self.punishment_threshold = math.floor(self.n / 2)
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        my_previous_action_idx = history.my_actions[current_round - 2]
        my_previous_action = Action(my_previous_action_idx)
        my_previous_payoff = history.my_payoffs[current_round - 2]
        opponent_payoffs_prev_round = history.opponent_payoffs[current_round - 2, :]
        average_opponent_payoff_prev_round = np.mean(opponent_payoffs_prev_round)
        total_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 2, :])
        if current_round == self.r:
            return Action.D
        if current_round <= self.initial_rounds:
            return Action.C
        else:
            if my_previous_action == Action.C and average_opponent_payoff_prev_round > my_previous_payoff:
                return Action.D
            if total_cooperators_prev_round <= self.punishment_threshold:
                return Action.D
            return Action.C

class Strategy_AGGRESSIVE_69(BaseStrategy):
    """
    The "Predator" strategy is designed for maximum individual gain in a repeated
    Public Goods Game. It aggressively defects by default, but will temporarily
    cooperate if it observes that cooperating was significantly more profitable
    for others than defecting would have been. If this "bait" condition is not
    met, or if it perceives it was exploited, it permanently reverts to defecting.
    It always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        previous_round_idx = current_round - 1
        my_action_prev = self.bool_to_action(history.my_actions[previous_round_idx])
        my_payoff_prev = history.my_payoffs[previous_round_idx]
        num_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :]) + (1 if history.my_actions[previous_round_idx] else 0)
        expected_payoff_if_defect_in_prev = 1.0 + self.k / self.n * num_cooperators_prev
        bait_condition_met = num_cooperators_prev > self.n / self.k
        if bait_condition_met:
            return Action.C
        elif my_action_prev == Action.C:
            if my_payoff_prev < expected_payoff_if_defect_in_prev:
                return Action.D
        return Action.D

    def bool_to_action(self, b: bool) -> Action:
        """Converts boolean (True=C, False=D) to Action enum."""
        return Action.C if b else Action.D

class Strategy_AGGRESSIVE_70(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy for the N-Player Public Goods Game.
    This strategy prioritizes maximizing individual gain by exploiting cooperative
    tendencies in others, while minimizing vulnerability to exploitation. It is
    aggressive and defaults to defection, only cooperating under very specific
    and rare conditions that guarantee a significant immediate benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_player_index = -1
        self.cooperation_benefit_threshold = 1.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None:
            return Action.D
        current_round = state.round_number
        previous_round_actions = history.opponent_actions[current_round - 1]
        my_previous_action = history.my_actions[current_round - 1]
        my_previous_payoff = history.my_payoffs[current_round - 1]
        all_previous_actions = np.append(previous_round_actions, my_previous_action)
        num_cooperators_prev_round = np.sum(all_previous_actions)
        if current_round == r:
            return Action.D
        action_to_take = Action.D
        if my_previous_action == Action.C.value and num_cooperators_prev_round == n:
            payoff_if_defected_prev = 1 + k / n * (num_cooperators_prev_round - 1)
            if my_previous_payoff > payoff_if_defected_prev * self.cooperation_benefit_threshold:
                action_to_take = Action.C
        if action_to_take == Action.D:
            min_cooperators_for_benefit = math.ceil(n / k)
            if num_cooperators_prev_round < min_cooperators_for_benefit:
                action_to_take = Action.D
        return action_to_take

class Strategy_AGGRESSIVE_71(BaseStrategy):
    """
    The "Always Exploit, Then Retaliate" strategy.

    This strategy defaults to defecting to maximize individual gain,
    unless all players (including itself) cooperated in the previous round.
    In that specific case, it cooperates strategically to maintain the
    potential for the highest collective payoff, which is beneficial
    to its own total accumulation. It defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        prev_round_idx = current_round - 1
        all_actions_prev_round = np.append(history.opponent_actions[prev_round_idx, :], [history.my_actions[prev_round_idx]])
        num_cooperators_last_round = np.sum(all_actions_prev_round)
        if num_cooperators_last_round == self.n:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_72(BaseStrategy):
    """
    Aggressive strategy: "Exploit and Endure".
    This strategy defaults to Defect (D), only cooperating when it demonstrably
    increases the player's own payoff in the current round. It punishes
    defection swiftly and is impatient with free-riders. Cooperation is tactical
    and conditional on a clear benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MAX_ROUNDS_STICKY_DEFECTION = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.n
        k = self.k
        r = self.r
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        opp_actions_prev_round = history.opponent_actions[prev_round_idx, :]
        all_actions_prev_round = np.append(my_action_prev_round, opp_actions_prev_round)
        total_cooperators_last_round = np.sum(all_actions_prev_round)
        num_defectors_last_round = n - total_cooperators_last_round
        consecutive_defections = 0
        if not my_action_prev_round:
            for i in range(prev_round_idx, -1, -1):
                if not history.my_actions[i]:
                    consecutive_defections += 1
                else:
                    break
        if my_action_prev_round and num_defectors_last_round > 0:
            return Action.D
        if total_cooperators_last_round > n / 2:
            my_potential_cooperate_payoff = 1 - 1 + k / n * (total_cooperators_last_round + 1)
            my_potential_defect_payoff = 1 - 0 + k / n * total_cooperators_last_round
            if my_potential_cooperate_payoff > my_potential_defect_payoff:
                return Action.C
        if consecutive_defections >= self.MAX_ROUNDS_STICKY_DEFECTION and num_defectors_last_round >= n / 2:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_73(BaseStrategy):
    """
    Opportunistic Exploiter strategy for the N-Player Public Goods Game.

    This strategy defaults to defection, aiming to maximize individual payoff
    by exploiting cooperative outcomes when profitable. It only cooperates
    under a specific "sweet spot" condition where a significant majority
    of players have cooperated in the previous round, and this cooperation
    yields a payoff at least as good as a guaranteed individual gain.
    It is aggressive by always prioritizing self-interest and never
    initiating cooperation to fix a collapsed game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_last_action == Action.C else 0)
        cooperate_condition = num_cooperators_last_round > self.n - self.k and self.k / self.n * (num_cooperators_last_round + 1) >= 1
        if cooperate_condition:
            self.my_last_action = Action.C
            return Action.C
        else:
            self.my_last_action = Action.D
            return Action.D

    @property
    def my_last_action(self) -> Action:
        """Helper to get the agent's last action."""
        if hasattr(self, '_my_last_action'):
            return self._my_last_action
        return Action.D

    @my_last_action.setter
    def my_last_action(self, action: Action):
        """Helper to set the agent's last action."""
        self._my_last_action = action

class Strategy_AGGRESSIVE_74(BaseStrategy):
    """
    The "Exploitative Retaliator" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual gains by exploiting cooperation
    while retaliating against widespread defection and self-preserving
    when personal payoffs are too low despite others cooperating.

    Core Philosophy:
    - Exploit the benevolent: When others cooperate, defect to maximize individual gain.
    - Deter and punish: If personal payoffs indicate being exploited or if overall
      cooperation is low, react defensively.
    - Self-preservation: If the game consistently results in low payoffs,
      cooperate to attempt to boost the public good.

    Decision Rules (for round t > 1):
    1. Get previous round's total cooperators (C_t-1) and own payoff (pi_i,t-1).
    2. If C_t-1 < floor(n/2): Play Defect (D). (Aggressive default)
    3. Else if pi_i,t-1 < exploitation_threshold AND C_t-1 < n: Play Cooperate (C).
       (Safety net: Cooperate if personal payoff was unexpectedly low and not everyone cooperated)
    4. Else: Play Defect (D). (Aggressive exploitation when others cooperate and you're doing well)

    Edge Cases:
    - First Round (t=1): Cooperate (C) to gather information and avoid immediate retaliation.
    - Last Round (t=r): Defect (D) to maximize final round payoff.

    Exploitation Threshold:
    - Defined as 1 + (k/n) * (n-1) - epsilon, where epsilon is a small buffer.
      This represents the expected payoff when defecting while n-1 others cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 1
        total_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        my_action_prev = history.my_actions[previous_round_idx]
        total_cooperators_prev_with_self = total_cooperators_prev + my_action_prev
        my_payoff_prev = history.my_payoffs[previous_round_idx]
        exploitation_threshold = 1.0 + self.k / self.n * (self.n - 1.0) - self.epsilon
        if total_cooperators_prev_with_self < math.floor(self.n / 2.0):
            return Action.D
        elif my_payoff_prev < exploitation_threshold and total_cooperators_prev_with_self < self.n:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_75(BaseStrategy):
    """
    Aggressive "Exploit & Retaliate" strategy for the N-Player Public Goods Game.
    This strategy defaults to Defect (D) and only cooperates if specific conditions
    are met: the number of cooperators in the previous round was above the
    individually rational threshold (n/k), and the player's own payoff in the
    previous round was less than the maximum (2). It's aggressive by exploiting
    profitable situations and conditionally retaliating through default defection
    when conditions aren't met.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.n_div_k_threshold = self.n_players / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if history is None:
            return Action.D
        if current_round_idx == self.n_rounds:
            return Action.D
        previous_round_idx = current_round_idx - 1
        total_cooperators_previous = np.sum(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx])
        my_payoff_previous = history.my_payoffs[previous_round_idx]
        if total_cooperators_previous > self.n_div_k_threshold and my_payoff_previous < 2.0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_76(BaseStrategy):
    """
    Aggressive "Exploiter" Strategy for the N-Player Public Goods Game.

    This strategy prioritizes individual payoff by defecting unless there's a
    clear and direct benefit to cooperating. It aims to exploit high levels of
    cooperation from others to maximize personal returns. Cooperation is viewed
    as a tool for profit, not as a sign of trust or a means to build collective
    welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.rounds_until_last_round_check = math.floor(game_description.n_rounds / 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: Historical data of actions and payoffs for all players.
                     None for the first round.

        Returns:
            Action.C if Cooperate, Action.D if Defect.
        """
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == r - 1:
            return Action.D
        my_past_cooperators = np.sum(history.my_actions[:current_round])
        opponent_past_cooperators_per_round = np.sum(history.opponent_actions[:current_round, :], axis=1)
        total_cooperators_in_previous_rounds = my_past_cooperators + opponent_past_cooperators_per_round
        average_cooperators_in_previous_rounds = np.mean(total_cooperators_in_previous_rounds) if current_round > 0 else 0
        break_even_threshold = n * (k - 1) / k
        if average_cooperators_in_previous_rounds < break_even_threshold:
            return Action.D
        else:
            if current_round > 0:
                last_round_my_action = history.my_actions[current_round - 1]
                last_round_opponent_actions = history.opponent_actions[current_round - 1, :]
                last_round_defectors = n - 1 - np.sum(last_round_opponent_actions)
                if not last_round_my_action:
                    last_round_defectors += 1
            else:
                last_round_defectors = n
            if last_round_defectors == 0:
                return Action.C
            if current_round % self.rounds_until_last_round_check == 0:
                return Action.C
            return Action.D

class Strategy_AGGRESSIVE_77(BaseStrategy):
    """
    The "Opportunistic Predator" strategy for the N-Player Public Goods Game.
    This strategy is designed to maximize individual payoff by defaulting to defection,
    cooperating only in rare, highly strategic situations, and employing harsh,
    permanent punishment for perceived betrayals by the group.

    Core Philosophy: "Maximize personal gain. Exploit hyper-cooperation. Punish any deviation."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.is_permanently_defecting = False
        self.cooperation_threshold = math.floor(self.game_description.k)

    def _count_cooperators_in_prev_round(self, history: PlayerHistory) -> int:
        """Counts the number of cooperators in the previous round among all players."""
        if history.round_number == 0:
            return 0
        previous_round_idx = history.round_number - 1
        num_opp_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
        if history.my_actions[previous_round_idx]:
            return num_opp_cooperators + 1
        else:
            return num_opp_cooperators

    def _get_total_cooperators_this_round(self, history: PlayerHistory) -> int | None:
        """
        Attempts to get the total number of cooperators in the current round.
        This is usually unavailable for simultaneous move games until the next round.
        However, if the history object is designed to reflect outcomes up to the *end* of the current round
        before the strategy is called for the *next* round, this might be possible.
        For this implementation, we'll assume we can access it if available.
        If not available, we'll return None.
        """
        return None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.is_permanently_defecting:
            return Action.D
        if history is None or state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        n = self.game_description.n_players
        k = self.game_description.k
        num_cooperators_prev_round = self._count_cooperators_in_prev_round(history)
        should_tentatively_cooperate = num_cooperators_prev_round == n - 1
        action_to_take = Action.D
        if should_tentatively_cooperate:
            action_to_take = Action.C
            if num_cooperators_prev_round <= self.cooperation_threshold:
                self.is_permanently_defecting = True
                return Action.D
        return action_to_take

class Strategy_AGGRESSIVE_78(BaseStrategy):
    """
    Aggressive Strategy: "Exploiter with a Trigger" for N-Player Public Goods Game.
    This strategy prioritizes maximizing individual payoff by defecting opportunistically,
    punishing perceived exploitation, and adapting to the game environment.

    Core Philosophy:
    - Maximize individual gain.
    - Exploit cooperation when it's individually rational.
    - Punish defection or exploitation aggressively.
    - Adapt to the observed behavior of other players.

    Decision Rules Summary:
    - Round 1: Always Defect (D).
    - Last Round: Always Defect (D).
    - Punishment Mode: Always Defect (D). Triggered by being exploited. Exits if majority of others also defect for consecutive rounds.
    - Normal Mode: Cooperate (C) if the proportion of cooperators in the previous round
      exceeds a dynamic `cooperation_threshold`. Otherwise, Defect (D).
    - `cooperation_threshold` dynamically adjusts: decreases with sustained cooperation,
      increases with sustained defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.punishment_mode = False
        self.cooperation_threshold = 0.8
        self.punishment_rounds_consecutive_defection = 0
        self.punishment_exit_majority_threshold = math.ceil((self.n_players - 1) / 2)
        self.punishment_exit_consecutive_rounds_needed = 2
        self.cooperation_met_consecutive = 0
        self.cooperation_missed_consecutive = 0
        self.threshold_adjustment_frequency = 3

    def _get_player_action_bool(self, action: Action) -> bool:
        """Converts Action enum to boolean (True for C, False for D)."""
        return action == Action.C

    def _get_action_from_bool(self, bool_action: bool) -> Action:
        """Converts boolean to Action enum."""
        return Action.C if bool_action else Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds:
            return Action.D
        if history is None:
            self.punishment_mode = False
            self.cooperation_threshold = 0.8
            self.punishment_rounds_consecutive_defection = 0
            self.cooperation_met_consecutive = 0
            self.cooperation_missed_consecutive = 0
            return Action.D
        last_round_index = current_round_number - 1
        my_action_last_round = self._get_action_from_bool(history.my_actions[last_round_index])
        my_payoff_last_round = history.my_payoffs[last_round_index]
        total_opponents = self.n_players - 1
        opponents_actions_last_round = history.opponent_actions[last_round_index, :]
        total_cooperators_last_round = np.sum(opponents_actions_last_round)
        proportion_cooperators_last_round = total_cooperators_last_round / total_opponents if total_opponents > 0 else 0
        potential_payoff_if_defected_last_round = 1 + self.k / self.n_players * total_cooperators_last_round
        if not self.punishment_mode and my_action_last_round == Action.C and (my_payoff_last_round < potential_payoff_if_defected_last_round):
            self.punishment_mode = True
            self.punishment_rounds_consecutive_defection = 0
            self.cooperation_met_consecutive = 0
            self.cooperation_missed_consecutive = 0
        if self.punishment_mode:
            action_to_take = Action.D
            if my_action_last_round == Action.D:
                other_players_defecting_last_round = total_opponents - total_cooperators_last_round
                if other_players_defecting_last_round >= self.punishment_exit_majority_threshold:
                    self.punishment_rounds_consecutive_defection += 1
                else:
                    self.punishment_rounds_consecutive_defection = 0
                if self.punishment_rounds_consecutive_defection >= self.punishment_exit_consecutive_rounds_needed:
                    self.punishment_mode = False
                    self.punishment_rounds_consecutive_defection = 0
                    self.cooperation_met_consecutive = 0
                    self.cooperation_missed_consecutive = 0
            else:
                self.punishment_rounds_consecutive_defection = 0
            return action_to_take
        else:
            if proportion_cooperators_last_round >= self.cooperation_threshold:
                self.cooperation_met_consecutive += 1
                self.cooperation_missed_consecutive = 0
            else:
                self.cooperation_met_consecutive = 0
                self.cooperation_missed_consecutive += 1
            if self.cooperation_met_consecutive >= self.threshold_adjustment_frequency:
                self.cooperation_threshold = max(0.3, self.cooperation_threshold - 0.05)
                self.cooperation_met_consecutive = 0
            elif self.cooperation_missed_consecutive >= self.threshold_adjustment_frequency:
                self.cooperation_threshold = min(0.95, self.cooperation_threshold + 0.05)
                self.cooperation_missed_consecutive = 0
            if proportion_cooperators_last_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_79(BaseStrategy):
    """
    The "Enforcer" strategy for the N-Player Public Goods Game.

    This strategy is aggressive, prioritizing immediate individual gain.
    It defects in the first and last rounds. In intermediate rounds,
    it defects if any player defected in the previous round while it cooperated,
    or if defecting provides a strictly higher payoff than cooperating,
    using an aggressive tie-breaker in favor of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.n_players
        k = self.k
        r = self.n_rounds
        if history is None:
            return Action.D
        if current_round == r:
            return Action.D
        previous_actions_self = history.my_actions[current_round - 1]
        previous_actions_opponents = history.opponent_actions[current_round - 1, :]
        previous_actions_self_int = int(previous_actions_self)
        previous_actions_opponents_int = previous_actions_opponents.astype(int)
        previous_cooperators_count = np.sum(previous_actions_opponents_int) + previous_actions_self_int
        num_defecters_last_round = n - previous_cooperators_count
        if num_defecters_last_round > 0 and previous_actions_self_int == 1:
            return Action.D
        payoff_cooperate = 1 - 1 + k / n * (previous_cooperators_count + 1)
        payoff_defect = 1 - 0 + k / n * previous_cooperators_count
        if payoff_defect > payoff_cooperate:
            return Action.D
        elif payoff_cooperate > payoff_defect:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_80(BaseStrategy):
    """
    The Exploiter strategy for the N-Player Public Goods Game.

    This strategy is aggressive and prioritizes individual payoff.
    It defaults to defection, cooperating only when the potential payoff
    is significantly higher than defecting, after accounting for risk.
    It includes a punishment mechanism for exploitation and a subtle reward
    for sustained group cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._base_profitability_margin = 0.1
        self._punishment_duration = min(3, math.ceil(self.r / 4))
        self._cooperation_streak_length = min(5, math.ceil(self.r / 3))
        self._reward_margin_reduction = 0.05
        self._punishment_counter = 0
        self._current_profitability_margin = self._base_profitability_margin
        self._current_cooperation_streak = 0
        self._last_round_total_cooperators = 0
        self._my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self._my_last_action = Action.D
            return Action.D
        if current_round == self.r - 1:
            self._my_last_action = Action.D
            return Action.D
        if self._punishment_counter > 0:
            self._punishment_counter -= 1
            self._my_last_action = Action.D
            return Action.D
        my_actions = history.my_actions
        my_payoffs = history.my_payoffs
        opponent_actions = history.opponent_actions
        if current_round > 0:
            self._last_round_total_cooperators = np.sum(my_actions[current_round - 1]) + np.sum(opponent_actions[current_round - 1, :])
        if self._my_last_action == Action.D:
            self._current_cooperation_streak = 0
        elif self._my_last_action == Action.C and self._last_round_total_cooperators >= self.n - 1:
            self._current_cooperation_streak += 1
        elif self._my_last_action == Action.C and self._last_round_total_cooperators < self.n - 1:
            self._current_cooperation_streak = 0
        if self._current_cooperation_streak >= self._cooperation_streak_length:
            self._current_profitability_margin = max(self._base_profitability_margin - self._reward_margin_reduction, 0.01)
        else:
            self._current_profitability_margin = self._base_profitability_margin
        pi_D = 1 + self.k / self.n * self._last_round_total_cooperators
        pi_C = 0 + self.k / self.n * (self._last_round_total_cooperators + 1)
        action_to_take = Action.D
        temptation_occurred = False
        if self._my_last_action == Action.C:
            hypothetical_defect_payoff_last_round = 1 + self.k / self.n * self._last_round_total_cooperators
            if my_payoffs[current_round - 1] < hypothetical_defect_payoff_last_round:
                temptation_occurred = True
        if temptation_occurred:
            self._punishment_counter = self._punishment_duration
            action_to_take = Action.D
        elif pi_C > pi_D + self._current_profitability_margin:
            action_to_take = Action.C
        else:
            action_to_take = Action.D
        self._my_last_action = action_to_take
        return action_to_take

class Strategy_AGGRESSIVE_81(BaseStrategy):
    """
    Ruthless Exploiter strategy:
    Defects in the first round.
    Cooperates only if the majority of OTHER players cooperated in the previous round.
    Otherwise, defects. This is unforgiving of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = math.ceil((self.n - 1) * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_82(BaseStrategy):
    """
    "Exploiter's Echo" strategy for the N-Player Public Goods Game.

    This strategy is designed to be assertive and opportunistic. It cooperates
    in the first round to gauge the group's behavior. In subsequent rounds,
    it cooperates only if all other players cooperated in the previous round.
    If even one player defects, it retaliates by defecting in the current
    round and continues to defect until universal cooperation is restored.
    In the final round, it always defects to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: The history of actions and payoffs for all players.
                     If None, it's the first round (round_number = 0).

        Returns:
            The action (Action.C or Action.D) for the current round.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_number = state.round_number
        if history is None:
            return Action.C
        if current_round_number == r - 1:
            return Action.D
        previous_round_all_actions = np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :]))
        num_cooperators_previous_round = np.sum(previous_round_all_actions)
        if num_cooperators_previous_round == n:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_83(BaseStrategy):
    """
    "Unleash the Herd": An aggressive, self-interested, and adaptive strategy
    for the N-Player Public Goods Game. It aims to maximize individual payoff
    by exploiting cooperative tendencies, punishing defection, and leveraging
    the temptation to defect when most profitable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def _calculate_thresholds(self) -> tuple[int, int]:
        """
        Calculates the Cooperation Threshold (CT) and Punishment Trigger (PT).

        Returns:
            A tuple containing (CT, PT).
        """
        CT = max(1, math.floor(self.n * (self.k - 1) / self.k))
        PT = math.floor(self.n * (1 - self.k / self.n))
        return (CT, PT)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state (e.g., round number).
            history: The history of past actions and payoffs, or None for the first round.

        Returns:
            The action (Action.C or Action.D) to take.
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r:
            return Action.D
        my_last_action_bool = history.my_actions[current_round - 1]
        my_last_action = Action.C if my_last_action_bool else Action.D
        total_cooperators_last_round = np.sum(history.my_actions[current_round - 1]) + np.sum(history.opponent_actions[current_round - 1, :])
        total_defectors_last_round = self.n - total_cooperators_last_round
        CT, PT = self._calculate_thresholds()
        if my_last_action == Action.C:
            if total_cooperators_last_round >= CT:
                return Action.C
            elif total_defectors_last_round >= PT:
                return Action.D
            else:
                return Action.D
        elif total_cooperators_last_round >= CT and total_defectors_last_round < PT:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_84(BaseStrategy):
    """
    "The Opportunist": An aggressive strategy for the N-Player Public Goods Game.
    This strategy defects by default, and cooperates only when it demonstrably
    benefits the player more than defecting, with a strong bias towards maximizing
    immediate private gain. It aims to exploit cooperation when profitable and avoid
    being exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def _calculate_expected_payoff(self, action: Action, avg_cooperators: float) -> float:
        """
        Calculates the expected payoff for a given action based on the average
        number of cooperators observed in previous rounds.
        """
        if action == Action.C:
            total_cooperators = avg_cooperators + 1
            return self.k / self.n * total_cooperators
        else:
            total_cooperators = avg_cooperators
            return 1 + self.k / self.n * total_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        num_completed_rounds = history.round_number
        total_cooperators_all_rounds = np.sum(history.my_actions.astype(float) + np.sum(history.opponent_actions, axis=1))
        avg_cooperators_prev_rounds = total_cooperators_all_rounds / num_completed_rounds if num_completed_rounds > 0 else 0.0
        if self.k / self.n <= 1:
            return Action.D
        else:
            expected_payoff_C = self._calculate_expected_payoff(Action.C, avg_cooperators_prev_rounds)
            expected_payoff_D = self._calculate_expected_payoff(Action.D, avg_cooperators_prev_rounds)
            cooperation_threshold_average = self.n / 3.0
            cooperation_trend_increasing = False
            if num_completed_rounds >= 2:
                my_recent_actions = history.my_actions[-2:]
                if np.all(my_recent_actions) or (my_recent_actions[0] == 0 and my_recent_actions[1] == 1):
                    cooperation_trend_increasing = True
                recent_cooperator_counts = []
                for r in range(max(0, num_completed_rounds - 2), num_completed_rounds):
                    cooperators_in_round = history.my_actions[r] + np.sum(history.opponent_actions[r, :])
                    recent_cooperator_counts.append(cooperators_in_round)
                if len(recent_cooperator_counts) >= 2 and recent_cooperator_counts[-1] > recent_cooperator_counts[-2]:
                    cooperation_trend_increasing = True
            if expected_payoff_C > expected_payoff_D and (avg_cooperators_prev_rounds >= cooperation_threshold_average or (cooperation_trend_increasing and avg_cooperators_prev_rounds >= self.n / 5.0)):
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_85(BaseStrategy):
    """
    The "Exploiter-with-a-Price" strategy.
    This strategy is aggressive and adaptive, prioritizing defection unless
    cooperation demonstrably leads to a higher payoff or is very nearly equal,
    and others are cooperating significantly. It uses a history-based average
    to predict other players' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.n_burn_in = min(self.r, 3)
        self.avg_coop_others = 0.0
        self.initialized = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.initialized = True
            return Action.D
        self._update_state(history)
        if state.round_number <= self.n_burn_in:
            return Action.D
        if state.round_number == self.r:
            if self.k / self.n > 1.0:
                return Action.C
            else:
                return Action.D
        predicted_n_others = round(self.avg_coop_others)
        predicted_n_others = max(0, min(self.n - 1, predicted_n_others))
        payoff_cooperate = self.k / self.n * (1 + predicted_n_others)
        payoff_defect = 1 + self.k / self.n * predicted_n_others
        tolerance = 0.01
        if payoff_cooperate > payoff_defect - tolerance:
            return Action.C
        else:
            return Action.D

    def _update_state(self, history: PlayerHistory):
        """
        Updates the average number of other players who cooperated in previous rounds.
        """
        num_rounds_completed = history.round_number
        if num_rounds_completed == 0:
            self.avg_coop_others = 0.0
            return
        total_other_cooperators_sum = 0
        for round_idx in range(num_rounds_completed):
            current_round_cooperators = np.sum(history.opponent_actions[round_idx, :])
            total_other_cooperators_sum += current_round_cooperators
        self.avg_coop_others = total_other_cooperators_sum / num_rounds_completed

class Strategy_AGGRESSIVE_86(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy for the N-Player Public Goods Game.
    This strategy is aggressive, defaulting to defection to maximize personal gain.
    It cooperates only in a very specific scenario where all other players cooperate,
    guaranteeing the highest possible individual payoff (k). It retaliates by defecting
    if a majority of players defected in the previous round or if it was exploited
    while cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        previous_round_idx = current_round - 1
        total_cooperators_prev = np.sum(history.my_actions[:current_round]) + np.sum(history.opponent_actions[:current_round, :])
        my_action_prev = history.my_actions[previous_round_idx]
        num_other_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        num_other_defectors_prev = self.n - 1 - num_other_cooperators_prev
        if num_other_cooperators_prev == self.n - 1:
            if my_action_prev == Action.D.value:
                return Action.C
            return Action.C
        total_defectors_prev = self.n - 1 - num_other_cooperators_prev + (1 if my_action_prev == Action.D.value else 0)
        if total_defectors_prev >= math.ceil(self.n / 2.0):
            return Action.D
        if my_action_prev == Action.C.value and num_other_defectors_prev >= 1:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_87(BaseStrategy):
    """
    The "Opportunistic Retaliator" strategy for the N-Player Public Goods Game.
    This strategy defects whenever not all players cooperated in the previous round.
    It cooperates only when universal cooperation was achieved in the prior round,
    unless it was personally exploited in the round before last, in which case it retaliates.
    It defects unconditionally in the first and last rounds to maximize immediate gain
    and exploit the lack of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 1:
            return Action.C
        if current_round == r:
            return Action.D
        if history is None:
            return Action.C
        last_round_idx = history.round_number - 1
        num_cooperators_last_round = int(np.sum(history.opponent_actions[last_round_idx, :])) + (1 if history.my_actions[last_round_idx] else 0)
        if num_cooperators_last_round < n:
            return Action.D
        elif history.round_number >= 2:
            round_before_last_idx = history.round_number - 2
            player_i_action_t_minus_2 = history.my_actions[round_before_last_idx]
            num_cooperators_t_minus_2 = int(np.sum(history.opponent_actions[round_before_last_idx, :])) + (1 if player_i_action_t_minus_2 else 0)
            if player_i_action_t_minus_2 == Action.D.value and num_cooperators_t_minus_2 == n - 1:
                return Action.D
            else:
                return Action.C
        else:
            return Action.C

class Strategy_AGGRESSIVE_88(BaseStrategy):
    """
    Aggressive "Exploitative Opportunist" strategy for the N-Player Public Goods Game.
    This strategy defaults to defecting and only cooperates if a clear, immediate
    payoff advantage is present, and a high threshold of cooperation from others is met.
    It punishes perceived exploitation or failed cooperation by reverting to defection
    for a sustained period.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.my_player_index = None
        self.punishment_duration = max(2, math.floor(self.n_rounds * 0.1))
        self.cooperation_threshold_ratio = 0.7
        self.current_punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_num = state.round_number
        if self.current_punishment_counter > 0:
            self.current_punishment_counter -= 1
            return Action.D
        if round_num == self.n_rounds:
            return Action.D
        if round_num == 1:
            return Action.D
        actual_cooperators_last_round_excluding_me = 0
        if history is not None and len(history.my_actions) > 0:
            last_round_opp_actions = history.opponent_actions[round_num - 2, :]
            actual_cooperators_last_round_excluding_me = np.sum(last_round_opp_actions)
        cooperate_is_potentially_beneficial = False
        if self.n_players - 1 > 0 and actual_cooperators_last_round_excluding_me / (self.n_players - 1) >= self.cooperation_threshold_ratio:
            payoff_if_cooperate_this_round = self.k / self.n_players * (actual_cooperators_last_round_excluding_me + 1)
            payoff_if_defect_this_round = 1.0 + self.k / self.n_players * actual_cooperators_last_round_excluding_me
            if payoff_if_cooperate_this_round > payoff_if_defect_this_round:
                cooperate_is_potentially_beneficial = True
        if cooperate_is_potentially_beneficial:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_89(BaseStrategy):
    """
    "Exploitative Opportunist" strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain by cooperating initially to gather data,
    then defects proactively when exploitative opportunities arise or cooperation falters.
    It is aggressive by aiming to extract maximum value from cooperative players
    and always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_round = 1
        self.observation_window = min(5, self.r - 1) if self.r > 1 else 1
        self.exploitation_profit_margin = 1.5
        self.cooperation_stability_threshold = None

    def _calculate_cooperation_stability_threshold(self) -> float:
        """
        Initializes or recalculates the threshold for stable cooperation.
        Aims to ensure the public good is at least somewhat beneficial.
        """
        return self.n * (self.k / self.n) * 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round = state.round_number + 1
        if self.cooperation_stability_threshold is None:
            self.cooperation_stability_threshold = self._calculate_cooperation_stability_threshold()
        if self.current_round == 1:
            return Action.C
        if self.current_round == self.r:
            return Action.D
        total_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1])
        num_past_rounds = history.round_number
        start_index = max(0, num_past_rounds - self.observation_window)
        recent_cooperator_counts = []
        recent_cooperator_counts.extend(np.sum(history.opponent_actions[start_index:num_past_rounds, :], axis=1) + history.my_actions[start_index:num_past_rounds])
        if not recent_cooperator_counts:
            recent_cooperation_average = 0
        else:
            recent_cooperation_average = np.mean(recent_cooperator_counts)
        avg_cooperator_payoff_prev = self.k / self.n * total_cooperators_prev_round
        avg_defector_payoff_prev = 1 + self.k / self.n * total_cooperators_prev_round
        if avg_defector_payoff_prev > avg_cooperator_payoff_prev * self.exploitation_profit_margin:
            return Action.D
        if recent_cooperation_average < self.cooperation_stability_threshold:
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_90(BaseStrategy):
    """
    Opportunistic Exploiter Strategy:
    This strategy aims to maximize individual gain by being aggressive by default,
    opportunistically free-riding on cooperation, and swiftly retaliating against defection.
    It adapts its cooperation threshold based on historical observations and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_round = 0
        self.defect_streak = 0
        self.cooperation_threshold_factor = 0.7
        self.punishment_rounds = 3
        self.initial_defection_rounds = min(3, self.r // 2)
        self.cooperation_history = []
        self.observed_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round = state.round_number
        if history is None:
            self.cooperation_history = []
            self.observed_payoffs = []
            self.defect_streak = 0
            return Action.D
        else:
            self.cooperation_history.append(np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0))
            self.observed_payoffs.append(history.my_payoffs[-1])
            if self.current_round == self.r - 1:
                return Action.D
            if self.defect_streak > 0:
                self.defect_streak -= 1
                if self.defect_streak > 0:
                    return Action.D
            avg_cooperators = np.mean(self.cooperation_history) if self.cooperation_history else 0
            if avg_cooperators > self.n / 2 and self.k / self.n * avg_cooperators >= 1:
                return Action.C
            if len(self.observed_payoffs) > 0 and self.observed_payoffs[-1] < 1.5:
                self.defect_streak = self.punishment_rounds
                return Action.D
            return Action.D

class Strategy_AGGRESSIVE_91(BaseStrategy):
    """
    Aggressive Strategy: "Exploit and Punish"
    This strategy aims to maximize individual payoff by exploiting cooperation
    when safe and retaliating swiftly and decisively against defectors.
    It cooperates initially to gather information, benefits from majority
    cooperation, and always defects in the last round. Any defection triggers
    a punishment phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = 3
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None:
            self.punishment_countdown = 0
            return Action.C
        if current_round == r:
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        last_completed_round_idx = history.round_number - 1
        all_previous_actions = np.concatenate([history.my_actions[last_completed_round_idx:last_completed_round_idx + 1], history.opponent_actions[last_completed_round_idx, :]])
        last_round_cooperators = np.sum(all_previous_actions)
        if last_round_cooperators < n:
            self.punishment_countdown = 2
            return Action.D
        consistent_cooperation = True
        if self.m > 1:
            start_idx = max(0, history.round_number - self.m)
            for round_idx in range(start_idx, history.round_number):
                past_actions = np.concatenate([history.my_actions[round_idx:round_idx + 1], history.opponent_actions[round_idx, :]])
                if np.sum(past_actions) < n:
                    consistent_cooperation = False
                    break
        if last_round_cooperators > n / 2:
            return Action.C
        elif consistent_cooperation:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_92(BaseStrategy):
    """
    The "Defect Dominance" strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain through an aggressive,
    initially distrustful approach. It cooperates conditionally,
    punishes low cooperation, and exploits favorable conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.COOP_THRESHOLD = 0.6
        self.PUNISHMENT_ROUNDS = 3
        self.RECENT_TREND_WINDOW = 5
        self.PERSONAL_BENEFIT_MIN_GAIN = 0.0
        self.punishment_phase_counter = 0

    def _get_opponent_cooperation_stats(self, history: PlayerHistory) -> tuple[float, float, int]:
        """Calculates average opponent cooperation ratio and trend."""
        if history.round_number == 0:
            return (0.0, 0.0, 0)
        opponent_actions_history = history.opponent_actions[:history.round_number, :]
        opponent_coop_ratios_per_round = np.mean(opponent_actions_history, axis=1)
        avg_opponent_coop_ratio = np.mean(opponent_coop_ratios_per_round)
        trend_window = min(self.RECENT_TREND_WINDOW, len(opponent_coop_ratios_per_round))
        recent_coop_ratios = opponent_coop_ratios_per_round[-trend_window:]
        if len(recent_coop_ratios) < 2:
            recent_opponent_cooperation_trend = 0.0
        else:
            recent_opponent_cooperation_trend = recent_coop_ratios[-1] - recent_coop_ratios[0]
        total_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) if history.round_number > 0 else 0
        return (avg_opponent_coop_ratio, recent_opponent_cooperation_trend, total_cooperators_last_round)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if history is None:
            self.punishment_phase_counter = 0
            return Action.D
        if self.punishment_phase_counter > 0:
            self.punishment_phase_counter -= 1
            return Action.D
        avg_opponent_coop_ratio, recent_opponent_cooperation_trend, total_cooperators_last_round = self._get_opponent_cooperation_stats(history)
        estimated_other_cooperators = total_cooperators_last_round if history.round_number == 0 else np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == Action.C.value and history.round_number > 0:
            estimated_other_cooperators -= 1
        estimated_other_cooperators = max(0, estimated_other_cooperators)
        total_cooperators_if_i_cooperates = estimated_other_cooperators + 1
        expected_payoff_cooperating = self.k / self.n * total_cooperators_if_i_cooperates
        payoff_defecting = 1.0
        should_cooperate = False
        if avg_opponent_coop_ratio > self.COOP_THRESHOLD and recent_opponent_cooperation_trend >= -0.1 and (expected_payoff_cooperating > payoff_defecting + self.PERSONAL_BENEFIT_MIN_GAIN):
            should_cooperate = True
        if not should_cooperate and total_cooperators_last_round < self.n * 0.3 and (history.round_number > 0):
            self.punishment_phase_counter = self.PUNISHMENT_ROUNDS
            return Action.D
        if should_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_94(BaseStrategy):
    """
    Aggressive Exploiter Strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual gain by defecting by default
    and cooperating only when the collective cooperation level is low enough
    that its own cooperation is likely to be a net positive contribution.
    It dynamically adjusts a 'threat threshold' to adapt to the behavior of
    other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threat_threshold = self.n_players * 0.6
        self.cooperation_adjustment_factor = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.n_rounds:
            return Action.D
        num_my_cooperators_prev_round = int(history.my_actions[current_round - 1])
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        total_cooperators_prev_round = num_my_cooperators_prev_round + num_opponent_cooperators_prev_round
        if total_cooperators_prev_round < self.threat_threshold:
            action = Action.C
            if total_cooperators_prev_round > self.threat_threshold * 1.2:
                self.threat_threshold += self.threat_threshold * self.cooperation_adjustment_factor
        else:
            action = Action.D
            if total_cooperators_prev_round < self.threat_threshold * 0.8:
                self.threat_threshold -= self.threat_threshold * self.cooperation_adjustment_factor
                self.threat_threshold = max(self.threat_threshold, 1.0)
        return action

class Strategy_AGGRESSIVE_95(BaseStrategy):
    """
    The Opportunistic Exploiter strategy is an aggressive, self-interested
    approach to the N-Player Public Goods Game. It aims to maximize its own
    payoff by defecting by default. It will only cooperate if there is a
    guaranteed net gain from its own cooperation, meaning the expected
    share of the public good from its own contribution is strictly greater
    than the private endowment received from defecting. It exploits the
    cooperation of others without engaging in trust-building or reciprocal
    behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Opportunistic Exploiter strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: The current game state, including the round number.
            history: An object containing the history of actions and payoffs
                     for this player and opponents. None for the first round.

        Returns:
            Action.C if the player decides to cooperate, Action.D otherwise.
        """
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        num_opponent_cooperators_last_round = 0
        if history.round_number > 0:
            num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        expected_total_cooperators_if_i_cooperate = num_opponent_cooperators_last_round + 1
        payoff_if_i_cooperate = self.k / self.n * expected_total_cooperators_if_i_cooperate
        payoff_if_i_defect = 1.0
        if payoff_if_i_cooperate > payoff_if_i_defect:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_96(BaseStrategy):
    """
    Aggressive Exploiter Strategy:
    This strategy always defects by default. It only cooperates if:
    1. The number of cooperators in the previous round met or exceeded a threshold (ceil(n/2)).
    2. Cooperating in the current round yields a strictly higher personal payoff than defecting,
       given the number of cooperators in the previous round.
    The strategy defects in the first and last rounds to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        C = Action.C.value
        D = Action.D.value
        if history is None:
            return Action.D
        if state.round_number == self.n_rounds:
            num_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            payoff_C = self.k / self.n_players * (num_cooperators_prev_round + 1)
            payoff_D = 1 + self.k / self.n_players * num_cooperators_prev_round
            if payoff_C > payoff_D:
                return Action.C
            else:
                return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_payoff_C = self.k / self.n_players * (num_cooperators_prev_round + 1)
        my_payoff_D = 1 + self.k / self.n_players * num_cooperators_prev_round
        if num_cooperators_prev_round >= self.cooperation_threshold:
            if my_payoff_C > my_payoff_D:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_97(BaseStrategy):
    """
    Aggressive "Exploiter-Punisher" Strategy:
    This strategy aims to maximize its own payoff by attempting to exploit
    cooperation when it's beneficial and aggressively punishing defection.
    It's built on a modified tit-for-tat with a strong bias towards personal gain
    and immediate reaction, driven by game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = max(1, math.floor(self.n / self.k) + 1)
        self.punishment_decay_window = 2
        self.averaging_window_size = 3
        self.punishment_window_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        opponents_defected_last_round = False
        if history.round_number > 0:
            for player_idx in range(self.n - 1):
                if history.opponent_actions[current_round - 1, player_idx] == Action.D.value:
                    opponents_defected_last_round = True
                    break
        if opponents_defected_last_round:
            self.punishment_window_counter = 0
            return Action.D
        self.punishment_window_counter += 1
        num_cooperators_in_window = 0
        rounds_to_consider = min(self.averaging_window_size, current_round)
        if rounds_to_consider > 0:
            for t in range(current_round - rounds_to_consider, current_round):
                if history.my_actions[t] == Action.C.value:
                    num_cooperators_in_window += 1
                for player_idx in range(self.n - 1):
                    if history.opponent_actions[t, player_idx] == Action.C.value:
                        num_cooperators_in_window += 1
            average_cooperator_count_per_round = num_cooperators_in_window / rounds_to_consider
        else:
            average_cooperator_count_per_round = 0
        if average_cooperator_count_per_round >= self.cooperation_threshold_count and self.punishment_window_counter >= self.punishment_decay_window:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_98(BaseStrategy):
    """
    The Ruthless Opportunist strategy defects by default. It only cooperates
    if cooperating in the previous round would have been strictly more profitable
    than defecting, given that all other players also cooperated in the previous round.
    It also defects in the last round to maximize immediate personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + int(my_action_prev_round)
        my_payoff_if_i_cooperated_prev = 0.0
        my_payoff_if_i_defected_prev = 1.0
        num_opponents_cooperated_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        my_payoff_if_i_cooperated_prev = self.k / self.n * (num_opponents_cooperated_prev + 1)
        my_payoff_if_i_defected_prev = 1.0 + self.k / self.n * num_opponents_cooperated_prev
        all_players_cooperated_prev = num_opponents_cooperated_prev == self.n - 1 and my_action_prev_round == Action.C.value
        cooperation_is_better_for_me = my_payoff_if_i_cooperated_prev > my_payoff_if_i_defected_prev
        if all_players_cooperated_prev and cooperation_is_better_for_me:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_99(BaseStrategy):
    """
    Dominant Defector strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain, retaliates swiftly against defectors,
    and only cooperates under specific, mutually beneficial conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_zone_length = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        previous_self_actions = history.my_actions
        previous_opponent_actions = history.opponent_actions
        previous_self_payoffs = history.my_payoffs
        previous_self_action = previous_self_actions[previous_round_idx]
        previous_total_cooperators = np.sum(previous_self_actions[:previous_round_idx + 1]) + np.sum(previous_opponent_actions[previous_round_idx, :])
        previous_self_payoff = previous_self_payoffs[previous_round_idx]
        consecutive_defections = 0
        for i in range(state.round_number - 1, -1, -1):
            if previous_self_actions[i] == Action.D.value:
                consecutive_defections += 1
            else:
                break
        if previous_self_action == Action.D.value and previous_total_cooperators < n:
            if np.any(previous_opponent_actions[previous_round_idx, :] == Action.D.value):
                return Action.D
        if consecutive_defections >= self.punishment_zone_length:
            if previous_total_cooperators == n:
                hypothetical_cooperate_payoff = k
                if previous_self_payoff >= hypothetical_cooperate_payoff:
                    return Action.C
        return Action.D

class Strategy_AGGRESSIVE_100(BaseStrategy):
    """
    Aggressive Exploitation Strategy:
    Always defects in the first and last rounds.
    In intermediate rounds, cooperates if and only if the cooperation
    yields a strictly higher payoff than defecting, given the actions
    of other players in the previous round.
    Specifically, cooperates if k * (count of other cooperators in round t-1 + 1) > n.
    Punishes any defection against it in the previous round by defecting
    in the current round, regardless of the profitability condition.
    Retains a defection if it defected in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None or current_round == 0:
            return Action.D
        if current_round == r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round - 1]
        my_previous_action = history.my_actions[current_round - 1]
        other_cooperators_previous_round = np.sum(previous_round_actions)
        exploited_in_previous_round = False
        if my_previous_action == Action.C.value:
            if np.any(previous_round_actions == Action.D.value):
                exploited_in_previous_round = True
        if exploited_in_previous_round:
            return Action.D
        should_cooperate_based_on_profitability = k * (other_cooperators_previous_round + 1) > n
        if should_cooperate_based_on_profitability:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_101(BaseStrategy):
    """
    The Dominator strategy. This strategy is designed for maximum self-interest
    and exploitation in the N-Player Public Goods Game. It prioritizes personal gain,
    leveraging the game's parameters to punish defection and reward perceived
    cooperation when it benefits the Dominator, but always maintaining a strong
    bias towards defection to secure private gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_high = 0.5
        self.cooperation_threshold_low = 0.25
        self.punishment_sustained_low_payoff_threshold = 1.5
        self.payoff_gain_for_cooperation_threshold = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        previous_round_idx = current_round - 1
        my_prev_action = 1 if self.n == 1 else 0
        if history.my_actions.size > previous_round_idx:
            my_prev_action = history.my_actions[previous_round_idx]
        opponent_cooperators_prev = 0
        if history.opponent_actions.shape[0] > previous_round_idx and history.opponent_actions.shape[1] > 0:
            opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_prev = my_prev_action + opponent_cooperators_prev
        average_cooperation_rate_prev_round = total_cooperators_prev / self.n
        my_average_payoff_prev = 0.0
        if history.my_payoffs.size > 0:
            my_average_payoff_prev = np.mean(history.my_payoffs)
        decision = Action.D
        payoff_if_i_cooperate = 0 + self.k / self.n * (total_cooperators_prev + 1)
        payoff_if_i_defect = 1 + self.k / self.n * total_cooperators_prev
        if average_cooperation_rate_prev_round > self.cooperation_threshold_high:
            if payoff_if_i_cooperate > payoff_if_i_defect + self.payoff_gain_for_cooperation_threshold:
                decision = Action.C
        if average_cooperation_rate_prev_round < self.cooperation_threshold_low:
            decision = Action.D
        if my_average_payoff_prev < self.punishment_sustained_low_payoff_threshold and payoff_if_i_defect < self.punishment_sustained_low_payoff_threshold:
            decision = Action.D
        if decision == Action.D:
            for i in range(self.n - 1):
                if history.my_actions.size > previous_round_idx and history.opponent_actions.shape[0] > previous_round_idx and (history.opponent_actions.shape[1] > i):
                    my_last_action = history.my_actions[previous_round_idx]
                    opponent_last_action = history.opponent_actions[previous_round_idx, i]
                    if my_last_action == Action.C.value and opponent_last_action == Action.D.value:
                        decision = Action.D
                        break
        if decision == Action.C:
            if payoff_if_i_cooperate <= payoff_if_i_defect:
                decision = Action.D
        if average_cooperation_rate_prev_round < self.cooperation_threshold_low and payoff_if_i_cooperate <= payoff_if_i_defect:
            decision = Action.D
        return Action(decision)

class Strategy_AGGRESSIVE_102(BaseStrategy):
    """
    Exploitative Leverage Strategy for N-Player Public Goods Game.

    This strategy initially cooperates for two rounds to observe other players'
    behavior. From the third round onwards, it defects if any player cooperated
    in the previous round, aiming to exploit their contributions. In the final
    round, it always defects as there are no future repercussions. If everyone
    defected in the previous round, it also defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: Current game state (round_number).
            history: History of past actions and payoffs. None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_number = state.round_number
        if current_round_number < 2:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round_number - 1]
        num_cooperators_prev_round = np.sum(previous_round_actions)
        if num_cooperators_prev_round > 0:
            return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_103(BaseStrategy):
    """
    The Opportunistic Dominator strategy.
    This strategy defaults to defection, only cooperating if the number of
    cooperators in the previous round was sufficiently high to ensure a
    per-player public good payoff of at least 1. It defects in the first
    and last rounds, and when the collective cooperation is too low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        previous_round_index = current_round_number - 1
        cooperators_in_previous_round = 0
        if previous_round_index < len(history.opponent_actions):
            cooperators_in_previous_round += np.sum(history.opponent_actions[previous_round_index, :])
        if previous_round_index < len(history.my_actions):
            if history.my_actions[previous_round_index]:
                cooperators_in_previous_round += 1
        if cooperators_in_previous_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_104(BaseStrategy):
    """
    The "Exploiter" strategy is an aggressive approach for the N-Player Public Goods Game.
    It defaults to defecting, prioritizing individual gain. Cooperation is rare,
    only occurring if expected personal payoff from cooperation is strictly greater
    than defection, and defection itself is still perceived as rewarding. It
    also has a rare, risky move to cooperate to potentially 'reset' the game for
    future exploitation if consistent cooperation is observed. Punishment for
    defection is immediate and permanent against the offending player.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punish_player_id = -1

    def _calculate_expected_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        """
        Calculates the expected payoff of cooperating and defecting based on historical average.
        Returns (expected_payoff_C, expected_payoff_D).
        """
        num_past_rounds = history.round_number
        if num_past_rounds == 0:
            return (0.0, 1.0)
        all_past_actions = np.concatenate((history.my_actions.reshape(-1, 1), history.opponent_actions), axis=1)
        total_past_contributions = np.sum(all_past_actions)
        average_cooperator_proportion = total_past_contributions / (num_past_rounds * self.n)
        expected_total_cooperators_if_C = 1 + average_cooperator_proportion * (self.n - 1)
        expected_payoff_C = 1 - 1 + self.k / self.n * expected_total_cooperators_if_C
        expected_total_cooperators_if_D = average_cooperator_proportion * (self.n - 1)
        expected_payoff_D = 1 - 0 + self.k / self.n * expected_total_cooperators_if_D
        return (expected_payoff_C, expected_payoff_D)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) based on the "Exploiter" strategy.
        """
        current_round = state.round_number
        n_players = self.n
        k_factor = self.k
        n_rounds = self.r
        if history is None:
            self.punish_player_id = -1
            return Action.D
        if current_round == n_rounds:
            return Action.D
        if self.punish_player_id != -1:
            if self.punish_player_id < n_players - 1:
                return Action.D
        expected_payoff_C, expected_payoff_D = self._calculate_expected_payoffs(history)
        action = Action.D
        if expected_payoff_C > expected_payoff_D and expected_payoff_D > 1:
            action = Action.C
        consistent_cooperators_threshold = 0.7
        recent_rounds_window = 3
        if current_round > recent_rounds_window:
            num_consistent_cooperators = 0
            for player_idx in range(n_players - 1):
                all_cooperated_in_window = True
                for r_idx in range(current_round - recent_rounds_window, current_round):
                    if history.opponent_actions[r_idx, player_idx] == Action.D.value:
                        all_cooperated_in_window = False
                        break
                if all_cooperated_in_window:
                    num_consistent_cooperators += 1
            if num_consistent_cooperators / (n_players - 1) >= consistent_cooperators_threshold:
                action = Action.C
        if action == Action.C:
            last_round_actions = history.opponent_actions[-1, :]
            for player_idx, player_action in enumerate(last_round_actions):
                if player_action == Action.D.value:
                    self.punish_player_id = player_idx
                    return Action.D
        return action

class Strategy_AGGRESSIVE_105(BaseStrategy):
    """
    Aggressive "Exploit and Punish" Strategy for N-Player Public Goods Game.
    This strategy defaults to defecting, only cooperating when the previous round's
    collective outcome was demonstrably beneficial for cooperators and sustained.
    It punishes perceived exploitation or a drop in cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_player_index = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        prev_round_idx = state.round_number - 1
        my_action_prev = Action.D if not history.my_actions[prev_round_idx] else Action.C
        total_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        if my_action_prev == Action.C:
            total_cooperators_prev += 1
        beneficial_cooperation_threshold_count = self.n / self.k
        cooperation_was_beneficial = False
        if total_cooperators_prev > 0:
            avg_cooperator_payoff_prev = self.k / self.n * total_cooperators_prev
            if avg_cooperator_payoff_prev > 1.0:
                cooperation_was_beneficial = True
        punish = False
        if not cooperation_was_beneficial:
            punish = True
        else:
            if my_action_prev == Action.C and total_cooperators_prev < self.n:
                punish = True
            if total_cooperators_prev < math.ceil(beneficial_cooperation_threshold_count):
                punish = True
        if punish:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_106(BaseStrategy):
    """
    The Ruthless Exploiter strategy is an aggressive approach for the N-Player
    Public Goods Game. It prioritizes maximizing personal payoff by defecting
    whenever it yields a higher immediate return. Cooperation is only considered
    if it's demonstrably more profitable due to a high level of observed
    cooperation from other players, effectively exploiting their tendencies.
    It defaults to defection in the first round and aggressively defects in the
    last round to maximize final scores.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.exploitation_threshold = 0.8

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        num_other_players = self.n - 1
        if len(history.my_actions) < 1:
            return Action.D
        other_players_actions_last_round = history.opponent_actions[current_round - 1, :]
        num_cooperators_last_round = np.sum(other_players_actions_last_round)
        my_payoff_if_C = 1 - 1 + self.k / self.n * (1 + num_cooperators_last_round)
        my_payoff_if_D = 1 - 0 + self.k / self.n * num_cooperators_last_round
        if my_payoff_if_D > my_payoff_if_C:
            return Action.D
        current_cooperation_ratio = 0
        if num_other_players > 0:
            current_cooperation_ratio = num_cooperators_last_round / num_other_players
        if my_payoff_if_D > my_payoff_if_C:
            return Action.D
        else:
            current_cooperation_ratio = 0
            if num_other_players > 0:
                current_cooperation_ratio = num_cooperators_last_round / num_other_players
            if my_payoff_if_C >= my_payoff_if_D and current_cooperation_ratio > self.exploitation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_107(BaseStrategy):
    """
    The "Ruthless Exploiter" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by exploiting high cooperation
    levels from others while punishing defection aggressively. It defaults to
    defecting unless specific conditions for profitable cooperation are met,
    or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.player_id = -1

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        my_actions_list = history.my_actions.tolist()
        opponent_actions_list = history.opponent_actions.tolist()
        recent_defection_occurred = False
        if opponent_actions_list:
            last_round_opponent_actions = opponent_actions_list[-1]
            for opponent_action in last_round_opponent_actions:
                if opponent_action is False:
                    recent_defection_occurred = True
                    break
        if recent_defection_occurred:
            return Action.D
        total_past_cooperators = 0
        num_past_rounds = len(my_actions_list)
        for my_action in my_actions_list:
            if my_action is True:
                total_past_cooperators += 1
        for round_actions in opponent_actions_list:
            for opp_action in round_actions:
                if opp_action is True:
                    total_past_cooperators += 1
        estimated_total_cooperators_per_round = total_past_cooperators / num_past_rounds if num_past_rounds > 0 else 0
        total_others_cooperation_count = 0
        for round_actions in opponent_actions_list:
            total_others_cooperation_count += sum(round_actions)
        average_others_cooperation_rate = total_others_cooperation_count / (num_past_rounds * (self.n - 1)) if num_past_rounds > 0 and self.n > 1 else 0
        payoff_if_cooperate = self.k / self.n * (estimated_total_cooperators_per_round + 1)
        payoff_if_defect = 1 + self.k / self.n * estimated_total_cooperators_per_round
        high_cooperation_threshold = 0.85
        if payoff_if_cooperate > payoff_if_defect and average_others_cooperation_rate > high_cooperation_threshold:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_108(BaseStrategy):
    """
    "Exploitative Aggressor" strategy for the N-Player Public Goods Game.
    This strategy prioritizes maximizing individual payoff by defaulting to defection
    and cooperating only when there's a significant individual gain, while punishing
    any perceived exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_premium = 1.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.D
        if current_round_t == r - 1:
            return Action.D
        prev_round_idx = current_round_t - 1
        my_action_prev_round = Action.C if history.my_actions[prev_round_idx] else Action.D
        num_cooperators_prev_round = 0
        exploited_me_in_prev_round = False
        total_actions_prev_round = np.concatenate(([history.my_actions[prev_round_idx]], history.opponent_actions[prev_round_idx, :]))
        num_cooperators_prev_round = np.sum(total_actions_prev_round)
        if my_action_prev_round == Action.C:
            for opp_idx in range(n - 1):
                if not history.opponent_actions[prev_round_idx, opp_idx]:
                    exploited_me_in_prev_round = True
                    break
        guaranteed_private_payoff = 1.0
        expected_coop_payoff = k / n * (num_cooperators_prev_round + 1)
        if my_action_prev_round == Action.C and exploited_me_in_prev_round:
            return Action.D
        if expected_coop_payoff > guaranteed_private_payoff * self.cooperation_threshold_premium:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_109(BaseStrategy):
    """
    Opportunistic Defection: This strategy aggressively pursues personal gain.
    It defects by default, only cooperating when past cooperation has been
    demonstrably beneficial and the player was a participant. Cooperation is
    a conditional, short-term tactic, not a default behavior. In the final
    round, it always defects to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        action = Action.D
        if history is None:
            return Action.D
        if state.round_number == self.n_rounds:
            return Action.D
        prev_round_idx = state.round_number - 2
        player_cooperated_prev_round = history.my_actions[prev_round_idx] == Action.C.value
        my_action_prev_is_C = history.my_actions[prev_round_idx] == Action.C.value
        total_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + my_action_prev_is_C
        if total_cooperators_prev_round >= self.cooperation_threshold and player_cooperated_prev_round:
            action = Action.C
        return action

class Strategy_AGGRESSIVE_110(BaseStrategy):
    """
    The Opportunist strategy for the N-Player Public Goods Game.
    This strategy is aggressive, prioritizing immediate gains and punishing deviations.
    It defects in the first and last rounds. In intermediate rounds, it defects if
    perfect cooperation wasn't achieved in the previous round or if it cooperated
    and not everyone else did. It cooperates only if perfect cooperation occurred
    in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round = state.round_number
        previous_round = current_round - 1
        if current_round == self.r:
            return Action.D
        my_previous_action_cooperated = history.my_actions[previous_round]
        previous_total_cooperators = np.sum(history.my_actions[previous_round:previous_round + 1]) + np.sum(history.opponent_actions[previous_round, :])
        if my_previous_action_cooperated and previous_total_cooperators < self.n:
            ideal_cooperation_payoff_per_player = self.k
            sum_previous_payoffs = np.sum(history.my_payoffs[previous_round:previous_round + 1]) + np.sum(history.opponent_payoffs[previous_round, :])
            average_previous_payoff = sum_previous_payoffs / self.n
            return Action.D
        else:
            if previous_total_cooperators == self.n:
                return Action.C
            if my_previous_action_cooperated and previous_total_cooperators < self.n:
                return Action.D
            elif not my_previous_action_cooperated or previous_total_cooperators == self.n:
                return Action.C
            else:
                pass
        return Action.D

class Strategy_AGGRESSIVE_111(BaseStrategy):
    """
    The "Opportunistic Enforcer" strategy is designed to maximize individual gain
    by exploiting opportunities for high payoffs while punishing defection.
    It defaults to defecting but cooperates strategically when there's a clear
    individual benefit from others' cooperation or as a calculated risk to
    shift a low-cooperation environment, with no forgiveness for past defections.
    It aggressively pursues self-interest and avoids being exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.n
        r = self.r
        k = self.k
        if history is None:
            return Action.D
        if current_round == r:
            return Action.D
        my_previous_action_bool = history.my_actions[current_round - 1]
        my_previous_payoff = history.my_payoffs[current_round - 1]
        my_previous_cooperation_contribution = 1 if my_previous_action_bool else 0
        previous_total_cooperators = np.sum(history.opponent_actions[current_round - 1, :]) + my_previous_cooperation_contribution
        payoff_if_i_had_cooperated_given_actual_cooperators = k / n * previous_total_cooperators
        if previous_total_cooperators > n / 2 and my_previous_payoff < payoff_if_i_had_cooperated_given_actual_cooperators:
            return Action.C
        guaranteed_payoff_defect = 1.0
        if previous_total_cooperators <= n / 2 and my_previous_payoff < guaranteed_payoff_defect:
            potential_cooperation_payoff_this_round = k / n * (previous_total_cooperators + 1)
            if previous_total_cooperators > 0 and potential_cooperation_payoff_this_round > guaranteed_payoff_defect:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_112(BaseStrategy):
    """
    The "Exploitative Maximizer" strategy aims to maximize personal payoff
    in a repeated N-Player Public Goods Game. It's a conditional cooperator
    with a strong bias towards defecting. It attempts to initiate cooperation
    when beneficial and exploits periods of widespread cooperation, but defects
    aggressively upon detecting a breakdown or presence of defectors.

    Key Features:
    - Proactive cooperation in the first round to gather information.
    - Unconditional defection in the last round to maximize private gain.
    - Dynamic Cooperation Threshold (T) adapting to group behavior.
    - Defection Counter (D_count) to trigger aggressive punishment for sustained defection.
    - Forgiving of slight dips but unforgiving of prolonged cooperation failures.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.total_cooperators_in_previous_round = 0
        self.d_count = 0
        self.cooperation_threshold_t = math.floor(self.n_players / 2)
        self.threshold_for_aggressive_defection = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self.total_cooperators_in_previous_round = 0
            self.d_count = 0
            self.cooperation_threshold_t = math.floor(self.n_players / 2)
            return Action.C
        if current_round == self.n_rounds:
            return Action.D
        if current_round > 0:
            my_previous_action = self.my_actions_history[-1] if hasattr(self, 'my_actions_history') and self.my_actions_history else Action.C
            all_previous_actions = np.append(my_previous_action, history.opponent_actions[current_round - 1, :]).astype(bool)
            self.total_cooperators_in_previous_round = np.sum(all_previous_actions)
            if my_previous_action == Action.D:
                self.d_count += 1
            else:
                self.d_count = 0
            if self.total_cooperators_in_previous_round >= math.floor(self.n_players / 2):
                self.cooperation_threshold_t = min(self.n_players, self.cooperation_threshold_t + 1)
            elif self.total_cooperators_in_previous_round < math.floor(self.n_players / 4):
                self.cooperation_threshold_t = max(0, self.cooperation_threshold_t - 1)
        if self.d_count > self.threshold_for_aggressive_defection:
            return Action.D
        elif self.total_cooperators_in_previous_round < self.cooperation_threshold_t:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_113(BaseStrategy):
    """
    Aggressive Exploiter Strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting cooperation
    while being resilient to defection. It uses a dynamic cooperation threshold
    and a punishment counter to adapt to the observed behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_proportion = 0.1
        self.punishment_counter = 0
        self.max_punishment_rounds = math.ceil(self.r / 4)
        self.initial_exploration_rounds = min(3, self.r)
        self.payoff_function = lambda player_action_value, total_cooperators: 1 - player_action_value + self.k / self.n * total_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r:
            return Action.D
        if history is None:
            self.cooperation_threshold_proportion = 0.1
            self.punishment_counter = 0
            self.initial_exploration_rounds = min(3, self.r)
            return Action.D
        last_round_my_action = self.game_description.n_rounds - 1
        if history.round_number == 0:
            last_round_cooperators_count = 0
            last_round_my_payoff = 0.0
            last_round_my_action_value = 0
        else:
            last_round_cooperators_count = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            last_round_my_payoff = history.my_payoffs[-1]
            last_round_my_action_value = 1 if history.my_actions[-1] else 0
        expected_defect_payoff_last_round = self.payoff_function(0, last_round_cooperators_count)
        if current_round_number < self.initial_exploration_rounds or self.punishment_counter >= self.max_punishment_rounds:
            action = Action.D
        elif last_round_cooperators_count >= self.cooperation_threshold_proportion * self.n:
            action = Action.C
        else:
            action = Action.D
        if not history.my_actions[-1]:
            self.punishment_counter += 1
        elif last_round_my_payoff < expected_defect_payoff_last_round:
            self.cooperation_threshold_proportion *= 1.5
            self.punishment_counter += 1
        else:
            self.cooperation_threshold_proportion = max(0.05, self.cooperation_threshold_proportion * 0.9)
            if self.punishment_counter > 0:
                self.punishment_counter -= 1
        self.cooperation_threshold_proportion = min(1.0, self.cooperation_threshold_proportion)
        return action

class Strategy_AGGRESSIVE_114(BaseStrategy):
    """
    The "Opportunistic Retaliator" strategy for the N-Player Public Goods Game.

    This strategy defaults to defecting, ensuring a baseline payoff. It only
    considers cooperating if a majority of players cooperated in the previous
    round, and if cooperating would strictly increase its own payoff compared
    to defecting. It is punitive towards insufficient collective action and
    never forgives. Cooperation is only for immediate personal gain when the
    conditions are highly favorable and supported by observed majority action.
    It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.n
        r = self.r
        k = self.k
        if current_round == 0:
            return Action.D
        if current_round == r - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_action_prev_round = Action.C if history.my_actions[prev_round_idx] else Action.D
        opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
        total_cooperators_prev_round = np.sum(opponent_actions_prev_round)
        if my_action_prev_round == Action.C:
            total_cooperators_prev_round += 1
        if total_cooperators_prev_round < math.ceil(n / 2):
            return Action.D
        estimated_other_cooperators = total_cooperators_prev_round
        payoff_if_cooperate = k / n * (1 + estimated_other_cooperators)
        payoff_if_defect = 1 + k / n * estimated_other_cooperators
        if payoff_if_cooperate > payoff_if_defect:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_115(BaseStrategy):
    """
    The "Aggressor's Reckoning" strategy is designed for an aggressive player in the
    N-Player Public Goods Game. It prioritizes maximizing individual gains while
    being willing to exploit cooperators and punish defectors, but only after a
    certain threshold of trust has been broken. The core idea is to be a
    "conditional cooperator" but with a very low tolerance for being taken
    advantage of, and a strong inclination to retaliate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.early_phase_end_ratio = 0.25
        self.early_phase_cap = 5
        self.late_phase_start_ratio = 0.5
        self.late_phase_cap = 5
        self.high_cooperation_threshold = 0.7
        self.moderate_cooperation_threshold = 0.5
        self.late_game_betrayal_threshold = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        current_round_number = state.round_number
        previous_round_number = current_round_number - 1
        my_action_prev_round = history.my_actions[previous_round_number]
        opponent_actions_prev_round = history.opponent_actions[previous_round_number, :]
        total_cooperators_prev_round = np.sum(opponent_actions_prev_round) + my_action_prev_round
        cooperation_ratio_prev_round = total_cooperators_prev_round / self.n
        my_coop_payoff_hypothetical = 1 - 1 + self.k / self.n * total_cooperators_prev_round
        my_defect_payoff_hypothetical = 1 - 0 + self.k / self.n * total_cooperators_prev_round
        if current_round_number == self.r - 1:
            return Action.D
        early_phase_end_round = min(math.floor(self.r * self.early_phase_end_ratio), self.early_phase_cap)
        late_phase_start_round = max(math.floor(self.r * self.late_phase_start_ratio), self.r - self.late_phase_cap)
        if current_round_number < early_phase_end_round:
            if cooperation_ratio_prev_round > self.high_cooperation_threshold and my_coop_payoff_hypothetical > my_defect_payoff_hypothetical:
                return Action.C
            else:
                return Action.D
        elif current_round_number >= late_phase_start_round:
            if my_action_prev_round == Action.C.value and cooperation_ratio_prev_round < self.late_game_betrayal_threshold:
                return Action.D
            if cooperation_ratio_prev_round > self.high_cooperation_threshold and my_coop_payoff_hypothetical > my_defect_payoff_hypothetical:
                return Action.C
            else:
                return Action.D
        else:
            if my_action_prev_round == Action.C.value and cooperation_ratio_prev_round < self.moderate_cooperation_threshold:
                return Action.D
            if cooperation_ratio_prev_round > self.high_cooperation_threshold:
                return Action.C
            elif cooperation_ratio_prev_round > self.moderate_cooperation_threshold:
                return Action.D
            else:
                return Action.D

class Strategy_AGGRESSIVE_116(BaseStrategy):
    """
    The "Opportunistic Predator" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to defection,
    but cooperates strategically when it perceives a sufficient level of group
    cooperation. It punishes exploitation by reverting to defection and has
    an adaptive threshold for cooperation based on past group behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.history_storage = []

    def _calculate_cooperation_threshold(self, avg_coop_rate: float | None) -> int:
        """
        Calculates the cooperation threshold based on game parameters and historical cooperation.
        """
        if avg_coop_rate is None:
            return math.ceil(self.n * (1 - self.k / self.n))
        else:
            adaptive_threshold = math.floor(self.n * (1 - self.k / self.n) * (avg_coop_rate + 0.1))
            return max(1, adaptive_threshold)

    def _get_average_cooperation_rate(self, history: PlayerHistory) -> float | None:
        """
        Calculates the average cooperation rate across all players in past rounds.
        Returns None if no history is available.
        """
        if history.round_number == 0:
            return None
        total_cooperators_sum = 0
        num_past_rounds = history.round_number
        total_cooperators_sum += np.sum(history.my_actions[:num_past_rounds])
        if history.opponent_actions.shape[0] > 0:
            total_cooperators_sum += np.sum(history.opponent_actions[:num_past_rounds, :])
        total_possible_cooperators = num_past_rounds * self.n
        if total_possible_cooperators == 0:
            return 0.0
        return total_cooperators_sum / total_possible_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        avg_coop_rate = self._get_average_cooperation_rate(history)
        cooperation_threshold = self._calculate_cooperation_threshold(avg_coop_rate)
        retaliate = False
        if history.round_number > 0:
            avg_coop_rate_before_last = None
            if history.round_number > 1:
                avg_coop_rate_before_last = self._get_average_cooperation_rate(PlayerHistory(my_actions=history.my_actions[:history.round_number - 1], my_payoffs=history.my_payoffs[:history.round_number - 1], opponent_actions=history.opponent_actions[:history.round_number - 1, :], opponent_payoffs=history.opponent_payoffs[:history.round_number - 1, :]))
            if history.round_number == 1:
                previous_decision_threshold = self._calculate_cooperation_threshold(None)
            else:
                previous_decision_threshold = self._calculate_cooperation_threshold(avg_coop_rate_before_last)
            last_round_total_cooperators = np.sum(history.my_actions[history.round_number - 1])
            if history.opponent_actions.shape[0] > 0:
                last_round_total_cooperators += np.sum(history.opponent_actions[history.round_number - 1, :])
            if history.my_actions[history.round_number - 1] == Action.C.value and last_round_total_cooperators < previous_decision_threshold:
                retaliate = True
        if retaliate:
            return Action.D
        if history.round_number > 0:
            last_round_total_cooperators = np.sum(history.my_actions[history.round_number - 1])
            if history.opponent_actions.shape[0] > 0:
                last_round_total_cooperators += np.sum(history.opponent_actions[history.round_number - 1, :])
            if last_round_total_cooperators >= cooperation_threshold:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_117(BaseStrategy):
    """
    The "Exploitative Retaliator" strategy for the N-Player Public Goods Game.
    This strategy prioritizes maximizing its own payoff. It defaults to defecting,
    only cooperating if it is demonstrably more profitable than defecting.
    It employs an unforgiving retaliation mechanism: if it cooperates and any
    opponent defects, it will defect for all subsequent rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.action_cooperate = Action.C.value
        self.action_defect = Action.D.value
        self.permanently_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if self.permanently_defect:
            return Action.D
        if current_round == self.r:
            return Action.D
        if history is None:
            return Action.D
        my_last_action = self.action_cooperate if history.my_actions[-1] else self.action_defect
        if my_last_action == self.action_cooperate:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            if np.any(opponent_actions_last_round == self.action_defect):
                self.permanently_defect = True
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_118(BaseStrategy):
    """
    Exploitative Retaliator strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by being opportunistic and
    retaliatory. It cooperates early if a majority cooperates, defects when
    cooperation is not dominant, and always defects in the final round.
    It punishes deviations from strong cooperation by also defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        last_round_idx = current_round - 1
        num_cooperators_last_round = np.sum(history.my_actions[last_round_idx] == Action.C.value) + np.sum(history.opponent_actions[last_round_idx] == Action.C.value)
        if current_round == self.r:
            return Action.D
        if current_round <= self.r / 4:
            if num_cooperators_last_round > self.n / 2:
                return Action.C
            else:
                return Action.D
        elif current_round <= 3 * self.r / 4:
            if num_cooperators_last_round == self.n:
                return Action.C
            elif num_cooperators_last_round > self.n / 2:
                return Action.C
            elif num_cooperators_last_round > 0:
                return Action.D
            else:
                return Action.D
        elif num_cooperators_last_round == self.n:
            return Action.C
        elif num_cooperators_last_round > 0:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_119(BaseStrategy):
    """
    Aggressive Strategy: "Opportunistic Exploiter & Ruthless Retaliator"

    This strategy prioritizes individual gain by defecting by default.
    It considers cooperation only when a strong cooperative signal from others
    suggests a significant personal benefit, and retaliates ruthlessly
    by defecting if it was exploited in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.cooperation_gain_threshold = 0.25
        self.cooperation_majority_threshold = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        num_previous_rounds = history.round_number
        previous_round_index = num_previous_rounds - 1
        my_previous_payoff = history.my_payoffs[previous_round_index]
        if my_previous_payoff < 1.0:
            return Action.D
        my_previous_action_was_C = history.my_actions[previous_round_index]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_previous_action_was_C else 0)
        payoff_if_cooperate_current = self.k / self.n * (total_cooperators_prev_round + 1)
        payoff_if_defect_current = 1.0 + self.k / self.n * total_cooperators_prev_round
        cooperation_majority_met = total_cooperators_prev_round / self.n >= self.cooperation_majority_threshold
        cooperation_gain = payoff_if_cooperate_current - payoff_if_defect_current
        if cooperation_majority_met and cooperation_gain > self.cooperation_gain_threshold:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_120(BaseStrategy):
    """
    The Exploiter-Defender strategy aims to maximize individual payoff by
    defaulting to defection, conditionally cooperating only when the collective
    benefit is high and the risk of exploitation is low. It retaliates
    immediately and permanently against perceived exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.permanent_defect = False
        self.required_prior_cooperators_for_potential_cooperation = math.ceil(self.n_players * (1 - self.k / self.n_players))
        self.punishment_threshold = self.n_players - self.required_prior_cooperators_for_potential_cooperation

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if self.permanent_defect:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        if history is None:
            return Action.D
        previous_cooperators = np.sum(history.opponent_actions[current_round - 1, :])
        cooperation_beneficial_threshold = self.n_players / self.k - 1
        if previous_cooperators >= self.required_prior_cooperators_for_potential_cooperation:
            if previous_cooperators > cooperation_beneficial_threshold:
                return Action.C
        my_last_action_was_C = history.my_actions[current_round - 1]
        if my_last_action_was_C:
            total_contributions_in_previous_round = np.sum(np.concatenate(([history.my_actions[current_round - 1]], history.opponent_actions[current_round - 1, :])))
            if total_contributions_in_previous_round <= self.punishment_threshold:
                self.permanent_defect = True
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_121(BaseStrategy):
    """
    Aggressive strategy for the N-Player Public Goods Game.
    This strategy defaults to defecting, prioritizing individual gain.
    It will only cooperate under specific, calculated conditions:
    1. If the previous round had n-1 or n cooperators, and this player defected,
       they will cooperate in the current round to maximize their share of the
       (now guaranteed) high collective payoff. This is an aggressive move to
       lock in the best possible outcome from others' cooperation.
    2. The first and last rounds are always defect rounds due to lack of history
       or future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: PlayerHistory | None) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        num_cooperators_prev = np.sum(history.my_actions[current_round - 1])
        my_last_action_was_C = history.my_actions[current_round - 1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[current_round - 1, :])
        total_cooperators_prev = (1 if my_last_action_was_C else 0) + num_opponent_cooperators_prev
        my_last_action = Action.C if my_last_action_was_C else Action.D
        decision = Action.D
        payoff_if_cooperate_this_round = self.k / self.n * (total_cooperators_prev + 1)
        payoff_if_defect_this_round = 1 + self.k / self.n * total_cooperators_prev
        if total_cooperators_prev >= self.n - 1 and my_last_action == Action.D:
            if payoff_if_cooperate_this_round > payoff_if_defect_this_round:
                decision = Action.C
        if total_cooperators_prev >= self.n - 1 and my_last_action == Action.D:
            if payoff_if_cooperate_this_round > payoff_if_defect_this_round:
                decision = Action.C
        return decision

class Strategy_AGGRESSIVE_122(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize personal gain in the
    N-Player Public Goods Game. It prioritizes defection when individually
    profitable, punishes perceived exploitation by others, and capitalizes on
    opportunities in full cooperation scenarios. It defaults to defection
    and acts aggressively in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tolerance_threshold = 0.1
        self.defection_threshold_percentage = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        current_round = state.round_number
        if history is None or current_round == 1:
            return Action.D
        if current_round == r:
            return Action.D
        my_payoff_prev_round = history.my_payoffs[-1]
        actions_prev_round_flat = np.concatenate([history.opponent_actions[-1, :], [history.my_actions[-1]]])
        num_cooperators_prev = np.sum(actions_prev_round_flat)
        num_defectors_prev = n - num_cooperators_prev
        total_payoff_prev = np.sum(history.my_payoffs[-1]) + np.sum(history.opponent_payoffs[-1, :])
        avg_other_payoff_prev = 0
        if n > 1:
            avg_other_payoff_prev = (total_payoff_prev - my_payoff_prev_round) / (n - 1)
        if avg_other_payoff_prev > my_payoff_prev_round + self.tolerance_threshold:
            return Action.D
        if num_cooperators_prev == n and my_payoff_prev_round == k / n * n:
            payoff_if_i_defected = 1 + k / n * (n - 1)
            if payoff_if_i_defected > my_payoff_prev_round:
                return Action.D
        if num_defectors_prev / n > self.defection_threshold_percentage:
            return Action.D
        my_current_potential_payoff_if_defect = 1 + k / n * num_cooperators_prev
        my_current_potential_payoff_if_cooperate = k / n * (num_cooperators_prev + 1)
        i_was_exploiter = True
        if avg_other_payoff_prev <= my_payoff_prev_round:
            i_was_exploiter = False
        if my_current_potential_payoff_if_defect - my_current_potential_payoff_if_cooperate < 0.05 and i_was_exploiter:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_123(BaseStrategy):
    """
    The "Opportunistic Exploiter" strategy is designed to maximize personal payoff
    by defecting by default, only cooperating under specific, advantageous circumstances.
    It is vigilant against exploitation and punishes deviations that hurt its interests,
    while cautiously exploring cooperation when collective benefits are high and sustained.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.epsilon = 0.01
        self.has_been_exploited = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_index = current_round - 1
        my_payoff_prev_round = history.my_payoffs[prev_round_index]
        my_action_prev_round = Action.C if history.my_actions[prev_round_index] else Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :])
        if my_action_prev_round == Action.C:
            num_cooperators_prev_round += 1
        total_payoff_prev_round = my_payoff_prev_round + np.sum(history.opponent_payoffs[prev_round_index, :])
        avg_group_payoff_prev_round = total_payoff_prev_round / self.n
        if my_payoff_prev_round < 1.0 - self.epsilon:
            self.has_been_exploited = True
        if self.has_been_exploited and num_cooperators_prev_round >= math.ceil(self.n / 2.0) and (my_payoff_prev_round >= 1.0 - self.epsilon):
            self.has_been_exploited = False
        if self.has_been_exploited:
            return Action.D
        if num_cooperators_prev_round < math.ceil(self.n / 2.0):
            return Action.D
        elif avg_group_payoff_prev_round > 1.5 and my_payoff_prev_round > 1.0 - self.epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_124(BaseStrategy):
    """
    "The Opportunist Predator" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting cooperation
    when it's demonstrably profitable and defecting otherwise to punish or protect.

    Decision Rules:
    - Round 1: Always Defect (D).
    - Last Round: Always Defect (D).
    - Intermediate Rounds:
        - If (Total Cooperators in Previous Round) > (n / k): Play Cooperate (C).
        - Else: Play Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperation_benefit_threshold = self.n / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        previous_round_idx = current_round - 1
        my_previous_action = history.my_actions[previous_round_idx]
        opponent_previous_actions = history.opponent_actions[previous_round_idx, :]
        total_cooperators_previous_round = 0
        if my_previous_action == Action.C.value:
            total_cooperators_previous_round += 1
        total_cooperators_previous_round += np.sum(opponent_previous_actions)
        if total_cooperators_previous_round > self.cooperation_benefit_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_125(BaseStrategy):
    """
    Aggressive "Tit-for-Tat with a Sting" Strategy for the N-Player Public Goods Game.

    This strategy prioritizes personal gain and punishes perceived exploitation.
    It defects in the early rounds and the last round. It retaliates against
    any defection with a sustained period of defection. Cooperation is only
    considered if all other players have consistently cooperated and have a
    high "trust score".
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.r_s = 3
        self.c_threshold = 0.8
        self.t_initial = 3
        self.initial_trust = 1.0
        self.trust_decay_rate = 0.2
        self.trust_reward_rate = 0.1
        self.high_trust_cutoff = 0.7
        self.individual_trust_scores = {}
        self.retaliation_timers = {}

    def _initialize_state_if_needed(self, n_players: int):
        """Initializes trust scores and retaliation timers if they haven't been."""
        if not self.individual_trust_scores:
            for i in range(n_players - 1):
                self.individual_trust_scores[i] = self.initial_trust
                self.retaliation_timers[i] = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            self._initialize_state_if_needed(self.n_players)
            return Action.D
        if current_round == self.n_rounds:
            return Action.D
        prev_round_actions = history.opponent_actions[current_round - 1]
        num_opponents = self.n_players - 1
        opponents_defected_in_prev_round = []
        total_cooperators_prev = 0
        for i in range(num_opponents):
            if not prev_round_actions[i]:
                opponents_defected_in_prev_round.append(i)
                self.individual_trust_scores[i] = max(0, self.individual_trust_scores[i] - self.trust_decay_rate)
            else:
                total_cooperators_prev += 1
                if self.retaliation_timers[i] <= 0:
                    self.individual_trust_scores[i] = min(1.0, self.individual_trust_scores[i] + self.trust_reward_rate)
        for i in range(num_opponents):
            if self.retaliation_timers[i] > 0:
                self.retaliation_timers[i] -= 1
                return Action.D
        if current_round < self.t_initial:
            return Action.D
        if opponents_defected_in_prev_round:
            for opponent_id in opponents_defected_in_prev_round:
                self.retaliation_timers[opponent_id] = self.r_s
            return Action.D
        proportion_cooperators = total_cooperators_prev / num_opponents if num_opponents > 0 else 1.0
        all_opponents_cooperated = len(opponents_defected_in_prev_round) == 0
        if all_opponents_cooperated and proportion_cooperators >= self.c_threshold:
            all_trust_high = True
            for i in range(num_opponents):
                if self.individual_trust_scores[i] < self.high_trust_cutoff:
                    all_trust_high = False
                    break
            if all_trust_high:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_127(BaseStrategy):
    """
    The Ruthless Maximizer strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by cooperating only when
    the expected payoff from cooperating, based on historical average cooperation,
    exceeds the guaranteed payoff from defecting. It defects in the first and
    last rounds, and defaults to defection when cooperation is not sufficiently
    profitable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.total_cooperators_history = 0.0
        self.current_round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round_number = state.round_number
        if self.current_round_number == 0 or self.current_round_number == self.n_rounds - 1:
            return Action.D
        if history is not None:
            my_cooperators_last_round = self.my_action_to_int(history.my_actions[-1])
            opponent_cooperators_last_round = np.sum(history.opponent_actions[-1])
            total_cooperators_last_round = my_cooperators_last_round + opponent_cooperators_last_round
            self.total_cooperators_history += total_cooperators_last_round
        num_previous_rounds = self.current_round_number
        avg_cooperators = self.total_cooperators_history / num_previous_rounds if num_previous_rounds > 0 else 0
        expected_payoff_cooperate = self.k / self.n_players * avg_cooperators
        payoff_defect = 1.0
        if expected_payoff_cooperate > payoff_defect:
            return Action.C
        else:
            return Action.D

    def my_action_to_int(self, action: bool) -> int:
        """Converts boolean action (True=C, False=D) to integer (1 or 0)."""
        return 1 if action else 0

class Strategy_AGGRESSIVE_128(BaseStrategy):
    """
    Aggressive "Exploiter-Punisher" strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual gain, punishes defection swiftly,
    and cooperates only when it's demonstrably profitable and safe.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == r:
            return Action.D
        if history is None:
            return Action.D
        current_round = state.round_number
        previous_round_idx = current_round - 1
        player_i_previous_payoff = history.my_payoffs[previous_round_idx]
        previous_actions = np.concatenate((history.my_actions[previous_round_idx:previous_round_idx + 1], history.opponent_actions[previous_round_idx, :]))
        any_opponent_defected_previous = False
        if any(previous_actions[1:] == Action.D.value):
            any_opponent_defected_previous = True
        if current_round < self.window_size:
            window_start_round = 0
        else:
            window_start_round = current_round - self.window_size
        total_cooperators_in_window = 0
        rounds_in_window = 0
        for t in range(window_start_round, current_round):
            rounds_in_window += 1
            round_actions = np.concatenate((history.my_actions[t:t + 1], history.opponent_actions[t, :]))
            total_cooperators_in_window += np.sum(round_actions == Action.C.value)
        average_cooperators_in_window = 0
        if rounds_in_window > 0:
            average_cooperators_in_window = total_cooperators_in_window / (rounds_in_window * n)
        if any_opponent_defected_previous:
            return Action.D
        if average_cooperators_in_window > 0.5 and player_i_previous_payoff > 1.0:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_23(BaseStrategy):
    """
    Tit-for-Tat Plus (TFT+) strategy for the N-Player Public Goods Game.

    TFT+ aims to foster cooperation by rewarding cooperative behavior and
    punishing defection, but with a more forgiving approach than traditional
    Tit-for-Tat. It prioritizes achieving the highest possible collective payoff
    for all players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_benefit_threshold = self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.C
        previous_round_index = current_round - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n
        if current_round == self.r:
            if num_cooperators_prev_round < self.n:
                return Action.D
            else:
                return Action.C
        if proportion_cooperators_prev_round >= self.cooperation_benefit_threshold:
            return Action.C
        elif proportion_cooperators_prev_round >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_93(BaseStrategy):
    """
    The "Exploiter's Edge" strategy.
    This strategy prioritizes aggressive maximization of individual payoff.
    It defects by default, cooperating only when it demonstrably benefits
    the player more than pure defection, or as a coercive measure to
    incentivize higher cooperation from others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        previous_round_index = state.round_number - 1
        my_previous_payoff = history.my_payoffs[previous_round_index]
        my_previous_action_was_C = history.my_actions[previous_round_index]
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :])
        if my_previous_action_was_C:
            num_cooperators_previous_round += 1
        hypothetical_pure_defection_payoff = 1 + self.k / self.n * (num_cooperators_previous_round - (1 if my_previous_action_was_C else 0))
        if num_cooperators_previous_round <= self.n / 2 and my_previous_payoff < hypothetical_pure_defection_payoff:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_126(BaseStrategy):
    """
    The Opportunistic Enforcer: Aggressively defects by default,
    cooperates only when it secures a clear higher individual payoff
    due to near-universal cooperation in the previous round. Punishes
    any group defection swiftly and severely.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if history is None:
            return Action.D
        if current_round == self.r:
            return Action.D
        prev_round_idx = current_round - 1
        my_prev_action = self.action_to_bool(history.my_actions[prev_round_idx])
        opponent_prev_actions = history.opponent_actions[prev_round_idx, :]
        total_cooperators_prev_round = np.sum(opponent_prev_actions)
        if my_prev_action:
            total_cooperators_prev_round += 1
        if total_cooperators_prev_round == self.n:
            return Action.C
        elif total_cooperators_prev_round == self.n - 1:
            if my_prev_action:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

    def action_to_bool(self, action: Action) -> bool:
        """Converts Action enum to boolean (True for C, False for D)."""
        return action == Action.C