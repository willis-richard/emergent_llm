"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: OpenAI
- Model: gpt-5.2
- Game: CollectiveRiskDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: cooperate to probe feasibility / enable coordination.
    - Middle rounds: cooperate when last round was near threshold and recent success is decent;
      otherwise defect to avoid wasting contributions.
    - Limited "rescue" after a near-miss (up to 2 consecutive attempts).
    - Anti-free-riding tightening when there is chronic slack (many extra cooperators) while we
      have been cooperating a lot: become stricter about when we cooperate.
    - Last round: defect unless likely pivotal (predict others like last round).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.rescue_attempts = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            self.rescue_attempts = 0
            return Action.C
        t = int(state.round_number)
        last_opp_c = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        last_my_c = 1 if bool(history.my_actions[-1]) else 0
        x = last_opp_c + last_my_c
        success_last = x >= m
        W = min(5, max(1, r - 1))
        w = min(W, history.round_number)
        if w <= 0:
            S = 0
            you_recent_C = 0
        else:
            opp_recent = history.opponent_actions[-w:, :]
            my_recent = history.my_actions[-w:]
            coop_counts = np.sum(opp_recent, axis=1).astype(int) + my_recent.astype(int)
            S = int(np.sum(coop_counts >= m))
            you_recent_C = int(np.sum(my_recent))
        if t >= r:
            return Action.C if x == m - 1 else Action.D
        if x == m - 1 and (not success_last):
            self.rescue_attempts += 1
        else:
            self.rescue_attempts = 0
        slack = x - m if x >= m else 0
        high_free_ride = bool(success_last and slack >= 2 and (you_recent_C >= max(0, W - 1)))
        if x == m - 1 and (not success_last) and (self.rescue_attempts <= 2):
            return Action.C
        if x <= m - 2:
            return Action.D
        majority_needed = int((W + 1) // 2)
        if high_free_ride:
            if x >= m and S >= majority_needed:
                return Action.C
            return Action.D
        else:
            if x >= m - 1 and S >= majority_needed:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    Adaptive Threshold Guardian (ATG) for the Collective Risk Dilemma.

    Cooperate when likely pivotal (especially when last round had m-1 cooperators),
    sustain cooperation when success is robust, attempt controlled recovery after failure
    when near-threshold evidence exists, and avoid wasting contributions in "hopeless"
    environments with persistent low success and low cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._Wmax = 5
        self._hopeless_mode = False
        self._near_threshold_streak = 0
        n = float(self.game_description.n_players)
        m = float(self.game_description.m)
        self._p0 = min(1.0, m / n + 0.15)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number <= 0:
            return Action.C if random.random() < self._p0 else Action.D
        t = history.round_number + 1
        W = min(self._Wmax, history.round_number)
        opp_lastW = history.opponent_actions[-W:, :]
        my_lastW = history.my_actions[-W:]
        try:
            coop_counts = np.sum(opp_lastW, axis=1).astype(int) + my_lastW.astype(int)
        except Exception:
            coop_counts = []
            for idx in range(history.round_number - W, history.round_number):
                opp_row = history.opponent_actions[idx]
                opp_c = int(sum((bool(x) for x in opp_row)))
                me_c = 1 if bool(history.my_actions[idx]) else 0
                coop_counts.append(opp_c + me_c)
            coop_counts = np.array(coop_counts, dtype=int)
        s_last = int(coop_counts[-1])
        succ_rate = float(np.mean(coop_counts >= m)) if W > 0 else 0.0
        avg_coop = float(np.mean(coop_counts)) if W > 0 else 0.0
        stableC = 0
        if W > 0:
            try:
                coop_freqs = np.mean(opp_lastW.astype(float), axis=0)
                stableC = int(np.sum(coop_freqs >= 0.8))
            except Exception:
                stableC = 0
                n_opp = opp_lastW.shape[1] if hasattr(opp_lastW, 'shape') else n - 1
                for j in range(n_opp):
                    c = 0
                    for i in range(W):
                        if bool(opp_lastW[i][j]):
                            c += 1
                    if c / float(W) >= 0.8:
                        stableC += 1
        if s_last >= m - 1:
            self._near_threshold_streak += 1
        else:
            self._near_threshold_streak = 0
        if self._hopeless_mode and self._near_threshold_streak >= 2:
            self._hopeless_mode = False
        hopeless_trigger = succ_rate <= 0.2 and avg_coop <= m - 2
        if hopeless_trigger:
            self._hopeless_mode = True
        if t == r:
            if s_last >= m - 1 or avg_coop >= m - 1 or stableC >= m - 1:
                return Action.C
            return Action.D
        if s_last == m - 1:
            return Action.C
        if self._hopeless_mode:
            return Action.D
        if s_last >= m + 1:
            return Action.C
        if s_last == m:
            if succ_rate >= 0.6 or stableC >= m - 1:
                return Action.C
            return Action.D
        if avg_coop >= m - 1 or stableC >= m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Critical Mass, Guarded Reciprocity (CMGR) for the Collective Risk Dilemma.

    - Round 1: cooperate (seed).
    - Later rounds: cooperate when a threshold-reaching coalition is plausible via:
        (1) enough reliable others in a recent window,
        (2) recent momentum of successes plus being near/at threshold last round,
        (3) last-round near-miss (C_{t-1} == m-1).
      Otherwise defect.
    - Guard: if I've been cooperating a lot but there were zero successes recently, defect.
    - Endgame: last round cooperate only if success is very likely/pivotal; optional tightening on r-1.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.7

    def _safety_margin(self, k: float) -> int:
        return 1 if k >= 2.0 else 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        n = gd.n_players
        r = gd.n_rounds
        m = gd.m
        k = gd.k
        if history is None or history.round_number == 0:
            return Action.C
        t = state.round_number
        completed = history.round_number
        w = min(5, completed)
        s = self._safety_margin(k)
        start = completed - w
        end = completed
        opp_prev = history.opponent_actions[completed - 1, :]
        C_prev = int(np.sum(opp_prev)) + (1 if bool(history.my_actions[completed - 1]) else 0)
        opp_recent = history.opponent_actions[start:end, :]
        my_recent = history.my_actions[start:end]
        opp_counts = np.sum(opp_recent, axis=1).astype(int)
        my_counts = my_recent.astype(int)
        total_counts = opp_counts + my_counts
        S = int(np.sum(total_counts >= m))
        if w > 0:
            opp_rates = np.mean(opp_recent.astype(float), axis=0)
            R = int(np.sum(opp_rates >= self.theta))
            my_recent_C_rate = float(np.mean(my_recent.astype(float)))
        else:
            R = 0
            my_recent_C_rate = 0.0
        s_effective = s
        if t == r - 1:
            s_effective = s + 1
        if t == r:
            if C_prev >= m - 1 or R >= m - 1 + s:
                return Action.C
            return Action.D
        cooperate = False
        if R >= m - 1 + s_effective:
            cooperate = True
        if w > 0:
            if S >= int(math.ceil(w / 2.0)) and C_prev >= m - 1:
                cooperate = True
        if C_prev == m - 1:
            cooperate = True
        if cooperate and my_recent_C_rate >= 0.8 and (S == 0):
            cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    CART (Collective, Adaptive, Retaliatory Thresholding) for the Collective Risk Dilemma.

    - Cooperate when a trusted coalition can plausibly reach the threshold m (trusted others + self >= m).
    - Punish free-riders only when the group succeeded (threshold met) and they defected.
    - Enter failure mode after repeated failures; mostly defect, with occasional probing cooperation near threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.P_early = 1
        self.P_late = 2
        self.F = 3 if self.k >= 2.0 else 2
        self.q = 3
        self._initialized = False
        self.punish = []
        self.recentC = []
        self.failStreak = 0
        self.C_last = 0
        self.success_last = False

    def _lazy_init_from_history(self, history: PlayerHistory) -> None:
        if self._initialized:
            return
        n_opp = int(history.opponent_actions.shape[1])
        self.punish = [0 for _ in range(n_opp)]
        self.recentC = [0 for _ in range(n_opp)]
        self.failStreak = 0
        self.C_last = 0
        self.success_last = False
        self._initialized = True

    def _update_after_round(self, t_completed: int, history: PlayerHistory) -> None:
        opp_last = history.opponent_actions[-1, :]
        my_last = bool(history.my_actions[-1])
        C_last_total = int(np.sum(opp_last)) + (1 if my_last else 0)
        success_last = C_last_total >= self.m
        if success_last:
            self.failStreak = 0
        else:
            self.failStreak += 1
        P = self.P_early if t_completed <= 2 else self.P_late
        n_opp = len(self.punish)
        for j in range(n_opp):
            acted_C = bool(opp_last[j])
            if self.punish[j] > 0:
                self.punish[j] = max(0, self.punish[j] - 1)
            if success_last and (not acted_C):
                if self.punish[j] < P:
                    self.punish[j] = P
            self.recentC[j] = 1 if acted_C else 0
        self.C_last = C_last_total
        self.success_last = success_last

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        self._lazy_init_from_history(history)
        t_completed = int(history.round_number)
        if t_completed > 0:
            if not hasattr(self, 'last_updated_completed_round'):
                self.last_updated_completed_round = 0
            if t_completed > self.last_updated_completed_round:
                self._update_after_round(t_completed, history)
                self.last_updated_completed_round = t_completed
        t = t_completed + 1
        T = 0
        for j in range(len(self.punish)):
            if self.recentC[j] == 1 and self.punish[j] == 0:
                T += 1
        feasible = T + 1 >= self.m
        inFailureMode = self.failStreak >= self.F
        near_miss = self.C_last >= self.m - 1
        if t >= self.r:
            if feasible and (not inFailureMode) and (self.success_last or near_miss):
                return Action.C
            return Action.D
        if not inFailureMode:
            return Action.C if feasible else Action.D
        if feasible and near_miss:
            return Action.C
        if t % self.q == 0 and near_miss:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    Cautious Threshold Builder (CTB) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: cooperate (probe/seed).
    - If recent play meets or often meets the threshold: cooperate (commit/lock-in).
    - If near the threshold: cooperate (try to be pivotal).
    - If far below threshold: defect to avoid wasting contributions, with sparse re-tests.
    - Last round: cooperate only if success is likely/pivotal; otherwise defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        my_c = history.my_actions.astype(np.int64)
        opp_c = history.opponent_actions.astype(np.int64)
        c_counts = my_c + np.sum(opp_c, axis=1)
        t_completed = int(history.round_number)
        t = t_completed + 1
        lastC = int(c_counts[-1])
        w = min(3, t_completed)
        recent_C = c_counts[-w:]
        avgC = float(np.mean(recent_C)) if w > 0 else 0.0
        recent_S = (recent_C >= m).astype(np.float64)
        succRate = float(np.mean(recent_S)) if w > 0 else 0.0
        if t == r:
            if lastC >= m:
                return Action.C
            if lastC == m - 1:
                return Action.C
            return Action.D
        if lastC >= m:
            return Action.C
        if succRate >= 2.0 / 3.0:
            return Action.C
        if m - 1 <= lastC <= m + 1:
            return Action.C
        if avgC >= m - 1:
            return Action.C
        hopelessGap = max(2, int(math.ceil((n - m) / 3.0)))
        if lastC <= m - hopelessGap:
            if succRate == 0.0 and t % 3 == 1:
                return Action.C
            return Action.D
        if lastC >= m - 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    Threshold-Insurance with Credible Retaliation (TICR) for the Collective Risk Dilemma.

    Key behaviors:
    - Start by cooperating.
    - Cooperate when pivotal (near threshold) or when cooperation is stable.
    - Punish briefly when repeated failures coincide with persistent defectors.
    - After punishment, periodically "test" cooperation to enable recovery.
    - Issue one-round warnings when exactly at threshold but exploitation signs appear.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 3
        self.Q = 2
        self.P = 2
        self.T = 2
        self.Rtest = 3
        self.E = 2
        self.punish_until = 0
        self.last_test_round = 0
        n_opponents = max(0, int(self.game_description.n_players) - 1)
        self.defect_streak = [0 for _ in range(n_opponents)]
        self.history_C_counts = []
        self._last_processed_completed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            self.punish_until = 0
            self.last_test_round = 0
            self.history_C_counts = []
            for i in range(len(self.defect_streak)):
                self.defect_streak[i] = 0
            self._last_processed_completed_rounds = 0
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        if completed < self._last_processed_completed_rounds:
            self.punish_until = 0
            self.last_test_round = 0
            self.history_C_counts = []
            for i in range(len(self.defect_streak)):
                self.defect_streak[i] = 0
        if completed > 0 and completed > self._last_processed_completed_rounds:
            last_my = bool(history.my_actions[-1])
            last_opp = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
            n_opponents_obs = int(last_opp.shape[0]) if hasattr(last_opp, 'shape') else 0
            if n_opponents_obs != len(self.defect_streak):
                self.defect_streak = [0 for _ in range(n_opponents_obs)]
            for j in range(n_opponents_obs):
                if bool(last_opp[j]) is False:
                    self.defect_streak[j] += 1
                else:
                    self.defect_streak[j] = 0
            last_round_C_count = int(np.sum(last_opp)) + (1 if last_my else 0)
            self.history_C_counts.append(last_round_C_count)
            if len(self.history_C_counts) > self.W:
                self.history_C_counts = self.history_C_counts[-self.W:]
            self._last_processed_completed_rounds = completed
        if completed == 0 or len(self.history_C_counts) == 0:
            self._last_processed_completed_rounds = completed
            return Action.C
        last_round_C_count = int(self.history_C_counts[-1])
        bad_in_window = 0
        for x in self.history_C_counts:
            if int(x) < m:
                bad_in_window += 1
        persistent_defector_exists = False
        persistent_defectors_count = 0
        for ds in self.defect_streak:
            if ds >= self.Q:
                persistent_defector_exists = True
                persistent_defectors_count += 1
        many_persistent_defectors = persistent_defectors_count >= self.E
        if t > self.punish_until and bad_in_window >= self.Q and persistent_defector_exists:
            self.punish_until = t + self.P - 1
        if t <= self.punish_until:
            return Action.D
        test_due = last_round_C_count == m - 1 or t - self.last_test_round >= self.Rtest
        if last_round_C_count == m - 1:
            self.last_test_round = t
            return Action.C
        stable = False
        if len(self.history_C_counts) >= self.T:
            stable = True
            recent = self.history_C_counts[-self.T:]
            for x in recent:
                if int(x) < m:
                    stable = False
                    break
        if stable:
            return Action.C
        if last_round_C_count <= m - 2:
            if t == r:
                return Action.D
            if test_due:
                self.last_test_round = t
                return Action.C
            return Action.D
        if last_round_C_count == m:
            if many_persistent_defectors:
                return Action.D
            return Action.C
        if last_round_C_count >= m:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Pivotal-Reciprocal Thresholding (PRT) for the Collective Risk Dilemma.

    - Round 1: cooperate.
    - Rounds 2..r-1: cooperate if near-threshold last round (m-1 or m), or if
      threshold met in >=2 of last 3 rounds, or if enough reliable cooperators
      among opponents exist (coopRate >= theta) such that S >= m-1.
    - Final round: cooperate only if success is very plausible (last round met m,
      last round was m-1, or S >= m-1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        opp_last = history.opponent_actions[-1, :]
        c_last = int(np.sum(opp_last)) + (1 if bool(history.my_actions[-1]) else 0)
        opp_hist = history.opponent_actions
        denom = history.round_number
        if denom <= 0:
            s_reliable = 0
        else:
            coop_rates = np.sum(opp_hist, axis=0) / float(denom)
            s_reliable = int(np.sum(coop_rates >= self.theta))
        lookback = 3
        start_idx = max(0, history.round_number - lookback)
        opp_window = history.opponent_actions[start_idx:history.round_number, :]
        my_window = history.my_actions[start_idx:history.round_number]
        if len(my_window) == 0:
            success_count = 0
        else:
            total_coops = np.sum(opp_window, axis=1) + my_window.astype(np.int64)
            success_count = int(np.sum(total_coops >= m))
        if t == r:
            if c_last >= m:
                return Action.C
            if c_last == m - 1:
                return Action.C
            if s_reliable >= m - 1:
                return Action.C
            return Action.D
        if c_last == m - 1 or c_last == m:
            return Action.C
        if success_count >= 2:
            return Action.C
        if s_reliable >= m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Adaptive Pivotal Cooperation (APC) for the Collective Risk Dilemma.

    Core behavior:
    - Start with cooperation to invite coordination.
    - Track recent cooperation reliability of opponents (sliding window).
    - Cooperate when likely pivotal to reaching threshold m; defect when success looks unlikely.
    - React strongly to near-misses and fragile successes.
    - After repeated failures, occasionally probe to rebuild cooperation.
    - Endgame: prioritize only clearly feasible/pivotal cooperation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta = 0.6
        self.max_window = 5
        self.base_probe_period = 3
        self.fail_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            self.fail_streak = 0
            return Action.C
        t = int(state.round_number)
        completed = int(history.round_number)
        if t <= 1:
            t = completed + 1
        opp_last = history.opponent_actions[-1, :]
        my_last = bool(history.my_actions[-1])
        x_last = int(np.sum(opp_last) + (1 if my_last else 0))
        if x_last >= m:
            self.fail_streak = 0
        else:
            self.fail_streak += 1
        W = int(min(self.max_window, completed))
        if W <= 0:
            S = 0
        else:
            recent = history.opponent_actions[-W:, :]
            rel = np.mean(recent.astype(float), axis=0)
            S = int(np.sum(rel >= self.theta))
        P = self.base_probe_period
        if r <= 3:
            P = 10 ** 9
        if t == r - 1:
            p_support = 0.25
        else:
            p_support = 0.5
        if my_last and x_last <= m - 2 and (S != m - 1):
            if t == r and S >= m - 1:
                return Action.C
            return Action.D
        if x_last >= m:
            if x_last == m:
                return Action.C
        elif x_last == m - 1:
            return Action.C
        if self.fail_streak >= 2 and P > 0 and (t % P == 0):
            if S >= m - 2:
                return Action.C
            return Action.D
        if t == r:
            if S >= m - 1:
                return Action.C
            return Action.D
        if S >= m:
            return Action.D
        if S == m - 1:
            return Action.C
        if S == m - 2:
            return Action.C if random.random() < p_support else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    Pivotal-Then-Selective (PTS) for the Collective Risk Dilemma.

    Summary:
    - Track "reliable" cooperators among opponents using a recent window.
    - Defect when the threshold is likely met without us; cooperate when we appear pivotal (m-1).
    - Early "investment" only when the group was just one short last round.
    - Give up after repeated failures, with a one-shot test when last round was m-1.
    - Last round: purely pivotal.
    - Pivotal fatigue: if we were pivotal multiple times recently, defect once to test if others step up.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 3
        self.theta = 2.0 / 3.0
        self.F = 3
        self.fatigue_W = 3
        self.fatigue_limit = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        n = gd.n_players
        r = gd.n_rounds
        m = gd.m
        k = gd.k
        if history is None:
            return Action.C if k >= 2.0 else Action.D
        t = state.round_number
        completed = history.round_number
        if completed <= 0:
            return Action.C if k >= 2.0 else Action.D
        last_opp_coop = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        last_my_coop = 1 if bool(history.my_actions[-1]) else 0
        last_total_C = last_opp_coop + last_my_coop
        totals = []
        if history.opponent_actions.size:
            opp_totals = np.sum(history.opponent_actions.astype(int), axis=1)
        else:
            opp_totals = np.zeros(completed, dtype=int)
        my_totals = history.my_actions.astype(int)
        totals = list((opp_totals + my_totals).astype(int))
        success_streak = 0
        fail_streak = 0
        idx = completed - 1
        while idx >= 0 and totals[idx] >= m:
            success_streak += 1
            idx -= 1
        idx = completed - 1
        while idx >= 0 and totals[idx] < m:
            fail_streak += 1
            idx -= 1
        W = self.W
        w_start = max(0, completed - W)
        window_len = completed - w_start
        if window_len > 0 and history.opponent_actions.size:
            recent = history.opponent_actions[w_start:completed, :].astype(float)
            recent_rates = np.mean(recent, axis=0)
            reliable_mask = recent_rates >= self.theta
            pred = int(np.sum(reliable_mask))
        else:
            pred = 0
        fw = self.fatigue_W
        f_start = max(0, completed - fw)
        pivotal_recent = 0
        for rr in range(f_start, completed):
            if bool(history.my_actions[rr]) and totals[rr] == m:
                pivotal_recent += 1
        if t >= r:
            return Action.C if pred == m - 1 else Action.D
        if fail_streak >= self.F:
            return Action.C if last_total_C == m - 1 else Action.D
        if pivotal_recent >= self.fatigue_limit:
            return Action.D
        early_window = int(math.ceil(r / 3.0))
        if t <= early_window:
            if last_total_C == m - 1:
                return Action.C
            if pred >= m:
                return Action.D
            if pred == m - 1:
                return Action.C
            return Action.D
        if pred >= m:
            return Action.D
        if pred == m - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    Opportunistic Threshold Best-Response (OTBR) for the Collective Risk Dilemma.

    Core idea:
    - Maintain recency-weighted (smoothed) estimates of opponents' cooperation probabilities.
    - Model the number of other cooperators as a Poisson-binomial and approximate with a Normal.
    - Cooperate only when the expected marginal advantage of being pivotal exceeds a risk premium.
    - Add two behavioral guards:
        (1) Lock-in free-ride: if the group has been meeting the threshold without us recently, keep defecting.
        (2) Emergency rescue: if the group missed by exactly 1 last round, cooperate once with lower premium.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.p0 = 0.3
        self.alpha = 1.0
        self.Wmax_base = 10
        self.Wmax_volatile = 15
        self.theta_base = 0.05
        self.theta_last = 0.25
        self.L_lock = 3
        self.vol_window = 8
        self.vol_threshold = 0.35

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        n = gd.n_players
        r = gd.n_rounds
        m = gd.m
        k = gd.k

        def _norm_cdf(z: float) -> float:
            if z > 8.0:
                return 1.0
            if z < -8.0:
                return 0.0
            erf = getattr(math, 'erf', None)
            if erf is not None:
                return 0.5 * (1.0 + erf(z / math.sqrt(2.0)))
            return 1.0 / (1.0 + math.exp(-1.702 * z))

        def _prob_geq_normal(mu: float, sigma: float, threshold: int) -> float:
            z = (threshold - 0.5 - mu) / sigma
            return 1.0 - _norm_cdf(z)

        def _prob_between_normal(mu: float, sigma: float, lo: float, hi: float) -> float:
            z_lo = (lo - mu) / sigma
            z_hi = (hi - mu) / sigma
            p = _norm_cdf(z_hi) - _norm_cdf(z_lo)
            if p < 0.0:
                return 0.0
            if p > 1.0:
                return 1.0
            return p
        if history is None or history.round_number <= 0:
            p = [self.p0 for _ in range(n - 1)]
            mu = float(np.sum(p)) if len(p) > 0 else 0.0
            var = float(np.sum([pi * (1.0 - pi) for pi in p])) if len(p) > 0 else 0.0
            if var <= 0.0:
                s_det = int(round(mu))
                P_ge_m = 1.0 if s_det >= m else 0.0
                P_eq_m1 = 1.0 if s_det == m - 1 else 0.0
            else:
                sigma = math.sqrt(var)
                P_ge_m = _prob_geq_normal(mu, sigma, m)
                P_eq_m1 = _prob_between_normal(mu, sigma, m - 1.5, m - 0.5)
            theta = self.theta_base
            Delta = (k - 1.0) * P_eq_m1 - 1.0 * P_ge_m
            return Action.C if Delta >= theta else Action.D
        t = history.round_number
        if t >= self.L_lock:
            recent_my = history.my_actions[-self.L_lock:]
            recent_opp = history.opponent_actions[-self.L_lock:, :] if history.opponent_actions.size else None
            lock = True
            for idx in range(self.L_lock):
                if bool(recent_my[idx]):
                    lock = False
                    break
                opp_coop = int(np.sum(recent_opp[idx, :])) if recent_opp is not None else 0
                total_coop = opp_coop + 0
                if total_coop < m:
                    lock = False
                    break
            if lock:
                return Action.D
        is_volatile = False
        if history.opponent_actions.size:
            w = min(self.vol_window, t - 1)
            if w >= 1:
                recent = history.opponent_actions[-(w + 1):, :]
                flips = np.not_equal(recent[1:, :], recent[:-1, :])
                flip_rate = float(np.mean(flips)) if flips.size else 0.0
                if flip_rate >= self.vol_threshold:
                    is_volatile = True
        Wmax = self.Wmax_volatile if is_volatile else self.Wmax_base
        W = min(Wmax, t)
        alpha = self.alpha
        opp_actions = history.opponent_actions
        p_list = []
        if opp_actions.size:
            recent_opp_actions = opp_actions[-W:, :]
            sums = np.sum(recent_opp_actions, axis=0).astype(float)
            denom = 2.0 * alpha + float(W)
            p_arr = (alpha + sums) / denom
            p_arr = np.clip(p_arr, 0.0, 1.0)
            p_list = [float(x) for x in p_arr.tolist()]
        else:
            p_list = [self.p0 for _ in range(max(0, n - 1))]
        mu = float(np.sum(p_list)) if len(p_list) > 0 else 0.0
        var = float(np.sum([pi * (1.0 - pi) for pi in p_list])) if len(p_list) > 0 else 0.0
        if var <= 1e-12:
            s_det = int(round(mu))
            P_ge_m = 1.0 if s_det >= m else 0.0
            P_eq_m1 = 1.0 if s_det == m - 1 else 0.0
        else:
            sigma = math.sqrt(var)
            P_ge_m = _prob_geq_normal(mu, sigma, m)
            P_eq_m1 = _prob_between_normal(mu, sigma, m - 1.5, m - 0.5)
        theta = self.theta_base
        if is_volatile:
            theta = theta + 0.05
        if t >= r - 1:
            theta = max(theta, self.theta_last)
        last_total_coop = int(np.sum(history.opponent_actions[-1, :])) + (1 if bool(history.my_actions[-1]) else 0)
        if last_total_coop == m - 1:
            theta = 0.0
        Delta = (k - 1.0) * P_eq_m1 - 1.0 * P_ge_m
        return Action.C if Delta >= theta else Action.D

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    Pivotal / Free-ride strategy for the Collective Risk Dilemma.

    Maintains Laplace-smoothed cooperation probabilities for each opponent and
    decides each round by classifying the expected others' cooperation into:
      - safe success without me  -> Defect
      - hopeless even with me    -> Defect
      - pivotal/knife-edge zone  -> Cooperate

    Includes streak-based robustness:
      - after L consecutive successes, defect until a failure is observed
      - after L consecutive failures, avoid repeated "probe" cooperation more often than every B rounds

    Final round: defect unless the expected others' cooperation is near-pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 1.0
        self.z = 1.0
        self.L = 2
        self.B = 3
        self.last_round_buffer = 0.25
        n = self.game_description.n_players
        m = self.game_description.m
        self._c_count = np.zeros(n - 1, dtype=np.float64)
        self._t_obs = np.zeros(n - 1, dtype=np.float64)
        p0 = float(m) / float(n) if n > 0 else 0.5
        self._p = np.full(n - 1, p0, dtype=np.float64)
        self._success_streak = 0
        self._fail_streak = 0
        self._last_probe_round = -10 ** 9
        self._processed_rounds = 0

    def _threshold_met_last_round(self, history: PlayerHistory) -> bool:
        my_c = 1 if bool(history.my_actions[-1]) else 0
        opp_c = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        return my_c + opp_c >= self.game_description.m

    def _update_from_history(self, history: PlayerHistory) -> None:
        total_completed = history.round_number
        if total_completed <= self._processed_rounds:
            return
        start = self._processed_rounds
        end = total_completed
        for t in range(start, end):
            opp_row = history.opponent_actions[t, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
            if opp_row.size:
                self._t_obs += 1.0
                self._c_count += opp_row.astype(np.float64)
                denom = self._t_obs + 2.0 * self.alpha
                self._p = (self._c_count + self.alpha) / denom
            my_c = 1 if bool(history.my_actions[t]) else 0
            opp_c = int(np.sum(opp_row)) if opp_row.size else 0
            met = my_c + opp_c >= self.game_description.m
            if met:
                self._success_streak += 1
                self._fail_streak = 0
            else:
                self._fail_streak += 1
                self._success_streak = 0
        self._processed_rounds = total_completed

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            return Action.C if m > n // 2 else Action.D
        self._update_from_history(history)
        t = int(state.round_number)
        if self._p.size:
            E = float(np.sum(self._p))
            V = float(np.sum(self._p * (1.0 - self._p)))
        else:
            E = 0.0
            V = 0.0
        sigma = math.sqrt(V) if V > 0.0 else 0.0
        if self._success_streak >= self.L:
            return Action.D
        if self._fail_streak >= self.L and t - self._last_probe_round < self.B:
            return Action.D
        if t >= r:
            if E >= m - self.last_round_buffer and E < m:
                return Action.C
            return Action.D
        if E - self.z * sigma >= m:
            return Action.D
        if E + self.z * sigma < m - 1:
            return Action.D
        self._last_probe_round = t
        return Action.C

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    Pivotal-threshold, evidence-weighted strategy for the Collective Risk Dilemma.

    Maintains EWMA cooperation propensities per opponent, estimates the probability
    that exactly (m-1) other players cooperate (i.e., you are pivotal), and cooperates
    iff P(pivotal) > 1/k (with a conservative throttle after wasted cooperations and
    a small last-round conservatism bump).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.lambda_ewma = 0.8
        self.window_w = 5
        self.wasted_limit = 2
        self.burn_alpha = 0.7
        self.epsilon_last = 0.02
        n = int(self.game_description.n_players)
        self.p = np.full(max(0, n - 1), 0.5, dtype=float)

    def _prob_sum_equals(self, probs, target):
        """
        Poisson-binomial DP: returns P(sum X_i == target) for independent Bernoulli(probs[i]).
        Uses O(len(probs)*target) time and O(target) space.
        """
        if target < 0:
            return 0.0
        if target == 0 and len(probs) == 0:
            return 1.0
        if len(probs) == 0:
            return 0.0
        tmax = int(target)
        dp = np.zeros(tmax + 1, dtype=float)
        dp[0] = 1.0
        for p_i in probs:
            if p_i < 0.0:
                p_i = 0.0
            elif p_i > 1.0:
                p_i = 1.0
            q_i = 1.0 - p_i
            j = tmax
            while j >= 1:
                dp[j] = dp[j] * q_i + dp[j - 1] * p_i
                j -= 1
            dp[0] = dp[0] * q_i
        return float(dp[tmax])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is not None and history.round_number > 0 and (len(self.p) == max(0, n - 1)):
            last = history.round_number - 1
            last_opp = history.opponent_actions[last, :].astype(float)
            lam = self.lambda_ewma
            self.p = lam * self.p + (1.0 - lam) * last_opp
        wasted = 0
        if history is not None and history.round_number > 0:
            start = max(0, history.round_number - self.window_w)
            t = start
            while t < history.round_number:
                my_c = bool(history.my_actions[t])
                if my_c:
                    total_c = int(bool(history.my_actions[t])) + int(np.sum(history.opponent_actions[t, :]))
                    if total_c < m:
                        wasted += 1
                t += 1
        p_pivotal = self._prob_sum_equals(self.p, m - 1)
        threshold = 1.0 / k if k > 0.0 else 1.0
        if wasted > self.wasted_limit:
            alpha = self.burn_alpha
            if alpha > 0.0:
                threshold = threshold / alpha
        if state is not None and int(state.round_number) >= r:
            threshold = threshold + self.epsilon_last
        return Action.C if p_pivotal > threshold else Action.D

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    Pivotal Opportunist (PO) for the Collective Risk Dilemma.

    Maintains an exponential moving average (EMA) estimate of how many OTHER players
    typically cooperate, plus an EMA variability proxy. Each round it:
      - Defects if success is very likely without us (free-ride).
      - Defects if failure is very likely even with us (avoid wasting cost).
      - Otherwise cooperates only if the estimated chance of being pivotal makes it
        worth paying cost 1 for expected benefit k.

    Also:
      - First round "probe": cooperate if k is large (k >= 2), else defect.
      - Exploitation lock-in: if recent rounds consistently succeed and we have
        defected at least once while still succeeding, defect until success stops.
      - Cooldown: after cooperating but still failing twice recently, defect for a
        couple rounds to avoid repeated wasted contributions.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.3
        self.eps = 1e-06
        self.L = 3
        self.cooldown_len = 2
        self.mu = float(self.m - 1)
        self.d = 1.0
        self.cooldown = 0

    def _update_beliefs_from_history(self, history: PlayerHistory) -> None:
        """
        Recompute mu, d, and cooldown from scratch from the provided history.
        This avoids relying on persistent mutable state that might desync if the
        environment re-instantiates or calls irregularly.
        """
        mu = float(self.m - 1)
        d = 1.0
        coop_fail_rounds = []
        T = history.round_number
        if T <= 0:
            self.mu, self.d = (mu, d)
            self.cooldown = 0
            return
        for t in range(T):
            my_c = bool(history.my_actions[t])
            opp_c = int(np.sum(history.opponent_actions[t, :])) if history.opponent_actions.size else 0
            total_c = opp_c + (1 if my_c else 0)
            X = total_c - (1 if my_c else 0)
            mu_new = (1.0 - self.alpha) * mu + self.alpha * float(X)
            d_new = (1.0 - self.alpha) * d + self.alpha * abs(float(X) - mu_new)
            mu, d = (mu_new, d_new)
            if my_c and total_c < self.m:
                coop_fail_rounds.append(t)
        d = max(d, 0.5)
        cooldown = 0
        if len(coop_fail_rounds) >= 2:
            last = coop_fail_rounds[-1]
            prev = coop_fail_rounds[-2]
            if last >= T - 1 and prev >= T - 6:
                since_last = T - 1 - last
                remaining = self.cooldown_len - since_last
                if remaining > 0:
                    cooldown = remaining
        self.mu, self.d = (mu, d)
        self.cooldown = cooldown

    def _stable_success_lockin(self, history: PlayerHistory) -> bool:
        """
        If in last L rounds, threshold met each time AND we defected at least once
        among those rounds while still meeting threshold, then lock into defect.
        """
        T = history.round_number
        if T < self.L:
            return False
        recent_idx = list(range(T - self.L, T))
        all_succeed = True
        defected_and_succeeded = False
        for t in recent_idx:
            my_c = bool(history.my_actions[t])
            opp_c = int(np.sum(history.opponent_actions[t, :])) if history.opponent_actions.size else 0
            total_c = opp_c + (1 if my_c else 0)
            if total_c < self.m:
                all_succeed = False
                break
            if not my_c and total_c >= self.m:
                defected_and_succeeded = True
        return all_succeed and defected_and_succeeded

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C if self.k >= 2.0 else Action.D
        self._update_beliefs_from_history(history)
        if self.cooldown > 0:
            return Action.D
        if self._stable_success_lockin(history):
            return Action.D
        mu = float(self.mu)
        d = float(self.d)
        m = self.m
        k = self.k
        if mu - d >= m:
            return Action.D
        if mu + d <= m - 2:
            return Action.D
        p_piv = 1.0 - abs(m - 1 - mu) / (d + self.eps)
        if p_piv < 0.0:
            p_piv = 0.0
        elif p_piv > 1.0:
            p_piv = 1.0
        return Action.C if p_piv * k > 1.0 else Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    Pivotal-Then-Parasite (PTP) for the Collective Risk Dilemma.

    Maintains per-opponent smoothed cooperation probability estimates, computes the
    Poisson-binomial distribution for the number of other cooperators, and:
    - Defects if threshold is (almost) surely met without us (free-ride).
    - Defects if even with us the threshold is very unlikely (avoid wasting).
    - Otherwise cooperates only when likely pivotal: k * P(S = m-1) > 1 + tau(t).
    Includes a temporary caution bump after experiencing a failed threshold round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 1.0
        self.p_clip_low = 0.01
        self.p_clip_high = 0.99
        self.epsilon = 0.05
        self.delta = 0.05
        self._extra_caution_next = 0.0

    def _clip(self, x: float, lo: float, hi: float) -> float:
        if x < lo:
            return lo
        if x > hi:
            return hi
        return x

    def _tau(self, t: int) -> float:
        if t <= 2:
            base = 0.5
        elif t <= self.r - 2:
            base = 0.2
        else:
            base = 0.0
        return base + self._extra_caution_next

    def _poisson_binomial_pmf(self, ps):
        pmf = [1.0]
        for p in ps:
            q = 1.0 - p
            new = [0.0] * (len(pmf) + 1)
            for s in range(len(pmf)):
                new[s] += pmf[s] * q
                new[s + 1] += pmf[s] * p
            pmf = new
        return pmf

    def _estimate_opponent_ps(self, history: PlayerHistory):
        if history is None:
            return [0.5] * (self.n - 1)
        t_obs = int(history.round_number)
        if t_obs <= 0:
            return [0.5] * (self.n - 1)
        opp = history.opponent_actions
        n_opp = opp.shape[1] if len(opp.shape) == 2 else 0
        ps = []
        for j in range(n_opp):
            x = float(np.sum(opp[:, j]))
            p = (x + self.alpha) / (t_obs + 2.0 * self.alpha)
            p = self._clip(p, self.p_clip_low, self.p_clip_high)
            ps.append(p)
        while len(ps) < self.n - 1:
            ps.append(0.5)
        return ps[:self.n - 1]

    def _update_failure_caution(self, history: PlayerHistory):
        self._extra_caution_next = 0.0
        if history is None or history.round_number <= 0:
            return
        last_opp_coop = int(np.sum(history.opponent_actions[-1, :]))
        last_me_coop = 1 if bool(history.my_actions[-1]) else 0
        total_coop = last_opp_coop + last_me_coop
        threshold_met = total_coop >= self.m
        if not threshold_met:
            self._extra_caution_next = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None:
            self._update_failure_caution(history)
        else:
            self._extra_caution_next = 0.0
        ps = self._estimate_opponent_ps(history)
        pmf = self._poisson_binomial_pmf(ps)
        m1 = self.m - 1
        if 0 <= m1 <= self.n - 1:
            p_eq_m1 = float(pmf[m1])
            p_ge_m1 = float(sum(pmf[m1:]))
        else:
            p_eq_m1 = 0.0
            p_ge_m1 = 0.0
        if 0 <= self.m <= self.n - 1:
            p_ge_m = float(sum(pmf[self.m:]))
        else:
            p_ge_m = 1.0 if self.m <= 0 else 0.0
        if p_ge_m >= 1.0 - self.epsilon:
            return Action.D
        if p_ge_m1 <= self.delta:
            return Action.D
        tau = self._tau(t)
        if self.k * p_eq_m1 > 1.0 + tau:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    Pivotal Free-Rider with Reliability Gating (PFR-RG).

    Core behavior:
    - Default to Defect (D).
    - Cooperate (C) only when you are likely pivotal: expected other cooperators is near m-1,
      and recent group success rate is sufficiently high.
    - If you observe that your cooperation is repeatedly wasted (threshold still fails or was
      met without you), enter a short "hard defect" cooldown to avoid being farmed.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.4
        self.tau = 0.55
        self.tau_last = 0.7
        self.L = 3
        self.window = 6
        self.waste_trigger = 2
        self._initialized = False
        self.coop_rate = None
        self.p_success = 0.5
        self.hard_defect = 0
        self._last_processed_rounds = 0
        self._waste_fail = []
        self._waste_unneeded = []

    def _ensure_initialized(self, history: PlayerHistory):
        if self._initialized:
            return
        n_opponents = int(history.opponent_actions.shape[1])
        self.coop_rate = np.array([0.5] * n_opponents, dtype=float)
        self.p_success = 0.5
        self.hard_defect = 0
        self._last_processed_rounds = 0
        self._waste_fail = []
        self._waste_unneeded = []
        self._initialized = True

    def _append_rolling(self, arr, val):
        arr.append(bool(val))
        if len(arr) > self.window:
            arr.pop(0)

    def _count_true(self, arr):
        s = 0
        for v in arr:
            if v:
                s += 1
        return s

    def _update_from_history(self, history: PlayerHistory):
        """
        Process any new completed rounds since last call, updating:
        - opponent coop EWMAs
        - success EWMA
        - waste counters and hard-defect triggers
        """
        self._ensure_initialized(history)
        n = self.game_description.n_players
        m = self.game_description.m
        total_rounds = int(history.round_number)
        if self._last_processed_rounds >= total_rounds:
            return
        for t in range(self._last_processed_rounds, total_rounds):
            opp = history.opponent_actions[t, :]
            my_c = bool(history.my_actions[t])
            c_opp = int(np.sum(opp))
            c_total = c_opp + (1 if my_c else 0)
            success = 1.0 if c_total >= m else 0.0
            self.p_success = (1.0 - self.alpha) * self.p_success + self.alpha * success
            obs = opp.astype(float)
            self.coop_rate = (1.0 - self.alpha) * self.coop_rate + self.alpha * obs
            if my_c:
                wasted_fail = c_total < m
                wasted_unneeded = c_total - 1 >= m
                self._append_rolling(self._waste_fail, wasted_fail)
                self._append_rolling(self._waste_unneeded, wasted_unneeded)
            else:
                self._append_rolling(self._waste_fail, False)
                self._append_rolling(self._waste_unneeded, False)
            if self.hard_defect <= 0:
                if self._count_true(self._waste_fail) >= self.waste_trigger or self._count_true(self._waste_unneeded) >= self.waste_trigger:
                    self.hard_defect = int(self.L)
                    self._waste_fail = []
                    self._waste_unneeded = []
        self._last_processed_rounds = total_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            return Action.D
        self._update_from_history(history)
        if self.hard_defect > 0:
            self.hard_defect -= 1
            return Action.D
        t = int(state.round_number)
        is_last_round = t >= n_rounds
        if self.coop_rate is None:
            return Action.D
        E = float(np.sum(self.coop_rate))
        if is_last_round:
            if E >= m:
                return Action.D
            if E < m - 1:
                return Action.D
            return Action.C if self.p_success >= self.tau_last else Action.D
        if E >= m:
            return Action.D
        if E <= m - 2:
            return Action.D
        return Action.C if self.p_success >= self.tau else Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    Robust Threshold Best-Response (RTBR) for the repeated Collective Risk Dilemma.

    Maintains smoothed beliefs p_j about each opponent's probability of cooperating.
    Each round:
      - Defect if threshold is very likely met without us (safe free-ride),
      - Defect if threshold is very unlikely even with us (avoid wasting),
      - Otherwise (pivotal region), cooperate only if estimated pivotal probability
        exceeds 1/k (with a slightly stricter requirement in the last round).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha_base = 0.25
        self.z = 1.25
        self.delta_last = 0.1
        self.shock_lambda = 0.9
        self._p = None
        self._initialized_for_opponents = None

    def _ensure_beliefs(self, n_opponents: int):
        if self._p is None or self._initialized_for_opponents != n_opponents:
            self._p = [0.5 for _ in range(n_opponents)]
            self._initialized_for_opponents = n_opponents

    def _normal_cdf(self, x: float) -> float:
        return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None:
            if m > n / 2.0:
                return Action.C
            return Action.D
        opp_actions = history.opponent_actions
        n_opponents = int(opp_actions.shape[1]) if opp_actions is not None and opp_actions.ndim == 2 else max(0, n - 1)
        self._ensure_beliefs(n_opponents)
        t_completed = int(history.round_number)
        t_next = t_completed + 1
        is_last_round = t_next >= r
        if t_completed >= 1 and opp_actions is not None and (opp_actions.ndim == 2):
            last_opp = opp_actions[-1, :]
            last_total_coop = int(np.sum(last_opp)) + (1 if bool(history.my_actions[-1]) else 0)
            failed_last = last_total_coop < m
            alpha = self.alpha_base
            for j in range(n_opponents):
                aj = 1.0 if bool(last_opp[j]) else 0.0
                self._p[j] = (1.0 - alpha) * self._p[j] + alpha * aj
            if failed_last:
                for j in range(n_opponents):
                    self._p[j] *= self.shock_lambda
                    if self._p[j] < 0.0:
                        self._p[j] = 0.0
        mu = 0.0
        var = 0.0
        for pj in self._p:
            mu += pj
            var += pj * (1.0 - pj)
        sigma = math.sqrt(var) if var > 0.0 else 0.0
        if mu - self.z * sigma >= float(m):
            return Action.D
        if mu + self.z * sigma <= float(m - 1):
            return Action.D
        if sigma <= 1e-12:
            s_det = int(round(mu))
            q = 1.0 if s_det == m - 1 else 0.0
        else:
            upper = (float(m) - 0.5 - mu) / sigma
            lower = (float(m) - 1.5 - mu) / sigma
            q = self._normal_cdf(upper) - self._normal_cdf(lower)
            if q < 0.0:
                q = 0.0
            elif q > 1.0:
                q = 1.0
        thresh = 1.0 / k if k > 0.0 else 1000000000.0
        if is_last_round:
            thresh += self.delta_last
        return Action.C if q > thresh else Action.D

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    Threshold-Guardian with Forgiving Punishment (TGFP) for the Collective Risk Dilemma.

    - Cooperate by default after successful threshold rounds to maintain coordination.
    - After failure, cooperate only when plausibly pivotal (gap <= 1), cautiously attempt recovery when gap==2,
      otherwise defect to avoid wasted contributions.
    - Occasionally "reboot" cooperation during prolonged failure streaks via periodic test rounds.
    - Apply a one-round warning defection after success only under clear, widespread, persistent exploitation.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self._fail_streak = 0
        self._defect_streak = [0 for _ in range(max(0, self.n - 1))]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        n = self.n
        r = self.r
        m = self.m
        t = state.round_number
        last_idx = history.round_number - 1
        opp_last = history.opponent_actions[last_idx, :]
        x_others_last = int(np.sum(opp_last))
        my_last = bool(history.my_actions[last_idx])
        x_last = x_others_last + (1 if my_last else 0)
        success_last = x_last >= m
        if success_last:
            self._fail_streak = 0
        else:
            self._fail_streak += 1
        if len(self._defect_streak) != len(opp_last):
            self._defect_streak = [0 for _ in range(len(opp_last))]
        for j in range(len(opp_last)):
            if bool(opp_last[j]):
                self._defect_streak[j] = 0
            else:
                self._defect_streak[j] += 1
        gap = m - x_others_last
        one_third = int(math.ceil((n - 1) / 3.0))
        num_persistent_defectors = 0
        for s in self._defect_streak:
            if s >= 2:
                num_persistent_defectors += 1
        widespread_persistent = num_persistent_defectors >= one_third
        exploited_last = my_last and x_others_last <= n - 1 - one_third
        if t == r:
            if success_last:
                return Action.C
            return Action.C if gap <= 1 else Action.D
        if success_last:
            if exploited_last and widespread_persistent:
                return Action.D
            return Action.C
        if gap <= 1:
            return Action.C
        if gap == 2:
            return Action.C if self._fail_streak == 1 else Action.D
        T = max(2, int(math.floor(n / float(max(1, m - 1)))))
        if self._fail_streak >= 2 and T > 0 and (t % T == 0):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    Adaptive Pivotal Reciprocity (APR) for the Collective Risk Dilemma.

    Summary:
    - Round 1: Cooperate (seed cooperation).
    - After success (>= m cooperators): Cooperate (stabilize threshold).
    - After failure:
        * If exactly m-1 cooperators: Cooperate (pivotal).
        * If exactly m-2 cooperators: Cooperate only if failure streak is short (<= 1).
        * If far (<= m-3): Defect, but periodically "probe" with cooperation on a fixed interval
          when the failure streak is at least 2.
    - Last round: Cautious collective last-round rule as specified.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.probe_interval = max(2, int(math.floor(self.r / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        completed = history.round_number
        opp_coop_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        my_coop_last = 1 if bool(history.my_actions[-1]) else 0
        xprev = opp_coop_last + my_coop_last
        success_prev = xprev >= self.m
        streak_success = 0
        streak_fail = 0
        idx = completed - 1
        while idx >= 0:
            opp_coop = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            x = opp_coop + my_coop
            if x >= self.m:
                streak_success += 1
                idx -= 1
            else:
                break
        idx = completed - 1
        while idx >= 0:
            opp_coop = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            x = opp_coop + my_coop
            if x < self.m:
                streak_fail += 1
                idx -= 1
            else:
                break
        if t == self.r:
            if success_prev:
                return Action.C
            if xprev == self.m - 1:
                return Action.C
            return Action.D
        if success_prev:
            return Action.C
        if xprev == self.m - 1:
            return Action.C
        if xprev == self.m - 2:
            if streak_fail <= 1:
                return Action.C
            return Action.D
        if streak_fail >= 2 and t % self.probe_interval == 0:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Threshold-Conditioned Reciprocator (TCR) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: Cooperate (seed).
    - If last round met threshold (>= m): Cooperate (stabilize success).
    - If last round failed (< m): Cooperate only on near-miss (exactly m-1), else defect.
    - Discipline:
        * After 2 consecutive failures: defect unless the group is again near-miss or better.
        * After 2 consecutive near-misses (m-1 twice): defect for one round ("shock").
    - Last round: cooperate if last round was success or near-miss; otherwise defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        slack = self.n - self.m
        self.s = max(1, int(math.ceil(0.1 * slack)))
        self.fail_streak = 0
        self.near_miss_streak = 0
        self.shock_pending = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.fail_streak = 0
            self.near_miss_streak = 0
            self.shock_pending = False
            return Action.C
        t = int(state.round_number)
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :]
        c_prev = (1 if my_last else 0) + int(np.sum(opp_last))
        if c_prev < self.m:
            self.fail_streak += 1
        else:
            self.fail_streak = 0
        if c_prev == self.m - 1:
            self.near_miss_streak += 1
        else:
            self.near_miss_streak = 0
        if t >= self.r:
            if c_prev >= self.m or c_prev == self.m - 1:
                return Action.C
            return Action.D
        if self.shock_pending:
            self.shock_pending = False
            return Action.D
        if self.near_miss_streak >= 2:
            self.shock_pending = True
            return Action.D
        if self.fail_streak >= 2 and c_prev < self.m - 1:
            return Action.D
        if c_prev >= self.m:
            return Action.C
        delta = self.m - c_prev
        if delta == 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    Adaptive Threshold Reciprocator (ATR) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: Cooperate (signal willingness).
    - Early seeding (up to T_seed): Cooperate if last round was within 1 of threshold; else defect.
    - Thereafter: Cooperate only when (a) feasible to reach threshold with you, (b) you appear needed,
      (c) the group is not hopeless based on recent threshold hits or near-hits.
    - Softly punishes chronic defectors by defecting when too many make threshold structurally unlikely.
    - Last round: cooperate only when near-pivotal (feasible and needed).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        T_seed = min(3, int(math.ceil(r / 4.0)))
        x_prev = int(np.sum(history.opponent_actions[-1, :])) + (1 if bool(history.my_actions[-1]) else 0)
        if t <= T_seed:
            return Action.C if x_prev >= m - 1 else Action.D
        w = min(5, history.round_number)
        if w <= 0:
            return Action.D
        recent_opp = history.opponent_actions[-w:, :]
        recent_self = history.my_actions[-w:]
        recent_x = np.sum(recent_opp, axis=1).astype(int) + recent_self.astype(int)
        reached_once = bool(np.any(recent_x >= m))
        avg_x = float(np.mean(recent_x)) if w > 0 else 0.0
        likely_C = 0
        chronic_D = 0
        n_opp = history.opponent_actions.shape[1] if history.opponent_actions.ndim == 2 else 0
        for j in range(n_opp):
            if w > 0:
                p_recent = float(np.mean(recent_opp[:, j]))
            else:
                p_recent = 0.0
            opp_col = history.opponent_actions[:, j]
            Cj = int(np.sum(opp_col))
            Dj = int(len(opp_col) - Cj)
            p_overall = float((Cj + 1.0) / (Cj + Dj + 2.0))
            is_likely = w >= 2 and p_recent >= 0.6 or (t >= 4 and p_overall >= 0.6)
            if is_likely:
                likely_C += 1
            is_chronic = t >= 5 and p_recent <= 0.2 and (p_overall <= 0.3)
            if is_chronic:
                chronic_D += 1
        if n - chronic_D < m:
            return Action.D
        feasible = likely_C + 1 >= m
        needed = likely_C < m
        not_hopeless = reached_once or avg_x >= m - 1
        if t >= r:
            return Action.C if feasible and needed else Action.D
        return Action.C if feasible and needed and not_hopeless else Action.D

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Threshold-Trigger with Forgiveness (TTF) for the Collective Risk Dilemma.

    - Cooperate in round 1 to bootstrap coordination.
    - Track chronic free-riders (defecting on successful rounds).
    - Cooperate when reaching the threshold seems feasible and exploitation is absent.
    - Punish chronic free-riders with short (2-round) defection bursts, with forgiveness.
    - When failing, defect by default but cooperate on near-misses and periodic probes.
    - In the last round, cooperate only if success was achieved last round and no chronic free-riders exist.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.failure_streak = 0
        self.Lmax = 5
        self.probe_period = 3
        self.punishment_length = 2
        self.chronic_threshold = 2
        self.reliable_rate = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        last_idx = history.round_number - 1
        opp_last = history.opponent_actions[last_idx, :]
        my_last = bool(history.my_actions[last_idx])
        C_last = int(np.sum(opp_last)) + (1 if my_last else 0)
        S_last = 1 if C_last >= m else 0
        if S_last == 0:
            self.failure_streak += 1
        else:
            self.failure_streak = 0
        rounds = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions.astype(bool)
        total_C = np.sum(opp_actions, axis=1).astype(int) + my_actions.astype(int)
        success = total_C >= m
        if rounds > 0:
            free_ride_counts = np.sum(~opp_actions & success[:, None], axis=0).astype(int)
        else:
            free_ride_counts = np.zeros(opp_actions.shape[1], dtype=int)
        H = int(np.sum(free_ride_counts >= self.chronic_threshold))
        L = min(self.Lmax, rounds)
        if L > 0:
            recent_opp = opp_actions[rounds - L:rounds, :]
            recent_rates = np.mean(recent_opp, axis=0)
            R = int(np.sum(recent_rates >= self.reliable_rate))
        else:
            R = 0
        feasible = R + 1 >= m
        if self.punishment_timer > 0:
            if S_last == 1 and H == 0:
                self.punishment_timer = 0
                return Action.C
            self.punishment_timer -= 1
            return Action.D
        if t == r:
            if S_last == 1 and H == 0:
                return Action.C
            return Action.D
        if not feasible:
            if S_last == 0 and C_last == m - 1:
                return Action.C
            if self.failure_streak >= self.probe_period and self.failure_streak % self.probe_period == 0:
                return Action.C
            return Action.D
        if H > 0 and S_last == 1:
            self.punishment_timer = self.punishment_length
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Responsive Threshold Contributor (RTC)

    Cooperates when likely pivotal for meeting the threshold or sustaining a
    demonstrated coalition, defects when success is unlikely, when not needed,
    or for a one-round punishment if persistently exploited in successful rounds.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            if m <= (n + 1) // 2:
                return Action.C
            q = m / n
            return Action.C if random.random() < q else Action.D
        t = history.round_number + 1

        def _count_total_cooperators_at_round(round_idx: int) -> int:
            opp_c = int(np.sum(history.opponent_actions[round_idx, :])) if history.opponent_actions.size else 0
            my_c = 1 if bool(history.my_actions[round_idx]) else 0
            return opp_c + my_c

        def _threshold_met_at_round(round_idx: int) -> bool:
            return _count_total_cooperators_at_round(round_idx) >= m
        C_last = _count_total_cooperators_at_round(t - 2)
        W = min(5, t - 1)
        if W <= 0:
            W = 1
        start = t - 1 - W
        end = t - 1
        recent_opp = history.opponent_actions[start:end, :]
        if recent_opp.size and recent_opp.shape[0] == W:
            p_vec = np.mean(recent_opp.astype(np.float64), axis=0)
        else:
            p_vec = np.zeros((history.opponent_actions.shape[1],), dtype=np.float64)
        C_hat_minus_i = float(np.sum(p_vec))
        Rcount = int(np.sum(p_vec >= 0.6))
        if t == r:
            if C_last == m - 1:
                return Action.C
            if m - 1.2 <= C_hat_minus_i <= m - 0.2:
                return Action.C
            return Action.D
        if C_last == m - 1:
            return Action.C
        met_last2 = 0
        for tt in range(max(0, t - 1 - 2), t - 1):
            if _threshold_met_at_round(tt):
                met_last2 += 1
        if met_last2 >= 1 and Rcount >= m - 1:
            return Action.C
        if m - 1.2 <= C_hat_minus_i <= m - 0.2:
            return Action.C
        successful_rounds = []
        for rr_idx in range(0, t - 1):
            if _threshold_met_at_round(rr_idx):
                successful_rounds.append(rr_idx)
        if len(successful_rounds) >= 3:
            CiS = 0
            for rr_idx in successful_rounds:
                if bool(history.my_actions[rr_idx]):
                    CiS += 1
            if CiS / len(successful_rounds) >= 0.8:
                low_coop_others = 0
                for j in range(history.opponent_actions.shape[1]):
                    CjS = 0
                    for rr_idx in successful_rounds:
                        if bool(history.opponent_actions[rr_idx, j]):
                            CjS += 1
                    if CjS / len(successful_rounds) <= 0.2:
                        low_coop_others += 1
                if low_coop_others >= m:
                    return Action.D
        if C_hat_minus_i < m - 1.2 and Rcount < m - 1:
            return Action.D
        if C_hat_minus_i >= m and Rcount >= m:
            return Action.D
        if C_hat_minus_i >= m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    Adaptive Threshold Reciprocator (ATR) for the Collective Risk Dilemma.

    Core behavior:
    - Probe early to seed cooperation and assess feasibility.
    - If cooperation is working, maintain threshold with occasional soft deterrence when exploited.
    - If cooperation is failing, switch to anti-exploitation (mostly defect) but re-enter when pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.w = 3
        self.W = min(6, self.r)
        self.reliable_coop_thresh = 0.6
        self.reliable_defect_thresh = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1

        def _count_cooperators_in_round(idx: int) -> int:
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            return my_c + opp_c

        def _mean01(arr) -> float:
            if arr is None:
                return 0.0
            try:
                n = len(arr)
            except Exception:
                return 0.0
            if n <= 0:
                return 0.0
            return float(np.mean(np.array(arr, dtype=float)))
        C_last = _count_cooperators_in_round(-1)
        T = history.round_number
        C_hist = []
        S_hist = []
        for idx in range(T):
            c = _count_cooperators_in_round(idx)
            C_hist.append(c)
            S_hist.append(1.0 if c >= self.m else 0.0)
        w_eff = min(self.w, T)
        W_eff = min(self.W, T)
        succRate_short = _mean01(S_hist[-w_eff:]) if w_eff > 0 else 0.0
        succRate_long = _mean01(S_hist[-W_eff:]) if W_eff > 0 else 0.0
        avgC = _mean01(C_hist[-W_eff:]) if W_eff > 0 else 0.0
        reliable_defectors = 0
        if W_eff > 0 and history.opponent_actions.size:
            opp_window = history.opponent_actions[-W_eff:, :]
            p = np.mean(opp_window.astype(float), axis=0)
            reliable_defectors = int(np.sum(p <= self.reliable_defect_thresh))
        my_last_action_was_C = bool(history.my_actions[-1])
        probe_end = min(3, self.r)
        if t <= probe_end:
            if C_last >= self.m:
                return Action.C
            if C_last <= self.m - 2:
                return Action.D
            return Action.C
        collective_mode = succRate_short >= 2.0 / 3.0 or succRate_long >= 0.5
        anti_mode = succRate_short <= 1.0 / 3.0 and avgC < self.m - 1
        if t == self.r:
            return Action.C if succRate_short >= 2.0 / 3.0 else Action.D
        near_end = t == self.r - 1
        if collective_mode and (not anti_mode):
            if C_last >= self.m:
                if my_last_action_was_C and C_last == self.m and (reliable_defectors > 0):
                    p_defect = 0.5
                    if near_end:
                        p_defect = 0.65
                    return Action.D if random.random() < p_defect else Action.C
                return Action.C
            else:
                if C_last == self.m - 1:
                    return Action.C
                if C_last == self.m - 2:
                    thresh = 0.5
                    if self.k >= 2.0:
                        thresh = 0.4
                    if succRate_long >= thresh:
                        return Action.C
                    return Action.D
                return Action.D
        if C_last == self.m - 1:
            return Action.C
        if T >= 2:
            c1 = C_hist[-1]
            c2 = C_hist[-2]
            if c1 >= self.m - 1 and c1 > c2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    Critical-Mass Builder (CMB) for the Collective Risk Dilemma.

    Core idea:
    - Build: cooperate early to seed coordination.
    - Pivotal: cooperate when near the threshold to flip/maintain success.
    - Lock: default to cooperation when repeated success or enough reliable cooperators are detected.
    - Exit: defect after repeated clear failures far from threshold.
    - Sparse freeriding: defect only when success is strongly overdetermined.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.C

        def x_round(idx: int) -> int:
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            return my_c + opp_c
        last_idx = history.round_number - 1
        x_last = x_round(last_idx)
        i_coop_last = bool(history.my_actions[last_idx])
        succ_streak = 0
        fail_streak = 0
        seen_other = False
        for idx in range(history.round_number - 1, -1, -1):
            x = x_round(idx)
            success = x >= m
            if not seen_other:
                seen_other = True
            if success:
                if fail_streak > 0:
                    break
                succ_streak += 1
            else:
                if succ_streak > 0:
                    break
                fail_streak += 1
        n_opp = n - 1
        opportunities = [0] * n_opp
        coopcount = [0] * n_opp
        if n_opp > 0 and history.opponent_actions.size:
            for idx in range(history.round_number):
                x = x_round(idx)
                if x == m - 1 or x == m or x == m + 1:
                    row = history.opponent_actions[idx, :]
                    for j in range(n_opp):
                        opportunities[j] += 1
                        if bool(row[j]):
                            coopcount[j] += 1
        R_size = 0
        for j in range(n_opp):
            opps = opportunities[j]
            if opps >= 2:
                if coopcount[j] / opps >= 0.75:
                    R_size += 1
        T_build = min(3, max(1, r - 1))
        if t == 2 and x_last == 0:
            return Action.D
        if t <= T_build:
            return Action.C
        if t >= r:
            return Action.C if x_last >= m - 1 else Action.D
        if x_last < m and history.round_number >= 2:
            x_prev = x_round(history.round_number - 2)
            if x_prev >= m:
                if x_last == m - 1 or x_last == m:
                    return Action.C
        if fail_streak >= 2 and x_last <= m - 2:
            return Action.D
        if x_last == m - 1:
            return Action.C
        if x_last == m and i_coop_last:
            return Action.C
        if succ_streak >= 2 or R_size >= m - 1:
            if x_last >= m + 1 and succ_streak >= 3 and (R_size >= m):
                return Action.D
            return Action.C
        return Action.C if x_last >= m - 1 else Action.D

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    Adaptive Threshold Enforcer (ATE) for Collective Risk Dilemma.

    Core behavior:
    - Seed cooperation in round 1.
    - Track recent cooperation rates (window up to 5) and build a "reliable" set.
    - Cooperate when a coalition of (m-1) other reliable cooperators exists, or when close to threshold.
    - Defect when the threshold seems unreachable to avoid wasted contributions.
    - Soft punishment: temporarily raise reliability threshold when being exploited by chronic free-riders.
    - Limited pivotal rescue when last round was exactly one short of threshold.
    - Endgame: defect if threshold will be met without us; cooperate only if pivotal.
    - Sudden collapse cooldown: brief defection after an observed drop from meeting threshold to far below.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._theta_base = 0.7
        self._theta_strict = 0.85
        self._theta = self._theta_base
        self._punish_timer = 0
        self._rescue_streak = 0
        self._rescue_limit = 2
        self._collapse_cooldown = 0
        self._collapse_cooldown_len = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        Cprev = int(np.sum(history.my_actions[-1])) + int(np.sum(history.opponent_actions[-1, :]))
        if t <= 3:
            if Cprev >= m - 1:
                return Action.C
            return Action.D
        W = min(5, history.round_number)
        start = history.round_number - W
        my_recent = history.my_actions[start:history.round_number].astype(bool).reshape(W, 1)
        opp_recent = history.opponent_actions[start:history.round_number, :].astype(bool)
        full_recent = np.concatenate([my_recent, opp_recent], axis=1)
        rates = np.mean(full_recent, axis=0)
        if history.round_number >= 3:
            coop_counts = np.sum(full_recent, axis=1)
            met_count = int(np.sum(coop_counts >= m))
            if met_count >= int(math.ceil(W / 2.0)):
                my_C = int(np.sum(full_recent[:, 0]))
                player_C_counts = np.sum(full_recent, axis=0).astype(float)
                median_C = float(np.median(player_C_counts))
                free_riders_exist = bool(np.any(rates[1:] <= 0.2))
                if my_C > median_C and free_riders_exist and (self._punish_timer == 0):
                    self._theta = self._theta_strict
                    self._punish_timer = 2
        if self._punish_timer > 0:
            self._punish_timer -= 1
            if self._punish_timer == 0:
                self._theta = self._theta_base
        reliable = rates >= float(self._theta)
        reliable_others_count = int(np.sum(reliable[1:]))
        if self._theta != self._theta_base and history.round_number >= 2 * W and (Cprev >= m):
            prev_start = history.round_number - 2 * W
            my_prev = history.my_actions[prev_start:prev_start + W].astype(bool).reshape(W, 1)
            opp_prev = history.opponent_actions[prev_start:prev_start + W, :].astype(bool)
            full_prev = np.concatenate([my_prev, opp_prev], axis=1)
            rates_prev = np.mean(full_prev, axis=0)
            prev_reliable_others = int(np.sum(rates_prev[1:] >= float(self._theta)))
            if reliable_others_count > prev_reliable_others:
                self._theta = self._theta_base
                self._punish_timer = 0
        if t == r:
            if reliable_others_count >= m:
                return Action.D
            if reliable_others_count == m - 1:
                return Action.C
            return Action.D
        if history.round_number >= 2:
            prev2_coops = int(np.sum(history.my_actions[-2])) + int(np.sum(history.opponent_actions[-2, :]))
            if prev2_coops >= m and Cprev <= m - 2:
                self._collapse_cooldown = self._collapse_cooldown_len
        if self._collapse_cooldown > 0:
            self._collapse_cooldown -= 1
            self._rescue_streak = 0
            return Action.D
        if Cprev == m - 1 and self._rescue_streak < self._rescue_limit and (int(np.sum(reliable)) >= m - 1):
            self._rescue_streak += 1
            return Action.C
        if Cprev != m - 1:
            self._rescue_streak = 0
        if reliable_others_count >= m - 1:
            return Action.C
        if Cprev >= m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    Adaptive Threshold-Builder (ATB) for the Collective Risk Dilemma.

    Core idea:
    - Cooperate to establish/maintain meeting the threshold m whenever plausible.
    - If recent history shows no successes, mostly defect but periodically probe with cooperation.
    - Use a short rolling window (up to 5 rounds) with success rate, average cooperators, and trend.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        denom = self.n - self.m + 1
        if denom <= 0:
            self.probe_period = 2
        else:
            self.probe_period = max(2, int(math.ceil(self.n / denom)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        if t <= 1:
            return Action.C
        opp = history.opponent_actions
        if opp is None or len(opp.shape) != 2 or opp.shape[0] == 0:
            return Action.C
        rounds_completed = history.round_number
        my_last = bool(history.my_actions[-1])
        C_last = int(np.sum(opp[-1, :])) + (1 if my_last else 0)
        if rounds_completed >= 2:
            my_prev = bool(history.my_actions[-2])
            C_prev = int(np.sum(opp[-2, :])) + (1 if my_prev else 0)
        else:
            C_prev = C_last
        delta = C_last - C_prev
        w = min(5, rounds_completed)
        start = rounds_completed - w
        C_recent = []
        for idx in range(start, rounds_completed):
            my_a = bool(history.my_actions[idx])
            ccount = int(np.sum(opp[idx, :])) + (1 if my_a else 0)
            C_recent.append(ccount)
        successes = 0
        total_c = 0
        for ccount in C_recent:
            total_c += ccount
            if ccount >= self.m:
                successes += 1
        S = successes / w if w > 0 else 0.0
        C_bar = total_c / w if w > 0 else 0.0
        if C_last >= self.m and C_bar >= self.m - 0.5:
            return Action.C
        if C_last == self.m - 1:
            return Action.C
        if C_last >= self.m - 2 and delta > 0:
            return Action.C
        if t == self.r and C_last <= self.m - 3 and (S == 0):
            return Action.D
        if S == 0:
            return Action.C if t % self.probe_period == 0 else Action.D
        return Action.D

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Threshold-Reciprocator (TR-m) for the Collective Risk Dilemma.

    - Round 1: Cooperate.
    - Reinforce success: if last round reached threshold m, cooperate (subject to endgame filters).
    - Pivotal push: cooperate when last round had m-1 cooperators; when m-2, cooperate with prob 1/2.
    - Controlled rebuild: after failures, cooperate briefly if recent success rate suggests recovery is plausible.
    - Discipline: defect after 3 consecutive failures until a success occurs.
    - Endgame: in last 1-2 rounds, cooperate only if success has been stable.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        rem = self.r - t + 1
        opp_last = history.opponent_actions[-1, :]
        x_last = int(np.sum(opp_last)) + (1 if bool(history.my_actions[-1]) else 0)
        opp_sums = np.sum(history.opponent_actions.astype(np.int64), axis=1)
        my_int = history.my_actions.astype(np.int64)
        x_hist = opp_sums + my_int
        succ_streak = 0
        fail_streak = 0
        for x in x_hist[::-1]:
            if int(x) >= self.m:
                if fail_streak > 0:
                    break
                succ_streak += 1
            else:
                if succ_streak > 0:
                    break
                fail_streak += 1
        W = 5
        if history.round_number < W:
            W = history.round_number
        if W <= 0:
            succ_rate = 0.0
        else:
            recent = x_hist[-W:]
            succs = int(np.sum(recent >= self.m))
            succ_rate = succs / float(W)
        if rem == 1:
            if x_last >= self.m and fail_streak == 0:
                return Action.C
            return Action.D
        if rem == 2:
            if x_last >= self.m and succ_streak >= 2:
                return Action.C
            return Action.D
        if x_last >= self.m:
            return Action.C
        if x_last == self.m - 1:
            return Action.C
        if x_last == self.m - 2:
            return Action.C if random.random() < 0.5 else Action.D
        if fail_streak >= 3:
            return Action.D
        if succ_rate >= 0.4 and fail_streak <= 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Quota-Triggered Contribution (QTC) for the Collective Risk Dilemma.

    - Cooperate in round 1 to seed coordination.
    - Track consecutive successes/failures relative to threshold m.
    - After F consecutive failures, enter a CD-round cooldown of defection.
    - Immediately after cooldown ends, perform a one-round cooperative probe.
    - When succeeding:
        * If exactly at threshold (x=m), cooperate (pivotal maintenance).
        * If above threshold (x>m), mostly cooperate but occasionally defect
          if we've over-volunteered relative to a fairness heuristic.
    - When failing:
        * If near threshold (x=m-1), cooperate (pivotal recovery).
        * If far (x<=m-2), defect (avoid waste).
    - Last-round adjustment: if prior round succeeded and x>m, defect; if x==m, cooperate.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(int(game_description.n_rounds), 5)
        self.F = 2
        self.CD = 2
        self.fail_streak = 0
        self.success_streak = 0
        self.cooldown = 0
        self.was_in_cooldown_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            return Action.C
        last_my = bool(history.my_actions[-1])
        opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        x_prev = opp_coops + (1 if last_my else 0)
        if x_prev >= m:
            self.success_streak += 1
            self.fail_streak = 0
            self.cooldown = 0
        else:
            self.fail_streak += 1
            self.success_streak = 0
            if self.fail_streak >= self.F:
                self.cooldown = self.CD
        if self.cooldown > 0:
            self.cooldown -= 1
            self.was_in_cooldown_last_round = True
            return Action.D
        if self.was_in_cooldown_last_round:
            self.was_in_cooldown_last_round = False
            return Action.C
        if int(state.round_number) >= r:
            if x_prev >= m:
                if x_prev > m:
                    return Action.D
                return Action.C
            return Action.D
        if x_prev >= m:
            if x_prev == m:
                return Action.C
            W_eff = min(self.W, int(history.round_number))
            if W_eff <= 0:
                return Action.C
            recent_self = history.my_actions[-W_eff:]
            volunteer_count = int(np.sum(recent_self))
            fair_share = int(math.ceil(W_eff * (float(m) / float(n)))) + 1
            if volunteer_count > fair_share:
                return Action.D
            return Action.C
        if x_prev == m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    Threshold-TFT with Credible Cutoff (TTCC) for the Collective Risk Dilemma.

    Key ideas:
    - Cooperate when likely pivotal (near threshold).
    - Maintain cooperation when success is fragile; occasionally defect when there is slack.
    - Stop donating after repeated clear failures, unless there is a reliable near-threshold base.
    - Special handling for round 1 and the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._near_miss_limit = 2
        self._reliable_window_max = 5
        self._reliable_rate_threshold = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            t = 1
        else:
            t = int(history.round_number) + 1
        if history is None or history.round_number == 0:
            if m <= n // 2 + 1 + (1 if n % 2 == 1 else 0):
                return Action.C
            return Action.D
        last_my_c = bool(history.my_actions[-1])
        last_opp_actions = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
        c_opp_prev = int(np.sum(last_opp_actions)) if last_opp_actions.size else 0
        c_prev = c_opp_prev + (1 if last_my_c else 0)
        c_others_prev = c_prev - (1 if last_my_c else 0)
        success_prev = c_prev >= m
        fail_streak = 0
        for idx in range(history.round_number - 1, -1, -1):
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_c = 1 if bool(history.my_actions[idx]) else 0
            if opp_c + my_c < m:
                fail_streak += 1
            else:
                break
        near_streak = 0
        for idx in range(history.round_number - 1, -1, -1):
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_c = 1 if bool(history.my_actions[idx]) else 0
            if opp_c + my_c == m - 1:
                near_streak += 1
            else:
                break
        W = min(self._reliable_window_max, history.round_number)
        reliable_count = 0
        if history.opponent_actions.size and W > 0:
            window = history.opponent_actions[-W:, :]
            rates = np.mean(window.astype(np.float64), axis=0)
            reliable_count = int(np.sum(rates >= self._reliable_rate_threshold))
        if t == r:
            if c_prev >= m - 1:
                return Action.C
            if fail_streak >= 2 and c_prev <= m - 2:
                return Action.D
            return Action.C
        if c_others_prev == m - 1 or c_prev == m - 1:
            if near_streak <= self._near_miss_limit:
                return Action.C
            return Action.D
        if success_prev:
            if c_prev <= m:
                return Action.C
            denom = c_prev - m + 2
            q = 1.0 / float(denom) if denom > 0 else 0.0
            return Action.D if random.random() < q else Action.C
        if c_prev <= m - 2 and fail_streak >= 2:
            if reliable_count >= m - 1:
                return Action.C
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Stateful Threshold Reciprocity (STR) for the Collective Risk Dilemma.

    Core behavior:
    - Cooperate early (rounds 1-2).
    - Thereafter, cooperate when threshold success is plausible (momentum or a reliable base).
    - Stop donating in a clearly hopeless regime (multiple failures, far from threshold, weak reliable base).
    - Apply a one-round, credible punishment only when last round *exactly* met threshold (pivotal region),
      there exist chronic defectors, and we cooperated last round; then immediately return to cooperation.
    - In the last round, cooperate only if plausibly pivotal (last round had >= m-1 cooperators).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.theta_reliable = 0.6
        self.theta_chronic = 0.2
        self.W_max = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t_completed = int(history.round_number)
        t = t_completed + 1
        if t <= 2:
            return Action.C

        def bool_to_action(b):
            return Action.C if bool(b) else Action.D

        def count_cooperators_in_round(round_idx):
            opp_c = int(np.sum(history.opponent_actions[round_idx, :])) if history.opponent_actions.size else 0
            me_c = 1 if bool(history.my_actions[round_idx]) else 0
            return opp_c + me_c
        W = min(self.W_max, t_completed)
        S_last = count_cooperators_in_round(t_completed - 1)
        F = 0
        idx = t_completed - 1
        while idx >= 0:
            if count_cooperators_in_round(idx) < m:
                F += 1
                idx -= 1
            else:
                break
        start = t_completed - W
        me_recent = history.my_actions[start:t_completed]
        me_rate = float(np.mean(me_recent)) if W > 0 else 0.0
        opp_recent = history.opponent_actions[start:t_completed, :] if history.opponent_actions.size else np.zeros((W, n - 1), dtype=bool)
        opp_rates = np.mean(opp_recent, axis=0) if W > 0 and opp_recent.size else np.zeros((n - 1,), dtype=float)
        R = 0
        if me_rate >= self.theta_reliable:
            R += 1
        if opp_rates.size:
            R += int(np.sum(opp_rates >= self.theta_reliable))
        chronic_exists = False
        if opp_rates.size:
            chronic_exists = bool(np.any(opp_rates <= self.theta_chronic))
        my_last_action_c = bool(history.my_actions[t_completed - 1])
        if t == r:
            return Action.C if S_last >= m - 1 else Action.D
        if S_last >= m - 1:
            return Action.C
        if R >= m - 1:
            return Action.C
        if F >= 2:
            S_prev = count_cooperators_in_round(t_completed - 2) if t_completed >= 2 else S_last
            if max(S_prev, S_last) <= m - 2 and R <= m - 2:
                return Action.D
        if S_last == m and chronic_exists and my_last_action_c:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Adaptive Threshold Guard (ATG) for the Collective Risk Dilemma.

    Core idea:
    - Cooperate when near the threshold (knife-edge) or when likely pivotal.
    - In stable success, cooperate probabilistically to avoid collapse while limiting exploitation.
    - In unstable failure, mostly defect but initiate short "repair attempts" after near-misses or pivotal moments.
    - Track targeted "debt" of free-riders who defect during successful rounds; occasionally defect to apply pressure,
      but never when knife-edge or pivotal.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        m = int(game_description.m)
        k = float(game_description.k)
        self.n = n
        self.m = m
        self.k = k
        self.delta = max(1, int(math.ceil(0.15 * m)))
        self.D = 2
        self.x = max(1, int(math.ceil(0.2 * (n - 1))))
        self.repair_L = 3 if k >= 2.0 else 2
        self.repair_remaining = 0
        self.debt = [0 for _ in range(n - 1)]
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            p0 = min(1.0, float(self.m) / float(self.n))
            return Action.C if random.random() < p0 else Action.D
        t_completed = history.round_number
        t_next = t_completed + 1
        if self._last_processed_round < t_completed:
            for r_idx in range(self._last_processed_round, t_completed):
                my_c = 1 if bool(history.my_actions[r_idx]) else 0
                opp_c = int(np.sum(history.opponent_actions[r_idx, :])) if history.opponent_actions.size else 0
                S_r = my_c + opp_c
                success = S_r >= self.m
                if self.n - 1 > 0:
                    for j in range(self.n - 1):
                        opp_act_c = bool(history.opponent_actions[r_idx, j])
                        if success and (not opp_act_c):
                            self.debt[j] += 1
                        if opp_act_c:
                            self.debt[j] = max(0, self.debt[j] - 1)
            self._last_processed_round = t_completed
        if t_next == 2:
            S1 = (1 if bool(history.my_actions[0]) else 0) + int(np.sum(history.opponent_actions[0, :]))
            return Action.C if S1 >= self.m - 1 else Action.D
        if t_next == self.game_description.n_rounds:
            S_last = (1 if bool(history.my_actions[-1]) else 0) + int(np.sum(history.opponent_actions[-1, :]))
            return Action.C if S_last >= self.m - 1 else Action.D
        if self.repair_remaining > 0:
            S_last = (1 if bool(history.my_actions[-1]) else 0) + int(np.sum(history.opponent_actions[-1, :]))
            if S_last >= self.m:
                self.repair_remaining = 0
            else:
                self.repair_remaining -= 1
                return Action.C
        w = min(5, t_completed)
        start = t_completed - w
        my_slice = history.my_actions[start:t_completed].astype(int)
        opp_slice = history.opponent_actions[start:t_completed, :].astype(int)
        S_vals = my_slice + np.sum(opp_slice, axis=1)
        S_bar = float(np.mean(S_vals)) if len(S_vals) else 0.0
        stable = S_bar >= self.m + self.delta
        knife = self.m - 1 <= S_bar < self.m + self.delta
        unstable = S_bar < self.m - 1
        if t_completed > 0 and self.n - 1 > 0:
            p_rates = np.mean(history.opponent_actions[:t_completed, :], axis=0)
            E = float(np.sum(p_rates))
        else:
            E = 0.0
        pivotal = E < self.m and E + 1.0 >= self.m
        culprits = 0
        for d in self.debt:
            if d >= self.D:
                culprits += 1
        discipline_on = culprits >= self.x
        if knife:
            return Action.C
        if stable:
            if discipline_on and (not pivotal):
                return Action.D
            M = S_bar - float(self.m)
            q = min(1.0, float(self.delta + 1) / float(M + 1.0))
            return Action.C if random.random() < q else Action.D
        if unstable:
            near_miss_recent = False
            for s in S_vals:
                if float(s) >= float(self.m - 1):
                    near_miss_recent = True
                    break
            if pivotal or near_miss_recent:
                self.repair_remaining = max(0, self.repair_L - 1)
                return Action.C
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Critical-Mass Reciprocity (CMR) for the Collective Risk Dilemma.

    Core behavior:
    - Estimate whether at least (m-1) opponents are likely to cooperate using a
      recent-window cooperation rate p_j and a stability-based reliability weight w_j.
    - Cooperate if credible support is sufficient to reach the threshold m, with
      separate build vs maintain cutoffs.
    - Add safeguards: last-round tightening, cooldown after repeated failures,
      and a "pivotal sucker" one-round rest when others can likely sustain m without us.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 5
        self.L = 3
        self.cooldown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)

        def _delta(nn: int, mm: int) -> float:
            denom = nn - 1
            if denom <= 0:
                return 0.25
            return 0.25 + 0.25 * ((nn - mm) / denom)

        def _count_total_c(last_idx: int, h: PlayerHistory) -> int:
            me_c = 1 if bool(h.my_actions[last_idx]) else 0
            opp_c = int(np.sum(h.opponent_actions[last_idx, :])) if h.opponent_actions.size else 0
            return me_c + opp_c

        def _consecutive_failures(h: PlayerHistory) -> int:
            streak = 0
            for idx in range(h.round_number - 1, -1, -1):
                if _count_total_c(idx, h) >= m:
                    break
                streak += 1
            return streak

        def _pivotal_count(h: PlayerHistory, W: int) -> int:
            end = h.round_number
            start = max(0, end - W)
            cnt = 0
            for idx in range(start, end):
                if bool(h.my_actions[idx]) and _count_total_c(idx, h) == m:
                    cnt += 1
            return cnt

        def _opponent_scores(h: PlayerHistory, W: int):
            T = h.round_number
            start = max(0, T - W)
            scores = []
            if h.opponent_actions.size == 0:
                return scores
            n_opp = h.opponent_actions.shape[1]
            for j in range(n_opp):
                recent = h.opponent_actions[start:T, j]
                Lr = int(len(recent))
                if Lr <= 0:
                    p = 0.0
                    w = 1.0
                else:
                    p = float(np.mean(recent.astype(float)))
                    if Lr > 1:
                        flips = recent[1:] != recent[:-1]
                        flip_rate = float(np.mean(flips.astype(float)))
                    else:
                        flip_rate = 0.0
                    w = 1.0 - flip_rate
                    if w < 0.0:
                        w = 0.0
                    if w > 1.0:
                        w = 1.0
                s = p * w
                scores.append((s, j))
            scores.sort(key=lambda x: x[0], reverse=True)
            return scores
        if history is None:
            return Action.C if k >= 1.5 else Action.D
        if self.cooldown > 0:
            self.cooldown -= 1
            return Action.D
        t_completed = history.round_number
        t_next = t_completed + 1
        last_idx = t_completed - 1
        success_last = _count_total_c(last_idx, history) >= m
        scores = _opponent_scores(history, self.W)
        d = _delta(n, m)
        theta_build = m - 1 - d
        theta_maintain = m - 1 - 2.0 * d
        if t_next == r:
            theta_build += 0.25
            theta_maintain += 0.25
        top_m_1 = scores[:max(0, m - 1)]
        support = 0.0
        for s, _j in top_m_1:
            support += float(s)
        fail_streak = _consecutive_failures(history)
        if fail_streak >= self.L and support < theta_build:
            self.cooldown = 2
            return Action.D
        pivotal = _pivotal_count(history, self.W)
        top_m = scores[:max(0, m)]
        support_m = 0.0
        for s, _j in top_m:
            support_m += float(s)
        if success_last:
            if pivotal >= int(math.ceil(self.W / 2.0)) and support_m >= m - d:
                return Action.D
            return Action.C if support >= theta_maintain else Action.D
        else:
            return Action.C if support >= theta_build else Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    Adaptive Threshold Guardian (ATG) for the Collective Risk Dilemma.

    Key ideas:
    - Maintain/restore reaching at least m cooperators when pivotal.
    - Avoid wasting contributions when far below threshold.
    - Implicitly form a "core" of the top-m most reliable recent cooperators.
    - Apply a one-round "strike" when success is fragile (x==m) and chronic free-riders exist.
    - Endgame discipline: defect in the last round unless protecting a fragile, just-met threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 5
        self.free_ride_threshold = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        n_opps = n - 1
        my = history.my_actions.astype(bool)
        opp = history.opponent_actions.astype(bool) if n_opps > 0 else np.zeros((len(my), 0), dtype=bool)
        all_actions = np.concatenate([my.reshape(-1, 1), opp], axis=1)
        x_last = int(np.sum(all_actions[-1, :]))
        my_last = bool(my[-1])
        W = int(min(self.W_max, history.round_number))
        recent = all_actions[-W:, :]
        p = np.mean(recent, axis=0) if W > 0 else np.zeros(n, dtype=float)
        order = list(range(n))
        order.sort(key=lambda j: (-float(p[j]), j))
        core = set(order[:m])
        i_in_core = 0 in core
        x_recent = np.sum(recent, axis=1)
        successful_mask = x_recent >= m
        successful_rounds = recent[successful_mask, :] if np.any(successful_mask) else None
        freeriders_exist = False
        if successful_rounds is not None and successful_rounds.shape[0] > 0:
            succ = successful_rounds
            succ_len = succ.shape[0]
            for j in range(1, n):
                d = succ_len - int(np.sum(succ[:, j]))
                if succ_len > 0 and d / succ_len >= self.free_ride_threshold:
                    freeriders_exist = True
                    break
        if t == r:
            if x_last == m and my_last and i_in_core:
                return Action.C
            return Action.D
        near_end = t >= r - 1
        if x_last >= m:
            if x_last == m:
                if my_last and freeriders_exist:
                    return Action.D
                return Action.C
            elif x_last == m + 1:
                if near_end:
                    return Action.C if i_in_core and freeriders_exist is False else Action.D
                return Action.C if i_in_core else Action.D
            else:
                if near_end:
                    return Action.D
                return Action.C if i_in_core else Action.D
        if x_last == m - 1:
            if near_end:
                return Action.C
            return Action.C
        else:
            if W >= 2:
                x_prev = int(np.sum(all_actions[-2, :]))
                trend_up = x_last > x_prev
            else:
                trend_up = False
            if x_last <= m - 2 and trend_up and i_in_core and (not near_end):
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Pivotal-Trigger with Probation (PTP) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: Cooperate.
    - Usually cooperate only when likely pivotal (predicted others in [m-1, m)).
    - If last round was exactly one short (x = m-1): cooperate to tip.
    - If failures are far from threshold (x <= m-2): punish with P defections, then do
      a single repair cooperation, then revert to pivotal logic.
    - Endgame: last round mostly defect unless pivotal AND recent evidence of viability.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 5
        self.P = 2
        self._probation_left = 0
        self._forced_repair = False

    def _clip(self, x: float, lo: float, hi: float) -> float:
        if x < lo:
            return lo
        if x > hi:
            return hi
        return x

    def _estimate_E(self, t: int, history: PlayerHistory) -> float:
        """
        Estimate expected number of cooperating opponents next round.
        t is 1-indexed round about to be played.
        """
        n = self.game_description.n_players
        if t <= 1 or history is None or history.round_number <= 0:
            return (n - 1) * 0.5
        W = self.W_max
        if history.round_number < W:
            W = history.round_number
        if W <= 0:
            return (n - 1) * 0.5
        recent = history.opponent_actions[-W:, :]
        rates = np.mean(recent.astype(np.float64), axis=0)
        E = 0.0
        for p in rates:
            E += self._clip(float(p), 0.05, 0.95)
        return E

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is None or history.round_number == 0:
            return Action.C
        last_opp_coops = int(np.sum(history.opponent_actions[-1, :]))
        last_my = 1 if bool(history.my_actions[-1]) else 0
        x_last = last_opp_coops + last_my
        fail_streak = 0
        success_streak = 0
        for idx in range(history.round_number - 1, -1, -1):
            x_t = int(np.sum(history.opponent_actions[idx, :])) + (1 if bool(history.my_actions[idx]) else 0)
            if x_t < m:
                if success_streak > 0:
                    break
                fail_streak += 1
            else:
                if fail_streak > 0:
                    break
                success_streak += 1
        if t >= r:
            E = self._estimate_E(t, history)
            pivotal = E >= m - 1 and E < m
            viable_recent = success_streak >= 1 or x_last >= m - 1
            if pivotal and viable_recent:
                return Action.C
            return Action.D
        if self._probation_left > 0:
            self._probation_left -= 1
            return Action.D
        if self._forced_repair:
            self._forced_repair = False
            return Action.C
        W = self.W_max
        if history.round_number < W:
            W = history.round_number
        wasted_coop = 0
        if W > 0:
            for idx in range(history.round_number - W, history.round_number):
                x_t = int(np.sum(history.opponent_actions[idx, :])) + (1 if bool(history.my_actions[idx]) else 0)
                if bool(history.my_actions[idx]) and x_t < m:
                    wasted_coop += 1
        if wasted_coop >= 2 and fail_streak >= 2:
            if x_last >= m:
                return Action.D
            if x_last == m - 1:
                return Action.C
            return Action.D
        if x_last < m:
            if x_last == m - 1:
                return Action.C
            if t <= r - (self.P + 1):
                self._probation_left = self.P
                self._forced_repair = True
            return Action.D
        E = self._estimate_E(t, history)
        if E >= m:
            return Action.D
        if E <= m - 2:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    Responsive Threshold Contributor (RTC)

    Cooperate only when likely pivotal to reaching the threshold m:
    - Round 1: probabilistic seeding with q = min(0.5, m/n)
    - Early calibration (rounds 2-3): C iff last round had exactly m-1 cooperators
    - Thereafter: compute viability from a short window; if viable and last round was m-1,
      cooperate unless we've been "exploited" (two recent pivotal cooperations still failed).
    - Reset from exploited mode after observing success, or after defecting for 2 rounds and
      seeing another near-threshold (m-1) round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._exploited_mode = False
        self._defect_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        m = int(self.game_description.m)
        r = int(self.game_description.n_rounds)
        if history is None or history.round_number == 0:
            q = min(0.5, float(m) / float(n)) if n > 0 else 0.0
            return Action.C if random.random() < q else Action.D
        t = history.round_number + 1
        my = history.my_actions.astype(np.int32)
        opp = history.opponent_actions.astype(np.int32)
        S_history = (my + np.sum(opp, axis=1)).tolist()
        S_last = int(S_history[-1])
        if bool(history.my_actions[-1]) is False:
            self._defect_streak += 1
        else:
            self._defect_streak = 0
        w = min(5, history.round_number)
        recent = S_history[-w:] if w > 0 else [S_last]
        S_bar = float(np.mean(np.array(recent, dtype=np.float64))) if len(recent) > 0 else float(S_last)
        succ_bar = float(np.mean(np.array([1.0 if x >= m else 0.0 for x in recent], dtype=np.float64))) if len(recent) > 0 else 1.0 if S_last >= m else 0.0
        viable = S_bar >= m - 1 or succ_bar >= 0.5
        pivotal_tries = 0
        pivotal_failures = 0
        scan_limit = max(0, history.round_number - 10)
        j = history.round_number - 1
        while j >= scan_limit and pivotal_tries < 2:
            if int(S_history[j]) == m - 1 and bool(history.my_actions[j]) is True:
                pivotal_tries += 1
                if int(S_history[j]) < m:
                    pivotal_failures += 1
            j -= 1
        exploited_now = pivotal_tries >= 2 and pivotal_failures == 2
        if exploited_now:
            self._exploited_mode = True
        if self._exploited_mode:
            if S_last >= m:
                self._exploited_mode = False
            elif S_last == m - 1 and self._defect_streak >= 2:
                self._exploited_mode = False
        if t <= 3:
            if S_last >= m:
                return Action.D
            if S_last == m - 1:
                return Action.C
            return Action.D
        if t == r:
            if S_last == m - 1 and succ_bar >= 0.5 and (not self._exploited_mode):
                return Action.C
            return Action.D
        if not viable:
            return Action.D
        if self._exploited_mode:
            return Action.D
        if S_last == m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    Threshold Grim-Trigger with Probation (TGT-P) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: cooperate with probability q ~ (m-1)/(n-1) (clamped).
    - If group success is reliable (success streak >= 2 or success rate over last W rounds >= 0.7): cooperate.
    - If group has failed for F consecutive rounds: defect, but occasionally enter a probation window (length P)
      where we cooperate to test recovery (triggered by "close" failures or periodically).
    - Last round: cooperate only if reliability is high; otherwise defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        m = int(game_description.m)
        self.W = min(5, max(1, r - 1))
        self.F = 2
        self.P = 2
        if n <= 1:
            base_q = 0.5
        else:
            base_q = (m - 1) / (n - 1)
        if base_q < 0.05:
            base_q = 0.05
        if base_q > 0.95:
            base_q = 0.95
        self.q = float(base_q)
        self.probation_left = 0
        self._in_defect_mode = False
        self._probation_success = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            if random.random() < self.q:
                return Action.C
            return Action.D
        t = history.round_number + 1
        last_opp_coop = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        my_last = bool(history.my_actions[-1])
        S_last = last_opp_coop + (1 if my_last else 0)
        total_coops = np.sum(history.opponent_actions, axis=1).astype(int) + history.my_actions.astype(int)
        successes = total_coops >= m
        success_streak = 0
        i = len(successes) - 1
        while i >= 0 and bool(successes[i]):
            success_streak += 1
            i -= 1
        fail_streak = 0
        i = len(successes) - 1
        while i >= 0 and (not bool(successes[i])):
            fail_streak += 1
            i -= 1
        W_eff = min(self.W, len(successes))
        if W_eff <= 0:
            rate_success = 0.0
        else:
            rate_success = float(np.mean(successes[-W_eff:].astype(float)))
        reliable = success_streak >= 2 or rate_success >= 0.7
        if t >= r:
            return Action.C if reliable else Action.D
        if self.probation_left > 0:
            self.probation_left -= 1
            if S_last >= m:
                self._probation_success = True
            if self.probation_left == 0:
                if self._probation_success:
                    self._in_defect_mode = False
                else:
                    self._in_defect_mode = True
            return Action.C
        if reliable:
            self._in_defect_mode = False
            return Action.C
        if fail_streak >= self.F:
            self._in_defect_mode = True
        if self._in_defect_mode:
            close_failure = S_last == m - 1
            periodic_test = t % (self.P + 1) == 0
            if close_failure or periodic_test:
                self.probation_left = self.P
                self._probation_success = False
                self.probation_left -= 1
                if S_last >= m:
                    self._probation_success = True
                if self.probation_left == 0:
                    if self._probation_success:
                        self._in_defect_mode = False
                    else:
                        self._in_defect_mode = True
                return Action.C
            return Action.D
        if S_last == m - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Adaptive Threshold Builder (ATB) for the Collective Risk Dilemma.

    Core behavior:
    - Start by cooperating to seed coordination.
    - After failures: cooperate for up to 2 consecutive failures (recovery), then defect,
      except always cooperate when last round was pivotal (S_last == m-1).
    - After successes: cooperate at the threshold; with slack, cooperate until a short
      success streak is established, then opportunistically defect only with clear buffer.
    - Last round: pivotality-based finish (do not blindly defect).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.C
        last_my = 1 if bool(history.my_actions[-1]) else 0
        last_opp = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        S_last = last_my + last_opp
        S_all = history.my_actions.astype(np.int64) + np.sum(history.opponent_actions.astype(np.int64), axis=1)
        succ = S_all >= m
        success_streak = 0
        fail_streak = 0
        idx = len(succ) - 1
        if succ[idx]:
            j = idx
            while j >= 0 and succ[j]:
                success_streak += 1
                j -= 1
        else:
            j = idx
            while j >= 0 and (not succ[j]):
                fail_streak += 1
                j -= 1
        t = history.round_number + 1
        if t == r:
            if S_last >= m:
                if S_last == m:
                    return Action.C
                return Action.D
            else:
                if S_last == m - 1:
                    return Action.C
                return Action.D
        if S_last == m - 1:
            return Action.C
        if S_last < m:
            if fail_streak <= 2:
                return Action.C
            return Action.D
        if S_last == m:
            return Action.C
        if success_streak < 2:
            return Action.C
        if S_last >= m + 2:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    Quota-Guardian with Forgiveness (QGF) for the Collective Risk Dilemma.

    Core behavior:
    - Maintain EWMA estimates p_j of each opponent's cooperation probability.
    - Compute E = expected number of OTHER cooperators next round.
      * If E >= m: defect (threshold likely met without us).
      * If m-1 <= E < m: cooperate (pivotal), unless fatigue triggers a one-round withdrawal.
      * If E < m-1: defect, except limited recovery attempts after repeated failures when close.
    - Recovery: after fail_streak >= 2, cooperate up to B times if E >= m-2.
    - Fatigue cap: if pivotal cooperation happened >=3 times in last W=5 rounds, defect once
      in the next pivotal situation (warning withdrawal), then resume.
    - First round: cooperate if m/n <= 0.5 else defect.
    - Last round: no extra generosity; only cooperate if pivotal and not fatigued.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.35
        self.B = 2
        self.W = 5
        self.fatigue_limit = 3
        self._p = None
        self._success_streak = 0
        self._fail_streak = 0
        self._recovery_C_used_in_current_fail = 0
        self._pivotal_C_hist = []
        self._last_E = None
        self._last_action = None
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None:
            self._p = [0.5 for _ in range(max(0, n - 1))]
            self._success_streak = 0
            self._fail_streak = 0
            self._recovery_C_used_in_current_fail = 0
            self._pivotal_C_hist = []
            self._last_E = None
            self._last_action = None
            self._last_processed_rounds = 0
            difficulty = float(m) / float(n) if n > 0 else 1.0
            action = Action.C if difficulty <= 0.5 else Action.D
            self._last_action = action
            self._last_E = sum(self._p) if self._p is not None else 0.0
            return action
        if self._p is None:
            self._p = [0.5 for _ in range(max(0, n - 1))]
            self._success_streak = 0
            self._fail_streak = 0
            self._recovery_C_used_in_current_fail = 0
            self._pivotal_C_hist = []
            self._last_E = None
            self._last_action = None
            self._last_processed_rounds = 0
        completed = int(history.round_number)
        if completed > self._last_processed_rounds:
            t = self._last_processed_rounds
            while t < completed:
                my_c = 1 if bool(history.my_actions[t]) else 0
                opp_c = int(np.sum(history.opponent_actions[t, :])) if history.opponent_actions.size else 0
                C_count = my_c + opp_c
                success = 1 if C_count >= m else 0
                if success:
                    self._success_streak += 1
                    self._fail_streak = 0
                    self._recovery_C_used_in_current_fail = 0
                else:
                    self._fail_streak += 1
                    self._success_streak = 0
                for j in range(len(self._p)):
                    a = 1.0 if bool(history.opponent_actions[t, j]) else 0.0
                    self._p[j] = (1.0 - self.alpha) * self._p[j] + self.alpha * a
                if self._last_E is not None:
                    did_C = bool(history.my_actions[t])
                    was_pivotal_band = self._last_E >= m - 1 and self._last_E < m
                    pivotal_indicator = 1 if did_C and was_pivotal_band else 0
                    self._pivotal_C_hist.append(pivotal_indicator)
                    if len(self._pivotal_C_hist) > self.W:
                        self._pivotal_C_hist = self._pivotal_C_hist[-self.W:]
                self._last_E = None
                self._last_action = None
                t += 1
            self._last_processed_rounds = completed
        current_round = int(state.round_number)
        E = float(sum(self._p)) if self._p is not None else 0.0
        fatigue = sum(self._pivotal_C_hist) >= self.fatigue_limit
        if current_round == r:
            if E >= m:
                action = Action.D
            elif E >= m - 1:
                action = Action.D if fatigue else Action.C
            else:
                action = Action.D
            self._last_E = E
            self._last_action = action
            return action
        if E >= m:
            action = Action.D
            self._last_E = E
            self._last_action = action
            return action
        if E >= m - 1:
            action = Action.D if fatigue else Action.C
            self._last_E = E
            self._last_action = action
            return action
        if self._fail_streak >= 2 and self._recovery_C_used_in_current_fail < self.B and (E >= m - 2):
            self._recovery_C_used_in_current_fail += 1
            action = Action.C
            self._last_E = E
            self._last_action = action
            return action
        action = Action.D
        self._last_E = E
        self._last_action = action
        return action

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    Risk-Weighted Pivotal Tit-for-Tat (RWP-TFT) for the Collective Risk Dilemma.

    Core behavior:
    - Start by cooperating to seed a threshold-reaching norm.
    - Maintain cooperation when recent play is stable (many successes).
    - Cooperate when near pivotal (group is close to the threshold).
    - When far from threshold and failures persist, withhold (defect), but run
      controlled probe cooperations after consecutive failures to attempt recovery.
    - Under stable but "barely-winning" coalitions, apply mild deterministic pressure
      (rare defections) only when there is slack.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = max(2, r // 5)
        self.L0 = 2
        self.probe_gap_cap = self.W
        self.consec_fail = 0
        self.next_probe_in = 0
        self.probe_gap = self.L0
        self.recent_recovery_probe = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            self.consec_fail = 0
            self.next_probe_in = 0
            self.probe_gap = self.L0
            self.recent_recovery_probe = 0
            return Action.C
        t = int(state.round_number)
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
        Ct_1 = int(my_last) + int(np.sum(opp_last))
        success_last = Ct_1 >= m
        start_idx = max(0, history.round_number - self.W)
        my_w = history.my_actions[start_idx:history.round_number].astype(np.int64)
        opp_w = history.opponent_actions[start_idx:history.round_number, :].astype(np.int64) if history.opponent_actions.size else np.zeros((len(my_w), 0), dtype=np.int64)
        Ct_window = my_w + np.sum(opp_w, axis=1)
        S = int(np.sum(Ct_window >= m))
        stable = S >= (2 * self.W + 2) // 3
        need_build = S < (self.W + 1) // 2
        if success_last:
            self.consec_fail = 0
            self.next_probe_in = 0
            self.probe_gap = self.L0
        else:
            self.consec_fail += 1
            if self.next_probe_in > 0:
                self.next_probe_in -= 1
        if self.recent_recovery_probe > 0:
            self.recent_recovery_probe -= 1
        close_to_threshold = Ct_1 >= m - 1 and Ct_1 <= m
        if t >= r:
            return Action.C if Ct_1 >= m - 1 else Action.D
        if stable and (not success_last):
            severe = Ct_1 <= m - 2
            if not severe or self.recent_recovery_probe == 0:
                self.recent_recovery_probe = 2
                return Action.C
        if stable:
            barely = int(np.sum(Ct_window == m))
            if barely >= (self.W + 1) // 2 and Ct_1 >= m + 1:
                if self.W > 0 and t % self.W == 0:
                    return Action.D
            return Action.C
        if close_to_threshold:
            return Action.C
        if t <= min(3, r):
            if Ct_1 >= m - 1:
                return Action.C
            return Action.C if t % 2 == 0 else Action.D
        if not success_last:
            if self.consec_fail >= self.probe_gap and self.next_probe_in == 0:
                self.next_probe_in = 1
                self.probe_gap = min(self.probe_gap + 1, self.probe_gap_cap)
                return Action.C
        if Ct_1 <= m - 2 and (not need_build):
            return Action.D
        return Action.C if t % 2 == 0 else Action.D

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    Critical-Mass Conditional Cooperator (CMCC) for the Collective Risk Dilemma.

    Core ideas:
    - Start by cooperating to probe for a cooperative coalition.
    - Maintain cooperation only when a "trusted" coalition can plausibly meet threshold m.
    - Build cooperation when close to threshold or when trust suggests m is reachable.
    - After a collapse from success to failure, attempt short recovery for R rounds.
    - Endgame: defect in final round unless likely pivotal for meeting threshold.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 3
        self.R = 2
        self.recovery_left = 0

    def _count_cooperators_last_round(self, history: PlayerHistory) -> int:
        return int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))

    def _trusted_set_mask(self, history: PlayerHistory, t: int) -> NDArray[np.bool_]:
        """
        Returns a boolean mask over opponents (length n_players-1) indicating Trusted.

        t is 1-indexed current decision round number (i.e., next round to play).
        history.round_number == t-1 completed rounds.
        """
        n_opp = history.opponent_actions.shape[1]
        if t <= 1:
            return np.zeros(n_opp, dtype=bool)
        start_round_1idx = max(1, t - self.W)
        start_idx = start_round_1idx - 1
        end_idx = t - 1
        recent = history.opponent_actions[start_idx:end_idx, :]
        if recent.size == 0:
            return np.zeros(n_opp, dtype=bool)
        coop_counts = np.sum(recent, axis=0)
        L = recent.shape[0]
        if L < 3:
            return coop_counts >= 1
        return coop_counts >= 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        m = gd.m
        r = gd.n_rounds
        if history is None:
            self.recovery_left = 0
            return Action.C
        t = history.round_number + 1
        C_prev = self._count_cooperators_last_round(history)
        trusted_mask = self._trusted_set_mask(history, t)
        T = int(np.sum(trusted_mask))
        if t == r:
            if C_prev == m - 1:
                return Action.C
            if T + 1 >= m and C_prev >= m - 1:
                return Action.C
            return Action.D
        if t >= 3:
            C_prev2 = int(history.my_actions[-2]) + int(np.sum(history.opponent_actions[-2, :]))
            if C_prev2 >= m and C_prev < m and (self.recovery_left == 0):
                self.recovery_left = self.R
        if self.recovery_left > 0:
            self.recovery_left -= 1
            return Action.C if T + 1 >= m else Action.D
        if C_prev >= m:
            return Action.C if T + 1 >= m else Action.D
        if C_prev == m - 1:
            return Action.C
        if C_prev >= m - 2 and T + 1 >= m:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    Threshold Parasite with Credible Punishment (TPCP)

    Core behavior:
    - Default: defect to free-ride whenever the threshold is likely met without us.
    - Cooperate only when pivotal (our C flips failure->success) or when "rescuing" a
      just-missed-by-1 failure to restore a profitable regime.
    - If we have been cooperating and the group is still failing frequently, switch to
      Always Defect for the rest of the game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.mode = 'normal'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        m = int(self.game_description.m)
        r = int(self.game_description.n_rounds)
        if history is None or history.round_number == 0:
            return Action.D
        if self.mode == 'always_defect':
            return Action.D
        t = history.round_number + 1
        last_idx = history.round_number - 1
        opp_last = history.opponent_actions[last_idx, :]
        my_last = bool(history.my_actions[last_idx])
        c_last = int(np.sum(opp_last)) + (1 if my_last else 0)
        last_success = c_last >= m
        W = 5
        if history.round_number < W:
            W = history.round_number
        if W > 0:
            recent_my = history.my_actions[-W:]
            recent_my_C = int(np.sum(recent_my))
            opp_recent = history.opponent_actions[-W:, :]
            opp_counts = np.sum(opp_recent, axis=1).astype(int)
            my_counts = recent_my.astype(int)
            total_counts = opp_counts + my_counts
            recent_successes = int(np.sum(total_counts >= m))
            if recent_my_C >= 2 and recent_successes < W / 2.0:
                self.mode = 'always_defect'
                return Action.D
        if my_last:
            c_pred_D = c_last - 1
        else:
            c_pred_D = c_last
        if c_pred_D < 0:
            c_pred_D = 0
        c_pred_C = c_pred_D + 1
        safe_to_free_ride = c_pred_D >= m
        pivotal = c_pred_D == m - 1
        wasted = c_pred_C < m
        if not last_success and c_last == m - 1:
            return Action.C
        if t >= r:
            return Action.C if pivotal else Action.D
        if safe_to_free_ride:
            return Action.D
        if pivotal:
            return Action.C
        if wasted:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    Exploit-the-Threshold (XtT): free-ride whenever success is reliable without you;
    cooperate only when likely pivotal (others at m-1) in a stable environment; otherwise defect.
    Includes a short early probe and endgame tightening, plus a simple pump-then-dump loop.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.free_ride_req = 0.6
        self.pivotal_req = 0.5
        self.pivotal_req_near_end = 0.6
        self.stability_req = 0.6
        self.max_window = 5
        self.probe_p = 0.25
        self.pump_gap = 0.3
        self.dump_len = 2
        self._dump_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        r = int(gd.n_rounds)
        m = int(gd.m)
        k = float(gd.k)
        if k <= 1.0:
            return Action.D
        if history is None:
            if random.random() < self.probe_p:
                return Action.C
            return Action.D
        t = int(history.round_number) + 1
        opp_coop_by_round = np.sum(history.opponent_actions.astype(np.int32), axis=1)
        my_coop_by_round = history.my_actions.astype(np.int32)
        total_coop_by_round = opp_coop_by_round + my_coop_by_round
        w = min(self.max_window, max(1, t - 1))
        probe_rounds = min(3, max(1, r - 1))
        start = len(total_coop_by_round) - w
        end = len(total_coop_by_round)
        last_total = total_coop_by_round[start:end]
        last_my = my_coop_by_round[start:end]
        last_wo = last_total - last_my
        count_wo = int(np.sum(last_wo >= m))
        count_one = int(np.sum(last_wo == m - 1))
        R_wo = count_wo / float(w)
        R_one = count_one / float(w)
        if w <= 1:
            stability = 1.0
        else:
            diffs = np.abs(np.diff(last_total.astype(np.int32)))
            stability = float(np.sum(diffs <= 1)) / float(w - 1)
        C_total_prev = int(total_coop_by_round[-1])
        my_prev = bool(history.my_actions[-1])
        C_wo_prev = C_total_prev - (1 if my_prev else 0)
        if t == r:
            if C_wo_prev >= m:
                return Action.D
            if C_wo_prev == m - 1 and stability >= self.stability_req:
                return Action.C
            return Action.D
        if t <= probe_rounds:
            if C_wo_prev >= m:
                return Action.D
            if C_wo_prev == m - 1:
                return Action.C
            return Action.D
        if self._dump_remaining > 0:
            if C_total_prev < m:
                self._dump_remaining = 0
            else:
                self._dump_remaining -= 1
                return Action.D
        pivotal_req = self.pivotal_req_near_end if t == r - 1 else self.pivotal_req
        if R_wo >= self.free_ride_req:
            return Action.D
        if stability < self.stability_req:
            return Action.D
        last_success = last_total >= m
        mask_c = last_my == 1
        mask_d = last_my == 0
        if np.any(mask_c):
            succ_c = float(np.mean(last_success[mask_c]))
        else:
            succ_c = 0.0
        if np.any(mask_d):
            succ_d = float(np.mean(last_success[mask_d]))
        else:
            succ_d = 0.0
        if succ_c - succ_d >= self.pump_gap:
            if R_one >= pivotal_req * 0.8:
                self._dump_remaining = self.dump_len
                return Action.C
        if R_one >= pivotal_req:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    Threshold-Leech with Credible Punishment (TL-CP) for the Collective Risk Dilemma.

    Core behavior:
    - Defect by default (including round 1 and the final round).
    - Cooperate only when likely pivotal to meeting the threshold and success is plausible.
    - Always defect when the group is comfortably meeting the threshold (slack >= 1).
    - If we cooperated and the round still failed, enter a short punishment (defect) period,
      with early exit if others succeed anyway.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_until = 0
        self.P = 2
        self.p_threshold = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        t = int(state.round_number)
        if history is None or history.round_number == 0 or t <= 1:
            return Action.D
        if t >= r:
            return Action.D
        my_prev_c = bool(history.my_actions[-1])
        opp_prev_c_count = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        S_prev = opp_prev_c_count + (1 if my_prev_c else 0)
        if my_prev_c and S_prev < m:
            candidate = t + self.P
            if candidate > self.punish_until:
                self.punish_until = candidate
        if t < self.punish_until:
            if S_prev >= m:
                self.punish_until = 0
            else:
                return Action.D
        if t <= 3:
            return Action.C if S_prev == m - 1 else Action.D
        if t == r - 1:
            recent = min(3, history.round_number)
            if recent <= 0:
                return Action.D
            my_recent = history.my_actions[-recent:].astype(np.int32)
            opp_recent = history.opponent_actions[-recent:, :].astype(np.int32) if history.opponent_actions.size else np.zeros((recent, 0), dtype=np.int32)
            S_recent = np.sum(opp_recent, axis=1) + my_recent
            any_success = bool(np.any(S_recent >= m))
            if S_prev == m - 1 and any_success:
                return Action.C
            return Action.D
        w = min(5, t - 1, history.round_number)
        if w <= 0:
            p = 0.0
            E = 0.0
        else:
            my_w = history.my_actions[-w:].astype(np.int32)
            opp_w = history.opponent_actions[-w:, :].astype(np.int32) if history.opponent_actions.size else np.zeros((w, 0), dtype=np.int32)
            S_w = np.sum(opp_w, axis=1) + my_w
            p = float(np.mean(S_w >= m))
            if opp_w.shape[1] > 0:
                q = np.mean(opp_w, axis=0)
                E = float(np.sum(q))
            else:
                E = 0.0
        if S_prev >= m + 1:
            return Action.D
        plausible = p >= self.p_threshold
        near_threshold_last = S_prev == m - 1 or S_prev == m
        near_threshold_expected = E >= m - 1.2 and E <= m + 0.2
        if plausible and (near_threshold_last or near_threshold_expected):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    Pivot Exploiter (PE) for the Collective Risk Dilemma.

    Core idea:
    - Estimate opponents' near-term cooperation propensities from a short window.
    - Approximate pivotality via Monte Carlo of a Poisson-binomial model.
    - Cooperate only when likely pivotal, not too easy to free-ride, and not hopeless.
    - Regime control:
        * FARM: if others have been reliably meeting the threshold while carrying you,
          defect for 2 rounds.
        * PUNISH: if you invested but the group underdelivered, defect for 2 rounds.
    - Always defect in the final round; special caution in the second-to-last round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._mode = 'EXPLOIT'
        self._mode_timer = 0
        self._mc_samples = 120

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if history is None:
            if k >= 1.5 and m <= math.ceil(n / 2):
                return Action.C
            return Action.D
        completed = int(history.round_number)
        if t >= r:
            return Action.D
        if self._mode_timer > 0:
            self._mode_timer -= 1
            if self._mode in ('FARM', 'PUNISH'):
                return Action.D
            self._mode = 'EXPLOIT'
        if self._mode_timer == 0 and self._mode != 'EXPLOIT':
            self._mode = 'EXPLOIT'
        if completed <= 0:
            if k >= 1.5 and m <= math.ceil(n / 2):
                return Action.C
            return Action.D
        L = 3
        if completed >= L:
            recent_my = history.my_actions[-L:]
            recent_opp = history.opponent_actions[-L:, :]
            recent_C = np.sum(recent_opp, axis=1) + recent_my.astype(int)
            threshold_met_last3 = recent_C >= m
            count_threshold_met = int(np.sum(threshold_met_last3))
            my_C_count_last3 = int(np.sum(recent_my))
            if count_threshold_met == L and my_C_count_last3 <= 1:
                self._mode = 'FARM'
                self._mode_timer = 2
                return Action.D
            if my_C_count_last3 >= 2 and count_threshold_met <= 1:
                self._mode = 'PUNISH'
                self._mode_timer = 2
                return Action.D
        theta_free = 0.75
        theta_hope = 0.35
        theta_piv = min(0.25, 1.0 / k) if k > 0 else 0.25
        if t == r - 1:
            theta_free = 0.6
            theta_piv = min(0.35, 1.0 / k) if k > 0 else 0.35
        W = min(5, completed)
        opp_recent = history.opponent_actions[-W:, :]
        p = np.mean(opp_recent.astype(float), axis=0)
        samples = self._mc_samples
        met_without = 0
        met_with = 0
        if m <= 0:
            P_met_without = 1.0
            P_met_with = 1.0
        elif m > n - 1 + 1:
            P_met_without = 0.0
            P_met_with = 0.0
        else:
            for _ in range(samples):
                others_C = 0
                for pj in p:
                    if random.random() < float(pj):
                        others_C += 1
                if others_C >= m:
                    met_without += 1
                if others_C >= m - 1:
                    met_with += 1
            P_met_without = met_without / float(samples)
            P_met_with = met_with / float(samples)
        P_piv = P_met_with - P_met_without
        if P_met_without >= theta_free:
            return Action.D
        if P_met_with <= theta_hope:
            return Action.D
        if P_piv >= theta_piv:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    Ruthless Pivotal Free-Rider (RPF) for the Collective Risk Dilemma.

    Core behavior:
    - Seed with C in round 1.
    - Free-ride (D) when others reliably meet the threshold without you.
    - Cooperate only when likely pivotal (especially at x = m-1, sometimes m-2 with upward trend).
    - Stop wasting cooperation in hopeless environments (others never get near m-1).
    - Last round: defect unless last round suggests you are exactly pivotal (x = m-1).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = min(5, max(1, game_description.n_rounds - 1))
        self.theta = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        T = n_rounds
        opp = history.opponent_actions
        if opp is None or len(opp) == 0:
            x_history = []
        else:
            x_history = [int(np.sum(opp[i, :])) for i in range(opp.shape[0])]
        last_x = x_history[-1] if len(x_history) > 0 else 0
        recent = x_history[-self.W:] if len(x_history) > 0 else []
        if len(recent) > 0:
            freq_others_alone_success = float(sum((1 for x in recent if x >= m))) / float(len(recent))
            max_recent = max(recent)
        else:
            freq_others_alone_success = 0.0
            max_recent = -1
        if t == T:
            if last_x == m - 1:
                return Action.C
            return Action.D
        if len(recent) >= 2 and freq_others_alone_success >= self.theta:
            return Action.D
        if len(recent) >= 2 and max_recent < m - 1:
            return Action.D
        if last_x == m - 1:
            return Action.C
        if len(x_history) >= 2:
            prev_x = x_history[-2]
            if last_x == m - 2 and last_x > prev_x:
                return Action.C
        if t <= min(3, T - 1):
            if last_x == m - 2 or last_x == m - 1:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    Threshold Parasite with Credible Trigger (TPCT) for the Collective Risk Dilemma.

    Core behavior:
    - Default defect to free-ride whenever possible.
    - Limited early probing: cooperate only if last round opponents were exactly at m-1.
    - Main phase: defect if opponents often "carry" (>= m without us); cooperate only when
      near-threshold (often at m-1 / >= m-1 but not >= m), except near endgame.
    - Credible trigger: if we ever cooperate and the threshold is still missed, punish by
      defecting for a block of rounds.
    - Always defect in the final round; typically also defect very late-game.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t >= r:
            return Action.D
        if history is not None and history.round_number > 0:
            last_idx = history.round_number - 1
            my_last_c = bool(history.my_actions[last_idx])
            opp_last = history.opponent_actions[last_idx, :]
            x_last = int(np.sum(opp_last)) if opp_last.size else 0
            s_last = x_last + (1 if my_last_c else 0)
            if my_last_c and s_last < m:
                L = max(2, int(math.ceil(r / 6)))
                self.punish_until = max(self.punish_until, last_idx + 1 + L)
        if t <= self.punish_until:
            return Action.D
        if history is None:
            return Action.D
        W = min(10, max(3, int(math.ceil(r / 5))))
        probe_rounds = min(max(2, int(math.ceil(r / 10))), 5)
        if t <= probe_rounds:
            if history.round_number >= 1:
                x_prev = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
                if x_prev == m - 1:
                    return Action.C
            return Action.D
        lookback = min(W, history.round_number)
        if lookback <= 0:
            return Action.D
        opp_actions_lb = history.opponent_actions[-lookback:, :]
        x_counts = np.sum(opp_actions_lb, axis=1).astype(int)
        carry_rate = float(np.mean(x_counts >= m)) if lookback > 0 else 0.0
        support_rate = float(np.mean(x_counts >= m - 1)) if lookback > 0 else 0.0
        exact_pivotal_rate = float(np.mean(x_counts == m - 1)) if lookback > 0 else 0.0
        if carry_rate >= 0.5:
            return Action.D
        near_threshold = exact_pivotal_rate >= 0.3 or (support_rate >= 0.4 and carry_rate < 0.5)
        if near_threshold:
            if t >= r - 2:
                return Action.D
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    Opportunistic Threshold Parasite (OTP)

    Defects by default to free-ride on threshold success, cooperates only when
    near-pivotal (around m or m-1) and the group looks recoverable, retreats
    quickly when cooperation collapses, and always defects in the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = self.r
        m = self.m
        k = self.k
        if history is None:
            return Action.C if k >= 1.5 else Action.D
        if t == r:
            return Action.D
        n_completed = int(history.round_number)
        if n_completed <= 0:
            return Action.C if k >= 1.5 else Action.D
        opp = history.opponent_actions
        my = history.my_actions
        C_counts = []
        for j in range(n_completed):
            C_counts.append(int(np.sum(opp[j, :])) + (1 if bool(my[j]) else 0))
        last_C = C_counts[-1]
        if t == 2:
            if last_C == m - 1:
                return Action.C
        W = min(5, n_completed)
        recent = C_counts[-W:]
        avg_coop = float(sum(recent)) / float(W) if W > 0 else 0.0
        success_rate = float(sum((1 for x in recent if x >= m))) / float(W) if W > 0 else 0.0
        trend = 0
        if n_completed >= 2:
            trend = C_counts[-1] - C_counts[-2]
        if t == r - 1:
            if last_C == m - 1 and success_rate >= 0.6:
                return Action.C
            return Action.D
        if W > 0:
            successes = sum((1 for x in recent if x >= m))
            if successes >= 3 and avg_coop >= m:
                if last_C == m or last_C == m - 1:
                    return Action.C
                return Action.D
        if last_C >= m + 1 or avg_coop >= m + 1:
            return Action.D
        if (last_C == m or last_C == m - 1) and trend <= -2:
            return Action.D
        if last_C == m:
            return Action.C
        if last_C == m - 1:
            if trend >= 0 or success_rate >= 0.4 or t <= 2:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    Threshold Leecher with Credible Punishment (TLCP).

    Exploitative threshold strategy for the Collective Risk Dilemma:
    - Free-ride (defect) when recent cooperation is safely above threshold.
    - Cooperate only when likely pivotal (at/just below threshold) and only briefly.
    - After a collapse from a recently cooperative state, punish by defecting for P rounds,
      exiting early if others re-establish threshold without you.
    - Always defect in the last round; default defect in the first round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = int(min(5, max(1, r - 1)))
        self.P = 2
        self.L = 2
        self.punish_remaining = 0
        self.recover_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if state is not None and hasattr(state, 'round_number') and (state.round_number is not None):
            t = int(state.round_number)
        else:
            t = 1 if history is None else int(history.round_number + 1)
        if history is None or history.round_number == 0:
            self.punish_remaining = 0
            self.recover_remaining = 0
            return Action.D
        if t >= r:
            self.punish_remaining = 0
            self.recover_remaining = 0
            return Action.D
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
        C_last = int(my_last) + int(np.sum(opp_last))
        w = min(self.W, history.round_number)
        if w <= 0:
            was_coop_recently = False
        else:
            my_w = history.my_actions[-w:].astype(np.int64)
            opp_w = history.opponent_actions[-w:, :].astype(np.int64) if history.opponent_actions.size else np.zeros((w, 0), dtype=np.int64)
            totals = my_w + np.sum(opp_w, axis=1)
            was_coop_recently = bool(np.any(totals >= m))
        if self.punish_remaining > 0:
            if C_last >= m:
                self.punish_remaining = 0
            else:
                self.punish_remaining -= 1
                self.recover_remaining = 0
                return Action.D
        if C_last < m and was_coop_recently:
            self.punish_remaining = self.P
            self.punish_remaining -= 1
            self.recover_remaining = 0
            return Action.D
        if self.recover_remaining > 0:
            if C_last >= m:
                self.recover_remaining = 0
            else:
                self.recover_remaining -= 1
                return Action.C
        if C_last == m - 1:
            self.recover_remaining = max(0, self.L - 1)
            return Action.C
        if t == r - 1:
            return Action.D
        if C_last >= m + 1:
            return Action.D
        if C_last == m:
            return Action.C
        if C_last <= m - 2:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    Threshold Parasite with Credible Punishment (TPCP) for the Collective Risk Dilemma.

    Core behavior:
    - Defect by default and in the final round.
    - Estimate how many "reliable" cooperators exist among opponents based on a short window.
    - Cooperate only when likely pivotal (estimated reliable opponents == m-1), except in the last round.
    - After you cooperated and the group still failed, punish by defecting for a few rounds.
    - If the group succeeds comfortably without you, enter free-ride mode (defect) until two consecutive failures.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W_max = 5
        self.theta = 0.6
        self.free_ride_mode = False
        self.punish_until_round = 0
        self.prev_free_ride_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = int(state.round_number)
        if history is None or history.round_number == 0 or t == 1:
            return Action.D
        if t >= r:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        last_idx = history.round_number - 1
        if last_idx < 0:
            return Action.D
        my_c_last = 1 if bool(history.my_actions[last_idx]) else 0
        opp_c_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
        C_last = my_c_last + opp_c_last
        success_last = C_last >= m
        i_defected_last = my_c_last == 0
        if my_c_last == 1 and (not success_last):
            P = min(3, r - t + 1)
            self.punish_until_round = t + P - 1
            return Action.D
        if success_last and i_defected_last and (C_last >= m + 1):
            self.free_ride_mode = True
        if self.free_ride_mode:
            if history.round_number >= 2:
                idx1 = history.round_number - 1
                idx2 = history.round_number - 2
                my1 = 1 if bool(history.my_actions[idx1]) else 0
                my2 = 1 if bool(history.my_actions[idx2]) else 0
                opp1 = int(np.sum(history.opponent_actions[idx1, :])) if history.opponent_actions.size else 0
                opp2 = int(np.sum(history.opponent_actions[idx2, :])) if history.opponent_actions.size else 0
                C1 = my1 + opp1
                C2 = my2 + opp2
                if C1 < m and C2 < m:
                    self.free_ride_mode = False
                else:
                    return Action.D
            else:
                return Action.D
        W = min(self.W_max, history.round_number)
        start = history.round_number - W
        end = history.round_number
        reliable = 0
        if history.opponent_actions.size:
            opp_actions_window = history.opponent_actions[start:end, :]
            rates = np.mean(opp_actions_window.astype(float), axis=0) if W > 0 else np.zeros(opp_actions_window.shape[1])
            for rate in rates:
                if float(rate) >= self.theta:
                    reliable += 1
        S_hat = reliable
        if S_hat >= m:
            return Action.D
        if S_hat == m - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    Threshold-Sniper (TS) for the Collective Risk Dilemma.

    - Maintain Laplace-smoothed cooperation rates p_j for each opponent.
    - Predict expected other cooperators mu = sum p_j.
    - Play pivotal cooperation only when mu is near the threshold:
        * if mu >= m      -> Defect (free-ride)
        * if mu <= m - 2  -> Defect (can't save it)
        * else            -> Cooperate (likely pivotal)
      After two recent failures, require stronger evidence (mu >= m - 0.5).
    - Anti-sucker control: count "pivotal payments" (we cooperated and total_C == m),
      optionally also count wasted cooperation (we cooperated and total_C < m),
      and if payments exceed a shrinking budget, switch to permanent defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.shutdown = False
        self.pivotal_payments = 0
        self.alpha = 1.0
        self.probe_k_threshold = 1.5
        self.count_wasted_as_payment = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.shutdown:
            return Action.D
        if history is None or history.round_number == 0:
            if self.k >= self.probe_k_threshold and self.m <= int(math.ceil(self.n / 2.0)):
                return Action.C
            return Action.D
        t = int(state.round_number)
        if t < 1:
            t = history.round_number + 1
        last_total_c = int(np.sum(history.opponent_actions[-1, :]) + (1 if bool(history.my_actions[-1]) else 0))
        if bool(history.my_actions[-1]):
            if last_total_c == self.m:
                self.pivotal_payments += 1
            elif self.count_wasted_as_payment and last_total_c < self.m:
                self.pivotal_payments += 1
        remaining = self.r - t + 1
        if remaining < 1:
            remaining = 1
        budget = int(max(1, math.floor(remaining / 4.0)))
        if self.pivotal_payments > budget:
            self.shutdown = True
            return Action.D
        opp = history.opponent_actions
        rounds_completed = int(history.round_number)
        c_counts = np.sum(opp, axis=0).astype(float)
        denom = rounds_completed + 2.0 * self.alpha
        p = (c_counts + self.alpha) / denom
        mu = float(np.sum(p))
        failed_recently = False
        if rounds_completed >= 2:
            total_c_1 = int(np.sum(history.opponent_actions[-1, :]) + (1 if bool(history.my_actions[-1]) else 0))
            total_c_2 = int(np.sum(history.opponent_actions[-2, :]) + (1 if bool(history.my_actions[-2]) else 0))
            failed_recently = total_c_1 < self.m and total_c_2 < self.m
        if mu >= self.m:
            action = Action.D
        elif mu <= self.m - 2:
            action = Action.D
        elif failed_recently:
            action = Action.C if mu >= self.m - 0.5 else Action.D
        else:
            action = Action.C
        if t >= self.r:
            return action
        return action

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    SqueezethenStay (STS) for the Collective Risk Dilemma.

    Exploitative policy:
    - Start by defecting to test whether others can reach the threshold without you.
    - Free-ride (defect) whenever others can (or likely can) carry the threshold.
    - Only (probabilistically) "pivot cooperate" when the previous round was a near miss
      and you were the holdout (exactly m-1 others cooperated while you defected).
    - Never keep subsidizing: if you cooperated and it still failed, hard stop to defection.
    - Become more selfish near the end; last round almost always defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.m = game_description.m
        self.k = game_description.k
        self.others_can_do_it = False
        self.near_miss_count = 0
        self.fail_streak = 0
        self.hard_stop = 0

    def _p_pivot(self, t: int) -> float:
        R = self.r - t + 1
        p = 0.2 + 0.7 * (float(R) / float(self.r))
        return min(0.9, max(0.0, p))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.D
        if self.hard_stop > 0:
            self.hard_stop -= 1
            return Action.D
        last_idx = history.round_number - 1
        my_last_C = bool(history.my_actions[last_idx])
        a = 1 if my_last_C else 0
        opp_coops = int(np.sum(history.opponent_actions[last_idx, :]))
        x = opp_coops + a
        y = opp_coops
        if y >= self.m:
            self.others_can_do_it = True
        if x == self.m - 1:
            self.near_miss_count += 1
        if x < self.m:
            self.fail_streak += 1
        else:
            self.fail_streak = 0
        if my_last_C and x < self.m:
            self.hard_stop = 1
            return Action.D
        if t >= self.r:
            if x == self.m - 1 and (not my_last_C) and (y == self.m - 1):
                return Action.C if random.random() < 0.3 else Action.D
            return Action.D
        if y >= self.m:
            return Action.D
        if self.others_can_do_it and y >= self.m - 1:
            return Action.D
        if x <= self.m - 2:
            return Action.D
        if x == self.m - 1:
            if not my_last_C and y == self.m - 1:
                p = self._p_pivot(t)
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Threshold-Hijacker (TH) for the Collective Risk Dilemma.

    Core idea:
    - Track each opponent's cooperation propensity via EWMA.
    - Defect by default; cooperate only when likely pivotal and profitable.
    - Enter a temporary cold-defection mode if the group is consistently far from the threshold.
    - Become stricter near the end of the game; last round is purely myopic pivotal logic.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.3
        self.W = 3
        r = int(game_description.n_rounds)
        self.L = max(2, int(math.ceil(0.2 * r)))
        self._p = None
        self._cold_mode = 0
        self._last_processed_rounds = 0

    def _ensure_p(self, n_opponents: int):
        if self._p is None or len(self._p) != n_opponents:
            self._p = [0.5 for _ in range(n_opponents)]
            self._last_processed_rounds = 0

    def _update_p_from_history(self, history: PlayerHistory):
        opp = history.opponent_actions
        if opp is None:
            return
        n_rounds_done = int(history.round_number)
        if n_rounds_done <= self._last_processed_rounds:
            return
        n_opponents = int(opp.shape[1])
        self._ensure_p(n_opponents)
        for t in range(self._last_processed_rounds, n_rounds_done):
            for j in range(n_opponents):
                obs = 1.0 if bool(opp[t, j]) else 0.0
                self._p[j] = (1.0 - self.alpha) * self._p[j] + self.alpha * obs
        self._last_processed_rounds = n_rounds_done

    def _last_S_list(self, history: PlayerHistory, W: int):
        out = []
        if history is None:
            return out
        rounds_done = int(history.round_number)
        if rounds_done <= 0:
            return out
        start = max(0, rounds_done - W)
        for t in range(rounds_done - 1, start - 1, -1):
            my_c = 1 if bool(history.my_actions[t]) else 0
            opp_c = int(np.sum(history.opponent_actions[t, :])) if history.opponent_actions.size else 0
            out.append(my_c + opp_c)
        return out

    def _poisson_binomial_pmf_k(self, probs, k: int):
        if k < 0:
            return 0.0
        n = len(probs)
        if k > n:
            return 0.0
        dp = [0.0] * (k + 1)
        dp[0] = 1.0
        for p in probs:
            q = 1.0 - p
            upper = min(k, n)
            for s in range(min(k, n), 0, -1):
                dp[s] = dp[s] * q + dp[s - 1] * p
            dp[0] = dp[0] * q
        return float(dp[k])

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        gd = self.game_description
        n = int(gd.n_players)
        r = int(gd.n_rounds)
        m = int(gd.m)
        k_reward = float(gd.k)
        if history is None:
            if k_reward >= 3.0 and m <= int(math.floor(n / 2)):
                return Action.C
            return Action.D
        self._update_p_from_history(history)
        completed = int(history.round_number)
        t = completed + 1
        last_S = self._last_S_list(history, self.W)
        if len(last_S) == self.W:
            all_far = True
            for S in last_S:
                if not S < m - 1:
                    all_far = False
                    break
            if all_far:
                self._cold_mode = self.W
        if self._cold_mode > 0:
            self._cold_mode -= 1
            return Action.D
        probs = self._p if self._p is not None else [0.5 for _ in range(max(0, n - 1))]
        P_pivotal = self._poisson_binomial_pmf_k(probs, m - 1)
        if t == r:
            tau = 0.0
        else:
            last_block_start = r - self.L + 1
            if t >= last_block_start:
                tau = 0.5
            else:
                tau = 0.2
        if k_reward * P_pivotal > 1.0 + tau:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    Threshold Parasite with Credible Retaliation (TPCR)

    - Defect by default; cooperate only when estimated pivotal for meeting threshold.
    - If we cooperated and the group still failed, enter punishment mode (defect P rounds),
      then do a one-round re-entry test (defect) to see if the group can now carry us.
    - Always defect on the final round.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.delta = 0.4
        self.W_max = 5
        self.P = 2
        self.punish_remaining = 0
        self.reentry_test = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if state.round_number >= r:
            return Action.D
        if state.round_number == r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.reentry_test = True
            return Action.D
        if self.reentry_test:
            self.reentry_test = False
            return Action.D
        delta = self.delta
        if state.round_number == r - 1:
            delta = 0.2
        t_prev = history.round_number - 1
        opp_C_prev = int(np.sum(history.opponent_actions[t_prev, :])) if history.opponent_actions.size else 0
        i_C_prev = bool(history.my_actions[t_prev])
        C_prev_total = opp_C_prev + (1 if i_C_prev else 0)
        success_prev = C_prev_total >= m
        if i_C_prev and (not success_prev):
            self.punish_remaining = self.P
            return Action.D
        W = min(self.W_max, history.round_number)
        if W <= 0 or history.opponent_actions.size == 0:
            C_hat_others = 0.0
        else:
            window = history.opponent_actions[history.round_number - W:history.round_number, :]
            p = np.mean(window, axis=0)
            C_hat_others = float(np.sum(p))
        if C_hat_others >= m:
            return Action.D
        if C_hat_others < m - 1 - delta:
            return Action.D
        if m - 1 - delta <= C_hat_others <= m - 1 + delta:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    Threshold Hijacker (TH): exploitative threshold free-rider.

    Core behavior:
    - Default to Defect.
    - Free-ride if others typically reach the threshold without us.
    - Cooperate only when likely pivotal (frequent near-misses at m-1).
    - Withdraw (cooldown defection) if cooperation doesn't seem to help / we're being used.
    - If others appear responsive to our cooperation, use sparse "spark" pulses.
    - Always defect in the last round; mostly defect in second-to-last unless strong near-miss evidence.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self._cooldown = 0
        self._max_window = 10
        self._theta_near = 0.25
        self._theta_fail = 0.2
        self._theta_near_hopeless = 0.1
        self._delta_spark = 0.5
        self._cooldown_L = 2
        self._theta_near_endgame = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        t = int(state.round_number) if state is not None else 1
        if history is None or history.round_number <= 0 or t <= 1:
            self._cooldown = 0
            return Action.D
        if t >= r:
            return Action.D
        is_second_to_last = t == r - 1
        if self._cooldown > 0:
            self._cooldown -= 1
            return Action.D
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        completed = int(history.round_number)
        x = (np.sum(opp_actions, axis=1) + my_actions.astype(np.int64)).astype(np.int64)
        W = min(self._max_window, completed)
        if W <= 0:
            return Action.D
        start = completed - W
        recent_idx = np.arange(start, completed)
        x_recent = x[recent_idx]
        S_recent = (x_recent >= m).astype(np.float64)
        s_hat = float(np.mean(S_recent)) if W > 0 else 0.0
        near_recent = (x_recent == m - 1).astype(np.float64)
        p_near = float(np.mean(near_recent)) if W > 0 else 0.0
        xs_wo_me = []
        for j in recent_idx:
            if not bool(my_actions[j]):
                xs_wo_me.append(int(x[j]))
        x_wo_me_hat = float(np.mean(xs_wo_me)) if len(xs_wo_me) > 0 else None
        afterC = []
        afterD = []
        for j in recent_idx:
            if j - 1 < 0:
                continue
            if bool(my_actions[j - 1]):
                afterC.append(int(x[j]))
            else:
                afterD.append(int(x[j]))
        if len(afterC) > 0 and len(afterD) > 0:
            delta = float(np.mean(afterC) - np.mean(afterD))
        else:
            delta = 0.0
        if t == 2 and completed >= 1:
            if int(x[0]) == m - 1:
                return Action.C
            return Action.D
        if x_wo_me_hat is not None and x_wo_me_hat >= m:
            return Action.D
        if s_hat < self._theta_fail and p_near < self._theta_near_hopeless:
            return Action.D
        recent_myC = int(np.sum(my_actions[recent_idx]))
        if recent_myC >= 2 and s_hat < 0.5 and (p_near < 0.2):
            self._cooldown = self._cooldown_L
            return Action.D
        if is_second_to_last:
            if p_near >= self._theta_near_endgame and s_hat >= self._theta_fail:
                return Action.C
            return Action.D
        if p_near >= self._theta_near:
            return Action.C
        if delta >= self._delta_spark:
            return Action.C if t % 4 == 0 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    Threshold Parasite with Credible Punishment (TPCP)

    Core behavior:
    - FARM: defect whenever others meet threshold without us; cooperate only when pivotal (x=m-1),
      plus rare "insurance" cooperation when x==m and history looks unstable.
    - PUNISH: defect for a fixed block when coordination is unreliable.
    - RESET: short probing phase to see if others can meet threshold without us; optionally seed once.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.FARM = 0
        self.PUNISH = 1
        self.RESET = 2
        self.mode = self.FARM
        self.W = 5
        self.L = 2
        self.Rs = 2
        self.p_insure = 0.15
        self.punish_left = 0
        self.reset_left = 0
        self.streak_met = 0
        self.streak_fail = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number <= 0:
            self.mode = self.FARM
            self.punish_left = 0
            self.reset_left = 0
            self.streak_met = 0
            self.streak_fail = 0
            return Action.D
        t = history.round_number
        opp_last = history.opponent_actions[-1, :]
        x = int(np.sum(opp_last))
        my_last_c = bool(history.my_actions[-1])
        met_last = x + (1 if my_last_c else 0) >= m
        if met_last:
            self.streak_met += 1
            self.streak_fail = 0
        else:
            self.streak_fail += 1
            self.streak_met = 0
        W_eff = min(self.W, history.round_number)
        if W_eff <= 0:
            success_rate = 0.0
        else:
            opp_recent = history.opponent_actions[-W_eff:, :]
            my_recent = history.my_actions[-W_eff:]
            total_coop = np.sum(opp_recent, axis=1) + my_recent.astype(int)
            met_vec = (total_coop >= m).astype(float)
            success_rate = float(np.mean(met_vec))
        recent_instability = met_last and success_rate < 0.8
        rounds_left = r - t
        p_ins = 0.0 if rounds_left <= 2 else self.p_insure
        if self.mode == self.PUNISH:
            self.punish_left -= 1
            if self.punish_left <= 0:
                self.mode = self.RESET
                self.reset_left = self.Rs
            return Action.D
        if self.mode == self.RESET:
            if x >= m:
                self.mode = self.FARM
                self.reset_left = 0
                return Action.D
            act = Action.D
            if self.reset_left == 1 and x == m - 1 and (rounds_left >= 1):
                act = Action.C
            self.reset_left -= 1
            if self.reset_left <= 0:
                if x >= m - 1:
                    self.mode = self.FARM
                else:
                    self.mode = self.PUNISH
                    self.punish_left = self.L
            return act
        if self.streak_fail >= 2 or success_rate < 0.5:
            self.mode = self.PUNISH
            self.punish_left = self.L
            return Action.D
        if rounds_left == 1:
            if x == m - 1:
                return Action.C
            return Action.D
        if x >= m:
            return Action.D
        if x == m - 1:
            return Action.C
        if x == m and p_ins > 0.0 and recent_instability:
            if random.random() < p_ins:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    Opportunistic Threshold Exploiter (OTE) for the Collective Risk Dilemma.

    Core behavior:
    - Default to defecting (free-ride) when the group is likely to meet the threshold without us.
    - Cooperate mainly when likely pivotal (others at m-1), but avoid repeated subsidizing.
    - If we were pivotal in a successful round, enter a short "punish" phase of guaranteed defection.
    - Occasionally "rescue" after a narrow failure (others at m-1) with probability q = min(1, 1/k).
    - Last round is myopic: cooperate only if last observed others == m-1; otherwise defect.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 5
        self.P = 2
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        if history is None or history.round_number == 0:
            self.punish_timer = 0
            return Action.D

        def threshold_met_at(j: int) -> bool:
            my_a = 1 if bool(history.my_actions[j]) else 0
            opp_c = int(np.sum(history.opponent_actions[j, :])) if history.opponent_actions.size else 0
            return opp_c + my_a >= m
        t = int(history.round_number) + 1
        x_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        met_last = threshold_met_at(-1)
        my_last_c = bool(history.my_actions[-1])
        if my_last_c and met_last and (x_last == m - 1):
            self.punish_timer = self.P
        elif self.punish_timer > 0:
            self.punish_timer -= 1
        if t == r:
            if x_last >= m:
                return Action.D
            if x_last == m - 1:
                return Action.C
            return Action.D
        if t <= self.W + 1:
            if x_last >= m:
                return Action.D
            if x_last == m - 1:
                if self.punish_timer > 0:
                    return Action.D
                return Action.C
            return Action.D
        if self.punish_timer > 0:
            return Action.D
        w = min(self.W, int(history.round_number))
        xs = []
        mets = []
        start = int(history.round_number) - w
        for j in range(start, int(history.round_number)):
            opp_c = int(np.sum(history.opponent_actions[j, :])) if history.opponent_actions.size else 0
            xs.append(opp_c)
            mets.append(1 if threshold_met_at(j) else 0)
        x_bar = float(sum(xs)) / float(w) if w > 0 else 0.0
        p_meet = float(sum(mets)) / float(w) if w > 0 else 0.0
        pivotal_count = 0
        for xv in xs:
            if xv == m - 1:
                pivotal_count += 1
        p_pivotal = float(pivotal_count) / float(w) if w > 0 else 0.0
        if x_bar <= m - 2 and sum(mets) == 0:
            return Action.D
        if x_bar >= m:
            return Action.D
        if p_meet >= 0.7 and x_bar >= m - 0.5:
            return Action.D
        if (p_pivotal >= 0.4 or x_last == m - 1) and p_meet <= 0.7:
            return Action.C
        if x_last == m - 1 and (not met_last):
            q = 1.0 / k if k > 0 else 1.0
            if q > 1.0:
                q = 1.0
            if random.random() < q:
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    Opportunistic Threshold Harvester (OTH)

    Core behavior:
    - Default to defection (harvest) whenever threshold is likely met without us.
    - Cooperate only as a targeted, pivotal "investment" when gap<=1 and the group
      appears reliable enough to make the investment pay off.
    - After successful cooperation, try to defect next round unless predicted failure.
    - Permanently defect against chronic cooperators (easy harvesting).
    - Permanently defect if the group never meets threshold early (hopeless).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t < 1:
            t = 1
        if history is None or history.round_number == 0:
            if self.m >= self.n - 1:
                return Action.C
            return Action.D

        def threshold_met_round(idx: int) -> bool:
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            return my_c + opp_c >= self.m
        completed = int(history.round_number)
        if completed < 1:
            if self.m >= self.n - 1:
                return Action.C
            return Action.D
        if t >= 3:
            any_met = False
            i = 0
            while i < completed:
                if threshold_met_round(i):
                    any_met = True
                    break
                i += 1
            if not any_met:
                return Action.D
        L = min(5, completed)
        W = min(5, completed)
        recent_opp = history.opponent_actions[-L:, :] if L > 0 else history.opponent_actions[0:0, :]
        if L > 0 and recent_opp.size:
            p = np.mean(recent_opp.astype(float), axis=0)
        else:
            p = np.zeros((self.n - 1,), dtype=float)
        P = float(np.sum(p)) if p.size else 0.0
        meet_count = 0
        i = completed - W
        if i < 0:
            i = 0
        while i < completed:
            if threshold_met_round(i):
                meet_count += 1
            i += 1
        meet_rate = meet_count / float(W) if W > 0 else 0.0
        streak = 0
        i = completed - 1
        while i >= 0:
            if threshold_met_round(i):
                streak += 1
                i -= 1
            else:
                break
        gap = float(self.m) - P
        if t >= self.r:
            if P >= self.m:
                return Action.D
            if gap <= 1.0:
                likely_coops = int(np.sum(p >= 0.6)) if p.size else 0
                if likely_coops >= self.m - 1:
                    return Action.C
            return Action.D
        if p.size and int(np.sum(p >= 0.9)) >= self.m:
            return Action.D
        if bool(history.my_actions[-1]) and threshold_met_round(completed - 1):
            if P >= self.m:
                return Action.D
        if P >= self.m:
            return Action.D
        if gap > 1.0:
            return Action.D
        if completed >= 2:
            if not threshold_met_round(completed - 1) and (not threshold_met_round(completed - 2)):
                pass
        likely_coops = int(np.sum(p >= 0.6)) if p.size else 0
        B1 = streak >= 1 or likely_coops >= self.m - 1
        B2 = meet_rate >= 0.5
        if B1 and B2 and (gap <= 1.0):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    Pivot & Parasite (P&P) for the Collective Risk Dilemma.

    Core behavior:
    - Default to defecting.
    - Free-ride whenever the threshold seems reachable without us.
    - Cooperate only when likely pivotal AND recent play suggests the group is reliably near/at threshold.
    - Enter a temporary defection lock after cooperating into a failed round.
    - If repeated failure is detected, switch to a permanent-defect mode unless the group shows renewed near-threshold capacity.
    - If others alone have met the threshold for several consecutive rounds, stay in parasite mode (always defect) until that stops.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.alpha = 0.3
        self.W = 5
        self.theta = 0.6
        self.L = 3
        self.T_lock = 3
        self.fail_F = 4
        self.perma_defect = False
        self.hat_x = float(self.m - 1)
        self.lock = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :] if history.opponent_actions.size else np.array([], dtype=np.bool_)
        x_last = int(np.sum(opp_last))
        C_last = x_last + (1 if my_last else 0)
        self.hat_x = (1.0 - self.alpha) * self.hat_x + self.alpha * float(x_last)
        if my_last and C_last < self.m:
            self.lock = int(self.T_lock)
        total_C_hist = np.sum(history.opponent_actions, axis=1).astype(int)
        total_C_hist = total_C_hist + history.my_actions.astype(int)
        revive_signal = bool(np.any(np.sum(history.opponent_actions, axis=1) >= self.m - 1))
        if self.perma_defect:
            if revive_signal:
                self.perma_defect = False
            else:
                return Action.D
        recent_len = int(min(self.W, history.round_number))
        if recent_len > 0:
            recent_total_C = total_C_hist[-recent_len:]
            recent_failures = int(np.sum(recent_total_C < self.m))
            if recent_len == self.W and recent_failures >= self.fail_F and (not revive_signal):
                self.perma_defect = True
                return Action.D
        if self.lock > 0:
            self.lock -= 1
            return Action.D
        if history.round_number >= self.L:
            recent_others = np.sum(history.opponent_actions[-self.L:, :], axis=1)
            if bool(np.all(recent_others >= self.m)):
                return Action.D
        C_D = self.hat_x
        C_C = self.hat_x + 1.0
        if C_D >= self.m:
            return Action.D
        if C_C < self.m:
            return Action.D
        if recent_len > 0:
            recent_total_C = total_C_hist[-recent_len:]
            near_or_met = int(np.sum((recent_total_C >= self.m) | (recent_total_C == self.m - 1)))
            p = near_or_met / float(recent_len)
        else:
            p = 0.0
        if p >= self.theta:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    Last-Mover Parasite (LMP) for the Collective Risk Dilemma.

    Core behavior:
    - Default defect.
    - Cooperate only when likely pivotal (others predicted at exactly m-1), except:
      * Free-ride if others predicted to already meet m without us.
      * Enter "hopeless mode" after w consecutive failures with low predicted cooperation.
      * Second-to-last round: cooperate only if almost surely pivotal (L==U==m-1).
      * Last round: always defect.
    - Optional knife-edge sabotage: if the group often hits exactly m and we often
      were among cooperators, occasionally defect instead of pivotal cooperation to
      test if others will cover.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.alpha = 0.3
        self.w = 3
        self.theta = 0.4
        self.q = 0.3
        self.p = None
        self.c_counts = []
        self.hopeless = False
        self.strict_pivotal = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        n_opp = max(0, n - 1)
        if self.p is None:
            self.p = np.array([0.5] * n_opp, dtype=float)
        if history is None:
            t = 1
        else:
            t = int(history.round_number) + 1
        if t >= r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        completed = int(history.round_number)
        if len(self.c_counts) < completed:
            idx = completed - 1
            my_c = 1 if bool(history.my_actions[idx]) else 0
            opp_c = int(np.sum(history.opponent_actions[idx, :])) if n_opp > 0 else 0
            self.c_counts.append(my_c + opp_c)
            if n_opp > 0:
                last_opp = history.opponent_actions[idx, :].astype(float)
                self.p = (1.0 - self.alpha) * self.p + self.alpha * last_opp
            if not self.hopeless and len(self.c_counts) >= self.w:
                last_w = self.c_counts[-self.w:]
                all_failed = True
                for cc in last_w:
                    if cc >= m:
                        all_failed = False
                        break
                E_next = float(np.sum(self.p)) if n_opp > 0 else 0.0
                if all_failed and E_next < m - 1:
                    self.hopeless = True
        if self.hopeless:
            return Action.D
        E = float(np.sum(self.p)) if n_opp > 0 else 0.0
        L = int(math.floor(E))
        U = int(math.ceil(E))
        if len(self.c_counts) > 0:
            successes = 0
            for cc in self.c_counts:
                if cc >= m:
                    successes += 1
            S_rate = successes / float(len(self.c_counts))
        else:
            S_rate = 0.0

        def knife_edge_reliance() -> bool:
            if history is None:
                return False
            lookback = 5
            if len(self.c_counts) < 1:
                return False
            start = max(0, len(self.c_counts) - lookback)
            count = 0
            for rr in range(start, len(self.c_counts)):
                if self.c_counts[rr] == m and bool(history.my_actions[rr]):
                    count += 1
            return count >= 3
        if t == r - 1:
            if L == m - 1 and U == m - 1:
                if knife_edge_reliance() and random.random() < self.q:
                    return Action.D
                return Action.C
            return Action.D
        if L >= m:
            return Action.D
        pivotal = L == m - 1
        if self.strict_pivotal:
            pivotal = pivotal and U == m - 1
        if pivotal:
            worth_targeting = S_rate >= self.theta or U >= m - 1
            if worth_targeting:
                if knife_edge_reliance() and random.random() < self.q:
                    return Action.D
                return Action.C
        if not self.strict_pivotal and len(self.c_counts) >= 5:
            last5 = self.c_counts[-5:]
            exact_m = 0
            for cc in last5:
                if cc == m:
                    exact_m += 1
            if exact_m >= 3 and (not knife_edge_reliance()):
                self.strict_pivotal = True
        return Action.D

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    Pivot-Extortion with Safety Net (PESN) for the Collective Risk Dilemma.

    Core behavior:
    - Round 1: defect to probe.
    - Thereafter: defect whenever threshold will likely be met without us (slack),
      cooperate only when likely pivotal and recent coordination evidence exists,
      and apply short punishment via withholding after instability/failures.
    - Last round: default defect, with a conservative pivotal exception.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.m = int(game_description.m)
        self.k = float(game_description.k)
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0 or t <= 1:
            self.punish_remaining = 0
            return Action.D
        completed = int(history.round_number)

        def _my_prev_action(idx: int) -> Action:
            return Action.C if bool(history.my_actions[idx]) else Action.D
        my_int = history.my_actions.astype(np.int64)
        opp_int = history.opponent_actions.astype(np.int64)
        X = (my_int + np.sum(opp_int, axis=1)).astype(np.int64)
        x_last = int(X[-1])
        W = min(5, completed)
        recent = X[-W:]
        hit_rate = float(np.sum(recent >= self.m)) / float(W) if W > 0 else 0.0
        coord_evidence = hit_rate >= 0.5 or bool(np.any(recent >= self.m))
        min_coops = int(np.min(recent)) if W > 0 else 0
        if t >= self.r:
            if x_last == self.m - 1:
                saw_m_minus_1 = bool(np.any(recent == self.m - 1))
                if min_coops >= self.m - 1 and saw_m_minus_1:
                    return Action.C
            return Action.D
        if self.punish_remaining > 0:
            if x_last == self.m - 1 and hit_rate >= 0.6:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                return Action.C
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if completed >= 2:
            x_prev = int(X[-2])
            if x_prev >= self.m and x_last < self.m and (_my_prev_action(-1) == Action.D):
                self.punish_remaining = 2 - 1
                return Action.D
            if x_prev < self.m and x_last < self.m:
                self.punish_remaining = 2 - 1
                return Action.D
        if x_last >= self.m + 1:
            return Action.D
        if x_last == self.m:
            if _my_prev_action(-1) == Action.C:
                return Action.D
            return Action.C
        if x_last <= self.m - 2:
            return Action.D
        if x_last == self.m - 1:
            return Action.C if coord_evidence else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    Exploitative Pivotal Free-Rider (PFR) strategy for the Collective Risk Dilemma.

    Behavior:
    - Round 1: cooperate (probe).
    - Middle rounds: defect by default; cooperate only when recent history suggests
      we are frequently pivotal (others at m-1) and the group is viable.
    - If others often meet threshold without us: defect (free-ride).
    - Drop mode: if two consecutive failures where others weren't close (<= m-2),
      defect forever.
    - Last round: defect (endgame).
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.drop_mode = False
        self.window_max = 5
        self.free_ride_ge_m = 0.4
        self.pivotal_mminus1 = 0.3
        self.viable_success = 0.3
        self.unviable_success = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        if t >= n_rounds:
            return Action.D
        opp = history.opponent_actions
        if opp is None or len(opp) == 0:
            return Action.C
        x_hist = np.sum(opp.astype(np.int64), axis=1)
        my = history.my_actions
        if my is None or len(my) != len(x_hist):
            my = np.zeros(len(x_hist), dtype=np.bool_)
        total_coop = x_hist + my.astype(np.int64)
        S_hist = (total_coop >= m).astype(np.int64)
        if len(S_hist) >= 2:
            if S_hist[-1] == 0 and S_hist[-2] == 0 and (x_hist[-1] <= m - 2) and (x_hist[-2] <= m - 2):
                self.drop_mode = True
        if self.drop_mode:
            return Action.D
        w = min(self.window_max, len(x_hist))
        if w <= 0:
            return Action.D
        recent_x = x_hist[-w:]
        recent_S = S_hist[-w:]
        f_success = float(np.mean(recent_S)) if w > 0 else 0.0
        f_mminus1 = float(np.mean((recent_x == m - 1).astype(np.int64))) if w > 0 else 0.0
        f_ge_m = float(np.mean((recent_x >= m).astype(np.int64))) if w > 0 else 0.0
        if f_ge_m >= self.free_ride_ge_m:
            return Action.D
        if f_mminus1 >= self.pivotal_mminus1 and f_success >= self.viable_success:
            return Action.C
        if f_success <= self.unviable_success:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    Threshold Parasite with Punitive Trigger (TPPT)

    Core behavior:
    - Default to defection to test and to free-ride when others likely meet threshold without us.
    - Cooperate mainly when predicted to be pivotal (others ~= m-1).
    - Enter short punishment (defect P rounds) if recent success collapses, to deter/clarify.
    - Endgame: defect more, cooperate only if pivotal probability is high.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        self.W = max(1, min(5, max(1, r - 1)))
        self.E = max(1, int(math.floor(r * 0.2)))
        self.S = 1
        self.P = 2
        self.mode = 'NORMAL'
        self.punish_timer = 0
        self._last_x_other = None
        self._second_last_x_other = None
        self._last_success = None
        self._second_last_success = None

    def _round_success(self, x_other: int, my_action: Action) -> bool:
        m = int(self.game_description.m)
        my_c = 1 if my_action == Action.C else 0
        return x_other + my_c >= m

    def _predict_x(self, x_hist) -> int | None:
        if x_hist is None or len(x_hist) == 0:
            return None
        recent = x_hist[-self.W:] if len(x_hist) > self.W else x_hist
        return int(round(float(np.mean(recent))))

    def _p_pivotal(self, x_hist) -> float:
        if x_hist is None or len(x_hist) == 0:
            return 0.0
        m = int(self.game_description.m)
        arr = np.array(x_hist, dtype=int)
        return float(np.mean(arr == m - 1))

    def _recent_success_rate(self, success_hist) -> float:
        if success_hist is None or len(success_hist) == 0:
            return 0.0
        recent = success_hist[-self.W:] if len(success_hist) > self.W else success_hist
        return float(np.mean(np.array(recent, dtype=float)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        if history is None or history.round_number == 0:
            return Action.D
        opp = history.opponent_actions
        x_hist = [int(np.sum(opp[t, :])) for t in range(history.round_number)]
        my_act_hist = history.my_actions
        success_hist = []
        for t in range(history.round_number):
            my_a = Action.C if bool(my_act_hist[t]) else Action.D
            success_hist.append(1.0 if self._round_success(x_hist[t], my_a) else 0.0)
        if history.round_number >= 1:
            self._second_last_x_other = self._last_x_other
            self._second_last_success = self._last_success
            self._last_x_other = x_hist[-1]
            self._last_success = bool(success_hist[-1] >= 0.5)
        t_current = int(getattr(state, 'round_number', history.round_number + 1))
        if t_current <= 0:
            t_current = history.round_number + 1
        if self.mode == 'PUNISH':
            self.punish_timer -= 1
            if self.punish_timer <= 0:
                self.mode = 'NORMAL'
                self.punish_timer = 0
            return Action.D
        x_pred = self._predict_x(x_hist)
        p_piv = self._p_pivotal(x_hist)
        if t_current > r - self.E:
            if p_piv >= 0.35:
                chosen = Action.C
            else:
                chosen = Action.D
        elif x_pred is None:
            chosen = Action.D
        elif x_pred >= m + self.S:
            chosen = Action.D
        elif x_pred == m - 1:
            chosen = Action.C
        elif x_pred <= m - 2:
            chosen = Action.D
        else:
            q = min(0.3, p_piv + 0.1)
            chosen = Action.C if random.random() < q else Action.D
        recent_success_rate = self._recent_success_rate(success_hist)
        last_two_failed = False
        if history.round_number >= 2:
            last_two_failed = success_hist[-1] < 0.5 and success_hist[-2] < 0.5
        near_pivotal_recent = False
        if history.round_number >= 1:
            x_last = x_hist[-1]
            if x_last in (m - 2, m - 1):
                near_pivotal_recent = True
        if history.round_number >= 2:
            x_prev = x_hist[-2]
            if x_prev in (m - 2, m - 1):
                near_pivotal_recent = True
        if last_two_failed and near_pivotal_recent or recent_success_rate < 0.4:
            self.mode = 'PUNISH'
            self.punish_timer = self.P
        return chosen

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    ThreshShark (adaptive threshold parasite) for the Collective Risk Dilemma.

    Core behavior:
    - Default defect.
    - Free-ride (defect) when expected others' cooperation safely meets threshold.
    - Cooperate only when likely pivotal AND recent group success is reliable, with
      stricter reliability near endgame and extra conservatism in very early rounds.
    - Avoid becoming the group's main funder via an anti-sucker cap.
    - Last round uses a myopic best-response rule under the same expectation heuristic.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        m = int(self.game_description.m)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if history is None or getattr(history, 'round_number', 0) == 0:
            if m <= n // 3 and k >= 2.0:
                return Action.C
            return Action.D
        completed = int(history.round_number)
        W = min(5, completed)
        if W <= 0:
            return Action.D
        opp_last = history.opponent_actions[-W:, :]
        my_last = history.my_actions[-W:]
        try:
            p_vec = np.mean(opp_last.astype(float), axis=0)
            E_others = float(np.sum(p_vec))
        except Exception:
            E_others = 0.0
        try:
            opp_counts = np.sum(opp_last, axis=1).astype(int)
            total_counts = opp_counts + my_last.astype(int)
            met = (total_counts >= m).astype(float)
            S = float(np.mean(met))
        except Exception:
            S = 0.0
        rounds_left = r - t + 1
        S_min = 0.8 if rounds_left <= 3 else 0.6
        pivotal = E_others >= m - 1 and E_others < m + 0.5
        try:
            myC_rate = float(np.mean(my_last.astype(float)))
        except Exception:
            myC_rate = 0.0
        if myC_rate > 0.4 and S < 0.8:
            return Action.D
        if t >= r:
            if E_others >= m + 0.5:
                return Action.D
            if pivotal:
                return Action.C
            return Action.D
        if E_others >= m + 0.5:
            return Action.D
        if E_others < m - 1:
            return Action.D
        if pivotal:
            if S >= S_min:
                if t <= 3:
                    if not (E_others >= m - 0.2 and k >= 2.0):
                        return Action.D
                return Action.C
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    ProbeExploitPunish (PEP) for the Collective Risk Dilemma.

    Core behavior:
    - Default to defection; free-ride whenever others are estimated to meet the threshold without us.
    - Cooperate only when likely pivotal (others' estimated cooperators == m-1) and not in last round.
    - After any failed threshold, enter a short punishment phase where we always defect.
    - If we observe repeated successes while we defect, lock into permanent defection.
    """

    def __init__(self, game_description: CollectiveRiskDescription):
        self.game_description = game_description
        self.W = 5
        self.P = 2
        self.S = 3
        self.punish_left = 0
        self.lock_in_defect = False

    def _median_int(self, xs):
        if xs is None or len(xs) == 0:
            return 0
        ys = sorted(xs)
        n = len(ys)
        mid = n // 2
        if n % 2 == 1:
            return int(ys[mid])
        return int(round((ys[mid - 1] + ys[mid]) / 2.0))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        m = self.game_description.m
        if history is None or history.round_number == 0:
            return Action.D
        t = state.round_number
        if t >= r:
            return Action.D
        if self.lock_in_defect:
            return Action.D
        last_idx = history.round_number - 1
        my_last_c = bool(history.my_actions[last_idx])
        opp_last_c_count = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
        C_last = opp_last_c_count + (1 if my_last_c else 0)
        if C_last < m:
            self.punish_left = self.P
        elif self.punish_left > 0:
            self.punish_left -= 1
        if history.round_number >= self.S:
            start = history.round_number - self.S
            my_lastS = history.my_actions[start:history.round_number]
            if np.all(my_lastS == False):
                opp_lastS = history.opponent_actions[start:history.round_number, :] if history.opponent_actions.size else np.zeros((self.S, 0), dtype=bool)
                succ = True
                for j in range(self.S):
                    Cj = int(np.sum(opp_lastS[j, :])) + 0
                    if Cj < m:
                        succ = False
                        break
                if succ:
                    self.lock_in_defect = True
        if self.punish_left > 0:
            return Action.D
        W = min(self.W, history.round_number)
        recent_start = history.round_number - W
        my_recent = history.my_actions[recent_start:history.round_number]
        opp_recent = history.opponent_actions[recent_start:history.round_number, :] if history.opponent_actions.size else np.zeros((W, 0), dtype=bool)
        othersCs = []
        for idx in range(W):
            oppC = int(np.sum(opp_recent[idx, :]))
            othersC = oppC + 0
            othersCs.append(othersC)
        othersC_est = self._median_int(othersCs)
        if othersC_est >= m:
            return Action.D
        if othersC_est == m - 1:
            return Action.C
        return Action.D